[
  {
    "objectID": "posts/2024-04-11-what-if-recent-apartment-buildings-in-vancouver-were-20-taller/index.html",
    "href": "posts/2024-04-11-what-if-recent-apartment-buildings-in-vancouver-were-20-taller/index.html",
    "title": "What if recent apartment buildings in Vancouver were 20% taller?",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\n\nEarlier this year a report from the NSW Productivity Commission in New South Wales, Australia, included a useful estimate to illustrate the harm that’s being done by height restrictions in Sydney. We thought it might be helpful to replicate the analysis for the Vancouver context.\nTaking ideas from the report we set up a simple counter-factual question:\nWhat would rents be if every apartment building built in Metro Vancouver over the past five years had been on average 20% taller?\n\nTL;DR\nWe estimate that planning decisions preventing apartment buildings built in the past 5 years in Metro Vancouver from being on average 20% taller are resulting in an annual redistribution of income from renters to existing landlords on the order of half a billion dollars across the region via higher rents.\n\n\nEconomics 101\nTo answer our counter-factual question, it’s worth quickly reviewing some basic economics concept. Firstly, the demand curve for housing slopes down. That means that as the price or rent for housing falls, more people will look to buy or rent that housing. That generally leads more households to form, e.g. young adults moving out of their parent’s basement or out of roommate or other complex household situations to form their own households. And it could mean some people that would have otherwise been pushed out are now able to stay, and some people deterred by high housing costs can move into our region. And the reverse of all that will happen if housing prices and rents rise.\nHow do we know this? Economic theory tells us demand curves generally slope downward and both theory and data confirm the demographic implications. But to answer our question we need to quantify this more precisely. If we increase our housing stock by some percentage, by what percentage will prices and rents change? This is the (inverse) price elasticity of demand. It is the (inverse of the) slope to the demand curve, shown in black in the plot below, re-expressed in terms of the percentage change of demand and supply.\n\nThese type of elasticities are notoriously difficult to identify. We are interested in the “long run” effect, so effects after markets adjust to short-term supply shocks. The report we cite above uses an estimate of -0.4 for Sydney, and estimates done by CMHC for the whole province of British Columbia suggest a similar value of around -0.5. We have previously estimated the price elasticity of demand for Metro Vancouver to be around -0.25, albeit with large confidence intervals. For this analysis we will use the value of -0.4 which is closer to what the broader literature suggests, with the understanding that this estimate comes with uncertainties and actual effects might be a bit larger or a bit smaller. For reference, an estimate of -0.4 is equivalent to calculating that a 1% increase in supply would result in a 2.5% decrease in prices and rents. Since ours is a point estimate of price elasticity, these effects compound, and a 10% increase in supply would result in a 21% decrease in prices and rents.\nWe note that the supply curve in the graph above is to be taken with a grain of salt. While the demand curve for housing works roughly as one would expect, insofar as housing is largely allocated via market mechanisms, the supply of housing works differently. To list just two caveats, housing is a durable good, with the market largely composed of existing, rather than newly produced, housing. More importantly for our purposes, there are strong external constraints on how much new supply can be added through newly produced housing. Even where supply curves suggest developers would happily add more dwellings in response to prices, planning often prevents them from doing so.\n\n\nHeight restrictions in Vancouver\nFor this post height limitation plays the role of a simple proxy for the restrictiveness of zoning. A CMHC report from 2019 has found that height restrictions are strongly binding in Vancouver, meaning a developer would be very likely to build taller if they were allowed to do so.\nThe effect of zoning restrictions is generally difficult to observe when only looking at development proposals that make it into the city approval process, but we can still see some of the effect of height restrictions play out there.\nFor example the development at 1535-1557 Grant Street was initially proposed as a 6 storey apartment building as in principle enabled under the Grandview-Woodland community plan, but was subsequently cut down to 5 storeys in “direct response” to Urban Design Panel & staff review.\nThe project at 1805 Larch brought forward under the Moderate Income Rental Housing Pilot Project went from six to five due to opposition at the pre-application open house and was compelled to provide more parking spaces.\n\n\n\nOriginal 6 storey version\n\n\nThe development around 708 Renfrew Street proposed under the Affordable Housing Choices (AHC) Policy did not see a reduction in the nominal number of storeys, but saw significant massing changes with the original proposal 43% times larger than the version that eventually got approved. However, the reduction in housing units seems to have ultimately proven fatal to the project’s viability, and the developer abandoned the project.\nThis motivates our counter-factual of trying to understand what would have happened if recently constructed apartment buildings were on average 20% taller, and had (roughly) 20% more units.\n\n\nApartment construction\nTo estimate the impact we look at historical apartment construction in Metro Vancouver, and hypothetically add 20% more units for the past 5 years. This is a rough estimate of what taller buildings could provide, of course. The math linking unit numbers to heights can be more complicated. Taller buildings might’ve enabled more projects (like 708 Renfrew) to “pencil” potentially adding even more units than our estimate. On the other hand, CMHC also counts secondary suites as “apartment units”, which could lead to us overestimating, but that likely only causes a small distortion in the stats.\n\nOverall, there were 81,317 apartment completions in the past 5 years. Had the buildings containing these apartments been on average 20% taller there would have been 16,263 additional housing units, resulting in an 1.5% increase in total housing stock in Metro Vancouver. Using our common demand elasticity assumption, that additional housing supply would have led to a reduction of prices and rents by 3.7%.\n\n\nImpact on rents\nTo make this a little more concrete, we can use the average rent for 2-bedroom units in Metro Vancouver with a new rental contract in the preceding year as a proxy for the average rent in the region and estimate the counter-factual rents.\nCMHC is now collecting information of rents of units with new rental contracts, and the average such 2-bedroom unit in Metro Vancouver rented for $2,601 a month in 2023. If apartment buildings built during the past 5 years had been on average 20% taller, then the average rent would have been $93 a month lower, saving the average new renter about $1,120 a year. For comparison, BC’s new Renter’s Tax Credit maxes out at $400 per year.\nRents for long-time tenants tend to be lower than for new tenants under BC’s rent control, reflecting Metro Vancouver’s lengthy history of low vacancy rates driving rents for new contracts ever higher. Correspondingly, for tenants under existing rent-controlled tenancy agreements the benefits of adding supply are less direct than for new tenants. Long-time tenants primarily experience additional supply effects in terms of increased choice and lower rents if they are looking to change their living arrangements because of a lower overall moving penalty. But they also potentially gain in their landlords becoming a little less keen to encourage turnover.\nAt this point it is useful to point out that the aggregate effect of the added counter-factual housing is a net transfer of wealth from existing landlords to tenants. Rent control delays some of these effects, but scaling the rent saving over the existing renter households in Metro Vancouver yields a net transfer of $442M a year from existing landlords to tenants in terms of aggregate rent savings.\n\n\nImpact on broader housing outcomes\nOn top of that, the increase in housing stock will allow existing households to who are unhappy with their current living arrangements, for example adults living with parents, or roommates, or other complex living arrangements, to move into their own units. Toronto and Vancouver have elevated rates of such young adults not in “Minimal Household units”, and new housing units are needed for these young adults to form their own households.\n\nThis under-appreciated benefit of increasing the housing stock allows individuals and families to better match their housing to their needs and preferences.\n\n\nConclusion\nOne might be inclined to think that a reduction of average rents from $2,601 to $2,508 a month is quite small. And we agree and believe that our counter-factual increase in new housing by 20% is very modest. We should aim for a much larger increase in housing. Previous analysis we performed suggests developers would have built enough units to increase our housing stock by 250,000 units, or 23% if municipalities had let them. This would have resulted in a reduction of average rents by 40%, thus bringing Vancouver rents in line with Canada-wide average CMA rent and resulting in enormous benefits to renters.\nOur counter-factuals are purely hypothetical in the sense that we cannot turn back time and allow apartments that have already been built to be taller. But they illustrate the the benefits of building more housing, and the harm of restricting the amount of housing getting built.\nWe suspect that many municipal planners and politicians don’t understand the extent of the harm to renters or new buyers they are causing by reducing the height of buildings, or more generally by restricting the supply of housing. Planning documents where planners give reasons for significantly reducing height and form of proposed buildings, like the one for the MIRHPP at Larch and 3rd, are often justified largely on aesthetic grounds (“Substantial massing change is required to provide a compatible form of development […] in relation to the surrounding RT-8 and RM-4 developments”) and do not seem to fully weigh or even consider the tradeoffs of the effects on rents and broader housing outcomes, even in cases like this that contain dedicated below-market units.\nThe other takeaway from this analysis is that affordability comes from supply effects. In terms of affordability, the purpose of new housing is to make old housing cheaper. Often we hear people ask that new housing be affordable, or that new housing should be targeted at specific income groups. This is entirely appropriate and important when it comes to dedicated non-market housing. But trying to target new market housing at specific income bands fundamentally misunderstands housing, and is likely counter-productive. Or in the words of Michael Manville:\n\nIf we believe that cheap housing matters and expensive housing doesn’t, and we act on that belief, our primary accomplishment will be to make our cheap housing expensive.\n\nAnd not coincidentally we have been doing a bang-on job at making our cheap housing expensive across Metro Vancouver.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2024-04-11 13:56:07 PDT\"\n## Local:    master /Users/jens/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [0ec8ecd] 2024-01-17: fix latex snafu\n## R version 4.3.2 (2023-10-31)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Sonoma 14.4.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/Vancouver\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 cmhc_0.2.8               \n##  [3] cancensus_0.5.8           lubridate_1.9.3          \n##  [5] forcats_1.0.0             stringr_1.5.1            \n##  [7] dplyr_1.1.4               purrr_1.0.2              \n##  [9] readr_2.1.4               tidyr_1.3.0              \n## [11] tibble_3.2.1              ggplot2_3.5.0            \n## [13] tidyverse_2.0.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] gtable_0.3.4       xfun_0.41          bslib_0.6.1        htmlwidgets_1.6.4 \n##  [5] tzdb_0.4.0         vctrs_0.6.5        tools_4.3.2        generics_0.1.3    \n##  [9] parallel_4.3.2     curl_5.2.0         fansi_1.0.6        highr_0.10        \n## [13] cluster_2.1.4      pkgconfig_2.0.3    data.table_1.14.10 checkmate_2.3.1   \n## [17] lifecycle_1.0.4    git2r_0.33.0       compiler_4.3.2     farver_2.1.1      \n## [21] munsell_0.5.0      htmltools_0.5.7    sass_0.4.8         yaml_2.3.7        \n## [25] htmlTable_2.4.2    Formula_1.2-5      crayon_1.5.2       pillar_1.9.0      \n## [29] jquerylib_0.1.4    cachem_1.0.8       Hmisc_5.1-1        rpart_4.1.21      \n## [33] tidyselect_1.2.0   digest_0.6.33      stringi_1.8.3      bookdown_0.37     \n## [37] labeling_0.4.3     rprojroot_2.0.4    fastmap_1.1.1      grid_4.3.2        \n## [41] here_1.0.1         colorspace_2.1-0   cli_3.6.2          magrittr_2.0.3    \n## [45] base64enc_0.1-3    utf8_1.2.4         foreign_0.8-85     withr_2.5.2       \n## [49] scales_1.3.0       backports_1.4.1    bit64_4.0.5        timechange_0.2.0  \n## [53] httr_1.4.7         rmarkdown_2.25     bit_4.0.5          nnet_7.3-19       \n## [57] gridExtra_2.3      blogdown_1.19      sanzo_0.1.0        hms_1.1.3         \n## [61] evaluate_0.23      knitr_1.45         rlang_1.1.3        glue_1.7.0        \n## [65] vroom_1.6.5        rstudioapi_0.15.0  jsonlite_1.8.8     R6_2.5.1\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2024,\n  author = {von Bergmann, Jens and Lauster, Nathan},\n  title = {What If Recent Apartment Buildings in {Vancouver} Were 20\\%\n    Taller?},\n  date = {2024-04-11},\n  url = {https://doodles.mountainmath.ca/posts/2024-04-11-what-if-recent-apartment-buildings-in-vancouver-were-20-taller},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von, and Nathan Lauster. 2024. “What If Recent\nApartment Buildings in Vancouver Were 20% Taller?”\nMountanDoodles (blog). April 11, 2024. https://doodles.mountainmath.ca/posts/2024-04-11-what-if-recent-apartment-buildings-in-vancouver-were-20-taller."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html",
    "href": "posts/2023-08-17-housing-outcomes/index.html",
    "title": "Housing Outcomes",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nAlmost everyone agrees that we have a housing crisis in Canada, and that it has gotten progressively worse over recent history. But there is a problem. The metrics most commonly used don’t reflect that."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#tldr",
    "href": "posts/2023-08-17-housing-outcomes/index.html#tldr",
    "title": "Housing Outcomes",
    "section": "TL;DR",
    "text": "TL;DR\nMost commonly used metrics use existing households as the base of analysis, but households are a consequence of housing pressures. This kind of misspecification is a form of collider or selection bias that, especially in tight housing markets, misleads researchers toward faulty conclusions and policy recommendations. It blinds researchers to the struggles of people who are unhappy about their current household living arrangement, like young adults struggling to move out of their parent’s place or out of a bad roommate setup, as well as people who have left their desired region and moved away, or failed to move in, because of the lack of housing options.\nThis post explains the problem with analysis based solely on households in more detail, and explains why this will lead to incorrect diagnoses of our housing problems and misguided policy recommendations."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#core-housing-need",
    "href": "posts/2023-08-17-housing-outcomes/index.html#core-housing-need",
    "title": "Housing Outcomes",
    "section": "Core Housing Need",
    "text": "Core Housing Need\nThe most-cited metric in the context of problems in the housing market in Canada is Core Housing Need. But looking at the share of households in core housing need for the 2006 through 2021 censuses one would be hard-pressed to conclude that Core Housing Need got worse.\n\n2021 was a weird year, of course, and the CERB payments from 2020 likely lifted a lot of people out of Core Housing Need, so we should not focus too much on that drop. But even before the pandemic it seems difficult to argue that Core Housing Need got worse, despite almost everyone agreeing that housing outcomes deteriorated during that time period.\nBut housing is a local problem, maybe things get washed out when looking at the national level? Turning our lens to local levels doesn’t really change much. Looking at Census Metropolitan Areas we still don’t see a clear trend indicating rising levels of housing need. More often we see the opposite.\n\nMetro Toronto and Vancouver stand out as higher than other metros, as we might expect, but it doesn’t look like things are getting worse. Maybe the problem is not with the whole metro areas but with the central cities. But again, it’s hard to make out a negative trend. Just looking at Core Housing Need, a naive observer might assume that conditions are pretty stable, with need on the decline in many places. Housing crisis? What housing crisis?\n\nDoes that mean that things actually aren’t getting worse? Are we just imagining the housing crisis? No. We are looking at the wrong metrics.\nCore Housing Need has long had issues as a metric, as careful analysts have repeatedly pointed out over the years. In short, it attempts to establish a housing standard for all of Canada, matching existing households to need on the basis of suitably roomy (matched to National Occupancy Standards), adequately maintained (good repair), and affordable (shelter costs no more than 30% of income) housing, or affordable alternatives that might meet their needs nearby. To list a few criticisms of the measure:\n\n\n\naffordable alternatives aren’t actually assessed for availability\n\n\nlocation of housing impacts transportation costs\n\n\na 30% shelter cost to income cutoff lacks solid justification\n\n\nthe metric excludes many households (e.g. students, income lower than shelter cost)\n\n\nshelter costs are inconsistently assessed\n\n\nincomes are temporally mismatched (from previous year) relative to shelter costs (current)\n\n\nadequacy of maintenance (based on self-reported repair) misses important elements\n\n\nsuitable roominess (based on Canadian National Occupancy Standards) lacks solid justification\n\n\nexisting households might not be preferable arrangements\n\n\nexisting households miss in- and out- migration/mobility responses\n\n\nIn short, there is a lot more to housing outcomes than what is currently captured in Core Housing Need."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#existing-households-are-the-wrong-unit-of-measurement",
    "href": "posts/2023-08-17-housing-outcomes/index.html#existing-households-are-the-wrong-unit-of-measurement",
    "title": "Housing Outcomes",
    "section": "Existing households are the wrong unit of measurement",
    "text": "Existing households are the wrong unit of measurement\nIn most of Canadian data and theorizing, a household is defined as an occupied dwelling unit, although household is often also used as a shorthand for only private households. Households are the outcome of how people distribute over dwellings.\nBasing analysis entirely on existing households is selecting on a consequence of the outcome, it introduces selection or collider bias. This bias is substantial, the degree to which housing pressures impact household formation are large and growing, and an existing household-based analysis is blind to suppressed household formation, as well as to the effects of housing on migration.\nTo better understand collider bias in this context consider Model 18 in this excellent monograph on good and bad controls. Alternatively one can interpret selecting on households that have formed as survivorship bias that ignores households that would have formed if housing pressures were different. This also highlights why it is necessary to choose counter-factual scenarios of different housing pressures. For market housing these counter-factual scenarios should be chosen to be realistically achievable, by for example comparing with regions with different housing policies but similiar economic and demographic environments, or by estimating how much housing the market would produce if municipal regulations preventing housing were removed. Non-market housing counter-factual scenarios should similarly work toward being achievable in order to be taken seriously, and, when interpreted from a rights based approach, set in relation to housing outcomes achievable in market housing.\nOn a personal level, housing pressures are experienced through unaffordability when people or families can’t afford housing that fits their needs, be it proximity to jobs and amenities, or size and number of bedrooms. But their inability to afford said housing generally won’t show up in Core Housing Need because in most cases they don’t actually move into the housing they can’t afford. The same principle generally applies to waitlisting for non-market housing.\nThat’s all quite abstract, let’s unpack this a little.\nTake a roommate household made up of four roommates, where two people have coupled up. The couple would like to move out into their own place, but they can’t find a place within their budget that’s reasonably close to their jobs. So they stay in their roommate situation. They may be delaying their plans to have kids until they find a better housing situation that they feel secure about, and maybe has an extra bedroom to accommodate their future child. They are not in core housing need because the combined household income of all four roommates easily covers rent and utilities, but they do feel like their housing needs aren’t being met and it’s forcing them to adjust life plans. This kind of household would be classified as a one-census family household with additional persons, and across Canada there are around 666,000 of these, 493,000 of which have children present.\nTo take a different example, the CBC recently ran a piece on how former couples wishing to separate or divorce increasingly find themselves forced to either continue living together or move in with friends and relatives. This clearly relates to the rising difficulty of finding housing. But the Core Housing Need measure sees no problem with these situations, at least as long as there’s a spare bedroom satisfying the National Occupancy Standard criteria for uncoupled adults. The issues are further compounded when children are involved in split households, insofar as the need for roominess grows, and in joint custody cases, may even double. But unsurprisingly, the Core Housing Need metric also ignores custody questions (kids are only supposed to show up once as part of a Census household).\nWe take this opportunity to update our overview graph from a previous post on households by household type and census family structure using 2021 data.\n\nThis highlights some of the complexities of households, and how they don’t necessarily map onto family units. In particular we notice how Montréal has comparatively few complex households."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#minimum-household-units",
    "href": "posts/2023-08-17-housing-outcomes/index.html#minimum-household-units",
    "title": "Housing Outcomes",
    "section": "Minimum Household Units",
    "text": "Minimum Household Units\nIf households aren’t the right unit of analysis, then what is? We have discussed and compared different conceptualizations of what constitutes a basic unit in the past, including the OECD definition of a household that StatCan uses within their National Accounts framework, as well as one looking at census (or economic) families and unattached individuals that we already highlighted in our post on households. But even non-divorcing census families might feel the need to split up, for example if they contain adult children living with parents who might want to move out but can’t find suitable housing.\nErmisch and Overton introduced the concept of Minimal Household Units (MHU) to better understand household formation. The idea is to define elemental building blocks of households, which may combined to form “complex” households. They start with single adults, parent-child, and couple relationships where we typically assume people want to live together, leaving four basic building blocks for households as:\n\nAdult individuals\nLone parents with dependent children\nChildless married (or common law) couples\nMarried (or common law) couples with dependent children\n\nThis leaves out some complexities (e.g Living Apart Together setups where couples want to remain couples, but live separately), but is simple and works well for the purpose of analyzing household formation and affordability in the context of housing. The assumption is basically that household complexity beyond MHU is likely influenced (though not completely determined) by housing scarcity, revealing a portion of people living together who might prefer to live apart.\nLooking up at our graphs on household composition by age we note that the blue households approximate minimum household units, with the exception that some of them have adult children living at home. To better understand this we focus on 25 to 34 year olds and examine what share of them live in MHUs vs complex households, and how this has changed through time and across geographies.\n\nWe notice overall declining trends in the total proportion of 25 to 34 year olds in MHUs, and substantially lower shares in Toronto and Vancouver compared to the other large CMAs. Vancouver has higher shares of childless couples than Toronto, but lower shares of couples with children. By contrast, Montréal has the highest proportion of 25 to 34 year olds living within MHUs, driven largely by more people living alone and part of couples with children (though the latter is still on the decline). In the 35 to 44 year old age groups we still notice slightly declining trends in some CMAs, and again lower levels in Toronto and Vancouver. Once more, Montréal has the highest proportion of 35 to 44 year olds living in MHU.\nIf young adults aren’t living in MHUs, how do they live? Here are the living arrangements of the remaining young adults.\n\nThere’s no single category dominating trends or differences. The proportion of young adults living as roommates went down in Calgary, but up in Vancouver and Toronto. The proportion living in complicated family arrangements also generally rose. Compared to other metros, Toronto and Vancouver have higher shares in all categories, except people living in census families with additional persons and roommate households. The high share of people living with their parents stands out in these two metros, but also a higher share of multi-generational households. Of note, the share in multi-generational households has been declining for 25 to 34 year olds over this time period for Vancouver and Toronto, and it declined 2016-2021 for the 35 to 44 year old group, while most other categories increased. In particular, the increases in young adults in non-MHUs in Toronto and Vancouver cannot be attributed to multi-generational living arrangements.\nUnfortunately we don’t have this data at finer age groups; that would require a custom tabulation. But at the expense of not being able to distinguish couple MHUs with or without children, we can use a different table to take a quick look at how overall MHU shares progress through age cohorts over time and across geographies. Housing stress can lead to couples feeling without enough room or otherwise unprepared for children, so it’s unfortunate we lose an indicator of children for couples. On the bright side, the data allow us to make different interesting distinctions between married and common-law couples and add in the 20 to 24 year old age group.\n\nStepping up through age groups we see progressively higher shares of young adults living in MHU, as one would expect. However, within each age group the shares of young adults in MHUs has decreased over time, and there is large variation across metro areas. These trends aren’t all due to housing pressures, they get confounded by young adults spending more time in school and possibly by shifts in preferences, including by culture, but it seems far-fetched to attribute most of the variation across time and metro areas to this. Indeed, one of the most notable cultural differences appears in the distinction between common-law partnerships, which are far more common within Montréal (and across Quebec), and marriage, which has been more common elsewhere. The difference between common-law and marriage doesn’t matter for purposes of assigning people into MHU, but the relative decline of marriage is in keeping with Canada’s entry into the Second Demographic Transition, and given the connections between marriage with home ownership, it’s striking that married couples are the category most strongly driving the decline in MHU for young adults everywhere.\nOverall across Canada 52% of adults aged 20 to 34 lived in MHUs in 2021, down from 54% in 2016 and 56% in 2011. In contrast with Core Housing Need, here’s a clearer indicator of conditions getting worse over Canada’s recent history. How did those outside of MHUs live? What was the makeup of their complex households?\n\nWe see a striking rise in young adults outside of MHUs, especially those living with their parents or other family members. Toronto and Vancouver also stand out in this regard. There are several factors playing into temporal and regional patterns, but housing pressures are likely the most important driver. Yet metrics based on existing households, like Core Housing Need, will systematically overlook this kind of evidence of housing pressure."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#household-maintainer-rates",
    "href": "posts/2023-08-17-housing-outcomes/index.html#household-maintainer-rates",
    "title": "Housing Outcomes",
    "section": "Household maintainer rates",
    "text": "Household maintainer rates\nAnother way to think about this kind of housing pressure is to look at age-specific household maintainer rates, and how these differ across geographies and time. We have looked at this previously and documented how Montréal, Toronto, and Vancouver used to have fairly similar household maintainer rates in the 70s. But while Montréal’s rates have remained relatively stable, Vancouver and Toronto have seen significant drops in household maintainer rates, especially among younger adults, as illustrated in the following graph.\n\nThis remains a static picture, providing cross-sections of the overall state of the population in the given years. We can make this more dynamic by following fixed birth cohorts over time to see how their household maintainer rates change as they age. Comparing across birth cohorts gives us a better sense of generational change in household maintainer rates. Of note, we don’t have full data over the 50 years covered in this graph, so some of the trajectories are only partial.\n\nIn Montréal successive birth cohorts trace out similar trajectories, eclipsing the 1931 through 1945 cohorts but then not changing much starting with the 1946-1950 birth cohort. But in Toronto and Vancouver younger cohorts are experiencing significantly lower household formation rates compared to previous generations that persist as people age into their 40s.\nAnother way to slice the data is to look at how many years households in Toronto and Vancouver lag relative to Montréal’s 1956-1960 birth cohort, selecting an age cohort fully covered in our window of observation.\n\nHere we see how Toronto and Vancouver are lagging behind their Montréal peers when it comes to forming and maintaining a household, with millennial and GenX households lagging by over 10 years by the time they reach 40. In other words, Millennials and GenX household formation rates by the time they reach 40 in Vancouver and Toronto are about equivalent to what their peers in Montréal reached in their late 20s.\nWe have interpreted this as suppressed household formation and a partial measure of housing shortfall. While we believe that this interpretation is generally correct, a potential alternative interpretation is that this trend may be explained by different cultural preferences, which we’ve also attempted to quantify, and further work on this would be useful. Nonetheless, we believe that estimates of suppressed households offer a useful metric to assess housing shortages."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#household-tightness",
    "href": "posts/2023-08-17-housing-outcomes/index.html#household-tightness",
    "title": "Housing Outcomes",
    "section": "Household tightness",
    "text": "Household tightness\nSpecifying the role that housing plays in our suppressed household measure is hard to pin down. But we can look at related metrics of housing outcomes to see if there’s more evidence of squeeze. Maybe it’s not there. People could be sharing more often because there’s lots of room to do so. Imagine we have a large reservoir of big houses, and people choose to form more complex households as the average household size of MHUs has been declining over time due to dropping fertility rates, people coupling up later, and people living longer.\nIdeally we would want to look at data on how people distribute over floor space, but we simply don’t have reliable floor space data in Canada. The Canadian Housing Statistics Program has some information on living area, but only for select types of housing and only for recent years and not (yet) covering all of Canada. The Survey of Household Energy Use has some data on “heated living area” going back to the 1990s, with some aggregate estimates made available by NRCAN, but it is not clear how these estimates are derived and how reliable they are.\nAnother way to understand how cramped or roomy living situations are is to look at the number of rooms available by household size. The census keeps track of the number of rooms in a dwelling unit as well as household size, giving us long timelines to work with. This still poses some challenges insofar as people’s preferences for how to slice up homes have changed too, with for example people sometimes tearing down interior walls, e.g. between dining room and living room, when remodelling. This results in fewer rooms, but the home is still as “roomy” as before. Some of that can be controlled for by comparing across different regions in Canada.\nYet another complication is that real incomes have grown over time, and housing demand increases strongly with income, with most estimates pegging the demand elasticity of income to be greater than 1. This means that we should expect to see an increase in roominess over time as incomes grow.\nUnderstanding these complications, let’s take a look at the distribution of number of rooms by household size, and how this varies across census years and metro areas.\n\nA lot of data is captured here, and overall this suggests that 2006 was peak “roominess” across Canada, having generally near the maximum number of rooms for a given household size and metro area. Roominess was growing in the period before 2006, and it has been declining since. The decline in roominess is unexpected, given rising real incomes over this time period. This could reflect trends in hardship not well captured by Core Housing Need, but possibly also an increasingly urban population, making trades of room for centrality. At the same time, even this measure may understate the extent of rising hardship. After all, we’re still only counting children in joint custody situations once, and there has been no decline in single parent households or joint custody arrangements accompanying the decline in household roominess. Moreover, the share of homes in Canada not occupied by usual residents was stable (actually declining slightly) over the 2006-2021 time period, indicating that secondary residences or vacation homes are unlikely to have offset this decline in roominess.\nThe peak in 2006 is also interesting for many reasons, not least of which because of what happened between 2006 and 2011. 2008-2009’s Great Recession and Financial Crisis saw a dramatic drop in the construction of housing. This carried over from the USA into Canada even though our housing prices didn’t drop as much and we had nowhere near similar foreclosure rates. If, as Kevin Erdmann makes the case, rising construction rates up to the 2008-2009 crisis were actually justified in the USA, then the case is potentially even stronger for Canada. And we can see the effects of the pullback in building since 2006 on the decreasing roominess of housing relative to household size.\n\nRaw starts and completions should be interpreted with caution, and do not account for demolitions or changes in CMHC estimation methods. But we can clearly see here the dramatic drop-off in starts after the Financial Crisis, followed by a decline in completions. We can only wonder where Canada would be now in terms of housing had the Crisis not spread North from the USA.\nOf course, in addition to historical variation, we’ve also got variation in roominess by region. We can take a closer look at how this compares across metro areas in 2021, and add in variations by tenure.\n\nHere we see that renter households tend to have significantly less room than owner households, aligning with our previous observation that renter households tend to be larger given a fixed number of bedrooms. Owner households tend to be richer, and richer people consume more housing.\nHowever, this basic fact does not hold when comparing across metro areas. On average people in Vancouver and Toronto have higher incomes and higher wealth than their counterparts in Montreal, but lower levels of roominess. It’s tricky to argue that people in Vancouver and Toronto simply prefer to have fewer rooms than people in other urbanized parts of Canada. This is very likely yet another sign of a housing shortage and people economizing in response to rising prices.\nInterestingly, the Core Housing Need metric does incorporate a measure of roominess (suitability, as determined by whether a given household can be understood as fitting into bedrooms according to National Occupancy Standards), but the crudeness of the metric, in conjunction with the alternative affordable housing rule and the taking of existing households as given, all means it doesn’t pick up the decline in household roominess since 2006 or variations in roominess by metro area."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#migration-impacts",
    "href": "posts/2023-08-17-housing-outcomes/index.html#migration-impacts",
    "title": "Housing Outcomes",
    "section": "Migration impacts",
    "text": "Migration impacts\nWe’ve seen how relying upon existing households for metrics of housing need creates problems insofar as existing residents of a place may wish to distribute themselves differently. But what about non-residents? What about those forced out or never able to arrive?\nLeaving Vancouver letters seem to have moved from novelty to well worn cliche and now to the point where people just move away without the fanfare. Housing always featured prominently in the Leaving Vancouver letters, but motivations are complex and the effects of housing on migration are difficult to identify. For this post we will have to be satisfied with some weak evidence showing that shifts in migration over time are consistent with the idea that housing pressures impact migration paths, but we don’t present enough evidence to claim that housing pressures are the cause of that shift.\nWe conceptualize housing pressures to impact migration most at the local and regional level, and less so at inter-provincial and international levels. This is consistent with “reason for move” data, where housing reasons for move dominate more local levels, but recede as a deciding factor as move distance increases. So here we focus on intra-provincial migration. Are we seeing less net in-migration to Metro Vancouver from other parts of the province than we used to see in the past?\n\nThis shows a declining trend in intra-provincial in-migration, and net-migration, and an increasing trend in out-migration between 1999 and 2021, the last year for which we have gross migration data. To better understand these shifts we can look at the difference in age-specific intra-provincial net migration rates at the beginning and the end of our time frame.\n\nIntra-provincial migration rates have dropped for almost all age groups, but not by the same amount. The drop in net migration rates is most pronounced for young children, neutral for ages starting university (hello UBC!), and quickly dropping off again for young adults thereafter. Around age 40, the drop-off from 2001/2002 rates reduces, before again dropping below the baseline trend. We might imagine this as reflecting the decline in housing accessibility for young adults, matching a corresponding decline in perceptions of achievable opportunity in Metro Vancouver. By 40, selection effects may have kicked in, with only those who have achieved decent housing outcomes or adjusted to alternatives remaining. This suggests how causal forces within the relationship may flip after age 40, with older home owners increasingly moving out over recent history as a way of cashing in on selling their homes. After the age of 55 the net migration rates once again converge toward the 2001/2002 levels.\nThe narrative we offer tying intra-provincial migration patterns to housing affordability seems plausible, but requires more research. Moreover, it’s worth keeping in mind that 2021/2022 was still a year where people were responding to the upheavals of the pandemic. Given possible pandemic impacts on migration patterns, we repeat this with 2018/2019 as the final year, which shows very similar patterns, showing that especially young adults are increasingly leaving the region on net, with the exception of people around the age they start university and people around 40 years old.\n\nThese patterns are consistent with the notion that people are leaving the region due to housing pressures, where they leave the region in the ages where family formation happens or they can’t find adequate housing for their growing family. But the link remains weakly established, and it’s not yet tested enough to make clear causal claims. We believe this is a useful area for future research."
  },
  {
    "objectID": "posts/2023-08-17-housing-outcomes/index.html#the-upshot",
    "href": "posts/2023-08-17-housing-outcomes/index.html#the-upshot",
    "title": "Housing Outcomes",
    "section": "The upshot",
    "text": "The upshot\nExisting households within a region are an outcome of housing pressures, and selecting on existing households as the unit for analysis introduces collider bias. We believe this bias is substantial.\nMuch of Canadian housing discussions focuses on affordability as experienced by existing households. This is understandable because affordability is the main channel through which individuals experience housing pressures. But focusing on existing households as the base metric here can be very misleading. In particular, tracing shelter-cost-to-income ratios over time for the households that manage to form within a region shows little to no worsening in housing affordability. At best, it blinds researchers to a large part of how housing pressures manifest themselves. At worst, it leads to grotesque misdiagnoses arguing that Canada has a sufficient supply of housing (e.g. “because Canada puts more people in each home, it requires fewer homes per capita”).\nHousing is a central part of everyone’s lives, and Canada’s housing struggles are having increasingly severe impacts on the lives of Canadians. Housing analysis, especially when meant to inform policy, needs to do better at understanding the metrics it uses, how to use them, and the limits of their usefulness.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2023-08-18 05:42:56 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [c8f529e] 2023-08-17: housing outcomes post\n## R version 4.3.0 (2023-04-21)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Ventura 13.5\n## \n## Matrix products: default\n## BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/Vancouver\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] statcanXtabs_0.1.2 cmhc_0.2.6         cansim_0.3.15      cancensus_0.5.6   \n##  [5] lubridate_1.9.2    forcats_1.0.0      stringr_1.5.0      dplyr_1.1.2       \n##  [9] purrr_1.0.1        readr_2.1.4        tidyr_1.3.0        tibble_3.2.1      \n## [13] ggplot2_3.4.2      tidyverse_2.0.0   \n## \n## loaded via a namespace (and not attached):\n##  [1] sass_0.4.5                utf8_1.2.3               \n##  [3] generics_0.1.3            xml2_1.3.4               \n##  [5] blogdown_1.18             stringi_1.7.12           \n##  [7] hms_1.1.3                 digest_0.6.31            \n##  [9] magrittr_2.0.3            timechange_0.2.0         \n## [11] evaluate_0.20             grid_4.3.0               \n## [13] bookdown_0.34             fastmap_1.1.1            \n## [15] jsonlite_1.8.4            httr_1.4.5               \n## [17] rvest_1.0.3               fansi_1.0.4              \n## [19] scales_1.2.1              jquerylib_0.1.4          \n## [21] cli_3.6.1                 rlang_1.1.1              \n## [23] munsell_0.5.0             withr_2.5.0              \n## [25] cachem_1.0.8              yaml_2.3.7               \n## [27] tools_4.3.0               tzdb_0.3.0               \n## [29] colorspace_2.1-0          vctrs_0.6.2              \n## [31] R6_2.5.1                  git2r_0.32.0             \n## [33] lifecycle_1.0.3           pkgconfig_2.0.3          \n## [35] pillar_1.9.0              bslib_0.4.2              \n## [37] gtable_0.3.3              glue_1.6.2               \n## [39] xfun_0.39                 tidyselect_1.2.0         \n## [41] rstudioapi_0.14           knitr_1.42               \n## [43] htmltools_0.5.5           rmarkdown_2.23           \n## [45] mountainmathHelpers_0.1.4 compiler_4.3.0"
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html",
    "title": "Metro Vancouver Planning Regimes",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nIn a previous post we looked at the history of planning regimes in the City of Vancouver. Similar shifts happened in other municipalities in the region, and they also fit into a broader shift in planning at the regional level. Regional level planning is less concerned with zoning and the regulations that govern housing production, and more with coordinating services and the broader guiding principles applying to municipal policies. Service provision means regional level planning has an interest in keeping track of population. But how far does that interest go?\nFirst let’s take a look at our history of regional population growth, shown here for the Vancouver Census Metropolitan Area (CMA). The Vancouver CMA now matches governing level of the Greater Vancouver Regional District (GVRD) recently rebranded as Metro Vancouver, but this has not always been the case. Moreover, even the Vancouver CMA boundaries have changed, experiencing an expansion in 1971, and we break the census count timelines at 1966 based on the respecitve census geographies.\nSome of the intricacies of measuring population, let alone projecting its future growth, become evident in this graph. Geographic extent, census count procedures, and population estimation methods all matter. We also note that the growth rate has varied, and shows several inflection points, including a slowing of the growth rate in 1971, an increase in the growth rate in 1986 and a slowing again in 1996. This population history brings us back to consideration of the history of regional planning.\nRegional planning bodies sit between the provincial and municipal levels. Their structure is governed by the province. In BC they have been set up as federations of municipalities, with dedicated staff responding largely to the politicians elected to represent each contained municipality. The Lower Mainland Regional Planning Board (LMRPB) began operating in this capacity in 1949, mostly in response to catastrophic flooding all along the Fraser River the year prior. The province set up regional districts more broadly in 1965, but then subdivided the LMRPB in 1969 into four separate regional district governments, including the Greater Vancouver Regional District (GVRD), recently renamed Metro Vancouver (more on Metro Vancouver governance here).\nThere are several good overviews of the history of Vancouver regional planning, including Metro Vancouver’s own Creating Our Future: The History, Status, and Prospects for Regional Planning, Tom Hutton’s look at the regional planning between 1976 to 2011 and the entertaining if somewhat more partisan City Making in Paradise. The latter, written by former mayor and NDP premier Mike Harcourt in conjunction with Metro planner Ken Cameron and reporter Sean Rossiter, makes the case that the shift from LMPRB to GVRD was politically motivated and consequential, especially for planning around ports. In this post we want to take a different angle with a focus on housing. In particular we want to look at shifts in planning objectives in relation to housing that occurred between the 1966 regional plan (under the LMRPB) and the 1975 regional plan (under the GVRD). We’ll argue that these shifts were similarly consequential."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#the-1966-official-regional-plan",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#the-1966-official-regional-plan",
    "title": "Metro Vancouver Planning Regimes",
    "section": "The 1966 Official Regional Plan",
    "text": "The 1966 Official Regional Plan\nIn 1952, shortly after the creation of the LMRPB with its mandate to “prepare plans for the physical development of the region,” the Planning Commission published The Lower Mainland looks ahead report. It describes the basic conditions of the lower mainland, a largely resource-driven economy at the time, and lays out some principles to guide development. After collecting considerably more information in conjunction with consultation, the work of the LMRPB accumulated in the 1966 Offical Regional Plan, which established a “guide for development, both public and private, in the Lower Mainland Region.”\nThe objectives of the 1966 LMRPB plan are laid out in the table above. The parts pertaining to residential development laid out a broad plan where development should be encouraged, e.g. the development of “compact regional towns” with surrounding areas developed via “staged expansion with subdivision”. Subdivision of land was to be strictly limited in absence of water pipe and sewerage extension. At the same time the plan forbid developments in the floodplains, except in parts “committed to urban development through early settlement, in which case further development for URBAN uses shall be contingent upon floodproofing.” The plan had little to say directly about housing relative to other “URBAN” uses of land, except that areas considered for expansion of the urban footprint should not be allowed to develop into multi-family residential forms. Within the existing urban footprint, municipal authorities were left to do pretty much as they wished, at least as long as they invested in protection from flooding.\nThe authority of the LMRPB was to be short-lived. The Lower Mainland Regional Planning Board dissolved in 1968 and was replaced with the four regional bodies spanning the Fraser Valley, with the largest being the Greater Vancouver Regional District (GVRD). These bodies would collaborate on a 1980 Plan, but otherwise governed their affairs separately. The end of the LMRPB may have followed simply from the expanding regulatory interests and capacities of the province, or it may have derived from the personal animosity of the premier at the time, as in the telling of City Making in Paradise. Either way, the premiership of W.A.C. Bennett wouldn’t last much longer than the LMRPB as the 1970s ushered in the tumultuous rise of new political powers both in the City of Vancouver (TEAM) and across the province of British Columbia (NDP)."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#the-1975-livable-region-plan",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#the-1975-livable-region-plan",
    "title": "Metro Vancouver Planning Regimes",
    "section": "The 1975 Livable Region Plan",
    "text": "The 1975 Livable Region Plan\nThe new GVRD began their planning efforts within a very different context than the old LMRPB. The new political parties in the province and local municipalities rode to power in part on the strength of resistance to development (see our discussion of the rise of a Spot Discretionary Regime in Vancouver). The NDP solidified the protections for agricultural land encouraged by the old LMRPB through formalization of the Agricultural Land Reserve, constraining further outward expansion of the region’s urban footprint. The culture within planning had changed as well, embracing many of the critiques launched by reformers like Jane Jacobs, and turning toward public consultation for solutions. Local activist planners like Walter Hardwick and Darlene Marzari circulated between academic, professional, and political positions. Of note, where the leading edge of the Baby Boom had just turned 20 by the time of the 1966 report, their demographic impact on housing demand was really picking up by the 1970s. Into this new moment, the GVRD launched a new consultation-heavy planning process, culminating in the 1975 Livable Region Plan, intended as a blueprint to carry metropolitan Vancouver forward from 1976 to 1986. Housing would figure prominently. But the document would place fears about growth front and centre in its introduction.\n\nWe have tried to make proposals rooted in the hopes and fears of the people living here who seem to have a modest but challenging dream: let Greater Vancouver continue to be a good place to live. Their fear is that growth will spoil the Region, and that governments at all levels cannot or will not act to maintain livability in the face of growth.\n\nWhile the 1975 Plan pointedly avoided a statement on whether growth was good or bad, GVRD staff began presenting policy proposals to their Board in 1974 for “bringing the overall growth rate under control.” The 1975 Plan summarized the bases for a shift in direction.\n\nBased on this series of studies, public meetings and seminars, the Board has adopted the following policy changes:\n\nOverall Growth Rate: From a policy of accepting all growth that comes, to a policy which recognizes that the overall rate of growth is capable of being controlled, but not by GVRD alone.\nSharing Growth: From a policy of accepting natural growth trends to a policy of determining the amount of growth each part of the Region can accommodate.\n\n\nThese key shifts show up in the first strategy forwarded by the 1975 Plan. The “growth targets” represented not aspirations, so much as caps, and the plan called very explicitly to limit population growth by rationing housing throughout the region. The plan elaborates on point 1 from the table up top:\n\nThe overall growth of the Region to 1986 should be shared among the municipalities on a rational basis. We propose population growth targets for each part of Greater Vancouver, based on 13 livability concerns such as the preservation of open space and recreation areas, minimizing travel time and inconvenience, and minimizing disruption to existing communities.\n\nBy contrast, strategy point 2 signalled a milder departure from the previous emphasis on economic development by subjugating jobs growth to the location of residential developments, indicating an eagerness to fragment the job market. As assessed by Tom Hutton “the idea of ‘deflecting’ office development from downtown Vancouver to the suburban RTCs suggested a naïve understanding of how commercial property markets operate, and more specifically the power of agglomeration in office location”. Points 3, 4, and 5 connected even more closely with prior goals from the 1966 LMRPB plan."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#transforming-a-projection-into-a-gate",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#transforming-a-projection-into-a-gate",
    "title": "Metro Vancouver Planning Regimes",
    "section": "Transforming a Projection into a Gate",
    "text": "Transforming a Projection into a Gate\nIn short order, the population targets of the 1975 GVRD Plan took a relatively crude set of population projections, lowered them, and turned them into a gate to control growth. The targets were couched in pseudo-scientific language and mechanisms that partially obscured the severity of the shift to population control as a objective of metro planning. The “rational basis” for the targets took the annual population growth rate between 1971 and 1976 (using population estimates for 1976) of 3.2% and reduced it to 2.1% for the next 10 years. The document grouped municipalities into sub-regions of the GVRD, and compared 1971 population counts from the census with estimated 1976 population estimates to project out “trends” to 1986.\nThe actual data work in the document remains opaque; the “trend” extrapolations given in the document follow neither geometric nor arithmetic population growth trends, with the region-wide growth rate explicitly fixed to the lower rate of 2.1% and the municipal trend adjusted to fit this in some obscure way that results in an increased trend for some municipalities and a decreased rate for others.\nThe 1975 Plan then proceeded to modify the regional trends according to ten criteria “based on opinions of the GVRD Planning Committee and the reports of the citizens’ advisory Policy Committees” that were translated into numeric population targets “with the help of a computer program”, but did so in way that left the (previously lowered) overall projected population for 1986 unchanged.\nThe plan did not explicitly translate these population targets into housing targets. The conversion of the population targets to housing units and floor space quotas was left to individual municipalities. Strikingly, at the same time as the plan set growth targets it also acknowledged a worsening of housing outcomes:\n\nFor the last few years we have had a shortage of affordable housing, especially rental housing. This shortage has helped to push up rents and house prices, and caused an increase in doubling-up and crowding of households in existing accommodation. These effects strike hardest at the poor, people who are in large part not newcomers to Vancouver.\n\nThe 1975 Plan presciently continued to explain how reducing growth via their new gates would adversely affect housing outcomes, in particular for lower income groups.\n\nTrying to keep out newcomers by reducing the amount of housing will only hurt our own lowest income groups more, and perhaps not stop very many newcomers.\n\nModeled on consultation only with existing residents, the 1975 Plan emphasized that it was not concerned with the fate of newcomers. But limiting housing would also hit the existing population.\n\nSuch a policy would make Greater Vancouver a lot less livable for many people who are already here, and our objective is to maintain livability for all who live in Vancouver.\n\nThe report did not provide a clear justification for how it arrived at its overall population projections, other than applying a lower growth rate than observed in the recent past. It justified the reduction with concerns about population growth while acknowledging that reducing housing development would hurt housing affordability, in particular for lower income people, which it claimed constituted a concern. To resolve this apparent contradiction the 1975 Plan suggested that:\n\nregional and municipal officials can work with senior levels of government to influence immigration and other factors which affect overall population growth.\n\nMore explicitly it suggested the federal government should:\n\n\nwork with provincial and regional governments to create a settlement policy for the country, just as the Regional District is working with its municipalities to develop a settlement policy in the Region,\nco-ordinate immigration policy with the national settlement policy when it is developed,\nwork to create growth centres outside of the fast-growing metropolitan areas, as part of the population settlement policy,\nfinance and participate in a study with the GVRD on the impact of immigration on the Vancouver Region.\n\n\nOf note, these suggestions for controlling the growth of cities look strikingly similar to those put forward by Harvey Molotch’s influential “Growth Machine” paper within Sociology, published in 1976.\nAll of the work of the 1975 Plan happened against a backdrop of already high housing prices, signalling pent-up demographic demand for housing both from suppressed households in the region as well as migration pressures. Planning ahead for this demographic demand would suggest scaling up the development of housing. But the 1975 Livable Region Plan did just the opposite, attempting to install a gate to control further growth.\nThe idea that the federal government should adjust immigration policy to fit the regional preference to build less housing, instead of fitting the housing strategy to accommodate federal immigration policy, is a theme in media and municipal policy-making that carries on through this day. As noted above, there were a lot of things happening in the 1970s, including a resurgent neo-Malthusianism directly referenced by the 1975 Plan. But it is possibly no coincidence that local calls to adjust federal immigration policy grew in conjunction with a more open shift in immigration policy and the arrival of many more immigrants from outside Europe and the United States as evidenced in the graph below showing the makeup of the foreign-born population in each census year."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#targets-vs-actuals",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#targets-vs-actuals",
    "title": "Metro Vancouver Planning Regimes",
    "section": "Targets vs actuals",
    "text": "Targets vs actuals\nDid the gate work? It’s always useful to check, after the fact, how targets compare to actual population growth. While municipal representatives to the GVRD expressed supported for limiting growth, it’s not clear the targets were at all binding, especially after regional plans were effectively nullified through provincial restraint bills in the 1980s. Perhaps more pressingly, there were no clear guides for how to translate population targets into housing approvals, especially given instability in household sizes. It’s doubtful any authorities adequately monitored how population aging and changes in family formation and childbearing implied more housing was going to be needed for every future person added to the population in the years to come.\n\nWhere an average dwelling might have contained nearly 4 residents in Canada in the 1960s, this figure dropped below 3 by the 1980s. While households were smaller in urban areas like Metro Vancouver, the fall in household size was similar precipitous in the lead up to 1976. But by 1986, household size began to rise again, with sizes in Metro Vancouver ultimately rising above the average for the rest of Canada (see also our post on age-specific household maintainer rates during these years). Overall it’s likely that the fuzziness of the methodology to set population targets, and the lack of guidance on how to translate these into housing units and approvals, led to all sub-regions, with the exception of the very small “Other” region (not shown in the graph below), failing to meet even their lowered GVRD targets by wide margins.\n\nIn conjunction with the planned reduction relative to the pre-1971 growth rate this further reduced the average annual growth between 1971 and 1986 that was actually achieved to 1.4%. One interpretation might be that the gate worked even better than planned. But many things were happening in the 1970s and 1980s, including rapid inflation, enormous hikes in interest rates, and a major recession that hit BC especially hard. So it’s probably too soon to celebrate or condemn the success of the Livable Region Plan’s gate."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#subsequent-regional-growth-strategies",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#subsequent-regional-growth-strategies",
    "title": "Metro Vancouver Planning Regimes",
    "section": "Subsequent regional growth strategies",
    "text": "Subsequent regional growth strategies\nIf the 1975 Plan attempted to transform a projection into a gate, subsequent plans would take that gate and elaborate upon it until they could pretend it was a projection again. The initial haphazard way in which population trends were projected into the future, and then linked to targets, was partially professionalized in subsequent iterations of the regional growth strategy and related planning documents. At the same time, equity concerns largely shifted from broader social equity to more specific inter-municipal equity. Regional planning documents downplayed recognition and concern that limiting housing could have large negative impacts on affordability, particularly for low income households. Illustrating these shifts, the 1996 GVRD plan attempted to take changing household sizes into account, but also took population trends and targets a step further to estimate “growth capacities” out to 2021, including separate capacities for ground-oriented and apartment households. The 1996 GVRD plan took a more pessimistic view of the prospects for controlling growth, but slowing and/or managing growth remained on the table, with little discussion of negative consequences.\n\nExperience around the world has shown that population growth cannot be stopped by rules and regulations. Even slowing growth is a challenge with many possible consequences. However, we are able to manage growth in a way that preserves the essential qualities that make our region such a special place.\n\nAside from minor shifts, the basic methods of projection and target setting have remained the same through the present. The Regional Growth Strategy does not engage with the question of demand for housing, but instead relies upon mechanistic population “projections” that naively extrapolate past patterns. In doing so, the Regional Growth Strategy continues to systematically enshrine past housing shortages and worsen them, although the technical machinery to do this has advanced considerably.\nIn effect, the technical machinery excludes everyone pushed out of the region in the past. The only ones who count are those who make it into the region and manage to stay in the region despite the growth reduction efforts of local authorities. The technical machinery also casts realized household size as perfectly matched to preferences. Everyone stuck living with their parents’ or in bad roommate situations is treated the same as if they were living out their dreams."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#city-development-and-regional-planning-regimes",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#city-development-and-regional-planning-regimes",
    "title": "Metro Vancouver Planning Regimes",
    "section": "City Development and Regional Planning Regimes",
    "text": "City Development and Regional Planning Regimes\nWe can now place a rough periodization of Regional Planning Regimes alongside our earlier periodization of City Development Regimes. Lack of regional coordination was linked to a permissive development regime through City and Metro’s earliest years. The City shifted in a more regulated regime first, adopting zoning as a key tool by 1931, primarily aimed at coordinating growth. The Region organized only after the 1948 flood, similarly focused on coordinating growth for major infrastructure. The City shifted to a Spot Discretionary Regime, whereby development was much more tightly controlled by 1972. Metro shifted toward a similar controlling stance, attempting to install gates to slow and manage growth, right around the same time.\n\n\n\n\nCity Development Regimes (Vancouver)\n\n\nRegional Planning Regimes (Metro Van)\n\n\n\n\n\n\n1886-1931 The Permissive Regime\n\n\n-1949 No Regional Regime\n\n\n\n\n1931-1972 The Zoning for Growth Regime\n\n\n1949-1968 The Infrastructure for Growth Regime\n\n\n\n\n1972-Present The Spot Discretionary Regime\n\n\n1968-Present The Gate Management Regime"
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#managing-regional-allocation",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#managing-regional-allocation",
    "title": "Metro Vancouver Planning Regimes",
    "section": "Managing regional allocation",
    "text": "Managing regional allocation\nThe gating work done by Metro Vancouver projections also remains tied to separate and managed allocation for each municipality. Allocations are partially informed by past trends, but also by evolving calculations of “land use capacity” linked to municipal community plans and inter-municipal negotiation. The results are complex, retaining some focus on the encouragement of compact urban communities (where we still see targets), but also essentially guiding housing and population growth into regions where there’s the least political resistance instead of focusing on demand for where people want to live. Exclusive and exclusionary municipalities are effectively encouraged to stay that way.\nTo understand the fundamental problem with how the Metro Vancouver growth strategy allocates housing targets to municipalities, consider how the Regional Growth Strategy to 2040 projections in the following municipalities stack up against actual growth over a ten year time period. Here we gloss over the issue with census undercounts, implicitly assuming that the ratio of the undercounts in each municipality did not change between the 2011 and 2021 censuses.\n\nTake Metro Vancouver A, the area around UBC. Population growth was substantially higher than projected, and this is entirely due to more housing getting built than projected. For the City of Vancouver, population growth came out very close to projections. Does that mean that Metro Vancouver properly estimated demand to live in the City of Vancouver? Of course not, prices and rents rose enormously over this period. It’s simply because the City of Vancouver built as much housing as Metro projected it would; it’s a self-fulfilling prophecy. Similarly for West Vancouver, Port Moody, Port Coquitlam, and Coquitlam the population came in significantly short of projections not because people don’t want to live there, but because the municipalities failed to build the projected amount of housing.\nThis raises two questions:\n\nWhy stick to flawed methodology for projections?\nDo projections and targets from the Regional Growth Strategy matter, or do municipalities do what they want anyway?\n\nWe will look at these in reverse order."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#do-rgs-projections-matter",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#do-rgs-projections-matter",
    "title": "Metro Vancouver Planning Regimes",
    "section": "Do RGS projections matter?",
    "text": "Do RGS projections matter?\nThe Regional Growth Strategy housing targets aren’t binding for municipalities. While some go over the targets, most stay below their targeted growth. Metro Vancouver does at times exert pressure if it feels that contained jurisdictions divert too much from their targets, as happened when UBC wanted to build more housing than what the Regional Growth Strategy called for. But there is little Metro Vancouver can do if municipalities decide to ignore targets. So how do they matter in maintaining and reinforcing our housing shortage if it all come down to municipalities?\nWhile it’s up to the municipalities how much housing to build, the Regional Growth Strategy plays two important roles. Individual municipalities have little chance to make a serious dent in the housing crisis by increasing the rate at which they add housing. It’s an uphill battle to start with given how large our housing deficit is, but if other municipalities are pulling in a different direction it’s next to impossible. Regional coordination matters. And if regional targets are set to reinforce housing scarcity all bets are off. At the same time, the low regional housing targets give individual municipalities engaging in fiscal zoning cover to keep housing scarce in their municipality."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#why-stick-to-flawed-methodology",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#why-stick-to-flawed-methodology",
    "title": "Metro Vancouver Planning Regimes",
    "section": "Why stick to flawed methodology?",
    "text": "Why stick to flawed methodology?\nIt is not uncommon that flawed methodology survives unquestioned for a long time. There is significant inertia inherent in established methods, and it can require concerted effort to overthrow them. In part it’s because technical detail helps wrap flaws up into an authoritative sounding package. But in part it’s also because the flawed projections have important defenders, serving a variety of roles for different groups involved. They enable staff to justify planning positions and point toward a source of authority and expertise; they enable politicians to begin negotiations over seemingly neutral starting projections while side-stepping very different views of the most desirable future, both between themselves and their constituents; finally, they enable both staff and municipal politicians to speak to provincial and federal governments in terms all parties can understand, even if they can’t agree upon desirable outcomes. This helps explain their inherent inertia. Even when they’re wrong, they can still be useful to the right people.\nThe bottom line is that the broad availability of housing, and the ensuing improved housing affordability it would provide, is not a universal good. A large portion of the voting population does not feel the negative consequences of housing scarcity because they already have secure housing, and they might even like the feeling of observing their primary asset steadily appreciating in value.\nThe nature of Metro Vancouver planning is that it relies on the individual municipalities signing off on any agreement. In order to fix the flawed planning and data processes and put them on a path of more realistically assessing the interplay between housing and population/demographics, one first needs to more clearly separate the planning and data processes from the political processes. Politicians may well choose to vote to continue to constrain housing growth and perpetuate our housing shortage, but they should do so explicitly instead of hiding their values under flawed data work and technocratic language."
  },
  {
    "objectID": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#upshot",
    "href": "posts/2023-06-20-metro-vancouver-planning-regimes/index.html#upshot",
    "title": "Metro Vancouver Planning Regimes",
    "section": "Upshot",
    "text": "Upshot\nOur focus in the post is on housing, and regional planning has an important role to play when it comes to coordinating planning for housing and population growth, especially in a heavily balkanized environment like Metro Vancouver (containing 21 municipalities, an electoral area, a First Nation Treaty Land, and, while not explicitly represented in Metro Vancouver planning processes, 16 First Nation reserves). We can see and clearly mark a regime change. Starting with plans from the 1970s, the GVRD moved away from coordinating for growth and toward gate management. This laid a strong foundation for housing scarcity, and Metro Vancouver has to this day continued to work toward perpetuating and deepening housing scarcity in the region through its projection and coordination work. While it is unclear how important the Metro Vancouver Regional Growth Strategies have been in coordinating municipal limits on housing development, they have at a minimum given cover to municipalities for perpetuating and deepening housing scarcity and spatial misallocation, shifting growth away from where people want to live.\nThe Province has already begun to look more closely at municipalities that have failed to accommodate more housing, and the inclusion on the “naughty list” of so many municipalities within Metro Vancouver should come as no surprise. But the regional planning body could also use an overhaul. A first step in reform should be reconfiguring the methods by which Metro Vancouver attempts to coordinate the housing targets of sub-regions. Past expert consultations (attended by one of the authors of this post) have identified problems with projections, making the key point that for a region with a severe housing shortage, housing development leads to population growth rather than the other way around. So far these critiques of Metro methods have not yet sunk in. Now might be a good time to shake things up.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2023-09-25 22:29:16 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [9251cc5] 2023-09-26: updates on target post\n## R version 4.3.0 (2023-04-21)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Ventura 13.6\n## \n## Matrix products: default\n## BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/Vancouver\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] kableExtra_1.3.4          mountainmathHelpers_0.1.4\n##  [3] cansim_0.3.15             cancensus_0.5.6          \n##  [5] lubridate_1.9.2           forcats_1.0.0            \n##  [7] stringr_1.5.0             dplyr_1.1.2              \n##  [9] purrr_1.0.1               readr_2.1.4              \n## [11] tidyr_1.3.0               tibble_3.2.1             \n## [13] ggplot2_3.4.2             tidyverse_2.0.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] sass_0.4.5        utf8_1.2.3        generics_0.1.3    xml2_1.3.4       \n##  [5] blogdown_1.18     stringi_1.7.12    hms_1.1.3         digest_0.6.31    \n##  [9] magrittr_2.0.3    timechange_0.2.0  evaluate_0.20     grid_4.3.0       \n## [13] bookdown_0.34     fastmap_1.1.1     jsonlite_1.8.4    httr_1.4.5       \n## [17] rvest_1.0.3       fansi_1.0.4       viridisLite_0.4.2 scales_1.2.1     \n## [21] jquerylib_0.1.4   cli_3.6.1         rlang_1.1.1       munsell_0.5.0    \n## [25] withr_2.5.0       cachem_1.0.8      yaml_2.3.7        tools_4.3.0      \n## [29] tzdb_0.3.0        colorspace_2.1-0  webshot_0.5.4     vctrs_0.6.2      \n## [33] R6_2.5.1          git2r_0.32.0      lifecycle_1.0.3   pkgconfig_2.0.3  \n## [37] pillar_1.9.0      bslib_0.4.2       gtable_0.3.3      glue_1.6.2       \n## [41] systemfonts_1.0.4 xfun_0.39         tidyselect_1.2.0  rstudioapi_0.14  \n## [45] knitr_1.42        htmltools_0.5.5   svglite_2.1.1     rmarkdown_2.23   \n## [49] compiler_4.3.0"
  },
  {
    "objectID": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html",
    "href": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html",
    "title": "A Brief History of Vancouver Planning & Development Regimes",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nSay you want to construct some multi-family housing in Vancouver. How long will it take? The answer is simple: it depends. There are many factors upon which it depends. Here we want to highlight one in particular: when you started.\nAs it turns out, it used to take a lot less time to build multi-family housing. There is reason to believe we could reduce that time again, but getting there involves gathering a better understanding of our current development regime, and placing it in historical perspective. We begin this process below, before diving deeper into two case studies of developments along Alma Street, located very near one another in space, but separated by some fifty years in start time. We’re going to look at the 14 storey rental building currently under construction at the intersection of Alma and Broadway, and the 12 storey rental building built in 1970 two blocks to the north at 3707 W 7th Ave.\nBut first some history! We’re going to divide time up into three rough regimes, which we call the Permissive Regime, the Zoning for Growth Regime, and the Spot Discretionary Regime."
  },
  {
    "objectID": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#the-permissive-regime",
    "href": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#the-permissive-regime",
    "title": "A Brief History of Vancouver Planning & Development Regimes",
    "section": "1886-1931 The Permissive Regime",
    "text": "1886-1931 The Permissive Regime\nThe City of Vancouver incorporated in 1886, and in its early years its development was relatively concentrated (mostly extending from the old site of Gastown to the East, and from the Railroad’s investments along Granville Street to the West). Bridges enabled leaps across the industrial sites surrounding False Creek, opening up further development to the South. The development regime quickly came under regulation after incorporation, but the form of regulation remained limited, with building permits linked to concerns about fire safety, gradually extending to building codes and the emergence of public health and nuisance regulations just starting to place limits on use (e.g. where a slaughterhouse could locate). Diverse forms of housing, including lots of multi-family, mixed within and beside other building forms and development often followed (and/or paid for the extension of) trolley lines. Early fire insurance maps - and some of the first moving picture footage shot from a trolley - demonstrate the diversity that accompanied Vancouver’s development boom.\nOur first development regime began to shift through the 1920s and here we put it to bed by 1931, after a series of preliminary zoning laws were joined to a new plan for the City of Vancouver which, though not fully adopted, would prove enormously influential in ushering in a new, and much stronger regulatory regime. Before we fully got there, 21% of our current building stock was constructed, not counting buildings that got torn down and redeveloped since. To illustrate that legacy, we highlight three examples of pre-1931 buildings below.\n  \nWhat these buildings have in common is that they mix residential and commercial uses. They’ve also stood the test of time and have proven useful to this day, often earning protected heritage designations along the way. But the new, post-zoning regulatory regime made these kind of buildings illegal to construct anew across most of the city. In fact, the first two buildings were rendered illegal to get rebuilt in place under the initial 1931 zoning, and none of these three buildings would be legal to build under their current zoning today."
  },
  {
    "objectID": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#the-zoning-for-growth-regime",
    "href": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#the-zoning-for-growth-regime",
    "title": "A Brief History of Vancouver Planning & Development Regimes",
    "section": "1931-1972 The Zoning for Growth Regime",
    "text": "1931-1972 The Zoning for Growth Regime\nThe series of zoning laws put in place, starting with their preliminary version in 1927, were adopted largely to prevent the incursion of multi-family apartments into single-family residential districts. But districting into zones also brought other changes, including efforts to more fully separate new development by land use, limiting mixing. Use districts were primarily divided into single-family & duplex residential (RS, RT), multi-family apartment (RM), commercial (C), and industrial (I). Building permits, previously tied to meeting evolving safety and nuisance concerns, became tied to compliance with far more rigid zoning regulations, laying out much narrower lists of allowed uses.\nThe planner hired to assist Vancouver in its transition to a zoning regime, Harland Bartholomew, also planned ahead, anticipating the future growth of the City. Given extensive surveys of existing uses, Bartholomew imagined the City could accommodate about thirty years of growth under the zoning enacted through the 1920s and 1930s before revisiting its plan. Somewhere around 1960, he thought Vancouver would have to upzone, accommodating a new period of growth.\nAs it turned out, Vancouver began a modernization of its zoning code in 1956, adopting new zoning district names and guidelines for metrics like Floor Space Ratios that would guide further development, and opening up new opportunities for tower form developments downtown. Through the 1960s, Vancouver would also begin to tentatively expand the range of its multi-family apartment (RM) districts, and also enable towers in these areas, upzoning to accommodate anticipated growth. But a backlash was brewing. A new council would sweep in with the 1972 election under the banner of TEAM, a reformist party opposed to the spread of towers. They placed themselves at odds with the planning regime of the City, downzoned the multifamily apartment districts once more, and ushered in a new regime characterized by much tighter controls on development and more direct negotiation with City Hall."
  },
  {
    "objectID": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#present-the-spot-discretionary-regime",
    "href": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#present-the-spot-discretionary-regime",
    "title": "A Brief History of Vancouver Planning & Development Regimes",
    "section": "1972-Present: The Spot Discretionary Regime",
    "text": "1972-Present: The Spot Discretionary Regime\nThe downzoning of residential multifamily (RM) apartment districts helped usher in a new planning and development regime. Its key features included a) the loss of outright development potential on most lots, b) the rise of discretionary development potential with planner approval, and c) the rise of spot zoning through comprehensive development districts with full city council approval.\nWhat we call the Spot Discretionary Regime emphasizes the two processes by which most development of new apartment buildings would proceed. Undeveloped old RM lots, intended to support apartment buildings, continued to see new construction, but generally at much lower heights and density than before and increasingly through discretionary approval processes within existing zoning, where developers had to get approval directly from planners before being able to proceed to getting a building permit. Some new RM lots were also added, expanding a few apartment districts. At the same time discretionary options to add residential units were opened up in other zones. These included, most prominently, the C (Commercial) zones along arterials, but also zones opened up by the rezoning of old industrial lands, especially around False Creek. These highly discretionary comprehensive neighbourhood zones (e.g. FCCDD & FM-1) highlighted evolving priorities of the city. They would effectively transform over time into smaller, lot-specific comprehensive development (CD) zones through a practice known as spot zoning that involved the discretion of both city planners and city council as a whole. The City also began to charge increasing Development Cost Levies (DCLs) for new multi-family apartments, adding Community Amenity Contributions (CACs) for most re-zonings and correspondingly lowering property taxes for existing residents.\nThe Spot Discretionary Regime has basically continued its evolution up into the present, for the most part progressively layering new rules, procedures, paperwork, and fees onto new multi-family apartment developments. Each of these layers has further oriented developers toward the bureaucracy and politics of City Hall, which have become increasingly important for developers to navigate to get anything built. It’s probably no accident that Vancouver’s housing crisis appears to have grown increasingly dire roughly in accordance with the evolution of the Spot Discretionary Regime. To this the Regime has responded not by loosening its hold over development, but instead by identifying new priorities to enforce through its controls. So it is that the City recently began to prioritize the construction of secured rental apartments, which is how we arrived, for instance, at the Moderate Income Rental Housing Pilot Program (MIRHPP), whereby developers are enabled density bonuses in exchange for building protected and moderately affordable rental apartments instead of condos.\nOverall, our different development regimes link to distinct approval processes and paper trails. Our Permissive Regime simply required building permits adhering to relatively simple safety and nuisance guidelines. Our Zoning for Growth Regime required permits to be scrutinized for adherence to relatively strict zoning guidelines, but left a fair number of lots open for outright redevelopment. Our Spot Discretionary Regime saw the closure of most opportunities for outright development of multi-family apartments, shifting developers into often complex negotiations with the City, generally involving either gaining chief planner approval (discretion) or gaining full council approval (spot re-zoning) before moving ahead. The evaluation of lengthening paper trails also involved lengthening approval times and correspondingly lengthening development schedules. As suggested by Ezra Klein’s recent NYTimes column on the Construction Industry, the shift in regimes isn’t unique to Vancouver. But instead of zooming out, let’s zoom in and return to the blocks containing our two nearby tower projects to compare what our Zoning for Growth Regime and our Spot Discretionary Regime look like on the ground!\nFor our case studies, we’re going to zoom in on the miniature Alma corridor between Broadway and 4th Avenue. Let’s have a look at how our regimes have played out via the history of zoning along this corridor. Here we draw heavily from our CMHC-funded project on zoning history.\n\nThe area is situated between Broadway to the south (bottom), 4th Avenue to the north (top), Alma St on the east (right), and Highbury on the west (left). West of Highbury Street start the Jericho Lands, which is the site of a future First Nation-led development by the MST development corporation, a collaboration between the Musqueam Indian Band, Squamish Nation and Tsleil-Waututh Nation.\nDuring our Permissive Regime, the Alma corridor remained relatively undeveloped, located on the outskirts at the border between the old City of Vancouver and the former self-styled residential municipality of Point Grey. Correspondingly very little remains from this era.\nThe amalgamation of Vancouver and Point Grey roughly coincided with the transition into our Zoning for Growth Regime. By 1931, Commercial hubs were zoned for Alma’s intersections with 4th and Broadway with a line of Apartment zoning between the two. The 1956 modernization of the zoning code (Bylaw 3575) replaced these with new C-2 (Commercial) and RM-3 (“Medium Density” Residential Multifamily) zones, installing restrictive RS (Single-Family) zones in the surrounding lots. At that time RM-3 allowed for a maximum height of 40ft and 1.3 FSR. But on July 4, 1961 council amended RM-3 zoning with bylaw 3962 to allow for heights up to 100ft and a change to maximum FSR that started out lower (1 FSR) but rewarded small site coverage, large lot area, and underground parking remaining below the building envelope in a formulaic and predictable way ultimately resulting in a significant FSR boost for towers. This zoning change enabled the 12 storey apartment building at 3707 W 7th Avenue to get built under outright zoning, as well as its mid-rise mate a block to the north.\nThis upzoning amounted to a significant expansion in developable floorspace across the city, leading to a development boom resulting in lots of new towers still standing today. It delineates two phases of the Zoning for Growth Regime: as thirty years of development filled out the initial plan, the modernization and 1960s upzoning “filled up the tank” of development capacity again.\nBut the towers prompted backlash, the rise of TEAM, and downzoning, setting up the transition to our Spot Discretionary Regime. Bylaw 4757, which came into effect in February 1974, downzoned the Alma corridor’s RM-3 to RM-3A. RM-3A was still a relatively new zone at the time (created in June 1972), containing similar FSR limits to RM-3, but reduced height to 35ft outright, which at the discretion of the Planning Board could be increased to only 40ft. In March 1976 the city introduced the further restrictive RM-3A1 and RM-3B zones and soon after started downzoning RM-3A areas to RM-3A1 or RM-3B while at the same time continuing to downzone remaining RM-3 areas to RM-3A. This process would take several years, catching up to our Alma corridor by December 1977 when it received its further downzone from RM-3A to RM-3B.\nThe Spot Discretionary Regime would continue to tinker with the RM codes, for instance adding and subtracting discretionary FSR for units earmarked for seniors, until our Alma corridor was transformed into RM-3A1 in January 1988, before the whole zone was renamed as RM-4 in April 1988. This coincided with the expansion of the zone into former RS lots under Gordon Campbell’s somewhat more developer friendly NPA administration, and a couple of low-rise condo developments were added along Highbury. This is where the RM lots show up in our 1990 map, and where they’ve remained basically through the present (The original RM-4 zoning, which covered the West End before it was replaced by RM-4A and then WED zoning, had been phased out in October 1984, and eventually the West End got downzoned to RM-5.) Meanwhile, the C-2 lands were increasingly allowed, at planner discretion, to accommodate residential apartments, as occurred for the lot on the NE corner of 5th & Highbury. Finally, a CD comprehensive development spot rezoning was introduced at the NW corner of Alma & Broadway in 2020, setting up our second development.\nWe can see the result today in this 3D view of the two sites we are interested in and the surrounding buildings.\n\nThe buildings are coloured by the period when they were built. Only one, in green, survives from the Permissive Regime. Here we divide up our Zoning for Growth Regime into pre-1961 and post-1961, reflecting our interest in the response to the 1961 upzoning. A number of smaller buildings and single family homes survive from before 1961, coloured in orange. Many more were produced after the upzoning, including towers laid out in purple. But not everything in RM came up towers during the 1961-1972 era; notice the multiple low-rise buildings along our strip. Nevertheless, nothing in RM was built to the tower form after the 1972 downzone. Indeed, not much got built at all for quite a while, with a flurry of buildings getting added between 1988 and 1990, including the new low-rise in lands rezoned from RS to RM and the C-2 zoned Jericho Centre and neighbouring condo building (which also benefited from a density transfer)."
  },
  {
    "objectID": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#a-tale-of-two-towers",
    "href": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#a-tale-of-two-towers",
    "title": "A Brief History of Vancouver Planning & Development Regimes",
    "section": "A Tale of two Towers",
    "text": "A Tale of two Towers\nSo let’s compare our two focus tower developments, the 1970 12-storey rental apartment on 7th and Alma and the currently under construction 14 storey MIRHPP at Broadway and Alma!\n\n3707 W 7th Avenue\nHere we have a 12 storey purpose-built rental apartment building, including some 86 apartments. This is just the kind of building that our MHRIPP policy is now meant to promote, constructed some fifty years ago. Of note, even then it could have been constructed as a condo building, though BC’s Strata Act was still relatively new at the time. We don’t know how long it took to pull together the financing for this building, but we do know that it was allowed as an outright use within the zoning code, requiring relatively little in the way of paperwork or discretionary review.\nSo how long did it take to build? The city archives date the building permit for it to October 1969 for 86 units, with permit value of $635k. The building completed only about one year later, around which time it got advertised in the newspapers, as in this excerpt from the Vancouver Sun from October 1970. In sum, the timing between the start of our paper record and building completion extends to only a single year.\nTo understand how the zoning enabled (and forced) the form of this 12 storey building, we can take a look at the parcel and building footprint. \nThe building had a site coverage of 16%, which gave it a density bonus of 0.51 FSR. The site area bonus fetched the maximum allowed 0.25 FSR, and additionally there was an FSR bonus of up to 0.2 for the share of parking that is directly under the building footprint, for a total of between 1.76 and 1.96 FSR, depending on how parking is configured. The total gross FSR for the 12 storey building, estimated from LIDAR data, is approximately 1.88 which falls right into that outright allowed range.\nBy contrast, as the area got downzoned, our example building became impossible to rebuild. The reduction of height made the FSR bonusing challenging, and building nearly any kind of apartment building came to rely upon the discretion of the Planning Board. Apartments had to spread out, reducing our lot coverage bonus, and limiting the maximum achievable density on the site to 1.6 FSR.\nLooking at lots nearby, low-rises later constructed within the downzoned RM-4 contained less floor space relative to lot size and significantly fewer apartments. And apartments in the area did not get cheaper. At the time we write this, a 635 sqft 1 BR apartment at 3707 w. 7th is currently on offer at a rent of $2,575/mo.\nThe lack of secured rental apartments in Vancouver is precisely what the MIRHPP policy was meant to address. So let’s take a look at how that’s going!"
  },
  {
    "objectID": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#the-point-grey-neighbourhood",
    "href": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#the-point-grey-neighbourhood",
    "title": "A Brief History of Vancouver Planning & Development Regimes",
    "section": "The Point Grey neighbourhood",
    "text": "The Point Grey neighbourhood\nLet’s take a look more broadly how this has played out in the Point Grey neighbourhood, where the Alma/Broadway project, as well as the apartment building from 1970 pictured above, are located. Here is how this neighbourhood is defined in CMHC data.\n\nHow many condo or rental apartments have been built in that area? CMHC data at the neighbourhood level goes back to 2010.\n\nAnd as one might have expected, very few apartments have been built. A recent low-rise mixed-use commercial (C-2 zoning) with 36 apartments over top shows up as a 2010 start (completed in 2012 near Discovery & 10th). Some of the rest of the low level noise is due to secondary suites, which CMHC data counts as single rental apartment units. The apartment ban gradually installed through the transition from the Zoning for Growth to the Spot Discretionary Regime has been very effective in Point Grey. As a result, the Broadway/Alma project registering as a 2022 start really stands out.\nAs for that Broadway/Alma project, it’s worth remembering that in the near future it should be sitting practically on top of a new SkyTrain Station, and just to the East of the soon-to-be-booming Jericho Lands. Point Grey is about to change. But it remains to be seen how fast. The clock has already started."
  },
  {
    "objectID": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#takeaway",
    "href": "posts/2023-02-06-a-brief-history-of-vancouver-planning-development-regimes/index.html#takeaway",
    "title": "A Brief History of Vancouver Planning & Development Regimes",
    "section": "Takeaway",
    "text": "Takeaway\nFrom 1970 to 2022 we shifted between two different planning and development regimes. The Zoning for Growth Regime definitely had its problems, but if it allowed you to build something, you knew you could build it, and you could build it quick. The zoning change in 1961 enabled and encouraged towers, and as far as we can tell, the developer of 3707 W. 7th needed only their building permit issued in 1969 to construct their 12-storey tower by 1970. Shortly thereafter we shifted into our current Spot Discretionary Regime. Here the developer purchased the assembled lot in 2011, began wading through political currents of rezoning in December 2016, only to dive in fully after policies shifted with a rezoning application in November 2019, approved in October 2020, with development permit and building permit processes holding up excavation till Spring of 2022. It won’t be done till the end of 2023 at the absolute earliest, some 4-12 years after the process began, depending upon when one starts the clock.\nIn the Zoning for Growth Regime developers were able to be responsive to changes in prices and add housing fast, purchasing development sites with full understanding what could be built and only needing a building permit. This reflects what economists call “supply elasticity,” or the ability for markets to serve as a mechanism linking increases or decreases in construction activity to increases or decreases in demand as expressed through changes in prices or rents. So long as outright zoning contained ample capacity to accommodate growth, generally reflected in upzoning in Vancouver until 1972, housing supply generally responded to housing demand.\nThe Spot Discretionary Regime directly targeted the market mechanisms enabling additional housing to be added in response to demand. The added uncertainty introduced by new discretionary processes required developers to speculate on what could be built when buying lots, adding risk. This process selected for developers well connected with City Hall and acquiring a deep understanding of the inner workings of the planning department. It also selected for developers who were well-capitalized to be able to handle the additional risk. The substantial lengthening of permitting times made an effective market response to changing prices next to impossible. Given 4+ years of process, the market at the time a new housing project completes could be quite different from the market when the housing project began.\nThis is not to say that market mechanisms are all that we should pay attention to, but they’re certainly key to getting more market affordability and evidence strongly suggests the Spot Discretionary Regime works no better in getting non-market housing built. This is also not to say that market mechanisms have disappeared. Indeed, they still work during downturns, when (well-capitalized) developers can abandon or delay projects working their way through the approvals timeline as economic conditions become less favourable. In effect, the Spot Discretionary Regime has broken broken supply elasticity and replaced it with a ratchet. On upswings, planners limit how much housing developers can initiate and on downswings developers withdraw from projects that look like losers. The end result: supply hardly ever catches up to demand. And housing keeps getting more expensive.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2023-02-07 09:46:33 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [8a5d962] 2023-02-07: link to nathan\n## R version 4.2.2 (2022-10-31)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Ventura 13.2\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] VancouvR_0.1.7            mountainmathHelpers_0.1.4\n##  [3] sf_1.0-9                  cancensus_0.5.5          \n##  [5] cmhc_0.2.4                forcats_1.0.0            \n##  [7] stringr_1.5.0             dplyr_1.1.0              \n##  [9] purrr_1.0.1               readr_2.1.3              \n## [11] tidyr_1.3.0               tibble_3.1.8             \n## [13] ggplot2_3.4.0             tidyverse_1.3.2          \n## \n## loaded via a namespace (and not attached):\n##  [1] httr_1.4.4          sass_0.4.5          jsonlite_1.8.4     \n##  [4] modelr_0.1.10       bslib_0.4.2         assertthat_0.2.1   \n##  [7] triebeard_0.3.0     urltools_1.7.3      googlesheets4_1.0.1\n## [10] cellranger_1.1.0    yaml_2.3.7          pillar_1.8.1       \n## [13] backports_1.4.1     glue_1.6.2          digest_0.6.31      \n## [16] rvest_1.0.3         colorspace_2.1-0    htmltools_0.5.4    \n## [19] pkgconfig_2.0.3     broom_1.0.3         haven_2.5.1        \n## [22] bookdown_0.32       scales_1.2.1        tzdb_0.3.0         \n## [25] git2r_0.31.0        timechange_0.2.0    proxy_0.4-27       \n## [28] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n## [31] cachem_1.0.6        withr_2.5.0         cli_3.6.0          \n## [34] magrittr_2.0.3      crayon_1.5.2        readxl_1.4.1       \n## [37] evaluate_0.20       fs_1.6.0            fansi_1.0.4        \n## [40] xml2_1.3.3          class_7.3-21        blogdown_1.16      \n## [43] tools_4.2.2         hms_1.1.2           gargle_1.3.0       \n## [46] lifecycle_1.0.3     munsell_0.5.0       reprex_2.0.2       \n## [49] compiler_4.2.2      jquerylib_0.1.4     e1071_1.7-13       \n## [52] rlang_1.0.6         classInt_0.4-8      units_0.8-1        \n## [55] grid_4.2.2          rstudioapi_0.14     rmarkdown_2.20     \n## [58] gtable_0.3.1        DBI_1.1.3           R6_2.5.1           \n## [61] lubridate_1.9.1     knitr_1.42          fastmap_1.1.0      \n## [64] utf8_1.2.3          KernSmooth_2.23-20  stringi_1.7.12     \n## [67] Rcpp_1.0.10         vctrs_0.5.2         dbplyr_2.3.0       \n## [70] tidyselect_1.2.0    xfun_0.37"
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html",
    "title": "New Premier New Housing Policy",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nIn this post, we take a moment to appreciate the first housing policy announcements from BC’s new Premier, offered up just days into his term. David Eby comes to the post fresh from his joint roles as Attorney General and Minister Responsible for Housing. In these roles, he was central to fashioning the teeth behind BC’s housing policy. Initially these teeth were directed at the private sector, with a special focus on rooting out the “toxic demand” thought to be leaving too many dwellings empty. The Speculation and Vacancy Tax (SVT) remains the most visible legacy of this approach, as we’ve written about before and we provide a brief update of SVT results based upon the latest release in our appendix below. But other legacies include things like BC’s beneficial ownership registry, insuring better transparency for private corporations.\nIt hasn’t all been teeth, though. Under the NDP, BC has also invested more heavily in social and affordable housing, with the party more than doubling the budget of BC Housing as public agency since taking over in 2017. These investments in social and affordable housing have directly contributed to improving peoples’ lives and we’d join those arguing that we’re still not building enough. But even here, the teeth came out when it came to the corporate side, largely directed at BC Housing’s operation as a Crown Corporation, as in this past July’s snap announcement that a new board would be taking over.\nThis week’s announcement doubles down on teeth. But Eby’s found a new target in municipal and strata corporations (i.e. condominium).\nThese two kinds of corporations are, in many respects, more similar than not. Both afford expansive local powers over land, development, property use, and servicing. Both are established via legislation as “creatures of provincial governments” (as Eby argued directly in a recent case in otherwise unlikely support of Doug Ford’s powers in Ontario). But strata corporations are generally nested within municipal corporations, and a key difference between the two has been in governance structure and exclusions carved out from Human Rights Codes. In particular, renters don’t get to vote in strata, unlike in municipalities. Renters could also be excluded by strata corporations entirely (via rental restriction bylaws). And strata could also put in place age restrictions on residents (unlike cities). As for municipal corporations, their means of exclusion remain more limited than strata corporations, though the scale of their exclusion has wider reach. Municipalities exclude mostly by preventing new housing from being built.\nAnd here’s where the teeth come in. New amendments to the Strata Act rip away existing strata corporation rights to exclude renters and discriminate by age (excepting for explicit retirement communities). Separately, the province is nipping away at municipal corporations’ independence in precisely those places where it’s being used to deny housing.\nSo can teeth grow housing?\nWe’re pretty sure that the answer is YES!\nBut it takes care and skill, because teeth only work to grow housing if they’re deployed against the right targets with the right pressure. Think of a well trained sheepdog nipping its flock into shape. Let’s take a closer look at what the new announcements portend in this regard, splitting out the strata corporation and municipal corporation parts separately."
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#strata-rental-restrictions",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#strata-rental-restrictions",
    "title": "New Premier New Housing Policy",
    "section": "Strata rental restrictions",
    "text": "Strata rental restrictions\nRemoving the option for rental restrictions from strata legislation is not aimed at growing new housing so much as adding currently (mostly) empty housing back to the market for habitation. In this way it’s most similar to the Speculation and Vacancy Tax, which aimed at the same target. Of note: rental restriction exemptions from the SVT also no longer apply as of 2022. This raises three questions.\n\nHow many properties are affected by this legislative change?\nHow many properties can we expect to get added to the occupied housing stock?\nWould these properties have ended up there anyway?\n\nIn answer to the first question, the premier cited a number of 300k strata units with rental restrictions that this law could apply to, but this is likely a substantial over-estimate. It appears that this is simply the number of strata units with plans registered before January 1st 2010. That’s when the law changed so developers could exempt new strata from rental restrictions over the long term regardless of whether strata councils attempted to pass such restrictions. Many developers did so, reducing the prevalence of rental restrictions past this point, but not (contra the impression from the provincial press conference), banning such restrictions. On the flipside, there are also many older strata corporations that don’t have rental restrictions, and for those that do there are a broad range of models for how they apply and how binding they are. Some strata corporations ban all rentals, others allow only a limited number or share of properties to be rented out, some allow up to half of strata lots being rented. The legislation will impact strata corporations with different types of rental restrictions to a different degree.\nThis makes it difficult to give a precise answer to the first question. But the second one might be a bit easier.\nAt least for the regions covered by the SVT we know how many strata units have been claiming the exemption from the tax due to strata rental restrictions, giving a proxy estimate of how many units are being held off the market because of these rules. It’s only a proxy insofar as claimed exemptions don’t necessarily reflect the underlying logic or situations of landlords; often it just comes down to which exemption is easiest to claim. In the press release and conference the premier noted that approximately 2,900 strata units claimed the rental restriction exemption in the SVT regions in 2021, the last year in which it was available. This appears to roughly match SVT technical report counts, except that there exemptions are reported in terms of the number of owners claiming the exemption rather than properties. Latest revised counts for 2018, 2019, 2020, and 2021 came in at 5,591, 5,194, 4,661, and 4.081 owners, respectively, showing a gradual decline in those taking the rental restriction exemption since the SVT began. It’s quite reasonable that the most recent figure equates to 2,900 strata units, as reported by Eby, if enough owners laid claim to the same properties.\nDrawing upon the latest report (just released after Eby’s press conference!) we can also check how rental restriction exemptions have been distributed spatially. Again, we draw upon owners claiming rental restrictions rather than strata units.\n\nHere we see most reporting municipalities have already witnessed a decline in units claiming the rental restriction exemption. But this data only covers the SVT regions. Of course, this is also where most strata condominium properties are located. We can use 2021 census data to estimate that roughly 87% of BC condo units are in the SVT area. It would be better to use BC Assessment data for estimates like this, but unfortunately our province keeps BC housing data locked up in order to be able to sell it (we’ll return to our pitch for more open data below). Overall we would expect somewhere between 3,000 and 4,000 rental units might get freed up were rental restricted units returned to the market. Probably closer to 3,000. That’s equivalent to a one-time supply shock of less than 10% of annual completions. Not nothing, and every bit helps, but on it’s own this will do little to get us to where we need to be with CMHC analysis suggesting we need to more than double housing completions over the next ten years to catch up on our housing deficit.\nBut were changes to the Strata Act necessary to bring these units back to the market, or would they have gone there anyway given the removal of the rental restriction exemption to the SVT in effect as of this year? This remains a good question. Certainly the removal of the rental restriction exemption within the SVT and the removal of strata rights to restrict rents can be expected to work together more smoothly now. Applying teeth to strata complement the teeth already applied to property owners, providing the latter with new options to rent out their units rather than paying the tax instead of just selling or moving in themselves. This makes sense from a policy perspective, lessening the impact of the SVT change on those most affected. But it’s hard to separate out how many units are likely to become newly occupied from the Strata Act change from how many units were already likely to shift into occupation due to the SVT change.\nEven if it doesn’t do a lot for supply, lifting rental restrictions in strata can still help a lot of renters, especially those caught between strata rules and landlord attempts to evade them. This should lead to greater renter security and (hopefully) less renter stigma, and we applaud Eby for this move, bringing BC’s policies more in line with places like Australia’s, where rental restrictions have long been disallowed within strata (more on comparative strata law here). Yet renters in condominium are still likely to remain second class citizens. They still won’t get to vote or have much of a say within the “fourth level of government” administered by the strata corporation."
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#age-restrictions",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#age-restrictions",
    "title": "New Premier New Housing Policy",
    "section": "Age restrictions",
    "text": "Age restrictions\nThe other part of the legislative changes addresses age restrictions. This probably won’t change the total number of units available, but it does address the question of how these units can be occupied. As our past research demonstrates via audit study, discrimination against children is a regular feature of the rental market. Mostly this is illegal, running against BC’s Human Rights Code. But as Doug Harris explained in a recent talk, BC is the only province in Canada enabling age restrictions in strata other than for senior housing, and there are questions around how this fits in with broader anti-discrimination rules in Canada. This was not addressed in the last overhaul of the condominium law which BC used to further clarify exactly how condo can discriminate against children and young adults, but it’s good to see the teeth of the province finally cutting away the ability for strata to engage in these discriminatory practices."
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#nipping-municipalities-into-shape",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#nipping-municipalities-into-shape",
    "title": "New Premier New Housing Policy",
    "section": "Nipping Municipalities into Shape",
    "text": "Nipping Municipalities into Shape\nWhat about municipalities? Here’s where we see the most potential for teeth to grow housing.\nAs our work on metropolitan zoning practices has amply demonstrated, municipalities have used the powers given to them by the province to tightly regulate the addition of new housing. Through rules attached to lots, municipalities narrowly circumscribe the locations in which new multi-unit housing can be added and prevent the subdivision of lots enabling more single-family housing. As a result, in many of our municipalities it often takes years of negotiations and approval processes to add new housing. Overall this makes it more expensive to construct housing and reduces the amount that gets built. Moreover, municipalities don’t seem to be fully aware of their role in our provincial housing shortage.\nAs it stands, the province has not yet committed to reforming municipal powers. Instead in 2019 the province began requiring municipalities to author Housing Needs reports, estimating how much new housing of what type was required to meet projected future needs. But the province did not require municipalities to adhere to common estimation standards or pre-zone for this new housing. Enter the newly announced “Housing Supply Act,” adding teeth to both Housing Needs reporting and its implications. Assuming it is enacted, the province will begin the process of auditing needs reports (choosing 8-10 recalcitrant municipalities) and working directly with municipal governments to insure the housing needed gets built, including by order-in-council, overriding municipal exclusion powers where necessary.\nIs this all bark and no bite? So far, yes. BUT… there’s a promise of a bite to come, and that might be enough for a skilled sheepdog to bring our flock of wayward municipalities to order. In the short term, there’s a hope that municipalities will begin planning for and approving more housing so as to avoid making “the list” of 8-10 municipalities at risk of having their control over development stripped away by the province. No one wants to be the example made to others. We see this as all to the good.\nIn the long term, as the province chooses its examples it will become forced to work through its own definitions of how to understand housing need and translate that need into development objectives. This is going to be hard and risky work. Even by itself, estimating housing need is really hard, as explained in a good piece in the Atlantic the other day. We have explained the complications associated with modelling housing demand in detail before, and highlighted how misspecifications of housing demand models by regional planners have systematically enabled the buildup of our housing shortage. We’ve also made a start at estimating aspects of housing need, like suppressed household formation. CMHC has also done some work on this front by providing estimates of the provincial supply gap and projecting what needs to be done to close our current gaps by 2030. But housing is inherently local, and work is underway to provide metro level supply gap estimates.\nThe province has indicated that they are looking for municipal level targets as it is municipalities that engage in the rationing of housing. But the province has not explained how these municipal level targets are linked. Right now municipalities create housing needs reports in isolation, but that’s not how housing works. When only modelling individual municipalities, the province won’t be able to distinguish if e.g. housing demand in Lion’s Bay is high because people want to live in that location, or if that’s only the fallback choice because West Vancouver failed to build housing. Going either by methodology prescribed by the province for their current municipal housing needs reports, or the news reporting on the proposed evaluation of municipal targets it looks like the province still has a ways to go in understanding how to model demand for housing. The current methodology does not adequately deal with suppressed household formation or changed in and out migration due to housing pressures, and they fail to recognize the role of rising incomes on housing consumption. As such they are bound to systematically under-estimate housing demand."
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#alternatives-to-rationing-housing",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#alternatives-to-rationing-housing",
    "title": "New Premier New Housing Policy",
    "section": "Alternatives to rationing housing",
    "text": "Alternatives to rationing housing\nIt’s important to note that the provincial approach is still grounded in our current planning regime built around rationing housing at the municipal level. Even the new provincial teeth are only nipping at the edges here, intervening via housing targets to prevent excessive rationing but not otherwise yet departing from the shape of existing practice. Policy is path-dependent, and given where we are this may be a defensible approach. But it’s worth pointing out alternative approaches. One approach would be to require municipalities to provide ample outright zoning to allow for organic housing growth. This could also be combined with overriding restrictive zoning, especially near central areas with provincially funded transit infrastructure. Effectively these kind of moves would follow the New Zealand approach, which is showing very encouraging first results both in terms of increase in housing production as well as effects on rents.\nIn his leadership race Eby did suggest to allow at a minimum for three (strata) units outright on every lot in “urban centres” in BC. There has been no policy brought forward to implement this (yet), but this would be a small step toward breaking out of the rationing regimen and allowing housing to grow via outright zoning. This falls short of what New Zealand implemented but does follow their direction.\nIn many urban municipalities enabling three units per lot outright would constitute a significant bump in allowable density, although cities could easily make it unviable through parking requirements or other zoning constraints. The City of Vancouver already technically allows three (or four) units per lot, but Eby’s proposal to allow strata dramatically changes how these properties could get used. To realize this density currently the City of Vancouver is asking an owner-investor family to take on a larger mortgage than what’s required for their family and also take on two rental properties on their lot and operate them as landlords. This incentivizes high debt levels and coerces people into becoming landlords. Over the years as the family pays down their mortgage and their income rises they often choose to stop renting out their suite, leading to suites being the most empty form of housing in Vancouver. The proposal by Eby would change this by allowing to stratify the property, with the family buying as much housing as they need and taking on a lower mortgage instead of relying on a secondary suite or laneway house as a “mortgage helper”. After all, the best mortgage helper is a lower mortgage. Moreover, the other units would now be separate taxable dwelling units, and thus subject to other regulations like the Empty Homes Tax and the Speculation and Vacancy Tax, and there are currently a lot more unoccupied suites than there are strata properties that sit empty because of rental restrictions.\nWe hope the proposal on allowing 3 units outright will come, and that it gets expanded on. In central cities like Vancouver we have already been discussing allowing 6 units per lot, with some parties campaigning on allowing lowrise apartments everywhere, which is more in line with the demand for housing in amenity and jobs rich Vancouver. New Zealand and California have tied some of their rules to their frequent transit network, some years back we have looked at where development within the frequent transit network should go if minimizing renter displacement were a policy objective (it should be!). Tackling BC’s housing shortage requires ambitious action. Our new Premier is off to a good start, but we hope he doesn’t stop now. And we’ll keep pointing toward New Zealand as a good place to go for policy innovation!"
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#less-toothy-housing-policy",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#less-toothy-housing-policy",
    "title": "New Premier New Housing Policy",
    "section": "Less Toothy Housing Policy",
    "text": "Less Toothy Housing Policy\nAll and all, we’re happy to see our new premier adding more teeth to BC’s housing policies, especially insofar as those teeth keep nipping away at the exclusionary powers of strata corporations and municipal corporations. We’d also like more non-toothy policies, including the long promised Renter’s Rebate and more support for building out the non-profit sector (which may be coming). When asked Eby did indicate that he had not forgotten about the renter’s rebate, but there is no timeline for when it will arrive. As a reminder, this was initially envisioned to equalize the homeowner grant that most homeowners enjoy, and provide a small step toward tenure neutrality and undoing the enormous imbalance in benefits higher levels of government bestow upon owners, most notably non-taxation of imputed rent and non-taxation of principal residence capital gains. But if designed right so it also collects information about rental accommodation and rents it could also provide an invaluable source of information on the rental market, as well as an enforcement mechanism (teeth!) for the Residential Tenancy Act in the hard to regulate secondary rental market, which we know is the segment where most evictions in Canada occur. As for an increased build-out of non-profit housing, it may also have to be accompanied by provincial overrides of municipal powers to block housing. So we’ll need the teeth even here."
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#a-new-ministry-of-housing",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#a-new-ministry-of-housing",
    "title": "New Premier New Housing Policy",
    "section": "A new Ministry of Housing",
    "text": "A new Ministry of Housing\nAnother announcement was the creation of a dedicated standalone ministry of housing, a role that previously was passed around as something closer to a side-job. We believe this is a positive development, housing is important to the wellbeing of the people in British Columbia, as well as the overall economy. And housing is complex, having a dedicated minister and staff should help create the expertise and knowledge within the government needed to insure it’s forming good policy and promoting that policy in an accurate manner to the public at large.\nHere we definitely see room for improvement. Not just when it comes to some of the issues already outlined above. It’s important for ministers to get simple things right, like the the unfortunate claim that “BC has more renters per capita than any part of Canada.” Which is obviously (to anyone who spent any time looking at housing across Canada) false no matter how one slices the data. Here is a simple graph of the share of the population in renter households across Canadian provinces and territories. \nAt this point we want to reiterate that we view data transparency and open data for housing (and other data) as extremely important. Current BC policies, while in principle making data available for researchers, impose significant friction that slow down and stymie housing research, and selling the data to private interests creates an information asymmetry with respect to the public as well as municipalities and governments without the same resources for utilizing the data. That information asymmetry erodes trust and is potentially harmful. We have made this point repeatedly on these pages, and have not lost hope that the BC government will change its mind on the role of housing data, especially during a housing crisis. Another item that a minister of housing might be well-positioned to finally address working with a supportive Premier!"
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#takeaway",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#takeaway",
    "title": "New Premier New Housing Policy",
    "section": "Takeaway",
    "text": "Takeaway\nWe’re happy to see our new Premier add new teeth to housing policy in the service of better supporting and de-stigmatizing renters within strata and bringing municipal corporations on side with getting more housing built. The province is well-positioned to shepherd its “creatures” - BC’s flock of municipal corporations - to where they need to be to get a lot more housing built. The flurry of changes that have been announces are generally good, but could use more work on the details. And we have high hopes we’ll also see more non-toothy announcements soon, including further direct investments in social and affordable housing.\nThanks for sticking with us. To reward you, here’s a video of some New Zealand sheepdogs at work.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt. This includes code for scraping SVT data out of their PDF reports which is necessary only because of BC broken data practices that default to not making data available but hiding it away in reports, if at all. In a similar vein, this post can’t provide more accurate estimates on strata properties with various forms of rental restrictions because this in part requires access to BC Assessment data which the BC government chooses to continue to keep locked up in order to sell it. (The other important ingredient for these estimates is building on work done by Doug Harris who has researched strata restrictions in BC in great detail.)\n\n\nReproducibility receipt\n\n## [1] \"2022-12-17 10:53:41 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [281f4e5] 2022-11-24: new-premier-new-housing-policy post\n## R version 4.2.2 (2022-10-31)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Ventura 13.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cmhc_0.2.4                mountainmathHelpers_0.1.4\n##  [3] cancensus_0.5.5           cansim_0.3.14            \n##  [5] forcats_0.5.1             stringr_1.4.1            \n##  [7] dplyr_1.0.10              purrr_0.3.5              \n##  [9] readr_2.1.3               tidyr_1.2.1              \n## [11] tibble_3.1.8              ggplot2_3.4.0            \n## [13] tidyverse_1.3.2          \n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.8.0     assertthat_0.2.1    digest_0.6.30      \n##  [4] utf8_1.2.2          R6_2.5.1            cellranger_1.1.0   \n##  [7] backports_1.4.1     reprex_2.0.1        evaluate_0.18      \n## [10] httr_1.4.4          blogdown_1.10       pillar_1.8.1       \n## [13] rlang_1.0.6         googlesheets4_1.0.0 readxl_1.4.0       \n## [16] rstudioapi_0.14     jquerylib_0.1.4     rmarkdown_2.14     \n## [19] googledrive_2.0.0   munsell_0.5.0       broom_1.0.0        \n## [22] compiler_4.2.2      modelr_0.1.8        xfun_0.34          \n## [25] pkgconfig_2.0.3     htmltools_0.5.3     tidyselect_1.2.0   \n## [28] bookdown_0.27       fansi_1.0.3         crayon_1.5.2       \n## [31] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n## [34] grid_4.2.2          jsonlite_1.8.4      gtable_0.3.1       \n## [37] lifecycle_1.0.3     DBI_1.1.3           git2r_0.30.1       \n## [40] magrittr_2.0.3      scales_1.2.1        cli_3.4.1          \n## [43] stringi_1.7.8       cachem_1.0.6        fs_1.5.2           \n## [46] xml2_1.3.3          bslib_0.4.0         ellipsis_0.3.2     \n## [49] generics_0.1.3      vctrs_0.5.1         tools_4.2.2        \n## [52] glue_1.6.2          hms_1.1.2           fastmap_1.1.0      \n## [55] yaml_2.3.6          colorspace_2.0-3    gargle_1.2.0       \n## [58] rvest_1.0.3         knitr_1.40          haven_2.5.0        \n## [61] sass_0.4.2"
  },
  {
    "objectID": "posts/2022-11-24-new-premier-new-housing-policy/index.html#appendix",
    "href": "posts/2022-11-24-new-premier-new-housing-policy/index.html#appendix",
    "title": "New Premier New Housing Policy",
    "section": "APPENDIX",
    "text": "APPENDIX\nAs a final note, we provide a quick update of Speculation and Vacancy Tax results based upon the newest report. Overall little has changed since our last look at three years of data, but more data, more up-to-date, is always better! So here’s a quick summary of where we’re at. The proportion of properties paying the tax continues to decrease, and is now well below 1% across most municipalities. But it was never very high to begin with."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html",
    "title": "Where did all the cheap rents go?",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nIt can be really useful to count things, but sometimes numbers end up causing confusion and misunderstanding rather than helping. Often this has to do with how the number is presented and attached to claims. Other times it has to do with problematic procedures used to obtain the number. Here we want to explore these problems more in detail concerning a claim that “Canada lost 322,000 affordable homes” between 2011 and 2016. This stat is generally made in reference to “private” rentals, and is contrasted to the number of non-market units built between 2011 and 2016, pegged at 60k units.\nAt its best, the claim has been used to advocate for more non-profit housing. This is a great thing to advocate for, and something we support. At its worst, the claim has been made to look like a popular NIMBY talking point: that development of new housing, including non-profit, is directly resulting in the loss of older more affordable housing stock, resulting in worse overall affordability. While this can occur in specific cases (especially when we only allow new apartment buildings to be developed on top of old apartment buildings), as a general claim it’s wrong and misleading on multiple levels. So let’s walk through where the number is coming from and how it’s being presented.\nThe headline number is based on an estimation by Steve Pomeroy:\nWe will temporarily set aside the problematic term “naturally occurring affordable housing,” except to link to this critical history of the term and note that left to nature, housing invariably decays (something that can also be observed in parts of Vancouver where zoning bylaws force unproductive land use). Let’s dive into the number. In the report referenced, this stat is only broadly sourced to the 2016 Census and 2011 NHS with no indication how it was estimated. But this document outlines one way that does line up with the headline number. It takes a data table from the 2016 census that enumerates non-subsidized rental households paying below $750 in rent and utilities, and a data table from the 2011 NHS with slightly different cutoffs where the $750 cutoff was estimated by linear interpolation between the rent cutoffs in the table.\nUsing linear interpolation assumes that shelter costs are uniformly distributed in the $600-$800 range, which is tenuous at best. Shelter costs will skew high within this band, which would under-estimate the “loss”. At the same time one has to ask why nominal shelter cost levels should be used for this instead of adjusting for inflation or income growth.\nSetting these questions aside, a quick cross-check with this method shows a net loss of 322,700 households in non-subsidized rental housing paying $750 or less on rent and utilities, matching the number cited in the research report (assuming the report truncated instead of rounded to the nearest 100). We will assume that this is how the number was constructed, as the estimated difference between 2011 to 2016 in households not living in subsidized housing or receiving rent subsidies paying at most $750 in rent and utilities. (If one were to adjust for inflation, using $701 as the starting rents in 2011, then the loss would amount to 103,000 households, adjusting in the other direction using $802 in 2016 this method comes out with a gain of 654,100 units, which is absurd and highlights the issue of using linear interpolation, in this case to estimate households below the $802 cutoff in the $750-$1000 band.)\nNow with the newest 2021 census data, we don’t need to interpolate using standard tables and can read off that between 2016 and 2021 another 230,500 fewer non-subsidized renter households were paying less than $750 in rent and utilities. So we can extend the claim forward. We can also push the claim back to 1996, although at the cost of grouping subsidized and non-subsidized rental housing and losing households with zero income in 2006 and earlier, to see a longer timeline of the number of units renting below $750.\nIn short, the number of tenant households in subsidized or market rental & paying rents below $750 has been declining for quite some time now."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#how-should-we-interpret-this-320k-statistic",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#how-should-we-interpret-this-320k-statistic",
    "title": "Where did all the cheap rents go?",
    "section": "How should we interpret this 320k statistic?",
    "text": "How should we interpret this 320k statistic?\nThe research note that introduced this loss of “322,600 units” interprets it in the following way:\n\nThese losses are driven chiefly by the financialization of rental housing – an asset class attracting investment from both large capital funds, as well as smaller investors, both seeking to capitalize on dramatically rising rents). A further contributor is the intensification and redevelopment of sites with older low-moderate rent properties.\n\nUnfortunately no argument is made to fully define or support these claims. And they’re pretty big claims involving a variety of potential processes and lots of room for confusion. Perhaps the biggest generator of confusion is that rents in Canada are generally not tied to units. Instead they’re mostly tied to rental contracts between landlords and tenant households. Here these rents are reported by tenant households. Rental contracts can provide stability and protect tenants from rent increases - especially in places like BC that have rent control. But they disappear when tenants move, resulting in turnover for units and new contracts offered to new tenants. And tenants move a lot.\nOverall, the decline in households reporting rents below $750 generally doesn’t reflect a loss of units, but rather turnover in rental contracts. Labeling our number properly as tied to tenancies, as we attempt to do above, may help ward off a lot of confusion. When tenants move out, landlords tend to set new rents for their units. In a remarkably predictable dynamic, these rents go up when vacancy rates are low, and go down when vacancy rates are high. Landlords setting higher rents on units that have turned over in conditions of scarcity probably explains most of the decline in rental contracts paying below $750 in rent. You could term this process “financialization” if you want to, but it’s not clear how it helps explain the underlying dynamic. “Filtering” is likely a better name for the process, reflecting how units tend to “filter down,” becoming cheaper as they age in places where housing is abundant, but can instead “filter up,” becoming more expensive, in places where housing is scarce.\nBut let’s get back to the number."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#affordability",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#affordability",
    "title": "Where did all the cheap rents go?",
    "section": "Affordability",
    "text": "Affordability\nThe stated motivation for this statistic is to understand “naturally occurring affordable housing”, and the $750 shelter cost threshold was chosen because this level is “affordable to households earning less than $30,000 per year.” This relies on the standard affordability criterion of spending no more than 30% of total income on shelter costs. The change in the number of households paying less than $750 on rent and utilities is one side of the affordability equation, the change in the humber of households earning less than $30k is the other. We could reasonably improve housing affordability either by working to keep rents low or by providing better income supports to people. So far we’ve only looked at rents, so let’s take a peek at households below $30,000 in income. In 2016 there were 128,500 fewer households in this income band than in 2011 living in non-subsidized housing. And by 2021 there were another 363,600 fewer households in that income range.\nAgain, if we look at all renter households, including those in subsidized housing, we can extend that timeframe back to 1996 (again dropping households with zero income in 2006 or earlier) and tally up how many households had income below $30k in the year preceding each census year.\n\nThe bump between 1996 and 2001 likely reflects simple household growth through a time period when $30,000 was still a decent income, but it could also result in part from a methods change in how households and incomes were counted. Regardless, since 2001 there’s been a strong decline in tenant households reporting less than $30,000 in income. It doesn’t fully match the more steady decline in rents less than $750, and may also be complicated by recent pandemic-related income supports, which together with methods changes make income harder to track than rents, but we can see real change. In other words, the decline in households paying low rents (&lt;$750) roughly tracked the decline in households with low incomes (&lt;$30,000). Nevertheless, there remain more of the latter than the former, suggesting real rent burdens.\nRather than examining changing rents and incomes in isolation, another approach is to look directly at rent burdens, i.e. what proportion of income is going toward rent for households with income below $30,000. Here we’ll only look at those in non-subsidized housing. We’ll also keep in mind that there’s an important lag between reporting of incomes (mostly drawn from tax records for the year prior to the Census) and rents (estimated as contemporaneous with the Census). As a result, some situations where it looks like people are paying a very high proportion of their incomes on rent may arise simply because they just graduated or moved to Canada and started a new job (with income not yet recorded) at the same time as they moved into a new place (where we get their new rent). We’ll come back to this point. But for now, keeping this caveat in mind, let’s take a look at the data!\n\nOverall, the most common situation for low-income households without subsidies is to be paying 30%-50% of incomes on rent. This doesn’t leave a lot of room for other expenses. On the bright side, we can see the decline in unsubsidized households with incomes below $30k, and we can also see that the low-income households remaining in 2021 are less and less likely to be paying more than half their income in rent, continuing a pattern of decline observable since 2011. So from that perspective, that’s an improvement. But we shouldn’t overstate it. Overall, looked at distributionally, little has changed concerning the rent burden for those households making under $30,000.\n\nThe relative lack of movement in this metric fits pretty well with the observable decline in both the number of households paying less than $750 in rent and the number of households making less than $30,000 in annual income. We’d like to see more improvement, but it’s not clear that things are getting noticeably worse."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#the-geography-of-cheap-rent",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#the-geography-of-cheap-rent",
    "title": "Where did all the cheap rents go?",
    "section": "The Geography of Cheap Rent",
    "text": "The Geography of Cheap Rent\nLet’s take a look at the geography of cheap rent. Where are the tenant households with shelter cost below $750, and where are we seeing the most losses of these kinds of rental contracts? We can pull this data from the cross tabulation we have used above. To reduce clutter we will break out the 20 biggest Metro Areas (CMAs) in Canada, and lump everything else together.\n\nWe see that cheap rent is most common outside of Metropolitan Canada (as represented by the top 20 CMAs), though Montréal also still retains a lot of tenant households paying less than $750 in rent. Correspondingly, Montréal and more rural parts of Canada are also where we’ve seen the biggest recent declines in low-rent households. Strange, then, that the focus on reporting about these losses often shifts to Vancouver or Toronto, where there are very few such rental contracts remaining. This speaks to a broader problem with the potential for any headline number for Canada to mislead insofar as it combines data from very different housing markets across the country.\nWe can look at this more directly by showing the loss of renter households with shelter cost less that $750 in each region and inter-census period.\n\nThis again highlights that most of the losses were in Montréal and regions outside of the main metro areas. The number of households with shelter cost below $750 in Calgary and Edmonton increasing 2016-2021 shows that this isn’t necessarily a one-way street. Units renting below $750 at any given point in time aren’t separate from the rest of the housing market, and their asking rents can go up or down depending upon what’s going on elsewhere in the market."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#further-exploration-into-low-rent",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#further-exploration-into-low-rent",
    "title": "Where did all the cheap rents go?",
    "section": "Further exploration into Low Rent",
    "text": "Further exploration into Low Rent\nLet’s dig a little deeper, drawing upon public use micro file (PUMF) data. Unfortunately this hasn’t been released yet for the 2021 Census, so we’ll only be able to look at 2011 and 2016 data, coming from the long-form NHS and Census samples. For 2016, these samples come in both individual and hierarchical form (including complete households), and we’ll include both here for reference. Using PUMF data adds uncertainty due to lower sample size, but adds much greater flexibility. First let’s look at a simple distribution of what rents those paying less than $750 a month are paying for each sample.\n\nWe can see that most rents are (perhaps not surprisingly) at the upper end of the distribution, with $700 a month being the most common. This suggests that there’s nothing intrinsically separating market housing renting under $750 from the rest of the housing market. There’s nothing “natural” about its affordability except that its on the low end of the overall distribution of rents. As the overall distribution shifts rightward, its affordable tail at the left end gets smaller and smaller.\nOther noticeable issues include the difference in results when using the individuals vs the hierarchical PUMF data, with only minimal overlap in the confidence intervals. This is a good reminder of the inherent difficulties when working with PUMF data. Another interesting observation is that renters renting below $300 have increased 2011-2016. These are likely non-arms length rentals, which we’ve yet to discuss another issue complicating analysis. Not all of these “market rentals” actually represent market processes.\nLet’s use our flexibility to break down tenancies with rents below $750 by age of primary household maintainer.\n\nThe zigzag (or double hump) pattern here is fascinating, with concentrations in both the early Millennial (born 1986-1991) and late Boomer (born 1956-1966). This may reflect simple demographics (these were larger cohorts than the intervening Gen Xers), but may also be picking up on other patterns, with variation in both quality and geographic distribution of low rent tenancies by age (keeping in mind that most of these renters are in more rural parts of Canada and bigger metros in Quebec). We’ll circle back to look at these patterns in a moment, but first let’s take a look at what was renting for less than $750 in 2016 by number of bedrooms.\n\nNo surprise, mostly 1BR and 2BR dwellings. We can also look at the type of dwelling households paying less than $750 in rent tend to live in.\n\nThe vast majority of low rent situations seem to involve non-condo apartments, representing both purpose-built rental buildings and secondary suites in homes. Once again, it’s worth keeping in mind that these are mostly showing up in Quebec and more rural parts of Canada.\nChecking by period of construction of the building we see that most of these households live in buildings constructed between 1946 and 1980 or 1990.\n\nMost low rent situations involve older buildings. But this age distribution also generally fits the age-distribution of our purpose-built rental stock in Canada, largely constructed during a mid-Twentieth Century build-out prior to the rise of the condominium (where we see far fewer being rented out at below $750). Aging stock is also more likely to be in poor condition, drawing a clear connection between rents and quality of housing. A fair amount of preventive maintenance is needed to keep old buildings habitable, and the more they fall into disrepair, the cheaper their rents tend to go relative to surrounding alternatives. There’s nothing “natural” about keeping older housing inhabitable."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#time-lag-bias-in-shelter-cost-to-income-ratios",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#time-lag-bias-in-shelter-cost-to-income-ratios",
    "title": "Where did all the cheap rents go?",
    "section": "Time Lag Bias in Shelter Cost to Income Ratios",
    "text": "Time Lag Bias in Shelter Cost to Income Ratios\nLet’s exploit the flexibility of our microdata to return to our exploration of those on the other side of the low rent affordability issue: households with incomes below $30k. As we noted before, some of the particularly high ratios, where it seems like tenants are paying extraordinary portions of their incomes on rents, may be accounted for by time lags between reporting incomes and rents. This can especially be an issue for recent immigrants and new graduates who might have landed both a good job and new apartment prior to the census, but lack income records from the year prior to the census. We can help account for this by pulling out recent movers (those moving in the last year). What do their shelter cost to income ratios look like if we actually separate non-movers from recent movers?\n\nWe can see that when we separate them out, non-movers in the past year with low household incomes (&lt;$30,000) reported for the year prior are mostly concentrated in the range of those paying between 30% to 50% of their income on rent. By contrast, the highest category for movers with low incomes is those paying 100% or more of their income on rent. This likely reflects the lag effect, where movers current incomes (at least within Canada) are likely much higher when they fill out Census forms about their rents, than in the year prior, when the Census attempts to assign their income.\nComputing the ratio of households that moved in the past year for each shelter cost to income band makes this relationship even more clear. The time lag between reported incomes and reported rents likely produces a real upward bias in shelter cost to income ratios. We can partly account for this bias by separating out mobility, but it’s imperfect, and analysts drawing upon this metric - including for the construction of Core Housing Needs measures! - should really keep this bias in mind."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#rents-in-the-primary-rental-market",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#rents-in-the-primary-rental-market",
    "title": "Where did all the cheap rents go?",
    "section": "Rents in the Primary Rental Market",
    "text": "Rents in the Primary Rental Market\nLet’s draw in one more source of data to keep triangulating our exploration of low rents. In particular, let’s look at the primary rental market. This gets surveyed annually by CMHC through their Rental Market Survey (RMS) and looking at rent bands by quartile gives some indication how this market has changed. Deeper access to CMHC data could answer the question how rent has changed in individual buildings, and how many of these have gotten torn down or received deep renovations. But publicly available data can give a broad overview.\n\nOur broadest overview of the primary rental market shows that the percentile bands of rent levels have continuously increased between 2006 and 2021. If we assume that the rents are evenly distributed between these 25th and 50th percentile we can estimate what share of the rental stock rented below $750 in 2011 and 2016, and folding in data on the total stock of purpose built rental housing we can estimate the loss of such units.\nThis method gives a rough estimate of 125,500 fewer primary market rental units in 2016 renting below $750 compared to 2011. This is based on rent, which may include some utilities, but generally not all of them, so it’s a slightly different estimate than what we used above.\nIn the purpose-built market the RMS estimated a total of around 842,864 units renting below $750 in 2011, compared to the census estimate of 1,408,306 households with shelter cost below $750 in 2011. As we noted those two aren’t directly comparable because the RMS rent does not include all utilities, but also because the RMS coverage has gaps outside of census metropolitan areas. But it’s probably fair to say that somewhere between half to maybe 60% of all households with shelter cost below $750 were renting in the primary market. Adding to that that some of the households with shelter cost below $750 were in non-arms length rentals, understanding what happened in the primary market can probably provide good context.\nAs noted above, housing markets are decidedly regional, so it might be useful to see how this pans out across different metropolitan areas.\n\nMatching our census-based analysis, the RMS data shows that Montréal has a high share of purpose-built rental units renting below $750 both in 2011 and 2016, whereas e.g. Toronto doesn’t. We can also see that in Winnipeg and Halifax about half of the units were renting below $750 in 2011, dropping to 25% in 2016 and likely very few left by 2021, whereas in Montréal about 75% of units were renting below $750 in 2011, with still a little over 50% renting below that cutoff in 2016 and over a quarter in 2021.\nBut how should one interpret this? That’s where things get tricky. RMS data alone does not answer why there are fewer units below this rent cutoff over the years. Or across metro areas. There are many different processes at play, from the location and state of repair/quality of rental units to the bedroom mix and size of units, and overall market conditions with scarcity disrupting filtering processes. But there’s no “natural” cut-off between apartments renting below $750 and the rest of the rental market. Instead, we can see that different price points in the distribution of rental contracts tend to move up and down together. In other words, we don’t see strong segmentation of the rental market, and instead see evidence of upward and downward filtering at work. So where there’s high demand, halting the addition of new apartments will place increased price pressure on old apartments, shifting their rents further upward.\nAnalyzing this in more detail unfortunately goes beyond the scope of this post, but we do want to quickly highlight how bedroom mix and change in overall purpose-built rental units vary across metro areas.\n\nMontreal and Québec have a relatively high share of of 2 and 3 bedroom units, which makes their high share of units renting below $750 even more impressive. They also have a high share of overall purpose-built rental units in their CMAs, and they have been adding on that over the years. Toronto and Vancouver, on the other hand, have been relatively flat in their overall stock of purpose-built rental units due to a mixture of modest additions and some loss due to conversion and demolitions.\nTo round this off we can narrow down to look specifically at the rent distribution of 2-bedroom purpose-built rental units across metro regions.\n\nOnly Montréal has a good share of their 2-bedroom purpose-built rental units renting below $750, with a little over 25% of them still renting below that cutoff in 2021. In Toronto and Vancouver 2-bedroom apartments renting below $750 have been exceedingly rare since at least 2006."
  },
  {
    "objectID": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#takeaway",
    "href": "posts/2022-09-27-where-did-all-the-cheap-rents-go/index.html#takeaway",
    "title": "Where did all the cheap rents go?",
    "section": "Takeaway",
    "text": "Takeaway\nWhat happens in the cheaper portions of the housing market is really important, demonstrating where the greatest risk of homelessness lies, and where the greatest need for non-market housing can be found. The headline number of 322,000 “lost units” renting below $750 calls useful attention to this portion of the housing market, but unfortunately it’s often misleading on multiple levels. To review just a few of the ways it can mislead:\n\nThe statistic is mostly about changes in contracts between tenants and landlords, specifying rents below $750, and is not generally otherwise tied to units of housing\nThe statistic refers to shelter cost of households, rather than referring directly to rents.\nThe $750 figure is nominal dollars, not adjusted for inflation or for growth in incomes.\nWithout consideration of incomes, the stat says little about change in affordability.\nInterpretations frequently advanced to explain the “loss” don’t hold up, there is no indication that this losses in rental contracts below $750 are driven by REITs or by demolitions of old rental buildings.\n\nThe much more immediate interpretation is that scarcity is driving up rents, leading to older units filtering up. This leads to the opposite implications of common NIMBY talking points that old housing is “naturally” affordable. It’s only affordable if enough new housing is built to soak up demand that would otherwise draw up the lower portions of the housing market into higher overall rents.\nAdditionally, using nominal rents is not very useful. We could adjust for inflation to (crudely) account for increased costs to build and operate rental buildings. Or adjust for income growth to account for people’s ability to pay. Instead of focusing on rent or shelter costs we should probably focus on affordability, as measured by shelter cost to income ratios. When looking at the impacts of “financialization” and/or REITs on the rental market we need to do better to support our arguments with data. (We will discuss this more in a separate post.)\nNon of this negates the dire need we have for more housing. The need to dramatically expand our non-market housing options was already clear in 2011, and it is still urgently needed in 2021. Acquiring old purpose-built rental housing is a fine stop-gap strategy to assist in stabilizing rents for current tenants, but by itself the strategy does little to add badly needed housing stock and often comes with outsized maintenance costs and accessibility concerns. We can and should build a lot more new non-market housing. We also need a dramatic expansion of our market housing, especially in high-demand areas, to slow or even reverse the growth in rents and prices beyond the increases in construction costs. Housing abundance, distributed both within and outside of market processes, probably offers our best shot at finding more cheap rent.\nAs usual, the code for this post is available on GitHub.\n\n\nReproducibility receipt\n\n## [1] \"2022-10-01 11:44:19 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [63c0f3b] 2022-10-01: fix authors\n## R version 4.2.1 (2022-06-23)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.6\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cancensus_0.5.3           cansim_0.3.12            \n##  [3] cmhc_0.2.3                mountainmathHelpers_0.1.4\n##  [5] statcanXtabs_0.1.2        forcats_0.5.1            \n##  [7] stringr_1.4.1             dplyr_1.0.9              \n##  [9] purrr_0.3.4               readr_2.1.2              \n## [11] tidyr_1.2.0               tibble_3.1.8             \n## [13] ggplot2_3.3.6             tidyverse_1.3.2          \n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.8.0     assertthat_0.2.1    digest_0.6.29      \n##  [4] utf8_1.2.2          R6_2.5.1            cellranger_1.1.0   \n##  [7] backports_1.4.1     reprex_2.0.1        evaluate_0.15      \n## [10] httr_1.4.4          blogdown_1.10       pillar_1.8.1       \n## [13] rlang_1.0.5         googlesheets4_1.0.0 readxl_1.4.0       \n## [16] rstudioapi_0.14     jquerylib_0.1.4     rmarkdown_2.14     \n## [19] googledrive_2.0.0   munsell_0.5.0       broom_1.0.0        \n## [22] compiler_4.2.1      modelr_0.1.8        xfun_0.33          \n## [25] pkgconfig_2.0.3     htmltools_0.5.3     tidyselect_1.1.2   \n## [28] bookdown_0.27       fansi_1.0.3         crayon_1.5.1       \n## [31] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n## [34] grid_4.2.1          jsonlite_1.8.0      gtable_0.3.1       \n## [37] lifecycle_1.0.2     DBI_1.1.3           git2r_0.30.1       \n## [40] magrittr_2.0.3      scales_1.2.1        cli_3.4.0          \n## [43] stringi_1.7.8       cachem_1.0.6        fs_1.5.2           \n## [46] xml2_1.3.3          bslib_0.4.0         ellipsis_0.3.2     \n## [49] generics_0.1.3      vctrs_0.4.1         tools_4.2.1        \n## [52] glue_1.6.2          hms_1.1.2           fastmap_1.1.0      \n## [55] yaml_2.3.5          colorspace_2.0-3    gargle_1.2.0       \n## [58] rvest_1.0.3         knitr_1.39          haven_2.5.0        \n## [61] sass_0.4.2"
  },
  {
    "objectID": "posts/2022-08-18-25-years-of-structural-change/index.html",
    "href": "posts/2022-08-18-25-years-of-structural-change/index.html",
    "title": "25 Years of Structural Change",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nHow are big Canadian Metros growing? Can we see different patterns? Here we want to provide a brief look back at the last 25 years, exploring change over time from 1996 to our most recent Census in 2021. This is also a test of R skills for one of us, who began this post as a learning exercise drawing upon Jens’ excellent CanCensus package and recent data updates.\nHere we’re simply going to compare housing stock by dwelling type in 1996 and 2021 for Canada’s big three metro areas (Toronto, Montréal, Vancouver) and Calgary (effectively tied with Ottawa & Edmonton for fourth place). What this allows for is a peek at how the housing stock of these metro areas has changed as they’ve grown. There are caveats to this quick look, of course. The two most prominent being that the definitions of housing structure and the boundaries of metro areas may have changed. We have written before about the change to Census methods and the problems this creates for analysis of housing stock (in particular we picked up a lot more secondary suites as Duplexes after 2006).\nThe Toronto, Vancouver and Calgary CMA boundaries have stayed the same across those 25 years, but the boundaries of Metro Montréal have changed significantly (more about the CMA concept here). As we will be taking differences of dwelling counts between these years it’s important to harmonize these boundaries, and we choose the 2021 CMA boundaries for that by simply intersecting 1996 CSDs with the 2021 CMA boundaries and keeping those with large overlap. That leaves us with some very minor mismatch where CSD boundaries have changed over the years which should not skew our results in meaningful ways. Keeping this in mind, let’s take a peek!\nWe can see that all four of our metro areas grew quite a bit between 1996 and 2021, adding hundreds of thousands of households. But they varied in the different kinds of dwelling type or structure new households occupied. Let’s shift our stacked bar chart so that we get percent of households occupying each type.\nHere we can more clearly see the different compositions of each Metro area. In particular, we can see that Calgary consistently has the largest proportion of households living in single-family detached houses, while Montréal remains predominantly low-rise and Toronto dominates the high-rise category. Vancouver increasingly appears somewhere between Toronto & Montréal in terms of its balance between low-rise and high-rise.\nOf particular interest, between 1996 and 2021 Vancouver surpassed both Toronto and Montréal to become the large North American metro area least dominated by single-family detached houses. But it did so in no small part by transforming (and having the Census reclassify) many of its single-family houses as duplexes, i.e. houses containing separate basement suites. As argued in Nathan’s book, this renovation of existing single-family housing stock remains the “sneakiest” way Vancouver has added to its density. But Vancouver has also continued to more visibly add low-rise and high-rise apartments.\nLet’s look more directly at how new households have been added across each of our big metros across the last quarter century. Below we simply subtract occupied housing stock in 1996 from occupied housing stock in 2021, leaving us with net change in each category.\nWe can see that Calgary has mostly been adding more single-family detached houses to accommodate its new households, continuing to sprawl outward. While Montréal also added a lot more single-family detached houses, some of this addition simply reflected its expanded boundaries, and it also added a relatively balanced mix of duplexes, low-rises, and high-rises. Toronto also sprawled further outward with many single-family detached houses, but also added a lot more row houses and high-rises. Meanwhile, Vancouver actually lost single-family detached houses. Some of these were simply re-categorized as duplexes, either rebuilt or simply recognized as such by the Census after the widespread legalization (or at least general acceptance) of secondary suites across the region. But the decline in single-family detached houses in Vancouver also reflects a very strong set of barriers against further suburban sprawl in the region. In particular, the Agricultural Land Reserve has prevented the further transformation of farmland to new subdivisions. As a result, most new growth has been spread across more dense forms of urban infill. Generally speaking, a success in terms of sustainability!\nLet’s track this a different way, by looking at the spread of built-up area within and surrounding each big CMA. This data comes from satellite imagery, coded and shared for public use. We’ve played around with it before. Below we match satellite imagery coding built-up area to 2021 CMA boundaries as determined by Statistics Canada, which is what we used for the comparative graphs above.\nWe can see larger sections of dark red, indicating recent built-up land, all around the outskirts of Toronto and Calgary. We can also see that the built up area of Toronto spills over its CMA boundaries along the shores of Lake Ontario into neighbouring Hamilton and Oshawa CMAs, while Calgary is near to spilling over its CMA boundary to the South toward Okotoks. The pattern of sprawling growth is more muted in Montréal, but we can see how outlying built-up areas might be integrating with the City, creating an expanded Metropolitan area. Vancouver demonstrates what looks like the least outward growth in built-up area, corresponding well with the dwelling type data from the Census. Those parts of the CMA boundaries still in gray are largely all protected by Vancouver’s various regulations, especially its Agricultural Land Reserve to the South and East, but also topography and parks or reserves to the North. The nearby CMA of Abbottsford is also visible outside of Vancouver CMA boundaries to the SE, and current commute patterns would force inclusion of Abbotsford-Mission into the Vancouver CMA if it wasn’t it’s own CMA already. Similar constraints prevent Toronto CMA expanding and swallowing or eating into neighbouring CMAs.\nTechnical restrictions of CMAs to “expand” into already established CMAs raise the question if we need an additional definition of “functional CMAs” that only look at current commute patterns and contemplate what CMA boundaries would look like if we took a clean approach that ignores precedents of historically established CMAs. We might come back to this in a future post.\nAs usual, the code for this post is available on GitHub."
  },
  {
    "objectID": "posts/2022-08-18-25-years-of-structural-change/index.html#appendix",
    "href": "posts/2022-08-18-25-years-of-structural-change/index.html#appendix",
    "title": "25 Years of Structural Change",
    "section": "Appendix",
    "text": "Appendix\nFinally, just because we can, we want to take a quick look at the geographic distribution of households by structural type of dwelling in Metro Vancouver using a dot density map. This is kinda cool, because apartment areas can be pulled out from lower-density detached house and related zoning. But we can also glimpse some townhouse neighbourhoods and even a mobile home park or two! Enjoy!\n\n\n\nReproducibility receipt\n\n## [1] \"2022-08-18 20:39:27 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [4525453] 2022-08-19: structural change post\n## R version 4.2.1 (2022-06-23)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.5\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 dotdensity_0.1.0         \n##  [3] rlang_1.0.4               cityDensities_0.1.0      \n##  [5] patchwork_1.1.1           sf_1.0-8                 \n##  [7] cancensus_0.5.3           forcats_0.5.1            \n##  [9] stringr_1.4.0             dplyr_1.0.9              \n## [11] purrr_0.3.4               readr_2.1.2              \n## [13] tidyr_1.2.0               tibble_3.1.8             \n## [15] ggplot2_3.3.6             tidyverse_1.3.2          \n## \n## loaded via a namespace (and not attached):\n##  [1] httr_1.4.4          sass_0.4.2          jsonlite_1.8.0     \n##  [4] modelr_0.1.8        bslib_0.4.0         assertthat_0.2.1   \n##  [7] googlesheets4_1.0.0 cellranger_1.1.0    yaml_2.3.5         \n## [10] pillar_1.8.0        backports_1.4.1     glue_1.6.2         \n## [13] digest_0.6.29       rvest_1.0.2         colorspace_2.0-3   \n## [16] htmltools_0.5.3     pkgconfig_2.0.3     broom_1.0.0        \n## [19] haven_2.5.0         bookdown_0.27       scales_1.2.0       \n## [22] tzdb_0.3.0          git2r_0.30.1        proxy_0.4-27       \n## [25] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n## [28] cachem_1.0.6        withr_2.5.0         cli_3.3.0          \n## [31] magrittr_2.0.3      crayon_1.5.1        readxl_1.4.0       \n## [34] evaluate_0.15       fs_1.5.2            fansi_1.0.3        \n## [37] xml2_1.3.3          class_7.3-20        blogdown_1.10      \n## [40] tools_4.2.1         hms_1.1.1           gargle_1.2.0       \n## [43] lifecycle_1.0.1     munsell_0.5.0       reprex_2.0.1       \n## [46] compiler_4.2.1      jquerylib_0.1.4     e1071_1.7-11       \n## [49] classInt_0.4-7      units_0.8-0         grid_4.2.1         \n## [52] rstudioapi_0.13     rmarkdown_2.14      gtable_0.3.0       \n## [55] DBI_1.1.3           R6_2.5.1            lubridate_1.8.0    \n## [58] knitr_1.39          fastmap_1.1.0       utf8_1.2.2         \n## [61] KernSmooth_2.23-20  stringi_1.7.8       Rcpp_1.0.9         \n## [64] vctrs_0.4.1         dbplyr_2.2.1        tidyselect_1.1.2   \n## [67] xfun_0.31"
  },
  {
    "objectID": "posts/2022-06-30-a-brief-history-of-canadian-real-estate-investors/index.html",
    "href": "posts/2022-06-30-a-brief-history-of-canadian-real-estate-investors/index.html",
    "title": "A brief history of Canadian real estate investors",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\n\nThe newest trend in the search for reasons for rising home prices is to look toward investors. The Bank of Canada released a report showing that the share of investors has risen over time. For this they took mortgage data from federally regulated financial institutions and matched them with credit history to determine if some of the buyers already owned property before they bought (during roughly the past 10 years) and kept it after they bought.\nThis is an interesting method, though we’ve got questions and concerns! The method does not capture all purchases. It excludes purchases that are all cash without a mortgage (e.g. downsizers) and it doesn’t capture unregulated mortgages (e.g. people that go to alternative lenders because they don’t meet down payment requirements). This method may also classify as investors people who are simply co-signing loans, like family members acting as mortgage guarantors, as this kind of support often shows up in the mortgage data. If we are mostly interested in timelines and not the overall level of these estimates then some of these concerns are secondary, unless there are shifts in the selection over time (e.g. rising importance of family assistance in home purchasing).\nLet’s pull their data using Jens’ handy canbank package and take a look at their results.\n\nIn the data we see that the share of first-time homebuyers has declined over time, holding fairly steady 2018 through 2020 but then entering an accelerated decline mid 2020. Making up for this is an increase in repeat homebuyers, and starting in late 2021, a particular increase in investor buyers who pushed past their previous high around 2018. Another interesting feature in the above data is the drop in first-time buyer share early 2018 through early 2019, which is made up for by a bump in repeat buyers while investor share dropped.\nTo understand what conclusions we can draw from this we need to consider some other data points. During the pandemic homebuying activity increased significantly and interest rates hit record lows while prices skyrocketed. In times like this the main barrier to homeownership becomes access to capital, which is fairly easy for investors or repeat buyers of those with family assistance and has plausibly pushed more first-time buyers into the unregulated mortgage market and thus shifted them outside of this dataset. This makes it very hard to draw definite conclusions from this data. Is the drop in overall first-time homebuyers as strong as this dataset suggests, or is it a product of selection bias in the dataset? Or a little bit of both?\nTo understand how interest rates shift the market toward investors and repeat buyers, consider the prevailing interest rates during this time span.\n\nWe note how the mortgage rate high point for the 3 and 5 year mortgages correspond to a drop in both first time buyers as well as investor buyers, with first time buyers recovering again after a slight drop from the plateau while investors only pick up after mortgage rates hit new lows.\nAn interesting question is how these timelines will evolve now with rising mortgage rates. We expect the share of investor buyers in this dataset declining again, for the same reasons discussed above that pushed them up during the record low interest rates. We will have to wait and see how this will pan out in the date over the next year or so.\nEither way, the Bank of Canada results are interesting, if not entirely straightforward to interpret. But the datasets drawn upon by the Bank of Canada analysis only go back to 2014. For the rest of this post, we thought it would be fun to take a longer view.\nThe longest view, of course, takes us back to how real estate investment was bound up with Canada’s ugly history of colonial expropriation and land dispossession. We’re not going to go that far back simply because we don’t have the data handy. Here we’ll explore what share of Canadian families have been real estate investors since the 1970s. We’ll define real estate investors for the rest of this post similarly to the Bank of Canada as families owning property other than their primary residence, except we are looking at the stock instead of the flow. For this we consult the Survey of Financial security, and two of it’s precursor surveys, which gives us information on this question in irregular intervals back to 1976.\nHere we’ll just do a provincial overview.\n\nWe start with a provincial breakdown. This is useful for drawing out some varying patterns. These include relative stability in investor ownership in Ontario, a general drop in Quebec and a general rise across the Prairies and Atlantic provinces. BC has perhaps the weirdest trajectory, following a sort of S curve, with investor ownership falling into the 90s, then rising to a peak in 2016 before dropping again to near 1976 levels by 2019. Interestingly, though many local observers think of Expo 86 as a turning point for real estate investment in BC, the share of families owning investment property actually dropped to its all-time low between 1984 and 1999. As a general caveat we should note that there is some uncertainty in these estimates based on the SFS or the precursor surveys, and we probably should not over-interpret small fluctuations.\nReturning back to the national level, and matching this up to Bank of Canada estimates, we can see that recent results from the SFS on current ownership (a stock measure) run a little lower than the 17%-20% of purchases by investors (a flow measure) estimated for the country as a whole between 2014-2021, but they’re in the same ballpark, and multiple purchases by investors could easily account for the differences. Also, people who make mortgage guarantees for family members or other people and show up in the Bank of Canada mortgage data as investors may not think of themselves as owning (part of) that secondary property and won’t list it in the SFS survey. On the other hand, the SFS also counts properties outside of Canada owned by Canadian residents, which might tilt the bias the other way.\nWe can break this down finer to also break out what share of family units own or rent their primary residence.\n\nMost Canadians do not own any investment property. The big long term story across most of Canada is either stability or a rise in owner-occupation and a decline in renter households (especially since peaks in 1984). The Atlantic provinces are the exception, with the trends running in the other direction.\nOf those owning an investment property, a real, but relatively small proportion of households rent, with long-term trends here either rising or stable. Most investors (as we’ve broadly defined them) also occupy housing they own.\nPutting these trends together, the rise in investor ownership seems especially strong in the Prairie and Maritime provinces, but possibly for opposite (and even interrelated) reasons. Rising oil patch wealth in the Prairies likely boosted real estate investment purchases. Meanwhile, the relative decline of opportunities in the Maritimes and their uneven impact may have contributed to more people looking for rental housing (because they couldn’t afford to buy) while others became landlords in response. Indeed, many from the Atlantic provinces may have saved up money while temporarily working in the oil patch to buy properties back home which could then be rented out (and this may account for some of the relatively high rates of ownership of properties elsewhere for renters in the Prairies).\nUnfortunately, the longest timelines from surveys are only at the provincial level. But in future posts, we may return to looking at metro areas and comparing once again with results from Bank of Canada methods, as well as CHSP.\nAs usual, the code for this post is available on GitHub for anyone to replicate or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2022-06-30 13:13:53 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [38cd45b] 2022-06-30: fix plot title\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.4\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] canpumf_0.1.5             canbank_0.1.2            \n##  [3] mountainmathHelpers_0.1.4 forcats_0.5.1            \n##  [5] stringr_1.4.0             dplyr_1.0.9              \n##  [7] purrr_0.3.4               readr_2.1.2              \n##  [9] tidyr_1.2.0               tibble_3.1.7             \n## [11] ggplot2_3.3.6             tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.8.0  assertthat_0.2.1 digest_0.6.29    utf8_1.2.2      \n##  [5] R6_2.5.1         cellranger_1.1.0 backports_1.4.1  reprex_2.0.1    \n##  [9] evaluate_0.15    highr_0.9        httr_1.4.2       blogdown_1.9    \n## [13] pillar_1.7.0     rlang_1.0.2      readxl_1.4.0     rstudioapi_0.13 \n## [17] MetBrewer_0.2.0  jquerylib_0.1.4  rmarkdown_2.13   labeling_0.4.2  \n## [21] bit_4.0.4        munsell_0.5.0    broom_0.8.0      compiler_4.2.0  \n## [25] modelr_0.1.8     xfun_0.31        pkgconfig_2.0.3  htmltools_0.5.2 \n## [29] tidyselect_1.1.2 bookdown_0.26    codetools_0.2-18 fansi_1.0.3     \n## [33] crayon_1.5.1     tzdb_0.3.0       dbplyr_2.1.1     withr_2.5.0     \n## [37] grid_4.2.0       jsonlite_1.8.0   gtable_0.3.0     lifecycle_1.0.1 \n## [41] DBI_1.1.2        git2r_0.30.1     magrittr_2.0.3   scales_1.2.0    \n## [45] cli_3.3.0        stringi_1.7.6    vroom_1.5.7      farver_2.1.0    \n## [49] fs_1.5.2         xml2_1.3.3       bslib_0.3.1      ellipsis_0.3.2  \n## [53] generics_0.1.2   vctrs_0.4.1      tools_4.2.0      bit64_4.0.5     \n## [57] glue_1.6.2       hms_1.1.1        parallel_4.2.0   fastmap_1.1.0   \n## [61] yaml_2.3.5       colorspace_2.0-3 rvest_1.0.2      knitr_1.39      \n## [65] haven_2.5.0      sass_0.4.1\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2022,\n  author = {von Bergmann, Jens and Lauster, Nathan},\n  title = {A Brief History of {Canadian} Real Estate Investors},\n  date = {2022-06-30},\n  url = {https://doodles.mountainmath.ca/posts/2022-06-30-a-brief-history-of-canadian-real-estate-investors},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von, and Nathan Lauster. 2022. “A Brief History of\nCanadian Real Estate Investors.” MountanDoodles (blog).\nJune 30, 2022. https://doodles.mountainmath.ca/posts/2022-06-30-a-brief-history-of-canadian-real-estate-investors."
  },
  {
    "objectID": "posts/2022-05-27-residential-mobility-in-canada/index.html",
    "href": "posts/2022-05-27-residential-mobility-in-canada/index.html",
    "title": "Residential mobility in Canada",
    "section": "",
    "text": "This excellent NYTimes article on mobility in the US coming out today nudged me into doing a quick post on residential mobility in Canada. While there are lots of similarities between Canada and the US, there are some important differences when it comes to residential mobility. A while back Nathan Lauster compared residential mobility between the two countries and noticed that the declining trend in US residential mobility is much more muted in Canada, and may have reversed by the 2016 census, the last year for which we currently have data in Canada. (The 2021 data is slated to come out on October 26, 2022.) Nathan’s post also takes a closer look at what has changed in the US and why residential mobility declined by examining the reasons people give for moving. And Yes, it’s housing related reasons that are driving the decline in moving.\nThe NYTimes article places this into a longer timeline, showing that this trend started well before the 2000-2001 year which Nathan used as a starting point. While we are waiting for the 2021 data we want to take this as an opportunity to push the Canadian data back a bit further to see how Canada’s residential mobility has evolved. And also look into how this varies across the country and depending on tenure and age."
  },
  {
    "objectID": "posts/2022-05-27-residential-mobility-in-canada/index.html#one-year-mobility",
    "href": "posts/2022-05-27-residential-mobility-in-canada/index.html#one-year-mobility",
    "title": "Residential mobility in Canada",
    "section": "One-year mobility",
    "text": "One-year mobility\nThe NYTimes article looks at 1-year mobility, which is a useful metric because few people move more than once within a year and thus gives a good measure of residential mobility. Unfortunately, this is not available prior to the 1991 census.\n\nOverall this paints a picture of declining residential mobility across Canada, with hints of a recovery during the year before 2016. But the level of residential mobility, as well as the shape and makeup of the decline, varies across the different CMAs broken out here. For the initial 1990-1991 movers data we don’t have a breakdown for people moving within the same municipality (“Non-migrants”) and people moving from elsewhere within the province (“Intraprovinical migrants”). Additionally, we need to be aware that some metro area have expanded over this time period, as have municipal boundaries – most notably the City of Toronto between 1996 and 2001. That complicates interpreting these timelines.\nAs Nathan already observed on his blog, most moves are local, and the reduction of mobility is mostly driven by local moves. To better zoom into this we can plot the data as a line graph, which does reveal some divergence from this pattern for some regions.\n\nIn this post we want to dive a little bit deeper into residential mobility, looking at longer timelines, and understand how this relates to age and tenure."
  },
  {
    "objectID": "posts/2022-05-27-residential-mobility-in-canada/index.html#five-year-mobility",
    "href": "posts/2022-05-27-residential-mobility-in-canada/index.html#five-year-mobility",
    "title": "Residential mobility in Canada",
    "section": "Five-year mobility",
    "text": "Five-year mobility\nIf we want to go further back in time we have to content ourselves with looking at 5-year mobility data, so the share of the population who lived at a different address 5 years prior. This is available in electronic form back to 1971.\n\nHere we have some finer categories, splitting up intraprovincial migrants into those staying in the same census district and those coming from a different census district in the same province.\nBut how does this look like across different parts of the country? We only have information for Montréal and Toronto all the way back to 1971, more metro areas get added to the PUMF in successive censuses.\n\nIn addition to the caveats we already mentioned above, that some metro areas as well as some municipal boundaries have changed over time, the relationship of metro areas and the cities within to census district also varies, with for Vancouver the census district and metro area coinciding, but for Toronto the City boundaries coinciding with their census district starting with 2001. That can lead to some artificial changes among the categories.\nEach of these regions shows an overall declining trend in residential mobility. We will look at how this interacts with tenure and age to better understand what is driving this."
  },
  {
    "objectID": "posts/2022-05-27-residential-mobility-in-canada/index.html#tenure",
    "href": "posts/2022-05-27-residential-mobility-in-canada/index.html#tenure",
    "title": "Residential mobility in Canada",
    "section": "Tenure",
    "text": "Tenure\nWe know that renters move more frequently than owners. This happens for many reasons, which are worth getting into in more detail, but we will leave this for another post.\n\nComparing the mobility rates we indeed see large differences by tenure that dominate the differences across metro areas. Renters move much more frequently, but the decline in mobility is visible in both categories."
  },
  {
    "objectID": "posts/2022-05-27-residential-mobility-in-canada/index.html#age",
    "href": "posts/2022-05-27-residential-mobility-in-canada/index.html#age",
    "title": "Residential mobility in Canada",
    "section": "Age",
    "text": "Age\n\nAgain we see a fairly consistent decline in mobility across age groups. Especially the 20-24 year old age group stands out, and the older age groups also show a clear drop in residential mobility.\nDropping the mobility status take a look how this differs across metro areas.\n\nThe trends are fairly consisted, although e.g. Montréal is showing lower drops than other metro areas, hinting that some of this effect may be due to suppressed household formation that we have observed in a previous post."
  },
  {
    "objectID": "posts/2022-05-27-residential-mobility-in-canada/index.html#putting-it-all-together",
    "href": "posts/2022-05-27-residential-mobility-in-canada/index.html#putting-it-all-together",
    "title": "Residential mobility in Canada",
    "section": "Putting it all together",
    "text": "Putting it all together\nLooking at the difference across age and tenure we can ask ourselves how much of the overall drop in residential mobility are explained by these two. This lets us remove the effect of change in tenure and age distribution across time and across geographies, using the 2016 overall Canadian distribution as a counterfactual demographic distribution.\n\nThis shows that some, but not all of the effect of the declining mobility is explained by shifts in demographics and housing tenure. But some differences remain, in particular it’s remarkable how low Montréal’s residential mobility is, especially when accounting for tenure. Which also hints that tenure itself is probably less of a driving factor of mobility, but that there are other factors apart from age that are a common cause of tenure and mobility. And the model adjusting for age and tenure should probably be treated with caution.\nOne last remaining question is that of the role of external migration. That does not quite fit into the concept of mobility for the purposes of this post since external migrants don’t corresponds to moves of people in Canada but rather moves into Canada. We can remove external migrants and see how that changes things.\n\nThis does somewhat depress the mobility rates of regions that see a lot of external migration, but the general pattern is quite similar."
  },
  {
    "objectID": "posts/2022-05-27-residential-mobility-in-canada/index.html#upshot",
    "href": "posts/2022-05-27-residential-mobility-in-canada/index.html#upshot",
    "title": "Residential mobility in Canada",
    "section": "Upshot",
    "text": "Upshot\nMoving is an important factor in our economy and our housing system. In our housing discussions, for example during the ongoing public hearing of the Broadway Plan, many people have a very naive understanding of how residential mobility works. There is often an implicit assumption that new buildings have to be matched with newcomers. We know this is by and large not how things work, the majority of people who move into new housing already lived in the region. Similarly, the vast majority of people who move move into old housing. One very obvious way to see this is that almost half of the population in the City of Vancouver lived at a different address five years prior. Which highlights the absurdity of people questioning who new housing is for. New housing only makes up a tiny portion of destinations for movers, it’s perfectly ok if it only suites a small portion of those people moving. That’s not to say we should not strive to create non-market housing whenever possible, we absolutely should. (Raise my property taxes to pay for it!) But denying new housing because e.g. median incomes can’t afford it misunderstands how housing and mobility work and is counter-productive.\nTrends in mobility are interesting because they have implication for the broader economy. If people can’t easily move, they may not be able to change jobs to one that better fits the skills and pays more, and on the flip side employers will have a harder time recruiting talent to increase their productivity. People may get stuck in housing situations for longer than they want, resulting in suppressed household formation. Adjusting for changes in demographics can help better understand these trends, and alleviate underlying causes of reduced mobility.\nAnother important direction is to understand the reasons for moving. In Canada we are only now getting decent data on why people move, with the CHS giving us a way to understand reasons for moving, in particular forced vs choice moves. This view allows us to shine a light onto the two sides of residential mobility, which can be bad if people are looking for stability and can’t find it because they are forced to move, or it can be good if it enables choice moves closer to work, family and friends, or amenities, or to adapt to changing family needs. We are looking forward to the next round of the CHS data getting released.\nA good discussion on declining residential mobility in the US and its implications and drivers can be found in a nice recent sudy by Dowell Myers at al, broken down in easily digestible form in a twitter thread.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-05-30 09:56:23 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [8b0def3] 2022-05-29: add sentence on non-market housing\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 cancensus_0.5.1          \n##  [3] forcats_0.5.1             stringr_1.4.0            \n##  [5] dplyr_1.0.8               purrr_0.3.4              \n##  [7] readr_2.1.2               tidyr_1.2.0              \n##  [9] tibble_3.1.7              ggplot2_3.3.6            \n## [11] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.2 xfun_0.30        bslib_0.3.1      haven_2.5.0     \n##  [5] colorspace_2.0-3 vctrs_0.4.1      generics_0.1.2   htmltools_0.5.2 \n##  [9] yaml_2.3.5       utf8_1.2.2       rlang_1.0.2      pillar_1.7.0    \n## [13] jquerylib_0.1.4  withr_2.5.0      glue_1.6.2       DBI_1.1.2       \n## [17] dbplyr_2.1.1     readxl_1.4.0     modelr_0.1.8     lifecycle_1.0.1 \n## [21] cellranger_1.1.0 munsell_0.5.0    blogdown_1.9     gtable_0.3.0    \n## [25] rvest_1.0.2      evaluate_0.15    knitr_1.38       tzdb_0.3.0      \n## [29] fastmap_1.1.0    fansi_1.0.3      broom_0.8.0      scales_1.2.0    \n## [33] backports_1.4.1  jsonlite_1.8.0   fs_1.5.2         hms_1.1.1       \n## [37] digest_0.6.29    stringi_1.7.6    bookdown_0.26    grid_4.2.0      \n## [41] cli_3.3.0        tools_4.2.0      magrittr_2.0.3   sass_0.4.1      \n## [45] crayon_1.5.1     pkgconfig_2.0.3  ellipsis_0.3.2   xml2_1.3.3      \n## [49] reprex_2.0.1     lubridate_1.8.0  assertthat_0.2.1 rmarkdown_2.13  \n## [53] httr_1.4.2       rstudioapi_0.13  R6_2.5.1         git2r_0.30.1    \n## [57] compiler_4.2.0"
  },
  {
    "objectID": "posts/2022-05-17-on-broadway/index.html",
    "href": "posts/2022-05-17-on-broadway/index.html",
    "title": "On Broadway",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWe have finally found some time to take a closer look at the Broadway Plan. There are many good things to say about the plan, it adds housing in an amenity and job rich area about to get a new subway line. It promises to not just undo the downzoning the city imposed on parts of the area in the 1970s but enables a bit more housing to make up for lost time.\nThe plan also tacks heavily against the displacement risk to renters in the established rental apartment areas by both 1) limiting the redevelopment potential in those areas and 2) increasing the strength of tenant relocation and right of return policies, a hard-learned lesson from the redevelopment activity around Metrotown in neighbouring Burnaby. In short, overall there’s a lot to like.\nIn this post we want to accomplish several somewhat diverse goals"
  },
  {
    "objectID": "posts/2022-05-17-on-broadway/index.html#the-broadway-corridor",
    "href": "posts/2022-05-17-on-broadway/index.html#the-broadway-corridor",
    "title": "On Broadway",
    "section": "The Broadway Corridor",
    "text": "The Broadway Corridor\nThe Broadway Plan document chooses to use census tracts as the basis for census data for the Broadway Plan, and to track changes in the corridor since 2001. This is a fairly coarse geography choice since census tracts are rather large and extend quite a bit beyond the outlined area. Dissemination area data is available for all censuses since 2001 and is much better suited to approximate the corridor. If only data on population, households or dwellings is required, then dissemination block data can give even better estimates.\n\nWe will get different census estimates for the Broadway Corridor depending on which of these we choose. The Census Tract based geography is quite coarse and will likely skew the estimates, the other two are probably better choices. To understand the difference let’s take a quick look at population estimates.\n\nAs we would expect, the number of people in the Broadway Corridor depends on the level of geography we chose to approximate the region, and the differences are noticeable. Close observers will note that in addition to the Broadway Plan area, the CT level includes additional parts of Kitsilano and Olympic Village, while the DA level mostly adds Olympic Village, largely accounting for differences in estimates and the larger jump in DA level post-Olympics. If we want to understand the population in a particular year instead of comparing it on a stable region across years we can do better and approximate the region by using the census geography for just that year instead of resorting to using a harmonized geography.\nHowever, these differences matter less when looking at rates, or at change over time as the following graph on the growth during the inter-census periods between 2001 and 2021 demonstrates.\n\nThese estimates are quite close and indicate that the damage from using a coarse Census Tract based approximation is probably not too large. But the inclusion of Olympic Village, in particular, likely boosts growth estimates in recent years."
  },
  {
    "objectID": "posts/2022-05-17-on-broadway/index.html#historical-zoning",
    "href": "posts/2022-05-17-on-broadway/index.html#historical-zoning",
    "title": "On Broadway",
    "section": "Historical zoning",
    "text": "Historical zoning\nUsing maps we’ve assembled from archival research for the Metro Vancouver Zoning Project, we can put together an historical comparison of how zoning along the Broadway Corridor has changed. This won’t catch all the change, and as we’ll discuss in a minute, that’s an important caveat! Nor have we fully back-coded our historical data to match with our most recent data, refining and harmonizing what our zoning codes are actually allowing. But we can still get a glimpse of some of the changes that have occurred just by looking at the maps.\nFrom the start, the Broadway Plan area mixed industrial uses to the North, with Apartment and Commercial districts in the middle (especially along Broadway), and Low-Density Single Family and Duplex Zoning to the South. The overall story emphasizes a history of experimentation with this mix, encouraged by de-industrialization but also constrained by reactionary moves to downzone and pull back, preserving single-family, duplex, and low-rise areas from change.\n\n\nIndustrial Expansion and Contraction\nThe biggest visible change probably concerns Industrial zoning. There was an initial expansion, from the modernization of the zoning by-law in 1956 to 1963, when Industrial zoning swept into the formerly residential zone to the NE of Cambie and Broadway. Only Jonathan Rogers Park remained zoned residential, though a few old houses were grandparented in and have now stuck around long enough to be considered heritage.\nAfter this initial expansion, we can see a gradual contraction of Industrial zones along False Creek. The contraction occurred in line with broader North American trends toward urban de-industrialization. Vancouver was notable mostly for its success in shifting so much former industrial land over to a mixture of residential and commercial use. South False Creek was probably the biggest example of this as a coordinated process along the Broadway Corridor, with the area given its own zone between 1974 and 1976, and supported across multiple levels of government as an experiment in residential mixing (with decidedly mixed results). But this area isn’t actually included in Broadway land use planning, so we’ll set it aside. One of the last big industrial areas to turn over to residential in the Broadway land use planning area was an old brewery on the western edge, tucked between 10th and 12th, which became Arbutus Walk in the 1990s.\n\n\nExpanding Apartment Options\nMid-Century planners looked to enable an update and expansion to Vancouver’s increasingly built-out apartment districts after the modernization of the Zoning Code in 1956. This was roughly in line with planner Harland Bartholomew’s expectations that the initial zoning plan would only last for about thirty years. Mid-century planners gradually edged the low-rise apartment RM-3 districts outward through the 1960s and early 1970s. This was coupled with an effort to enable towers as a new built form throughout old RM-3 zones via a change to maximum heights in 1961. Correspondingly, towers began to pop up in Kitsilano, Fairview, and Mount Pleasant.\n\n\nReactionary Downzoning\nThe 1970s brought in a new party strongly opposed to the proliferation of these new towers: TEAM. They took over council in 1973, installed a new planner, and began downzoning, especially across Kitsilano and Mount Pleasant, shifting high-rise RM-3 lands down to low-rise RM-4 districts. Only the RM-3 surrounding Granville remained. As a result, many of the old towers built from the mid-1960s to the early 1970s could no longer be re-built today, ranging from the apartment buildings near 2nd & Vine in Kitsilano to the Senior’s Housing built for the Lutheran Church across from City Hall at Yukon & 10th (the latter also featured as Stop Four in our Virtual Zoning Tour). In effect, the reactionary downzoning of the 1970s actively forbid the kinds of heights only now being re-enabled by the Broadway Plan today.\nWe see the old TEAM’s housing legacy, on the whole, as decidedly mixed; balancing downzoning of denser forms with support for low-rise variety and adding housing to former industrial lands (see Gordon Price & Sam Sullivan for a less charitable view). But regardless of the old TEAM’s legacy, if you think we’ve got more than enough housing now, then the newly resurrected TEAM for a Livable Vancouver Party might be for you. If you don’t think we’ve got enough housing today, well… we’d probably have a lot more older and more affordable housing now if we hadn’t downzoned so much of the Broadway Corridor fifty years ago.\nLooking forward, we can and should start building the housing we think people will want some thirty years into the future. That’s what the Broadway Plan aims to start enabling. As a reminder, we also want there to be a “some thirty years into the future” for people to live in, which the Broadway Skytrain expansion will help insure to the extent it gets people where they need to go without greenhouse gas emission. And we need people to live near the Skytrain for that to work."
  },
  {
    "objectID": "posts/2022-05-17-on-broadway/index.html#proposed-zoning-changes",
    "href": "posts/2022-05-17-on-broadway/index.html#proposed-zoning-changes",
    "title": "On Broadway",
    "section": "Proposed zoning changes",
    "text": "Proposed zoning changes\nWhile the plan at this stage does not provide the legal text for zoning changes, it does lay out in some detail how it envisions zoning to change in various areas. These changes are the last point we’ll address.\n\nConditional rules\nIt’s not clear from the plan how much the actual zoning rules will rely on conditional vs outright zoning. And if the plan envisions to rezone the area, or if it’s just left as a guiding document and every single development will require a council decision to move forward like in the Cambie corridor.\nFor Multiplexes and Townhouses (11.2), as well as Low-rise Apartments (11.3), the proposed rules in section 11 of the Broadway Plan do seem to be sufficiently clear in the density that’s allowed, a clear improvement over general practice in Vancouver to regulate multi-family housing via conditional zoning (or rezoning) only. However, when it comes to Mid to High-Rise Apartments (11.4), Mixed-use High-rise (11.6), Mixed-use Residential (11.7), Office (11.8), Industrial/Employment (11.9), or Transit Integration (11.10) areas no clear guidance on height and FSR is given. Others, like the Mixed-use Low-rise (11.5), while clearly intended for housing purposes refers to existing zoning documents that only allow housing conditionally. This continued over-reliance on conditional zoning and planner discretion in the process will server to further feed into Vancouver’s climate of speculation and developer concentration that planners have fostered for many decades now.\n\n\nInclusionary zoning\nThe plan has fairly aggressive inclusionary zoning requirements, which we are guessing have been tested by the housing economics group to ensure that projects are marginally viable. However, there seem to be no contingencies built into the plan to allow for market prices and rents to fall. If that were to happen (an outcome many would welcome), the projects allowed in the plan will most likely become unviable and development will pause until prices climb back up. We believe this is a major shortcoming of the plan. Even if falling market prices and rents aren’t a direct goal of the plan, maybe because of its limited geographic scope, it should be explicitly allowed by the plan. The plan should probably be amended to allow for the possibility of falling prices and rents. It’s worth noting that another way to insure we continue to build through any downturns, of course, might be to get government involved in building, but this, too, remains beyond the Plan."
  },
  {
    "objectID": "posts/2022-05-17-on-broadway/index.html#low-density-areas",
    "href": "posts/2022-05-17-on-broadway/index.html#low-density-areas",
    "title": "On Broadway",
    "section": "Low density areas",
    "text": "Low density areas\nA final part of the plan worth interrogating is how it deals with the low-density areas within the corridor. These are the remaining areas in yellow in our historical map. We support the move to protect and slow down development in the existing rental apartment areas, but a more balanced approach would offset these protections by allowing more density in the areas with minimal displacement risk. Even more disappointing is how these zones that allow higher and lower density apartments are delineated. They don’t follow planning patterns like proximity to subway stations, or jobs and amenities. Instead they’re simply based on continuing forward existing exclusionary and fairly arbitrary historic zoning patterns. In that sense they are largely backward-looking rather than forward-looking, baking in exclusionary decisions made by planners long-dead.\nJust to illustrate this problem, consider Fairview South - Area C. \nPlanners think the FSOB area just to the north is good for up to 6.5 FSR, with 20 storey towers conditionally allowed on some sites, but the highlighted FSOC areas only allow for a maximum of 6 storeys 2.7 FSR rental buildings. As an example, let’s take a look at the existing built form around the leftmost of the two FSOC areas looking north toward the FSOB.\n\n\n\nFSOC map view\n\n\nWe notice that just across from the FSOC area on the north side of 14th Ave there are existing 13 storey buildings. These combine a rental building constructed in 1972 (thanks 1960s upzoning!) with an additional infill rental building enabled in 2017 via spot-zoning. But now, 50 years and a subway line later, planners think only six storey rental buildings make sense across the street. It’s really hard to interpret this as anything but deference to decisions made by planners many decades ago of where to draw the fairly arbitrary lines delineating higher density from lower density zones, an act that seems more akin to religion than to planning."
  },
  {
    "objectID": "posts/2022-05-17-on-broadway/index.html#upshot",
    "href": "posts/2022-05-17-on-broadway/index.html#upshot",
    "title": "On Broadway",
    "section": "Upshot",
    "text": "Upshot\nWhile this post devotes quite a bit of space to what we feel are some of the weak points in the Broadway Plan, overall it is a vast improvement over the status quo. While it can’t undo all the damage done by the downzoning of much of the corridor in the 1970s, it does revert back to allowing buildings that could be built before the downzoning and adds more options. Allowing more people to live in a jobs and amenity rich area close to a new subway line is good.\nProtecting existing renters is good too, and we are happy to see that planners are avoiding some of the mistakes that were made in Metrotown.\nThe plan also takes a cautious step toward more predictable outright zoning, at least for the lower density parts of the plan, which is welcome.\nWe are hoping that our concerns around the ability to dynamically adapt inclusionary requirements to changing market conditions can be incorporated at the bylaw stage of the plan, planners should include contingencies that allow for falling market prices or rents.\nOur concerns about low-density areas, and how the delineation between planning areas are chosen, are painful reminders of how planning can become too focused on historical (right or wrong) paradigms and stay disconnected from the physical and economic realities. Hopefully the Vancouver Plan can do a better job at re-imagining our city from a forward-pointing rather than backward-looking perspective, and undo some of the arbitrary zoning divisions in the current Broadway Plan.\nTo sum up, one thing the Broadway Plan does well is tack against exclusions of existing renters by “the invisible hand of the market.” One improvement would be to also tack against “the invisible hand of planners past,” keeping our eyes more firmly fixed on securing a more inclusive future.\nAs usual, the code for this post is available on GitHub for anyone to replicate or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2022-05-17 13:45:38 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [14741e8] 2022-05-17: broadway post\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 sf_1.0-7                 \n##  [3] tongfen_0.3.5             cancensus_0.5.1          \n##  [5] forcats_0.5.1             stringr_1.4.0            \n##  [7] dplyr_1.0.8               purrr_0.3.4              \n##  [9] readr_2.1.2               tidyr_1.2.0              \n## [11] tibble_3.1.7              ggplot2_3.3.6            \n## [13] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8.3       lubridate_1.8.0    class_7.3-20       assertthat_0.2.1  \n##  [5] digest_0.6.29      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  \n##  [9] backports_1.4.1    reprex_2.0.1       e1071_1.7-9        evaluate_0.15     \n## [13] httr_1.4.2         blogdown_1.9       pillar_1.7.0       rlang_1.0.2       \n## [17] readxl_1.4.0       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.13    \n## [21] munsell_0.5.0      proxy_0.4-26       broom_0.8.0        compiler_4.2.0    \n## [25] modelr_0.1.8       xfun_0.30          pkgconfig_2.0.3    htmltools_0.5.2   \n## [29] tidyselect_1.1.2   bookdown_0.26      fansi_1.0.3        crayon_1.5.1      \n## [33] tzdb_0.3.0         dbplyr_2.1.1       withr_2.5.0        grid_4.2.0        \n## [37] jsonlite_1.8.0     gtable_0.3.0       lifecycle_1.0.1    DBI_1.1.2         \n## [41] git2r_0.30.1       magrittr_2.0.3     units_0.8-0        scales_1.2.0      \n## [45] KernSmooth_2.23-20 cli_3.3.0          stringi_1.7.6      fs_1.5.2          \n## [49] xml2_1.3.3         bslib_0.3.1        ellipsis_0.3.2     generics_0.1.2    \n## [53] vctrs_0.4.1        tools_4.2.0        glue_1.6.2         hms_1.1.1         \n## [57] fastmap_1.1.0      yaml_2.3.5         colorspace_2.0-3   classInt_0.4-3    \n## [61] rvest_1.0.2        knitr_1.38         haven_2.5.0        sass_0.4.1"
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html",
    "title": "Estimating Suppressed Household Formation",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#tldr",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#tldr",
    "title": "Estimating Suppressed Household Formation",
    "section": "TL;DR",
    "text": "TL;DR\nWe develop and elaborate a Montréal Method for estimating housing shortfalls related to constraints upon current residents who might wish to form independent households but are forced to share by local housing markets. Applying simple versions of the Montréal Method to Metro Areas across Canada suggests that Toronto has the biggest shortfall, which we estimate at 250,000 to 400,000 dwellings, depending upon assumptions. For Vancouver, the estimated shortfall range is narrower, from roughly 75,000 to 100,000 dwellings. But models suggest housing shortfalls remain widespread, and there is much room for further elaboration. Note: shortfalls estimated in this post only account for those due to suppressed household formation among residents and do not account for e.g. migration pressures, which means that overall housing shortfalls are likely much larger."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#introduction",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#introduction",
    "title": "Estimating Suppressed Household Formation",
    "section": "Introduction",
    "text": "Introduction\nIn a previous post we tried to explain what goes wrong with population and dwelling growth projections in planning. In this post we will examine household maintainer rates more closely, one of the two key places where implicit model assumptions perpetuate and amplify housing scarcity. We’ll also lay out adjustments to consider that we’ll call “The Montréal Method” for estimating housing needs based on suppressed household formation.\nHousehold maintainer rates, sometimes called headship rates, are the share of the population 15+, or within a specific age group, who are coded as “primary household maintainer” in the census. The concept draws upon an older & occasionally patriarchal notion of household head, where the census was interested in identifying who was “in charge” of the household, and updates it by merely identifying the first person listed who pays the bills. Often, of course, as with most couples, there are two or more people paying the bills. But only one gets identified as the “primary” household maintainer in order to help identify a single link between households and the people they contain. Correspondingly, household maintainer rates serve as a sort of missing link in how we fit populations into dwellings. This missing link also contains information about household size. And as we noted previously with respect to Metro population projections, assumptions about household maintainer rates have a significant impact on housing needs and demand estimates.\nOften housing needs and demand projections assume household maintainer rates will follow past trends, but as explained in our previous post this assumption tends to enshrine and worsen past housing shortages. If in the past housing was scarce, and household maintainer rates have been suppressed by, e.g. delaying independent household formation of young adults, simply projecting past trends forward would bake these past shortages into plans for the future. In other words, normal planning processes, as with Metro Vancouver’s growth projections, can serve to systematically enshrine housing shortages.\nAs a start, let’s estimate recent household maintainer rates in Canada for the population 15 years or older. Our figure comes out to 49%. That’s the factor we need to multiply the population 15 years or older by to get to our estimated number of private households. It tells us what to put into the ‘Household Maintainer Rate’ box on the diagram in the previous post. In other words, in Canada the average household size is just over 2, if we don’t count children under the age of 15.\nWe can think of this number as constituting model zero for how to integrate household maintainer rates into linking population to housing."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#household-maintainer-rates-vary-across-cmas",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#household-maintainer-rates-vary-across-cmas",
    "title": "Estimating Suppressed Household Formation",
    "section": "Household maintainer rates vary across CMAs",
    "text": "Household maintainer rates vary across CMAs\nModel zero provides a starting number to link population to dwellings. But household maintainer rates aren’t constant across the country; some CMAs have higher household maintainer rates than others. In fact, this is at the core of what we are interested in in this post. How should we understand this variation?\n\nThe central question is what causes discrepancies between e.g. Toronto with a household maintainer rate of 45% and Montréal with a rate of 51%? One candidate that immediately comes to mind is the availability of housing. We can encode that in an extension of our Model 0 by explicitly acknowledging that CMA can impact household maintainer rates.\n\nThis is still somewhat simplistic, but if we assume that our Model 1 is complete, that CMA variation can be explained by housing constraints, and that there aren’t any other important factors linking CMAs to household maintainer rates, we can ask how much more housing each of these CMAs would need in order to achieve the same household maintainer rates as any other given Metro Area.\nWhich metro area should we use as our counterfactual of unconstrained household formation? We could use Canada as a whole, of course, but that would just be balancing constrained and unconstrained areas (and there have recently been suggestions that the country as a whole should be understood as in housing deficit). Quebec City is tempting as the metro area with the highest household maintainer rate. But it’s also somewhat small and might be distinct for other reasons. By contrast, Montreal looks like a nice, big, reasonable counterfactual, enabling us to ask how much more housing would other metros need to reach the household maintainer rate of Montreal?\nTo see if this is reasonable, let’s do a quick preliminary check-in on vacancy rates in the lead-up to our 2016 household maintainer rates as another measure of how they might plausibly have been influenced by housing conditions.\n\nHere we can see patterns that generally support, but might also complicate our understanding of household maintainer rates being linked to housing conditions. Toronto and Vancouver, of course, have notoriously low rental vacancy rates, which match well onto their low household maintainer rates. People can’t readily find apartments to rent, and it’s been that way for awhile. But Victoria’s vacancy rates are also quite low, despite the area having a high household maintainer rate. Québec’s vacancy rates climbed to a high just prior to the 2016 Census that might help explain its high household maintainer rate, but leave concerns about whether it was a temporary effect. Yet Edmonton and Calgary’s rental vacancy rates climbed even more dramatically, while their household maintainer rates aren’t much different from Vancouver’s. It may be the case that fluctuations in vacancy rates take some time to translate into household maintainer rates, but we’re not sure.\nThis perhaps furthers the case for using Montréal as our counterfactual. Montréal is a big metro area with rental vacancy rates that have remained relatively stable within the 3%-4% range, with high household maintainer rates, if not the highest. So we’re going to go ahead and use Montréal as our baseline to provide estimates for how household formation might be suppressed elsewhere across Canada. And we’re going to call it The Montréal Method.\nHow much more housing would other metro areas need to reach household maintainer rates matching Montréal’s?\n\nInteresting! But our initial results from The Montréal Method could easily be questioned. Toronto and Vancouver registering with a large housing shortfall seems in principle plausible, but what about Calgary? Does Toronto really need 18% more dwellings to accommodate all those desiring an independent household? And why do Québec City and Victoria appear to have an excess of housing? We might be missing something.\nIt’s also a good time to remember that the housing shortfall estimated here is only the shortfall due to suppressed household formation, and it disregards housing shortfall due to other factors, for example suppressed in-migration or forced out-migration. Still, it’s a start."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#household-maintainer-rates-vary-by-age",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#household-maintainer-rates-vary-by-age",
    "title": "Estimating Suppressed Household Formation",
    "section": "Household maintainer rates vary by age",
    "text": "Household maintainer rates vary by age\nTime to get a bit more serious. Most household maintainer rate models, including the Metro Vancouver projection models, stratify by age groups. And for good reason! For any given year, age is the strongest predictor for household maintainer rates. The household maintainer rates for 15 year olds are essentially zero. But household maintainer rates generally increase with age and for seniors they rise well over 60%.\n\nCorrespondingly, if different CMAs have different age distributions, that might skew our estimates. This tells us we need to refine our Model 1 to account for Age. Here we think of people sorting into CMAs by age, with some CMAs skewing younger and others skewing older. Victoria, for instance, is regularly promoted as “Canada’s Retirement Capital.”\n\nPart of the justification of using household maintainer rates to translate between population and dwellings is the recognition that in most situations all household maintainers in a given household are of similar age, and thus the choice of “primary” household maintainer does not impact these kind of estimates much. Based on this model we can estimate age-adjusted household maintainer rates for each CMA by weighting age specific rates by the overall age distribution in Canada.\n\nAdjusting by age highlights how some of the difference in household maintainer rates can be accounted for by difference in age distribution across municipalities. In particular, Toronto has a younger than average age distribution whereas Victoria is on average older, so the adjustments go in opposite directions. Nevertheless, the differences across CMA are quite stubborn, only Montréal and Victoria flipped positions in our lineup.\nIn other words, adjusting for age reduces the variation in household maintainer rates across CMAs a little, by 27% to be precise, but there is still considerable variation left. We can again estimate how many more housing units we would need to achieve the same household maintainer rates as Montréal, after accounting for differences in the age distributions.\n\nAssuming age offers our only important household maintainer determinant and Montréal makes a good counterfactual for where other metro areas could be, this offers us a nice estimate of how much housing we would need to reach Montréal levels of maintainer rates. The Age-Adjusted Montréal Method estimates Vancouver is short well over 10% and Toronto by well over 15%, but shortfalls still appear to be common across most Canadian metro areas, save for Québec."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#age-specific-metro-household-maintainer-rates-across-history",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#age-specific-metro-household-maintainer-rates-across-history",
    "title": "Estimating Suppressed Household Formation",
    "section": "Age-Specific Metro Household Maintainer Rates across History",
    "text": "Age-Specific Metro Household Maintainer Rates across History\nLet’s examine our counterfactual a little more carefully by comparing age-specific household maintainer rates directly and taking them backward in time. History gives us another view into the differences in household maintainer rates across CMAs that we have observed above. Have these differences been stable over time, or have they changed significantly? Here we’ll just focus on the historical data we have on age-specific household maintainer rates from 1971-2016 PUMF data for Montréal, Toronto and Vancouver CMAs.\nOf note, we have to switch our data source to allow more flexibility. So far we have used data from census cross tabulations, going forward we need to cut data in more custom ways and will rely on PUMF data. In this overview we gloss over a couple of complications, we just use standard weights and don’t show confidence intervals for PUMF estimates, and we ignore that the geographic boundaries of CMAS, in particular Montréal, have changed over time. The broad effects we are trying to highlight here are large enough that they can’t be explained by variability in the PUMF, and we believe that the changing geography won’t have significant effects on the age-specific household maintainer rates in either direction. We talk about the different data sources more in our appendix.\nFirst we compare how age-specific household maintainer rates have changed over time in each of the CMAs.\n\nWhat does Montréal offer as a counterfactual? Strikingly, age-specific household maintainer rates in Montréal have remained fairly constant over time. The most notable trend is for older age groups, especially age 75+, where we can see a rise in independent living during the 1970s and 1980s, as reflected in rising household maintainer rates.\nComparing Montréal to Toronto and Vancouver, we can see that they had quite similar age-specific household maintainer rates in the 1970s. Then rates in Toronto and Vancouver began to plummet to quite a bit below Montréal’s rates from 2000 onward. These drops are most evident in the age ranges from 25-39, but they also show up in older ranges, with ages 75-79 most notable for Vancouver.\nWe can slice the data differently and compare the CMAs next to each other over the years.\n\nThis highlights how similar all three CMAs were in the 1970s, strengthening the case for Montréal as a reasonable counterfactual. Montréal didn’t pull ahead in the 1980s and 1990s so much as Toronto and Vancouver fell behind. The graphic effect is kind of like receding gums gradually leaving Montréal’s teeth exposed.\nLet’s leave dentistry behind and return to thinking about housing. Using our Montréal Method can provide estimates of how much housing would be needed to return Toronto and Vancouver to similar household maintainer rates as Montréal, and correspondingly to similar rates reflecting where they were in the 1970s. But is it reasonable to assume that the divergence between these three metro areas is entirely about housing constraints?\nLet’s add one more complicating factor: culture."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#cultural-differences",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#cultural-differences",
    "title": "Estimating Suppressed Household Formation",
    "section": "Cultural Differences?",
    "text": "Cultural Differences?\nPerhaps cultural differences emerging in Toronto and Vancouver have altered their trajectories away from their past patterns and away from Montréal’s. These could be associated with the quite different immigration patterns we’ve seen in Toronto and Vancouver. The important idea here is that maybe some of the variation in household maintainer rates has less to do with housing constraints than with different ideas about family and ideal living situations. Maybe some groups of young adults don’t want to live on their own? Maybe some groups of seniors similarly prefer to live with their children?\nThis leads us to the next refinement of our household maintainer model. There is literature on large cultural differences, and those differences have complex fault lines. For example young adults leave home much later in south and eastern European countries like Italy, Greece, or Spain than in north and western European countries. There are similar arguments to be made about various cultural differences across Asia. For instance, Statistics Canada estimates that South Asian Canadians are over three times as likely to live in complex, multi-family households compared to Canadians as a whole. So we should probably consider ethnic background, but also immigrant generation status. First generation immigrants may have few other options than forming households at high rates, as they often have no parents or support networks to fall back on. Second generation immigrants might stay longer living with their parents, whereas their third or higher generation counterparts may converge toward broader Canadian patterns. We will loosely refer to the intersections of these variables as potentially pointing out differences in Culture.\nThis tells us that ethnic background, as well as immigrant generation, will likely have an impact on household maintainer rates. We also think of immigrants sorting by cultural background into CMAs, for example based on existing networks.\n\nBut this is where things get tricky for several reasons. For one, the concepts of ethnic background are somewhat fluid and hard to nail down. A more technical reason is that we are forced to move the analysis to PUMF data, which is roughly a 1:40 subsample of the population, and we are quickly running up against model complexity limits. The PUMF data is simply not thick enough to be able to expect robust estimates of counterfactual model when simultaneously accounting for CMA, Age, and Culture unless we distill Culture& down to very broad categories. This is somewhat in keeping with our approach of successively refining our model.\nTo derive broad categories of Culture we first recode generation status into three groups, First Generation (born outside of Canada), Second Generation (both parents first generation) and all other. Then we manually group the 49 ethnic origin categories that are split out in the PUMF into broader groups. After that we cross the ethnicity categories with the generation status, and collapse some categories further in order to ensure the categories are thick enough to yield somewhat robust estimates. Then we take data from Canada outside of Vancouver and Toronto (where we know housing is constrained) to estimate overall household maintainer rates for these categories and cluster them to arrive at three broad Culture categories (C1, C2, and C3) we will use in our model, the results of which are depicted here.\n\nWe’re the first to admit that our C1, C2, and C3 “Cultural” groupings linked to different household maintainer patterns are rough. But they allow us to make another pass at understanding differences across metro areas and applying our Montréal Method. With this set up we now have three different models to estimate counter-factual household maintainer rates. The crude estimates, the age-adjusted model and the model adjusting for age and ethnic/immigration background.\nWe can express this in terms of household maintainer rates as observed using Montréal as counterfactual.\n\nOr, following our Montréal Method, we can express this directly as housing shortfall relative to our counter-factual of Montréal. This corresponds to three different versions of our Montréal Method which we can use to estimate the housing shortfall in other metro areas. Results from the Montréal Method are interpretable as the number of extra dwellings we’d need to enable people to maintain independent households like they do in Montréal. We’ve now established a crude version, and age adjusted version, and a version adjusted for both age and ethnic/immigration background.\n\nThis shows how our successively refined models lead to adjustments in the counterfactual effect of CMA on household maintainer rates. We can see how each refinement of the model adjusts the rates of household suppression and associated housing shortfall relative to the counter-factual of Montréal. Adjusting additionally for ethnic background and immigration generation leads to noticeable shifts both up and down. In particular, Toronto’s counter-factual estimates decrease significantly, along with estimates for Vancouver and Calgary. By contrast, Hamilton remains roughly the same.\nOverall, in terms of enabling people to form independent households, large swathes of Canada appear to be constrained relative to Montréal. The Québec City CMA alone consistently comes out with a negative housing shortfall, perhaps hinting that even Montréal may experience some pressures that suppress household formation. Nevertheless, as we discuss above, Montréal appears to offer a pretty decent counterfactual for the rest of Canada to aspire to.\nOur estimates of constraints in Toronto and Vancouver are largely to be expected. That they begin to converge with other metro areas as we elaborate our model suggests more widespread constraints. The prominence of Calgary, in particular, is somewhat unexpected. We plan to return to this in another post where we will go beyond household maintainer rates and focus in on particular living arrangements. One hypothesis would be that this is due to Calgary, as well as Edmonton, having a lot of roommate households in the key household formation age groups. This may be due to the particular dwelling stock, which has about half the share of 1-bedroom dwelling units compared to Vancouver or Toronto and lends itself to roommate households. Another potential issue could arise because of how the census codes the residence of students, which hits CMAs with a higher share of parents harder."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#what-does-this-mean",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#what-does-this-mean",
    "title": "Estimating Suppressed Household Formation",
    "section": "What does this mean?",
    "text": "What does this mean?\nHow should we make sense out of our Montréal Method estimates of housing shortfall related to constrained maintainer rates? To make them more interpretable, let’s translate translate the percentages into hard numbers. How many more dwellings are required to make up the housing shortfall due to suppressed household formation? Once again, this is only measuring part of our shortfall, so we’re assuming no change in migration patterns due to higher availability of housing.\n\nFor Toronto, the Montréal Method estimates a housing shortfall of 250,000 to 400,000 dwellings, depending upon assumptions. For Vancouver, the shortfall range is narrower, from roughly 75,000 to 100,000 dwellings. We should stress again that this is only trying to estimate the housing shortage due to suppressed household formation, if we were to actually make up the dwelling shortfall this would almost certainly also alter migration patterns and interfere with these estimates.\nAt the national level we don’t have to worry about impacts of migration, if we are able to build the needed housing in all the right places. For all of Canada this housing shortfall comes out to around 890k units for our Model 2 and, if we are comfortable with our assumptions of adding culture into the model, 830k for our Model 3.\nThis includes the negative shortfall in Québec, which reminds us that maybe Montréal was already experiencing housing pressures in 2016 (which have intensified in the meantime) and perhaps isn’t the only counterfactual we should consider, leading to possible under-estimates. Indeed, if we look back to the age-based household maintainer rate timelines for Montréal we can observe a drop in household maintainer rates in the 20 to 39 year old age groups in the 2016 census supporting this. Still, if nothing else, Montréal provides us a conservative estimate of what less constrained household formation could look like.\nHow could our estimated housing shortfall be fixed? To provide one last means of interpretation, we can compare our numbers above to the prevailing pace of housing completions. If we scaled up to the development industry to double the number of new dwellings completed circa 2016 - without accounting for demolitions - how long would it take to address the shortfall in each metro area?"
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#how-robust-are-these-results",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#how-robust-are-these-results",
    "title": "Estimating Suppressed Household Formation",
    "section": "How robust are these results?",
    "text": "How robust are these results?\nThis is a good start, but how realistic is our Model 3, and what might a more realistic Model 4 look like? And how would that effect the estimates?\nWe still have some concerns about Model 3. In particular, people are different both within and between any cultural groupings; hence assumptions about cultural groupings are tricky to get right. It’s difficult to adjust for possible differences in culture and living preferences between groups that also distribute themselves selectively across CMA. For example, people who move from places with large groups of co-ethnics to places with smaller groups may be distinct from those who stay in terms of their ideal living situations. This gets to a fundamental conundrum: the more realistic complexity we attempt to add to our model, the greater the risk we might make wrong assumptions about how that complexity actually works."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#possibly-more-realistic-models",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#possibly-more-realistic-models",
    "title": "Estimating Suppressed Household Formation",
    "section": "Possibly more realistic models",
    "text": "Possibly more realistic models\nGiven that we’ve laid out these risks, we can at least think through this a little more systematically and point at some other important factors we might want to consider if we want to add further complexity. We don’t have to worry about factors impacting just household maintainer rates as long as we think they don’t also impact, or are impacted by CMA in some way, and we will omit those. For example, we know that Gender impacts household maintainer rates, but we don’t believe CMA and Gender are strongly related, so for now we omit Gender from our model. We could model a set of important relationships as per below.\n\nThis model aims to capture the main effects impacting the relationship between CMA and Household Maintainer Rates, but still remains incomplete. It is still missing several factors that impact or confound the relationship between CMA and Household Maintainer Rates. For example, parental wealth likely interferes with household formation of young adults (e.g. the “gilded cage” effect) in ways that are not independent of CMA. The young adults in Vancouver and Toronto who don’t have help from parents in meeting their housing needs may choose at an increased rate to relocate to other CMAs and form households there. This kind of effect of people self-selecting into CMAs based on affordability issues would be captured broadly under CMA effects (and might increase noise).\nThough there are factors missing, we don’t believe that they are necessarily correlated with the other factors, Age, Income, School/Work/Neither status, or Culture in our model. We’d certainly be interested in seeing how the factors we attempt to include might shift, e.g. the model hints at how adjusting for culture in our Model 3 may well soak up some income effects. Armed with a model like this we could use census data to estimate the effect of CMA on household maintainer rates. Two effects actually, the total effect, including the effect mediated through differences in the income distribution which we assume to be partially caused by CMA level variables, and the direct effect (in red) that is not mediated through CMA-induced differences in the income distribution.\nBut there’s a further problem. While this model probably does a better job at capturing the complexities, we can’t expect PUMF data to produce reasonable estimates. The PUMF’s 1:40 subsample is simply not deep enough for the complexity of the model. To stand a fighting chance to estimate this model we might need to use the full census data at the RDC. Having access to the census in the RDC would also allow to explore further linkages in the data and add features of parents into the model that might interact with CMA, for example type of dwelling the parents live in."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#upshot",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#upshot",
    "title": "Estimating Suppressed Household Formation",
    "section": "Upshot",
    "text": "Upshot\nWe develop and elaborate the Montréal Method for estimating constrained household formation. Taken together, we interpret our results from applying the Method as evidence that housing shortage is likely constraining household formation quite a bit, especially in Toronto, but also Vancouver and elsewhere across Canada. Montréal makes for a decent and fun counterfactual, enabling us to estimate housing shortage. While it might have some cultural idiosyncrasies that limit its applicability as counterfactual, Montréal’s relatively stable and accessible (for Canada) vacancy rates; the fact that Halifax has similar overall household maintainer rates; and the fact that Montréal’s maintainer rates were quite similar to Toronto and Vancouver in the 1970s, all suggest the Montréal Method does its job ok.\nWe open up many more questions with our analysis so far. Returning to history, why did household maintainer rates change so much outside of Montréal, and how has this related to the housing market of other CMAs. While household maintainer rates remain useful, we may want to look at distinct lifecourse factors and situations to work this out in detail. For instance, we would like to spend more time observing the living arrangements of young adults at the ages 20 or 25 through 39 where the bulk of the household formation happens. Some transitions, e.g leaving home, tend to increase household formation, while others, e.g. coupling, reduce it. Connecting transitions to housing stock availability can offer deeper insight into the mechanisms by which household maintainer rates differ across regions, which can sometimes work in opposing ways. For example, Calgary has a lower rates of young adults living with their parents compared to Montreal, but a higher rate of young adults living as roommates. While these details are secondary to linking population to households, they offer important insights into the mechanisms behind household formation in different regions in Canada.\nAnother possibly fruitful angle of attack to identify the CMA effects in a more robust way in the face of our models getting entangled in complex revealed (constrained) preferences is to explicitly introduce prevailing levels of rent as a mediator between CMA and household maintainer rates and try to identify the effect mediated through rents.\nWe will explore some of this in a future blog post.\nAs usual, the code for this post is available on GitHub for anyone to replicate or adapt for their own purposes."
  },
  {
    "objectID": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#pumf-data",
    "href": "posts/2022-05-06-estimating-suppressed-household-formation/index.html#pumf-data",
    "title": "Estimating Suppressed Household Formation",
    "section": "PUMF data",
    "text": "PUMF data\nPublic use micro data (PUMF) is a (roughly) 1:40 representative subsample of individual level census responses, somewhat altered to preserve anonymity. This allows to generate flexible CMA level cross tabulations that we will need for more complex models, like the one including Culture. It is a good idea to do a cross-check to see how well the PUMF data lines up with the cross tabulation when just accounting for age to get a better understanding of the uncertainties and challenges when working with PUMF data.\n\nTo get a feel for how PUMF data compares to the full (25%) census data we compare the age-adjusted household maintainer rates by CMA estimated from the two different sources.\n\nGenerally they line up well. Here we pooled the 15-17 and 17-19 age groups in the PUMF to match the ones in the cross tabulation and added in the estimate based on standard weights in red. We do not that especially with the smaller CMAs we see wider confidence intervals from the PUMF estimates and some deviation.\n\n\nReproducibility receipt\n\n## [1] \"2022-05-12 14:11:24 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [69a5324] 2022-05-12: sp, thanks Nathan\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cmhc_0.1.0         cancensus_0.5.1    statcanXtabs_0.1.1 forcats_0.5.1     \n##  [5] stringr_1.4.0      dplyr_1.0.8        purrr_0.3.4        readr_2.1.2       \n##  [9] tidyr_1.2.0        tibble_3.1.7       ggplot2_3.3.6      tidyverse_1.3.1   \n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.8.0           assertthat_0.2.1         \n##  [3] digest_0.6.29             utf8_1.2.2               \n##  [5] R6_2.5.1                  cellranger_1.1.0         \n##  [7] backports_1.4.1           reprex_2.0.1             \n##  [9] evaluate_0.15             httr_1.4.2               \n## [11] blogdown_1.9              pillar_1.7.0             \n## [13] rlang_1.0.2               curl_4.3.2               \n## [15] readxl_1.4.0              rstudioapi_0.13          \n## [17] jquerylib_0.1.4           rmarkdown_2.13           \n## [19] bit_4.0.4                 munsell_0.5.0            \n## [21] broom_0.8.0               compiler_4.2.0           \n## [23] modelr_0.1.8              xfun_0.30                \n## [25] pkgconfig_2.0.3           htmltools_0.5.2          \n## [27] tidyselect_1.1.2          bookdown_0.26            \n## [29] fansi_1.0.3               crayon_1.5.1             \n## [31] tzdb_0.3.0                dbplyr_2.1.1             \n## [33] withr_2.5.0               grid_4.2.0               \n## [35] jsonlite_1.8.0            gtable_0.3.0             \n## [37] lifecycle_1.0.1           DBI_1.1.2                \n## [39] git2r_0.30.1              magrittr_2.0.3           \n## [41] scales_1.2.0              vroom_1.5.7              \n## [43] cli_3.3.0                 stringi_1.7.6            \n## [45] fs_1.5.2                  xml2_1.3.3               \n## [47] bslib_0.3.1               ellipsis_0.3.2           \n## [49] generics_0.1.2            vctrs_0.4.1              \n## [51] tools_4.2.0               mountainmathHelpers_0.1.4\n## [53] bit64_4.0.5               glue_1.6.2               \n## [55] hms_1.1.1                 parallel_4.2.0           \n## [57] fastmap_1.1.0             yaml_2.3.5               \n## [59] colorspace_2.0-3          rvest_1.0.2              \n## [61] knitr_1.38                haven_2.5.0              \n## [63] sass_0.4.1"
  },
  {
    "objectID": "posts/2022-03-29-what-s-up-with-squamish/index.html",
    "href": "posts/2022-03-29-what-s-up-with-squamish/index.html",
    "title": "What’s up with Squamish?",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nIn our previous post we have outlined the broad problems with the recent UBCM report, in this post we return to one particular one, the comparison of dwelling growth to population growth for “BC Major Census Metropolitan Areas” (Figure 2 in the report), paying particular attention to Squamish as the largest outlier. To start out, let’s take a comprehensive look at how dwelling and population growth play out across BC’s CMAs and CAs.\nWe see a strong correspondence between dwelling and population growth as one would expect. The UBCM report picked out the Major CMAs Vancouver and Victoria, and with Kelowna the next largest one. But after that UBCM breaks with both “major” (in size they are rank 8 and 18 among BC CMA/CAs, respectively) and “census metropolitan area” and includes two census agglomerations, Prince George and Squamish.\nSo why then were these census agglomerations added to this graph? What they lack in being “major” they make up for by having a large difference between population and dwelling growth (rank 4 and 1, respectively, with Dawson Creek (in a twisted way where both population and dwelling growth were negative) and Quesnel taking spots 2 and 3). In other words, this graph is the result of cherry picking, not of data analysis. And cherry-picking is unfortunately, and to much embarrassment for UCBM, a theme in the report.\nBut let’s ignore this for now and check what’s going on with Squamish, the CMA/CA with the largest difference between population and dwelling growth in BC. Squamish is also interesting for other reasons, it abuts BC’s largest CMA (Vancouver) and it’s (private) dwelling stock grew by an astonishing 30%, almost double that of the second (Kelowna) and triple of the third (Chilliwack) and far outpacing Victoria and Vancouver who rank 7th and 10th in dwelling growth among BC’s CMA/CAs.\nBefore we get started, we should remember that comparing population to dwelling growth does not tell us much about whether this is a problem or not, this stat is more a hallmark of shallow data BS than analysis. These two stats can diverge for three main of reasons, change in household size, change in dwellings not occupied by usual residents, and change in population not in private dwellings. This last one is simply a result of a mismatch of units. Population is comprised of population in private dwellings plus the population in collective dwellings, but we should only compare the population in private dwellings to the private dwelling stock, not the total population. The population in collective dwellings is typically quite small and for larger regions like a CMA or CA this last factor won’t matter much (though it can matter quite a bit on the local level, especially around, e.g. universities). Data on the population in private dwellings in 2021 is not yet available, but we can start to get a glimpse of the other two factors.\nDeclining household sizes is a long-standing trend in Canada and across almost all countries, and it naturally leads to dwelling growth outstripping population growth. This means that just to keep the population stable we need to add housing, and in areas that don’t add (much) housing, for example much of the West Side of the City of Vancouver, the population generally declines. Unless we want to draw upon fairly authoritarian policies it is hard to see what can be done about declining household sizes or why this would be seen as a problem.\nThe remaining issue, an increase in dwellings not occupied by usual residents, could potentially be problematic, so this is worth looking into in detail. An increase in vacancies could indeed indicate “empty housing,” maybe abandoned, maybe used only for seasonal use or short-term rentals, or maybe left empty for other reasons.\nTo understand how population and dwelling change relates our interactive Canada-wide map of Components of Population Change decomposes the change in population for each region into three components: change in population due to change in dwelling units, change of population due to change in household size, and change of population due to change in dwellings not occupied by usual residents. Looking at Squamish we see that the population changed by 30.3% due to the change in dwellings (matching the dwelling growth by definition), but declined by 4.4% due to a decrease in household size and declined by 4.1% due to an increase in dwelling units not occupied by usual residents.\nSo what’s going on with the dwellings not occupied by usual residents? In Squamish CA it increased from 325 (4% of dwelling units) to 725 (7% of dwelling units).\nLet’s take a look at the change in number of homes not occupied by usual residents at fine geographies based on dissemination blocks to understand where they are and get an idea of what is going on there.\nMost blocks saw small increases, some saw small decreases, but one particular area saw large increases in dwellings not occupied by usual residents. Let’s zoom in for a better view.\nThe change seems concentrated in area in the northeast portion of Garibaldi Highlands, an area built up before 2016, this change is not due to new construction like we have seen in parts of Vancouver with large change in the number of dwellings not occupied by usual residents. Scrolling through the area in Google Street View it does not look particularly “empty”. It’s mostly single family homes.\nPicking out the worst offenders in the region, here is a side by side comparison of the share of dwelling units not occupied by usual residents.\nBut there is something else funny going on, this area gained a lot of dwelling units without construction to account for this.\nThis looks like a reclassification issue, similar to what we have seen in e.g. the City of Vancouver, and what has tripped up lots of people before who have claimed that we have built enough housing based on sloppy use of census data. What most likely happened in this case is that the Census got better at locating secondary suites in Squamish, resulting in an increase in housing units without new construction. And we know well by now that secondary suites are the form of housing with the highest rates of being “unoccupied”, mostly because owners may choose to not rent them out and keep them for their own use. To confirm this theory we will have to wait until the next census data release when we learn about the structural type of housing to see if there has indeed been a shift away from “single-detached” toward “duplex” housing (the census classifies a single family home with a suite as two “duplex” units).\nIt’s probably no coincidence that the UBCM report cites a prime example of such misguided analysis, apparently unaware that John Rose’s Housing Supply Myth, Working Paper, Version 1 is deeply flawed to the extent that the conclusions are wrong. Overall, Census users should always read the fine print for methods changes when carrying out comparisons over time, and should also remain mindful that the Census cares a lot more about undercounting people than they do about overcounting dwellings (they primarily keep track of dwellings as a way to find people). As a result, dwelling data should be interpreted with some care."
  },
  {
    "objectID": "posts/2022-03-29-what-s-up-with-squamish/index.html#upshot",
    "href": "posts/2022-03-29-what-s-up-with-squamish/index.html#upshot",
    "title": "What’s up with Squamish?",
    "section": "Upshot",
    "text": "Upshot\nWhat does the cherry-picked stat about Squamish’s dwelling growth exceeding population growth tell us? Nothing, except once again that the authors of the report don’t understand housing.\nAs usual, the code for this post is available on GitHub for anyone to replicate or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2022-04-26 16:13:39 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [ac5107a] 2022-04-05: fix link to github code\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 tongfen_0.3.5            \n##  [3] cancensus_0.5.0           forcats_0.5.1            \n##  [5] stringr_1.4.0             dplyr_1.0.8              \n##  [7] purrr_0.3.4               readr_2.1.2              \n##  [9] tidyr_1.2.0               tibble_3.1.6             \n## [11] ggplot2_3.3.5             tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8.3       lubridate_1.8.0    class_7.3-20       assertthat_0.2.1  \n##  [5] digest_0.6.29      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  \n##  [9] backports_1.4.1    reprex_2.0.1       e1071_1.7-9        evaluate_0.15     \n## [13] httr_1.4.2         blogdown_1.9       pillar_1.7.0       rlang_1.0.2       \n## [17] readxl_1.4.0       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.13    \n## [21] munsell_0.5.0      proxy_0.4-26       broom_0.8.0        compiler_4.2.0    \n## [25] modelr_0.1.8       xfun_0.30          pkgconfig_2.0.3    htmltools_0.5.2   \n## [29] tidyselect_1.1.2   bookdown_0.26      fansi_1.0.3        crayon_1.5.1      \n## [33] tzdb_0.3.0         dbplyr_2.1.1       withr_2.5.0        sf_1.0-7          \n## [37] grid_4.2.0         jsonlite_1.8.0     gtable_0.3.0       lifecycle_1.0.1   \n## [41] DBI_1.1.2          git2r_0.30.1       magrittr_2.0.3     units_0.8-0       \n## [45] scales_1.2.0       KernSmooth_2.23-20 cli_3.3.0          stringi_1.7.6     \n## [49] fs_1.5.2           xml2_1.3.3         bslib_0.3.1        ellipsis_0.3.2    \n## [53] generics_0.1.2     vctrs_0.4.1        tools_4.2.0        glue_1.6.2        \n## [57] hms_1.1.1          fastmap_1.1.0      yaml_2.3.5         colorspace_2.0-3  \n## [61] classInt_0.4-3     rvest_1.0.2        knitr_1.38         haven_2.5.0       \n## [65] sass_0.4.1"
  },
  {
    "objectID": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html",
    "href": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html",
    "title": "Vacancy rates and rent change, 2021 update",
    "section": "",
    "text": "The new CMHC Rms data is out today, and it’s been three years since we did our post on vacancy rates and rent change. That post still gets a lot of views, so maybe it’s a good time for an update.\nThe Rms survey is carried out in October, and the results used to come out in the following month. Checking the date of our post from three years ago it was written on November 28th reporting on the Rms from October 2018. But last year we had to wait until January to get the data, and this year it came out in February. The Rms provides important data that informs a lot of decisions, and I am having a hard time understanding why this can’t be delivered in a more timely fashion.\nAdditionally, CMHC again failed to release the Rms data on their Housing Market Information Portal until later in the day, the only organized way CMHC data can get accessed, but scattered it across a bunch of multi-line header Excel files. It’s 2022 and apparently CMHC still thinks that xlsx is an acceptable data release format. It is not, and I am scared to think what this says about CMHC’s internal data analysis practices."
  },
  {
    "objectID": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html#rental-stock",
    "href": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html#rental-stock",
    "title": "Vacancy rates and rent change, 2021 update",
    "section": "Rental stock",
    "text": "Rental stock\nLet’s start out with a look at the total purpose-built rental stock across Canadian Cities.\n\nThose CMAs are quite different in size though, let’s look at the portion of dwelling units made up of purpose-built rental, using the fresh 2021 census data as a base.\n\nThat paints quite a different picture. Montreal still features prominently, but Toronto Vancouver don’t make it into the top 20. For reference, here is the share of dwelling units that are purpose-built rental in the regions with the highest total number of purpose-built rental units.\n\nTime for the main act. Our main interest is the Rms vacancy rate, the share of apartment units vacant at the time of the survey and not rented out. US readers should note that this differs from the definitions used in the US where a unit will also be considered vacant if it’s rented but the renter has not moved in yet or has already moved out. We will compare this to the fixed sample rent change, the the change in average rent for a common set of units across two consecutive years. This can be thought of as some sort of Case-Shiller index for rentals, it is not skewed by addition of new stock or changes in composition of rental units. One important consideration is rent control. Rent control legislation varies across Canada and across time, with many provinces having implemented rent increase moratoriums during COVID times. And correspondingly the ability of landlords to increase rents of existing renters also varies, although rents are free to change arbitrarily after turnover.\nThis is essentially a redo of the graph from our post three years ago, with three more years of data and also added data points for the April survey for the years where CMHC surveyed units twice a year. We break the time series for years where there was no reliable estimate for a period longer than one year.\n Generally it is well-understood that vacancy rates drive rent changes. Theory says it should and the correlations are strong. The reverse, assuming that increasing rents lowers the vacancy rate is absurd. But in theory there could be hidden confounders, and I still sometimes see people look at these correlations and refuse to see a causal picture, because “economics 101 does not apply to housing” or some other obscure reason. Unfortunately I haven’t seen anyone try to explain what could confound these estimates, other than some vague reference to “greedy landlords”, “financialization” or “commodification” of housing. This view proposes a that landlord behaviour is a hidden confounder that drives rent changes. Calgary or Edmonton provide good case studies because of the lack of rental control, relatively elastic housing supply, and strong economic shocks.\n\nIt is difficult to take this seriously unless people can provide a good explanations of why landlords go through these changes. And the mechanism how this would impact vacancy rates also remains unexplained."
  },
  {
    "objectID": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html#the-rest-of-the-rental-supply",
    "href": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html#the-rest-of-the-rental-supply",
    "title": "Vacancy rates and rent change, 2021 update",
    "section": "The rest of the rental supply",
    "text": "The rest of the rental supply\nSo far we have only talked about purpose-built market rentals. The main reason for this is that we have longer time lines and better quality data for purpose-built rental, and the idea is that purpose-built and “artisanal rentals” are fungible, so there should not be too much of a different. But in e.g. Vancouver that’s only a small fraction of the rental market. A question that gets brought up a lot is how representative is this really? Can we take the purpose-built rental market as a good barometer of the overall rental market? Theory says so, but it’s good to check the data. CMCH also survey the condo market vacancy rate, but data quality is much poorer.\nLet’s compare the purpose-built vacancy rate to the condo vacancy rate.\n\nOverall they line up reasonably well. The condo vacancy rate is much more noisy, but it does capture the general ups and downs of the purpose-built rental vacancy rate. We shoud also note that the condo supply generally differs significantly from the purpose-built rental supply, it tends to be newer and even when controlling for age condos tend to be higher quality. On the other hand, they come with reduced security of tenure."
  },
  {
    "objectID": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html#regional-variation-in-vacancy-rates",
    "href": "posts/2022-02-18-vacancy-rates-and-rent-change-2021-update/index.html#regional-variation-in-vacancy-rates",
    "title": "Vacancy rates and rent change, 2021 update",
    "section": "Regional variation in vacancy rates",
    "text": "Regional variation in vacancy rates\nLastly we want to take a quick look at the regional variation in Vacancy rates.\n This pretty much puts us back to pre-pandemic vacancy rates, with Port Moody being an outlier and not representative for rents in the survey zone as the next graph shows.\nWe can take it down to finer geographies, although data gets more wonky and prone to effects of e.g. new rental buildings coming online during the survey month. The CMHC Survey Zones might strike a good balance between geographic resolution in the central areas and accuracy.\n\nAs always, the code for the analysis is available on GitHub for anyone to reproduce, adapt or appropriate for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2022-02-18 16:58:33 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [d9602a5] 2022-02-18: census quirks\n## R version 4.1.2 (2021-11-01)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.2.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.3 cancensus_0.5.0          \n##  [3] cansim_0.3.10             cmhc_0.1.0               \n##  [5] forcats_0.5.1             stringr_1.4.0            \n##  [7] dplyr_1.0.8               purrr_0.3.4              \n##  [9] readr_2.1.1               tidyr_1.2.0              \n## [11] tibble_3.1.6              ggplot2_3.3.5            \n## [13] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8       lubridate_1.8.0  assertthat_0.2.1 digest_0.6.29   \n##  [5] utf8_1.2.2       R6_2.5.1         cellranger_1.1.0 backports_1.4.0 \n##  [9] reprex_2.0.1     evaluate_0.14    highr_0.9        httr_1.4.2      \n## [13] blogdown_1.6     pillar_1.6.4     rlang_1.0.1      readxl_1.3.1    \n## [17] rstudioapi_0.13  jquerylib_0.1.4  rmarkdown_2.11   munsell_0.5.0   \n## [21] broom_0.7.12     compiler_4.1.2   modelr_0.1.8     xfun_0.28       \n## [25] pkgconfig_2.0.3  htmltools_0.5.2  tidyselect_1.1.1 bookdown_0.24   \n## [29] codetools_0.2-18 fansi_1.0.2      crayon_1.4.2     tzdb_0.2.0      \n## [33] dbplyr_2.1.1     withr_2.4.3      grid_4.1.2       jsonlite_1.7.3  \n## [37] gtable_0.3.0     lifecycle_1.0.1  DBI_1.1.2        git2r_0.29.0    \n## [41] magrittr_2.0.1   scales_1.1.1     cli_3.1.0        stringi_1.7.6   \n## [45] farver_2.1.0     fs_1.5.1         xml2_1.3.3       bslib_0.3.1     \n## [49] ellipsis_0.3.2   generics_0.1.1   vctrs_0.3.8      tools_4.1.2     \n## [53] glue_1.6.1       hms_1.1.1        fastmap_1.1.0    yaml_2.2.1      \n## [57] colorspace_2.0-2 rvest_1.0.2      knitr_1.36       haven_2.4.3     \n## [61] sass_0.4.0"
  },
  {
    "objectID": "posts/2022-02-15-deadbeat-neighbourhoods/index.html",
    "href": "posts/2022-02-15-deadbeat-neighbourhoods/index.html",
    "title": "Deadbeat Neighbourhoods",
    "section": "",
    "text": "How much have City of Vancouver neighbourhoods changed 2016-2021? We have our interactive Canada-wide population change map on CensusMapper showing 2016-2021 population change down to the census tract level, and we have looked at finer geography population change using TongFen. But sometimes we don’t want maps but just a list of how city neighbourhoods changed.\nThe city pulls a custom tabulation for city neighbourhood geographies for every census, but that will still take more than year until that arrives. Until then we can use TongFen to get good estimates, just like what we did for our deadbeat zoning post to go down to block level and build a least common denominator geography for the 2016 and 2021 based on the respective dissemination blocks. Then we can categorize the geographies by which neighbourhood they (mostly) fall in. Strictly speaking we are going to look at the City of Vancouver together with Musqueam 2, which is part of the Dunbar-Southlands neighbourhood.\nLet’s take a look what that matchup between the (harmonized) dissemination blocks and the city neighbourhoods looks like.\nOverall the match is good, with the exception of a small areas of Kerrisdale bleeding into Dunbar-Southlands in this classification. We will ignore this for this post. Part of the advantage of creating the unified geography is that the population in that area will be consistently classified as Dunbar-Southlands for both censuses."
  },
  {
    "objectID": "posts/2022-02-15-deadbeat-neighbourhoods/index.html#absolute-population-change",
    "href": "posts/2022-02-15-deadbeat-neighbourhoods/index.html#absolute-population-change",
    "title": "Deadbeat Neighbourhoods",
    "section": "Absolute population change",
    "text": "Absolute population change\nFirst, let’s look at absolute population change. Which neighbourhood added the most people, and which added the least.\n\nIt’s probably not a surprise that Downtown, Mount Pleasant, and Marpole lead the pack. That’s where a lot of the development in this city has been concentrated over this cycle. On the other end of the spectrum we see neighbourhoods with population drops, with the “No Towers” neighbourhood of Grandview-Woodland seeing the highest population drop. In total, the census counted 28 more dwelling units in 2021 compared to 2016 in Grandview-Woodland, not nearly enough to make up for other demographic factors like declining household size.\nWe don’t have income data yet, but it will be interesting to see the effects of the gentrification pressure on Grandview Woodland resulting from this lack of adding housing. Looking at census tract level taxfiler data that takes us up to 2017 Grandview-Woodland stands out as one of the areas with the highest drop in low-income population and large increase in median income."
  },
  {
    "objectID": "posts/2022-02-15-deadbeat-neighbourhoods/index.html#relative-population-change",
    "href": "posts/2022-02-15-deadbeat-neighbourhoods/index.html#relative-population-change",
    "title": "Deadbeat Neighbourhoods",
    "section": "Relative population change",
    "text": "Relative population change\nNot all neighbourhoods are the same (population) size, so growth by the same number of people can feel different in different neighbourhoods. Let’s look at relative population change by neighbourhood.\n\nThat reshuffles things a little, with South Cambie rising to the top where densification along the Cambie Corridor has added significant population to the otherwise fairly low-population neighbourhood. At the bottom end Shaughnessey takes the lead, due to its already quite small starting population.\nWhen comparing to the city average growth rate there have to be neighbourhoods that punch above their weight and neighbourhoods that fall below the average. For the next round we should pay special attention to those neighbourhoods that have fallen behind, and especially to the deadbeat neighbourhoods that have gone the opposite direction and dropped population."
  },
  {
    "objectID": "posts/2022-02-15-deadbeat-neighbourhoods/index.html#components-of-population-change",
    "href": "posts/2022-02-15-deadbeat-neighbourhoods/index.html#components-of-population-change",
    "title": "Deadbeat Neighbourhoods",
    "section": "Components of population change",
    "text": "Components of population change\nOne useful way to slice the data is to break down the population change into three components:\n\npopulation change due to a (net) change in dwelling units,\npopulation change due to a change in household size,\npopulation change due to a change in dwellings unoccupied by usual residents.\n\nAt this point it’s a bit of a hack because we should be using population in private households instead of overall population, but that data is not yet available and the difference for larger areas like city neighbourhoods won’t be large and we will gloss over this detail. This gives a pretty good idea why population has changed.\n\nWe see that declining household size has been a major component in the population change in most areas. In particular the population loss in Victoria-Fraserview is entirely due to a drop in average household size in this period, while population increased due to increased dwelling stock and decrease in unoccupied dwellings. Grandview-Woodland really stands out with it’s dismal dwelling growth.\nWhat’s interesting is that the decrease in unoccupied dwellings, and associated increase in population, can be seen across almost all neighbourhoods, with the exception of the West End, Fairview and Grandview-Woodland. While it’s good to remember that this metric is difficult to interpret and outliers in some areas should be expected, it is consistent with the notion that the Empty Homes Tax and Speculation and Vacancy tax had some effect.\nWe also have an interactive Canada-wide map on CensusMapper showing these components of population change in each region down to the census tract level.\nAs usual, the code for this post, including the code to scrape the data out of the PDFs, is available on GitHub for anyone to reproduce or adapt."
  },
  {
    "objectID": "posts/2022-02-11-deadbeat-zoning/index.html",
    "href": "posts/2022-02-11-deadbeat-zoning/index.html",
    "title": "Deadbeat zoning",
    "section": "",
    "text": "With the first batch of data from the 2021 census we can start to answer some questions about how Vancouver has grown. One of these is how population growth relates to zoning as Gil Meslin reminded me today. It would be very useful to have a custom tabulation available for that, but it will still take a lot of time before 2021 custom tabulations will become available.\nIn the meantime, we can get a pretty good idea how low-density zoning has or has not contributed to Vancouver’s population growth by following a line of analysis like we did back when the 2016 data came out. The idea is quite simple, we are going to take block level population change data, and divided it into three categories. Block that lie entirely within low-density zones (“Core” blocks), blocks that have parts inside and parts outside of low-density zones (“Fringe” blocks), and blocks that are entirely outside of low-density zones (“Rest”). To refine this a little we will remove parks from the census data, and shrink the census blocks by 10m in all directions to avoid issues with boundaries along roads or other places being drawn slightly differently.\nOne minor complication is that we don’t have population change at the census block level, StatCan only provides back-calculated 2016 population counts on 2021 census tract geographies or higher. But not to despair, our {tongefen} package is made for just this application and it automates the creation of a common custom geography based on dissemination blocks from 2016 and 2021 where we can get counts from both censuses.\nhowever, nothing is perfect and during this process of taking out parks and messing with the geographies we are losing track of some. Most of these have zero population, but there are some caretaker residences in parks and some geocoding errors in StatCan Data like the strech of beach north of English Bay with a population of 15 people where clearly nobody lives. This kind of mixups happen. But overall regions that fall by the wayside in this analysis only account for 28 people in 2016 and 45 in 2021. Which we are happy to ignore for now.\nJust like we did last time around when we ran this analysis we are also going to look separately at low-density “Core” and “Fringe” areas on the West and East sides of the city.\nOne last question we have to answer is what version of zoning we should look at. We could take the areas zoned for low density housing, specifically the RS, RT and FSHCA districts, as of the starting point in 2016, or as of the endpoint in 2021. Taking the zoning at the end point would focus on population growth low-density zoning via organic growth within the zones, e.g. through addition of suites and laneways. Taking zoning as of the beginning of the period would additionally allow growth via rezonings.\nIdeally we would want to split specifically those rezoned areas, but our current data is not fine enough for this and this will require a custom tabulation.\nSo let’s take a look how this shakes out.\nThere is surprisingly little difference from the choice of vintage of the zoning fabric. Most of the difference will likely have been in the fringe for either of those two, just because that’s how we most of our rezonings have worked out, our rezonings focus on areas that are already adjacent to higher density zones.\nWhat stands out, just like last time we ran this analysis for the 2011-2016 period, that the Core areas don’t grow much. Low-density zoning is deadbeat zoning, they take up lots of space but don’t contribute to population growth. At least they are not losing population, adding suites and laneways manages to roughly balance out other demographic factors like declining household size. During the 2011-2016 cycle the core areas on the West side lost population and whereas the East side gained population. This time around it’s reversed. Either way, the effects were small and we chalk this up to statistical noise and there isn’t much of a difference.\nThere does seem to be a difference when it comes to fringe areas, which is likely due to the Cambie Corridor. The other big driver of growth in the fringe areas is C-2 zoning, which generally only extends half a block and will inevitably be caught in the fringe areas. We will need a custom tabulation to disentangle this.\nTo underline that point, let’s look at how much these various categories take up, after cutting out parks.\nOne issue here is that we are including agricultural zoning in Southlands in the “Rest” category, which inflates it’s area. Still, this shows what we already know, the deadbeat core low-density areas take up a huge portion of our city, and the low-density parts of the fringe will add to that. We can combine the data from these last to graphs to see how much population change per area we got for each of these categories.\nThis makes it very clear which land carries most of the weight when it comes to accommodating our population growth. Bottom line is that low density zoning does exactly what it was designed to do. Nothing. Unless it gets upzoned.\nOne last thing to do is a quick visual check which areas ended up in the Core, Fringe and Rest categories in relation to low-density zoning."
  },
  {
    "objectID": "posts/2022-02-11-deadbeat-zoning/index.html#update-feb-13th",
    "href": "posts/2022-02-11-deadbeat-zoning/index.html#update-feb-13th",
    "title": "Deadbeat zoning",
    "section": "Update (Feb 13th)",
    "text": "Update (Feb 13th)\nOf course we can ask how this pans out through other Metro Vancouver municipalities. Thanks to our UBC Socioology Zoning Project we can grab the data from our interactive metro wide zonig map and run this for all Metro Vancouver municipalities. We have investigated before how zoning impacts population growth, time to run it against 2021 data.\nAdditionally we will refine the above analysis by running a second version that includes agricultural zoning. This won’t really make much of a difference in Vancouver, but will matter for some other Metro Vancouver municipalities with large share of agricultural land. However, much of the agricultural zoning is in the flood plains, and the benefits of densifying in this area have to be carefully weighted against the risks. At some point we should come back to this with a more detailed analysis, for now we just run two versions, one with agricultural zoning included in “low density” areas, and one where it’s not included. This really only matters when looking at population change per hectare. (Update Fev 14: Additionally we cut out unzoned areas like roads, which removes some issues where people have been geocoded to highway medians but also dignificantly improves the match of census data to zoning data. However this also hints at data quality problems in the census that are likely to impact some of the analysis. We are removing Pitt Meadows from this analysis because of geocoding problems. A good reminder that census data is not always as accurate as we might light it to be, and visual inspection and ground-truthing of data is important.)\n\nThis paints a more varied picture of how growth has been allocated with respect to low-density zoning. Some municipalities, like Surrey and the District of Langley, do manage to add population in low-density zoning. We will need to look in more detail if this is happening due to greenfield development or densification, or due to increasing household sizes as initial naive data suggests. However it should not surprise that all municipalities are better at adding population to higher-density areas, especially when considering the Metro Vancouver Zoning Project zoning data that is pegged to summer 2021, so at the end of our 2016-2021 population change observation period.\nAgain, all these categories have different areas, and thus the impact of the population change can be quite different. To get a better view of this we can look at total population change per hectare of land in each of these categories, after removing schools and parks. This is where it will matter more if we count agrecultural zoning as part of the low-density areas or not.\n\nThis brings home the point that across the region most of the growth happens outside of low-density areas, especially if we include agricultural land in our low-density base.\nAs usual, the code for this post, including the code to scrape the data out of the PDFs, is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-05-11 08:35:12 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [f6dd25d] 2022-05-11: typo, thanks Bert!\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] sf_1.0-7                  mountainmathHelpers_0.1.4\n##  [3] VancouvR_0.1.7            tongfen_0.3.5            \n##  [5] forcats_0.5.1             stringr_1.4.0            \n##  [7] dplyr_1.0.8               purrr_0.3.4              \n##  [9] readr_2.1.2               tidyr_1.2.0              \n## [11] tibble_3.1.7              ggplot2_3.3.6            \n## [13] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8.3       lubridate_1.8.0    class_7.3-20       assertthat_0.2.1  \n##  [5] digest_0.6.29      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  \n##  [9] backports_1.4.1    reprex_2.0.1       e1071_1.7-9        evaluate_0.15     \n## [13] httr_1.4.2         blogdown_1.9       pillar_1.7.0       rlang_1.0.2       \n## [17] readxl_1.4.0       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.13    \n## [21] urltools_1.7.3     triebeard_0.3.0    munsell_0.5.0      proxy_0.4-26      \n## [25] broom_0.8.0        compiler_4.2.0     modelr_0.1.8       xfun_0.30         \n## [29] pkgconfig_2.0.3    htmltools_0.5.2    tidyselect_1.1.2   bookdown_0.26     \n## [33] fansi_1.0.3        crayon_1.5.1       tzdb_0.3.0         dbplyr_2.1.1      \n## [37] withr_2.5.0        grid_4.2.0         jsonlite_1.8.0     gtable_0.3.0      \n## [41] lifecycle_1.0.1    DBI_1.1.2          git2r_0.30.1       magrittr_2.0.3    \n## [45] units_0.8-0        scales_1.2.0       KernSmooth_2.23-20 cli_3.3.0         \n## [49] stringi_1.7.6      fs_1.5.2           xml2_1.3.3         bslib_0.3.1       \n## [53] ellipsis_0.3.2     generics_0.1.2     vctrs_0.4.1        tools_4.2.0       \n## [57] glue_1.6.2         hms_1.1.1          fastmap_1.1.0      yaml_2.3.5        \n## [61] colorspace_2.0-3   classInt_0.4-3     rvest_1.0.2        knitr_1.38        \n## [65] haven_2.5.0        sass_0.4.1"
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html",
    "title": "No shortage in Housing BS",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nSay you built a bunch of housing in a cornfield in the middle of rural Iowa. Would people come to live in it? Maybe. But probably not. Let’s imagine the same scenario scooted over to Vancouver. The conditions for our little field of dreams have changed. Here we’re pretty comfortable predicting: if you build it, they will come. Housing limits population growth here in a way it does not in rural Iowa.\nBut maybe we’re wrong?\nThere is a special brain worm making the rounds that we somehow built a bunch of housing in Vancouver and then no one came to live in it. This despite decades of extraordinary low rental vacancy rates, and successive reports from an Empty Homes Tax in place since 2017 and a Vacancy Tax since 2018, both of which consistently demonstrate very few empty dwellings.\nSo how would one, if one were so inclined, try to drum up support for such a position? Here is a good example:\nThe claim is that during 10 years roughly 55 thousand new units were built in the City of Vancouver, while the population grew by roughly 55 thousand people. Taking roughly 2 people per unit, that means we got twice as many units as we needed. That means, so the argument goes, that supply is double the demand, but prices still doubled. So this clearly shows that supply and demand don’t matter when it comes to housing, right?\nNo. This is a load of data bullshit that has been unleashed upon us. But if you bear with us, we will take this data bullshit as fertilizer and endeavour to grow from it something more useful."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#tldr",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#tldr",
    "title": "No shortage in Housing BS",
    "section": "TL;DR",
    "text": "TL;DR\nFor the purpose of a quick summary, here are all the elements that have gone into the data bullshit:\n\nThe mixing of two different time frames, years 2010 through 2019 for new buildings and between the 2006 and 2016 censuses for population change.\nThe use of building starts, when no one can live in a start. If anything, the relevant metric should be completions.\nThe overlooking of demolitions, when no one can live a demolished unit.\nThe ignorance of declining household size, necessitating additional units even just to keep population constant.\nThe neglect of fluff. There needs to be some extra fluff in the housing market. If there aren’t any extra vacant units it becomes logistically impossible for people to move. (And the more differentiated the housing market is, the more vacant units are needed.)\nThe equating of population growth with demand. Population growth reflects only the part of the demand that managed to come to or stay in Vancouver, but it ignores the money they express their demand through as well as all the people that have been pushed out by not having enough of it."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#population-growth-of-55-thousand",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#population-growth-of-55-thousand",
    "title": "No shortage in Housing BS",
    "section": "Population growth of 55 thousand",
    "text": "Population growth of 55 thousand\nThis stat is pretty simple, and the tweet’s author, Patrick Condon (Prof. of Landscape Architecture and all-around commentator-at-large), vaguely points to a City of Vancouver report, which cites census population counts, which has the City of Vancouver population changing by 53,445 between the 2006 and 2016 censuses."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#thousand-new-homes",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#thousand-new-homes",
    "title": "No shortage in Housing BS",
    "section": "55 thousand new homes",
    "text": "55 thousand new homes\nThis numbers is sourced from a PDF posted on BC government website detailing aggregate annual housing starts for BC cities for the ten years 2010 through 2019 These are just taken from the CMHC Scss, which makes it easier to get to the raw numbers.\n\nAggregating to annual starts we see the sharp dip during the crash in 2009 and the bumper year in 2016 in response to rising prices. For reference we shade the ten years 2010 through 2019 that Condon picked out and aggregate the counts."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#starts-vs-completions",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#starts-vs-completions",
    "title": "No shortage in Housing BS",
    "section": "Starts vs Completions",
    "text": "Starts vs Completions\nStarts are a poor metric for comparing to population growth, nobody can live in a building start. Let move on to look at completions, which is a much better metric for this and is readily available.\n\nSo we are left with about 50k newly constructed units."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#matching-timeframes",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#matching-timeframes",
    "title": "No shortage in Housing BS",
    "section": "Matching timeframes",
    "text": "Matching timeframes\nBut the time frame is off. Condon was using population numbers between 2006 and 2016 censuses, we should look at completions in that timeframe. We can think about what month we want to break the time series, but that won’t change the numbers much. For simplicity we just take completions for the years 2006 through 2015.\n\nFor that timeframe the total number of completions was around 42k. Breaking it out by type of dwelling we also notice that around 8 thousand of those completions were single detached, many of which which are likely one-for-one replacements. This points the issue that completions isn’t really the right metric to use to compare against population growth, we need to account for demolitions and take net new units."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#gross-vs-net-new-units",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#gross-vs-net-new-units",
    "title": "No shortage in Housing BS",
    "section": "Gross vs net new units",
    "text": "Gross vs net new units\nUnfortunately we don’t have good data on demolitions, but there is some and we can fold that in.\n\nThis leaves us with 33,217 completions net of demolitions. This is only a rough estimate, demolitions are prone to under-counting, and this does not account for conversions or de-conversions.\nWe can plot the quotient of demolitions to completions to better understand how these two relate.\n\nThis tells us that as a heuristic for the City of Vancouver we should expect about 20% of completions to replace demolitions, and 80% of gross completions amounting to net new dwellings."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#just-use-the-census-for-both-population-and-dwellings-counts",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#just-use-the-census-for-both-population-and-dwellings-counts",
    "title": "No shortage in Housing BS",
    "section": "Just use the census for both population and dwellings counts",
    "text": "Just use the census for both population and dwellings counts\nUltimately we should probably just go back to the census, which has estimates for dwelling units for exactly the timeframe we need. The census counted 35,614 more private dwelling units in 2016 than in 2006. This differed from our above estimate mostly because of problems with tracking demolitions, but also with failure to account for conversions and deconversions, as well as re-classifications of dwelling units in the census."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#relating-population-to-dwelling-change",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#relating-population-to-dwelling-change",
    "title": "No shortage in Housing BS",
    "section": "Relating population to dwelling change",
    "text": "Relating population to dwelling change\nNow that we have net counts of population and dwellings, we need to combine these two. The coupling depends on how people group up into households. For this kind of analysis it suffices to just look at the average household size, so the total population divided by the total number of households. But some dwellings are (necessarily) unoccupied as explained up top, so this gets complicated. We will have to take this slowly.\nThe above numbers leaves us naively with a net ratio of 1.5 net new people per net new dwelling unit. But what does that mean?\nBefore we move on, there is another problem. The census, as well as the construction numbers we have been using, only counts private dwellings, which excludes things like nursing homes. Which means we should not use the total population change, but only the change of population in private dwellings, which was 49,225 and brings our ratio to 1.38. (We will ignore the rabbit hole with dwellings having been re-classified from private to collective between these censuses.)\nBut what does this number mean? The average households size in the City of Vancouver in 2006 was 2.25, so have we been building too many dwellings? Not really. The average household size dropped to 2.18 by 2016. Which may not seem like much of a difference until one realizes that this change in household size means that if we had not built any extra dwelling we would have seen a population loss of 17,631! This effect can be seen by looking at longer census timelines at fine geography, where for example much of the West Side of Vancouver has lost population. That alone increases the population we effectively need to accommodate by new buildings by 36%.\nWe can take this calculus a step further and break up the change in population into three factors:\n\npopulation change due to a (net) change in dwelling units,\npopulation change due to a change in household size,\npopulation change due to a change in dwellings unoccupied by usual residents.\n\nThe latter of which can be split up into a change in dwellings that are split further into a change in dwellings that are occupied by temporarily present residents, for example students or seasonal workers, or change in truly unoccupied dwellings. We also made an interactive map to visualize these components separately for the 2011 to 2016 timeframe.\nThis accounting identity allows us to better understand why population changed.\n\nIf household size and share of unoccupied dwellings had remained constant, then Vancouver would have gained almost 75k people over this timeframe. But household size shrank, leading to a loss of 20k people. The share of dwelling units not occupied by usual residents also increased, leading to an additional loss of 5k people."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#focusing-in-on-dwellings-not-occupied-by-usual-residents",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#focusing-in-on-dwellings-not-occupied-by-usual-residents",
    "title": "No shortage in Housing BS",
    "section": "Focusing in on dwellings not occupied by usual residents",
    "text": "Focusing in on dwellings not occupied by usual residents\nSo what all of this comes down to is the population loss due to an increase in the rate of dwelling units not occupied by usual residents. Now we at least have the numbers right, but this still requires some thoughtful analysis as we have learned from the misinterpretations past.\nLet’s see what’s going on there.\n\nThis gives an accounting of the change in the total number of units not occupied by usual residents. Most of these are “duplex” units, which as we know maps onto units in single family homes where the census has found a suite at some point in time, and suites are the most empty form of housing in Vancouver. And some of that addition is simply due to the census updating its methods to discover more suites between years, rather than owners becoming less willing to rent them out.\nThe second largest category is highrise apartments. But that’s also the form of housing that we have been adding the most of, so it’s hard to interpret this, as it is with the suites. To get a better sense we can see if the share of units not occupied by usual residents has changed between those two censuses.\n\nLooking at the percentage point change we see that “duplex”, as well as “single detached” homes, have seen increases in the share of homes not occupied by usual residents. All other forms of housing have seen decreases. Which also underlines that housing is a system, and it’s not necessarily the new homes that end up being unoccupied (by usual residents), but could well be existing homes. And many of these suites in “duplexes” have probably just been re-absorbed for use by their owners rather than sitting empty."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#population-growth-vs-demand",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#population-growth-vs-demand",
    "title": "No shortage in Housing BS",
    "section": "Population growth vs demand",
    "text": "Population growth vs demand\nThe last point left is the equating of population growth with demand. In its balancing act with supply, demand contributes to prices. But we don’t (and really can’t) measure demand directly. Demand reflects more than simply population growth. It’s also dependent upon the ease of financing, as well as wealth and incomes of the population here now. But - and this is key - there’s still more to demand. There are people who continue to move to Vancouver, bringing their own wealth and income. We anticipate even more of them would come if prices and rents were lower. And there are others who invest in Vancouver, in part because prices and rents keep going up. All this works itself out through markets, matching supply to demand in setting prices.\nPrices, as Condon points out, are very high. And if you think that prices are too high (we definitely do), you should certainly advocate for more social housing (we definitely do). You can also advocate for higher property taxes that would cut into investment profits (we certainly do). But if you also want housing to remain accessible via markets, you have to take demand seriously, and not think you’re going to catch it all by the handy shortcut of looking at past population growth. Maybe you don’t care so much about the abstract vastness of demand, and you just want to make sure the housing we have gets used for housing? Given concerns about all the stuff investors might get up to, that seems reasonable. Try putting in place an Empty Homes Tax like we have here in Vancouver! Where we know very few homes remain empty for long.\nGiven what we know about supply and demand and the lack of empty homes in Vancouver, there’s no good path to lower prices that doesn’t run through adding a lot more housing. There are still bad paths, of course. We could make Vancouver a much less desirable place to live by, say, plunking down an enormous chicken factory farm in Shaughnessy. But that’s a topic for a different post."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#but-we-have-been-building-a-lot-right",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#but-we-have-been-building-a-lot-right",
    "title": "No shortage in Housing BS",
    "section": "But we have been building a lot, right?",
    "text": "But we have been building a lot, right?\nHave we been building a lot? Well, not really. It’s worth taking a step back to look at all of Metro Vancouver over longer timelines to put our current construction into context. The Vancouver of the 70s, or even earlier, was very different from the Vancouver of today, so we normalize by population.\n This shows that while construction’s gone up within the last decade, in historical context we aren’t building nearly as much as we did in decades past. The last year per-capita housing completions have been above the long-run average was 1996. And this is before accounting for demolitions, the rate of which has likely increased over time as we are running out of greenfield and brownfield development sites."
  },
  {
    "objectID": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#upshot",
    "href": "posts/2022-01-31-no-shortage-in-housing-bs/index.html#upshot",
    "title": "No shortage in Housing BS",
    "section": "Upshot",
    "text": "Upshot\nWhen we built housing in Vancouver, people came to live in it. We knew this already because hardly any of our housing is empty. So it is likely to continue for the foreseeable future: if we build it, they will come. Indeed, for the most part they’re already here.\nBut maybe it’s still worth wading through data bullshit suggesting otherwise, if only to fertilize more careful analysis. Hopefully we’ve managed to explain some of the complexities that render overly simplistic approaches to comparing dwelling growth to population growth useless. Given the many intersecting processes at work, it’s tricky to figure out how people sort themselves into homes, even when using correct numbers and not some diffusely assembled non-matching numbers.\nIf you take affordability and human hardship seriously, there is no question that we have a housing shortage. All data clearly point to this. Prominent commentators like Patrick Condon are now on the record attacking Housing Minister David Eby for taking this data seriously. Indeed, his tweet links back to his Tyee article with exactly this theme. But these attacks are largely based on data bullshit strung together with a bunch of faulty and rather idiosyncratic logic. When faced with this kind of housing shortage denial, coupled with demands we pause for a bespoke rethink of all our working models, we recommend the Eby response.\n\n“Well, I’ll be sure to mention that to all the people sleeping in their cars, and lining up to find rental units; that we are going to study the problem more.”\n\nAs usual, the code for this post, including the code to scrape the data out of the PDFs, is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-01-31 18:21:01 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [4108e69] 2022-02-01: minor edits\n## R version 4.1.2 (2021-11-01)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cansim_0.3.10   cmhc_0.1.0      cancensus_0.4.8 forcats_0.5.1  \n##  [5] stringr_1.4.0   dplyr_1.0.7     purrr_0.3.4     readr_2.1.1    \n##  [9] tidyr_1.1.4     tibble_3.1.6    ggplot2_3.3.5   tidyverse_1.3.1\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8       lubridate_1.8.0  assertthat_0.2.1 digest_0.6.29   \n##  [5] utf8_1.2.2       R6_2.5.1         cellranger_1.1.0 backports_1.4.0 \n##  [9] reprex_2.0.1     evaluate_0.14    httr_1.4.2       blogdown_1.6    \n## [13] pillar_1.6.4     rlang_0.4.12     readxl_1.3.1     rstudioapi_0.13 \n## [17] jquerylib_0.1.4  rmarkdown_2.11   munsell_0.5.0    broom_0.7.12    \n## [21] compiler_4.1.2   modelr_0.1.8     xfun_0.28        pkgconfig_2.0.3 \n## [25] htmltools_0.5.2  tidyselect_1.1.1 bookdown_0.24    fansi_1.0.2     \n## [29] crayon_1.4.2     tzdb_0.2.0       dbplyr_2.1.1     withr_2.4.3     \n## [33] grid_4.1.2       jsonlite_1.7.3   gtable_0.3.0     lifecycle_1.0.1 \n## [37] DBI_1.1.2        git2r_0.29.0     magrittr_2.0.1   scales_1.1.1    \n## [41] cli_3.1.0        stringi_1.7.6    fs_1.5.1         xml2_1.3.3      \n## [45] bslib_0.3.1      ellipsis_0.3.2   generics_0.1.1   vctrs_0.3.8     \n## [49] tools_4.1.2      glue_1.6.1       hms_1.1.1        fastmap_1.1.0   \n## [53] yaml_2.2.1       colorspace_2.0-2 rvest_1.0.2      knitr_1.36      \n## [57] haven_2.4.3      sass_0.4.0"
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#tldr",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#tldr",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "TL;DR",
    "text": "TL;DR\nWe now have three years of Speculation and Vacancy Tax data for BC, demonstrating generally less than one percent of properties pay the tax in most municipalities. We play around with the data we scraped from files released by the BC government to show:\n\n\nhow the federal CHSP program systematically overstates “foreign ownership”\n\n\nhow source of revenue estimates shift depending upon definitions and tax rates\n\n\nhow properties are moving into rentals and\n\n\nwhat else we can glean from exemptions and revenue data."
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#celebrating-three-years-of-data",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#celebrating-three-years-of-data",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Celebrating Three Years of Data",
    "text": "Celebrating Three Years of Data\nBC recently released round three of results from its Speculation and Vacancy Tax (SVT) to relatively little fanfare. Understandable, seeing as how we’ve been dealing with some stuff out here.\nTo recap: the Speculation and Vacancy Tax is an additional property tax mostly levied on a small proportion of properties deemed unjustifiably empty. In this, it’s like the City of Vancouver’s Empty Homes Tax (and is layered over top of that tax for the City). But the Speculation and Vacancy Tax works a little differently insofar as it sets different tax rates by property owner class, with the primary classes including BC Residents, Other Canadians, Foreign Owners, and Satellite Families. Satellite Families include married couples where the spouse making the most money works abroad and pays income tax in a different country.\nWe’ve explored earlier releases, and it’s worthwhile to dig in and take a look at this one as well, building up a longitudinal database of data over time. Of note, the data also keeps changing, though not (so far) in major ways. In general, each time a new round of the SVT data is released we see a revision resulting in a drop in taxpayers compared to the initial data release. This likely reflects people making late declarations or otherwise being exempted from the tax in a retroactive fashion. Importantly, we have not seen the retroactive rises in taxpayers that would indicate successful audits uncovering a great deal of tax fraud.\nSpeculation and Vacancy Tax Technical Reports are now available for 2018 ; 2019 ; and 2020, with supplementary data by municipality appended to the end of the 2018 report and separated out for 2019 and 2020, as per links by year. Staying true to BC government form regarding data openness and transparency of housing data, the government does not make this data easily available. Instead it requires scraping out of PDF reports, which we have done for the analysis below."
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#validating-foreign-ownership",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#validating-foreign-ownership",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Validating Foreign Ownership",
    "text": "Validating Foreign Ownership\nThe first thing we’d like to update is our simple comparison of SVT data with CHSP data on Foreign and Non-Resident Owners for BC. Since it’s based upon auditable declarations, as we’ve noted before, the Speculation and Vacancy Tax offers a valuable check on the Canadian Housing Statistics Program’s (CHSP) estimates of Non-Resident Owners. Often erroneously cast as a measure of “foreign ownership,” the CHSP measure of Non-Resident Owners attempts to match owners on property titles in the provinces of BC, ON, NB, and NS with income tax and related records in Canada. Where it fails to find a good match, it labels the owner Non-Resident. This creates two big possible source of error in the CHSP data as a measure of “foreign ownership.” First, it’s a residual measure, and it’s possible the failure to find a match reflects things as benign as administrative errors in inputting a name correctly rather than an owner’s residence outside of Canada. Second, many owners outside of Canada may still be Canadian citizens and Permanent Residents, and hence not “foreign” despite living abroad. We noted before that comparing SVT to CHSP data, it appeared that somewhere less than half of CHSP’s “Non-Resident Owners” show up as “Foreign Owners” within the SVT data. Using the most recent data from CHSP and SVT, we can see the pattern continues to hold below.\n\nWe can see that as a proxy for “foreign owners,” CHSP vastly and systematically overestimates the size of foreign ownership relative to SVT’s auditable declarations. We can also see that only a very small proportion of “Foreign Owned” properties are left vacant, and hence subject to taxation in BC from the SVT. Taxable “Foreign Owned” properties generally ran well below one percent of total properties in all municipalities in 2018, and have dropped even further to generally below half a percentage point by 2020."
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#where-does-revenue-come-from",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#where-does-revenue-come-from",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Where Does Revenue Come From?",
    "text": "Where Does Revenue Come From?\nThere’s lots of other data in the Speculation and Vacancy Tax technical reports! Let’s look at the revenue, which largely goes into BC Housing. BC’s reporting on the tax revenue has emphasized that relatively little of it comes from BC Residents. Instead the revenue comes mostly from those perceived as outsiders: Foreign Owners and Satellite Families. There are two problems with this way of presenting the data. First, and foremost, it alienates BC Residents who are deemed members of Satellite Families from being considered residents anymore. In our view, this is a serious problem. People should not be removed from consideration as BC Residents because of who they are married to. Period. Second, at this point, given the dramatic drop in foreign owners and satellite families and the rise in BC residents paying the tax, a decisive factor in BC’s revenue claims is that BC Residents pay such a low tax rate on properties left empty relative to other categories. Were tax revenues held constant at 1% for all categories, as per the simulation below, BC Residents would pay more tax than foreign owners, especially if Satellite Families were more correctly considered as Residents.\n\nThe shift away from Foreign Owners and Satellite Families (and “Mixed”), and toward BC residents can also be seen in the number of properties subject to the tax.\n\nNote that we’re generally supportive of Empty Homes Taxes, and their targeting toward insuring housing is used for long-term residential purposes seems justifiable in tight markets like BC’s. But if we’re treating all those non-exempt from the tax as speculators, then an important question remains as to whether BC is justified favouring its domestic speculators over other sets of speculators. What message is that sending exactly, and why?"
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#change-in-tenant-exemptions",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#change-in-tenant-exemptions",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Change in Tenant Exemptions",
    "text": "Change in Tenant Exemptions\nAnother often touted aspect of the Speculation and Vacancy Tax is the return of housing to the rental market. As we’ve often noted, this is definitely a potential benefit of vacancy taxes! They add pressure to owners of empty dwellings to rent them out while not in use or between sales. As we’ve noted before, there are multiple ways to attempt to see the relationship between the SVT and rentals. The simplest is to simply look at changes over time in Tenancy Exemptions. Unfortunately, we’re missing the information about tenancy prior to the imposition of the SVT that would be most helpful in assessing its full effects. But we can see how increasing proportions of properties are being rented out over time, as demonstrated via tenancy exemption to the SVT, in larger cities including Vancouver, Burnaby, and Richmond. This fits especially well with what we know about condominiums, insofar as they’re a major source of secondary market rental stock across BC.\n\nImportantly, the rise in exemptions for tenancy appears to reflect a broad shift toward renting of properties that might’ve happened regardless of SVT imposition. Supporting this interpretation, we see a matching downward trend in homes declared as owner-occupied, so this is likely more a shift from owner to tenant occupied homes than an effect of the SVT moving homes out of vacancy per se."
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#flows",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#flows",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Flows",
    "text": "Flows\nWhat about zooming in on change specific to homes taxed as vacant or otherwise non-exempt from taxation? Through the SVT technical reports released in 2019 and 2020, we can get a broad sense of change over time for taxed properties. In particular, how many properties taxed in 2018 and 2019 shifted to a different status, becoming exempt in 2019 and 2020, by category? This is tricky to do well, given the data provided. In particular, both the 2019 and the 2020 reports have cursed tables where the results, as provided via Technical Reports, are clearly wrong. In each case, the cursed table is Table 8, where totals should match Table 7 in 2019 and Table 9 in 2020. But they don’t. It looks to us like Tables 7 and 9 provide plausibly correct information, though. At the same time, those taxed also change between years. So we drop Table 7 and use the most recent results below, pulling together Table 9 from the 2020 report in conjunction with Tables 10 and 11 (in 2019) and 11 and 12 (in 2020) to provide a sense of change over time in taxed properties. Here we get a direct sense of how many properties taxed in 2018 shifted to a tenancy exemption in 2019. And we get it again between 2019 and 2020. It’s plausible that these properties might’ve shifted into the rental market without the SVT. But it’s also plausible that it helped lead them there.\nFlows demonstrate other interesting changes over time. Perhaps surprisingly, only a minority of properties remain persistently subject to taxation between years. But new properties are added to SVT taxation at the same time as most properties from the previous year are removed. In short, there’s substantial fluctuation in tax incidence. This has compositional effects. In particular, it appears that Foreign Owned properties and Satellite Family properties have gradually diminished to a smaller, but relatively stable low level. Between 2018 and 2020, they seem to have been largely replaced by Other properties, mostly owned by residents of BC and other parts of Canada. For the most recent year, a cynical read might suggest that the SVT simply replaced higher-taxed Foreign owners of empty homes with lower-taxed Canadian owners of empty homes.\n\nWorth noting that the 2020 results may end up overstating the final number of non-exempt properties, as has been the case for previous years. But we won’t find out till 2021 results are released! Until then, we can see that part of the rise in properties owned by BC Residents is a rise in “Other Residential” properties, a category containing vacant residential land that became subject to the tax starting with the 2020 tax year.\n\nWe can also look more broadly at the declaration status of the 2018 and 2019 SVT properties in the following year, checking in which way SVT properties moved out of the SVT tax paying stock.\n\nEspecially after the initial year Foreign Owners reacted by selling their properties and to an almost equal extent by renting them out. Again, this may be interpreted as a success of the SVT, but it may also just be part of the normal pattern of flows of properties. It is hard to make a definite call, we were unable to construct an at least somewhat robust counterfactual scenario."
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#change-and-variation-in-other-exemptions",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#change-and-variation-in-other-exemptions",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Change and Variation in Other Exemptions",
    "text": "Change and Variation in Other Exemptions\nFinally we demonstrate the range of other exemptions provided to the Speculation and Vacancy Tax. The list of exemptions is long, we will focus on the most common exemptions across all municipalities. The way the data is organized means that the “Other exemptions” category is unique to each municipality and contains all exemptions that aren’t shown in the graph for the respective municipality. For instance, in Belcarra, many properties are deemed exempt because they’re only accessible by water."
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#municipal-breakdown",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#municipal-breakdown",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Municipal breakdown",
    "text": "Municipal breakdown\nFor completeness we are adding municipal breakdowns of properties by municipality, keyed by the residency/tax declaration status of the respective owners, excluding the “BC Resident” category.\n\nThere is a bit of variation across municipalities, ranging from under 5% to over 10% of properties owned by other than a “BC Resident” as defined by the SVT, but things stay within the expected range and shares are surprisingly stubborn across the years. Of these, only a small fraction actually pay the SVT as can be seen in the next graph, now also including BC Residents.\n\nOverall, once again, very few properties now pay the tax. While it varies from municipality to municipality, it rarely exceeds one percent of properties. And only for the very tiny and very unusual municipality of Belcarra does it reach two percent. Notably, Victoria and some nearby communities have a larger share of tax-paying properties than Metro Vancouver municipalities that often take the spotlight in discussions around the tax.\nThis view switches when we don’t look at the number of properties subject to the tax and instead look at the revenue that the tax generates. For the revenue total number of properties and property valuations matter too, and Metro Vancouver municipalities with more expensive properties make a strong showing."
  },
  {
    "objectID": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#takeaway",
    "href": "posts/2021-11-21-three-years-of-speculation-vacancy-tax-data/index.html#takeaway",
    "title": "Three Years of Speculation & Vacancy Tax Data",
    "section": "Takeaway",
    "text": "Takeaway\nOverall, we’re glad the BC Government is releasing data on the Speculation and Vacancy Tax, though we wish they’d do so in an easier and less error-prone fashion (What is up with the Cursing of Table 8?) The results demonstrate very little “toxic demand” leading to vacancies in BC, and provide valuable checks on estimates of “foreign” ownership. We also get a fun look at how properties shift status between years. At the same time, we continue to urge the BC government to reconsider certain aspects of the tax and its presentation. In particular, is toxic demand better if it comes from BC? If not, why does the tax code treat it that way? Moreover the government should recognize that just because residents of BC have spouses working abroad does not mean they’re no longer residents of BC.\nAs usual, the code for this post, including the code to scrape the data out of the PDFs, is available on GitHub for anyone to reproduce or adapt."
  },
  {
    "objectID": "posts/2021-10-17-censusmapper-p-review/index.html",
    "href": "posts/2021-10-17-censusmapper-p-review/index.html",
    "title": "CensusMapper (p)review",
    "section": "",
    "text": "We started CensusMapper in 2015, and it’s hard to believe that it’s already six years old. And with the first release of the 2021 census just a few months away it’s a good time for a review. And a preview of what could possibly come in the future."
  },
  {
    "objectID": "posts/2021-10-17-censusmapper-p-review/index.html#how-it-started",
    "href": "posts/2021-10-17-censusmapper-p-review/index.html#how-it-started",
    "title": "CensusMapper (p)review",
    "section": "How it started",
    "text": "How it started\nCensusMapper started as a side project that got a bit out of control. Sometime in spring 2015 I became aware how difficult it is to access and use Canadian census data. In Canada we have a census every five years, and it’s an extremely rich resource. Yet even high-level numbers have been quite inaccessible outside a group of people with desktop GIS skills. Let alone being able to explore spatial variation of data. And even for GIS specialists the process of downloading and working with the data was cumbersome and time-consuming.\nWe felt that census data should be easily accessible to everyone. Not just high-level stats but also being able to explore fine-grained spatial variation. Based on this belief we developed CensusMapper, a web-based interactive map that covers all of Canada, all census variables, at (almost) all aggregation levels from all of Canada down to Dissemination Areas, the finest geography general census profile data is released on, and Dissemination Blocks, an even finer geography that only has population, household and dwelling counts.\n\nCensusMapper web maps\nInitially CensusMapper came with the 2011 census and NHS data, adding the 2006 census data in 2016, and the 2016 census in 2017 bit by bit as it came out. Later we added the 2001 and 1996 censuses to allow for longer timelines and context when viewing census data through time.\nInitially map-making was locked off, but we opened up the ability to make custom maps to everyone shortly after. Over time we added a number of features, but most of the deeper map-making features remained locked off, mostly because we did not have the time to make them user-friendly enough to be used easily.\n\n\nA flexible mapping tool\nUnder the hood the CensusMapper queries raw census data from the server and draws maps live in the user’s browser. To do this the map-maker has to specify a function based on census variables, which may be arbitrarily complex. The CensusMapper front-end will with query those variables for the current map view from the CensusMapper back-end, evaluate the function, apply the colour scale and draw the map. This means that CensusMapper is extremely flexible and a great tool for exploratory analysis. Maps are automatically Canada-wide and work at all geographic aggregation levels.\n\n\nCensusMapper APIs\n Maps are fun, but maps aren’t analysis. In 2017 I was working increasingly with census data, and I was pulling data out of the CensusMapper database as I needed it. This worked in principle, but in practice there were problems.\n\nIt wasn’t reproducible, I did not keep good track of how I pulled data out of the database\nIt wasn’t transparent, it required privileged access to the database and others could not check or reproduce my workflows\nIt wasn’t adaptable. Pulling data out of the database manually made it hard to easily adapt previous work for related projects\n\nThe solution to this was to build an API to have an organized and transparent way to pull data out of the CensusMapper database that others could use too. And to build an R package that wraps the API and enables people to easily share that uses and analyses Canadian census data fetching the data dynamically as needed."
  },
  {
    "objectID": "posts/2021-10-17-censusmapper-p-review/index.html#how-its-going",
    "href": "posts/2021-10-17-censusmapper-p-review/index.html#how-its-going",
    "title": "CensusMapper (p)review",
    "section": "How it’s going",
    "text": "How it’s going\nCensusMapper is now six years old, and six years is a lot of time in the world of web technology, in particular web maps. Modern web maps are a lot more flexible in how to display contextual data like roads and place names, have continuous zoom, allow 3D extrusion, and are much faster and performant. Additionally, CensusMapper was always more of a personal side-project spun out of control than a product developed for a broader set of users. The UX is clunky and unintuitive at times, the design is incoherent and wanting.\nIt’s pretty clear what the solution to this should be. A couple of things need updating:\n\nFront end map engine\nOverhaul of general UI/UX\nBetter and more user-friendly custom map creation and expansion of advanced mapping to the general public\nExpansion of custom map overlays to general public\nEnabling geographic filters for general users\nNon-map based summary information for cities\nBetter widgets and interactivity\nMoar data\nExpansion of the data API and python bindings (next to existing R bindings)\nPerformance boost\nBetter map projection\n\n\nMap engine\nThere are lots of modern map engines to choose from. What we will want is something that enables continuous zoom, ways to fine-tune the styling of contextual information like roads, place names and land use overlays, all in a snappy WebGL based rendering engine. There are several ways to do this, Mapbox is probably the best know one. This is for example what we used for the Metro Vancouver Zoning Map.\nWhile Mapbox is great for rendering traditional map data, it has some limitations for rendering data. Enter Deck.gl, the Uber mapping engine optimized for data that plays well with Mapbox. This gives a lot more flexibility in how to render and interact with the data, we have used this when mapping all of Nova Scotia residental properties and the effect of the CAP, the Nova Scotia version of California’s property tax reduction Prop 13.\nWith Deck.gl we can dynamically interweave census data layers with other layers, which allows individual styling of maps, and selective overlaying of contextual information.\nFor CensusMapper we need to adapt this to deal with tiled data, as well as work with the CensusMapper paradigm of strict separation of geographic and census variable data that is key to the high performance and low resource consumption architecture.\nIn principle that’s not too much work, we did a quick test to see what this would look like. It’s still has some issues, especially on mobile, but it gives a general idea of the direction we think CensusMapper maps should be heading.\n\n\n\n\n\nUI/UX overhaul\nThe overall CensusMapper UI was cooked up with minimal planning, and grew out of our personal mapping needs as they arose. IT needs a major cleanup and decluttering and a more seamless transition between mobile and desktop experiences. Some UI and visualization elements are clunky. We simply never spent the time to polish things up and come up with a clear and consistent design. There is no tutorial or basic walk-through that can help interested new users to find their way around CensusMapper and gently guide them toward how to create their own maps.\n\n\nAdvanced mapping\nRight now CensusMapper enables anyone to map single census variables, either directly or as a percentage (if appropriate). This is great and an easy way to explore basic census variables. But it barely opens the door Advanced mapping opens up the possibility to map more complicated relationships. For example, our very first map on CensusMapper, and part of the motivation for it’s design choices, was a map of the “Hidden Mortgage”, a map that estimates the transportation costs to work people in each region are paying by considering mode of transportation to work, commute time, and income.\nOn the map-creating side of things it needs a clearer and more purposeful user interaction that also enables the user to transition to more advanced mapping functionality. The advanced map-making module is extremely powerful, allowing for arbitrary functions to compute mappable values from any selection of census variables. Opening this up to general users needs thoughtful UI and guidance.\nThe payoff for enabling advanced map-making capabilities for general users is considerable, it puts the full power of CensusMapper maps into the hands of all Canadians.\n\n\nMap overlays\nAdding contextual information can be very useful, allowing custom overlays is a regularly requested feature. Currently CensusMapper has the ability to add overlays, but it’s clunky and locked up. Here is an example map that maps proportions of the population in low, middle and high adjusted family income groups and overlays City of Vancouver low-density zoning areas. This should be expanded to allow users to more easily upload or link custom datasets and style them.\n\n\nGeographic filters\nOne of the great features of CensusMapper is that any map created is automatically Canada-wide. But this can also be a disadvantage, sometimes maps are made with very particular geographies in mind only. And might not make much sense in other areas. And example would be a map showing location quotients, so ratios relative to some higher order geography. Take the map of age group location quotients for Metro Vancouver that looks at the share of the population in different age groups relative to the Metro Vancouver age distribution. This map does not make sense outside of Metro Vancouver, so it is filtered to only show information for within Metro Vancouver. While this functionality is already available in CensusMapper, it is not documented and currently locked to advanced map users. This is mostly because it’s only really needed in combination with advanced mapping of census variables, but also because it hasn’t been implemented in an intuitive and user-friendly way.\n\n\nNon-map based information\nWhile CensusMapper is mostly about maps, there is also value in simple overview census profiles for municipalities and other census regions that give a high-level visual overview of key statistics like income and age distributions and other features. The EveryCityCanda bot by Dmitry Shkolnik and myself implemented something like this, this could be expanded on and implemented in CensusMapper to give an easy and interactive overview of key metrics.\n\n\nBetter widgets and interactivity\nOne of the strength of CensusMapper is the way it can add widgets with data interactions linked across widgets and the map. By default CensusMapper displays a histogram of the values in the current map view, and hovering over map areas will indicate the position of the area in the histogram. And vice versa, hovering over the histogram will highlight corresponding map areas.\nCensusMapper also allows for the embedding of scatter plots into the map story side bar, and all widgets, including the histogram and the map data, interact with one another. An example of this can be seen in the active transportation map linked above, or the Local Affordability map. These kind of widgets can greatly enhance the understanding of the data and storytelling, making this more user-friendly and enabling adding of these widgets for all users would help others create compelling data stories.\n\n\nMoar data\nWe will definitely add the 2021 Census data as it rolls out, but there are lots of other datasets that would greatly enhance the usability of CensusMapper, both for mapping as well as a data server. We have already integrated several cross tabulations, for example one on document type by structural type to get a better reading of the overall housing stock. We have custom tabulations of T1FF tax filer data at the census tract level for all census years 2000 through 2018.\n\n\nAPI expansion\nNext to our traditional API services we have added an enpoint that indentifies census regions intersecting custom geographies. This is very useful when running analysis relating to custom geographies with a-priory unspecified spatial extent. Traditionally we would first have to identify and download data that covers our custom geographies and then run an analysis, with the new endpoint we can automate the identification of relevant geographies. There are other use cases like this, and the API can be tailored or expanded to better serve such needs.\nAt the same time, the most convenient way to access census data is via the cancensus R package. It would be useful to provide similar bindings to python, the other important statistical language with a mature ecosystem for geospatial analysis. While most people in the data world are bilingual, people have preferences and giving easier access to census data in python would be a lot more convenient for those preferring to work exclusively in python. Cleaning up the API web interface for data download for those looking to do this manually would also be useful, the current UI is quite intuitive.\n\n\nPerformance boost\nThe CensusMapper server runs on a $20/month cloud instance. This suffices for most uses, the map server starts to slow down noticeably at around 1000 simultaneous users. Which rarely happens. But it does necessitates some compromises, e.g. the throttling of API calls. We regularly get request for extended API quotas which we usually grant, but this is starting to put some strain on the server and could benefit from a performance boost. At the same time, we could invest some time to improve server logic to better deal with large simultaneous data requests to ensure smooth performance of the other server functions like web mapping.\n\n\nBetter map projections\nCurrently CensusMapper, like many web maps, uses the Mercator projection. This is particularly problematic for Countries as far away from the equator as Canada. Right now this is largely dictated by the availability of tile data for contextual map data like roads, place labels, land use and buildings. Having this available for other map projections would be useful for other Canadian mapping applications, and is in principle not hard to do. But it requires setting up a tile server for differently projected map data, and could be enhanced by dynamic re-projection of data to e.g. retain that North is pointing up (on average) in the visible map area for intuitive orientation."
  },
  {
    "objectID": "posts/2021-10-17-censusmapper-p-review/index.html#censusmapper-2.0",
    "href": "posts/2021-10-17-censusmapper-p-review/index.html#censusmapper-2.0",
    "title": "CensusMapper (p)review",
    "section": "CensusMapper 2.0",
    "text": "CensusMapper 2.0\nSo when will all this be ready? Here we run into the fundamental problem when turning a side project into an actual product. It requires time and effort. Not like we haven’t put any time into CensusMapper in the past, we definitely have. But there are limits to how far we can justify running this off the side of our desk. We are at the juncture where we have to either lock up parts of CensusMapper and monetize it, or get other funding to keep this project open and accessible to everyone. Or, given our past history a fairly likely variant, keep CensusMapper largely as it is, while maybe every now and then doing some work around the edges.\nThe key portion to make CensusMapper more useful, and that justifies other upgrades like a better mapping engine, is the UI/UX overhaul. It makes little sense to just swap out and spiff up the map and leave the rest as is. Without increases in usability there is not much value in doing this.\nWill CensusMapper 2.0 become reality? We don’t know. But at least we have a roadmap."
  },
  {
    "objectID": "posts/2021-09-25-elections-fun-2021-edition/index.html",
    "href": "posts/2021-09-25-elections-fun-2021-edition/index.html",
    "title": "Elections fun - 2021 edition",
    "section": "",
    "text": "We noticed a lot of recent traffic to our blog post on the 2019 elections, so maybe that means that, now that all districts have been called, we should update the post with 2021 data. We will just lazily run the code from the old post with the new data. And given that the outcome was overall quite similar, we can also leave the text/commentary largely unchanged. Which makes life nice and easy for us.\nElection maps in Canada tend to be somewhat terrible, Canada is a large country, with some reasonably densely populated regions, and large areas that are sparsely populated. That makes it hard to map things. CensusMapper, our project to flexibly map Canadian census data, struggles with that. The choropleth maps can be quite misleading. The same problem comes up when mapping Canadian election data.\nThis map makes it virtually impossible to get a good reading of the distribution of votes. There are a couple of ways around this.\nFor example, one could break out the areas with electoral districts too small to make a visible impact on the map, or use a cartogram, like the following two examples taken from the Wikipedia page of the 2021 federal election.\nThe first keeps the overall geographic context, although the metropolitan areas that are broken out are hard to interpret unless one if very familiar with each region. The cartogram distorts the areas to give each electoral district the same amopunt of space, and thus gives a proportional view of the number of seats each party won. In this version, the labels and breaks help delineate familiar geographies, but it can be hard to properly place them on a map.\nTo bridge the divide between overall geography and emphasis on treating each district separately, one can also animate the cartogram between the familiar map view and the cartogram view. In the following example that we built as an observable notebook we move between a map of Canada and a cartogram where each electoral district is a dot with size given by the total number of votes cast.\nWe include a video clip of the animation for convenience, it’s just a screenshot from the interactive live observable notebook.\nThis still loses a lot of nuance. The colour is determined by who won the district, the animation reveals no information on how wide or narrow the margin of victory was. Or how the other candidates performed."
  },
  {
    "objectID": "posts/2021-09-25-elections-fun-2021-edition/index.html#winning-vote-share",
    "href": "posts/2021-09-25-elections-fun-2021-edition/index.html#winning-vote-share",
    "title": "Elections fun - 2021 edition",
    "section": "Winning vote share",
    "text": "Winning vote share\nWith more than 2 candidates in each riding, one does not necessarily require a plurality of votes to win. Only 113 out of the 338 candidates won with over 50% of the vote in their district. The largest vote share any winning candidate got was 76%, the lowest was 29%.\n\nIt stands out that the top 4 spots were taken by the Conservatives."
  },
  {
    "objectID": "posts/2021-09-25-elections-fun-2021-edition/index.html#distribution-of-votes",
    "href": "posts/2021-09-25-elections-fun-2021-edition/index.html#distribution-of-votes",
    "title": "Elections fun - 2021 edition",
    "section": "Distribution of votes",
    "text": "Distribution of votes\nWe can expand this view to show the vote share by party for each district."
  },
  {
    "objectID": "posts/2021-09-25-elections-fun-2021-edition/index.html#wasted-votes",
    "href": "posts/2021-09-25-elections-fun-2021-edition/index.html#wasted-votes",
    "title": "Elections fun - 2021 edition",
    "section": "Wasted votes",
    "text": "Wasted votes\nWith our first-past-the-post system, we can also take a look at “wasted” votes. We define these as votes that have no bearing on the outcome. For winners it’s the vote margin by which they won, for the ones that did not win their district it’s the entirety of their votes.\n\nWasted votes is a somewhat artificial system that does not necessarily reflect how parties would have performed under a different voting system. We will need to take a closer look for that."
  },
  {
    "objectID": "posts/2021-09-25-elections-fun-2021-edition/index.html#first-past-the-post-vs-proportional-representation",
    "href": "posts/2021-09-25-elections-fun-2021-edition/index.html#first-past-the-post-vs-proportional-representation",
    "title": "Elections fun - 2021 edition",
    "section": "First-past-the-post vs proportional representation",
    "text": "First-past-the-post vs proportional representation\nHow would the parties have fared under a proportional representation system (PR) instead of first-past-the-post (FPTP)? There are many different kinds of proportional representation systems out there, but they generally try to approximate a seat distribution that mirrors the overall vote share. For simplicity we will simply take the overall vote share as a proxy for what a proportional representation system might have yielded. Well, kind of. The votes were cast under the expectation of a FPTP system, if we had switched to some kind of proportional representation or ranked ballot system some people would likely have voted differently and the outcome would have been different. We should view the PR alternative view as a first approximation to a PR outcome, if people were voting under a PR system the outcome could possibly be quite different.\n\nThe Liberals are the big winner of FPTP, as is Bloc Québécois. The Conservatives fair equally well under either system, and NDP and the Green Party are the losers under the current FPTP system. Which probably explains why the Liberals renegaded on their promise to replace FPTP with a more representative voting system.\nWe can run this by individual province to see how well each province is represented in terms of first-past-the-post vs proportional representation.\n\nThis reveals that Alberta and Saskatchewan voters are a lot more diverse than the FPTP vote system may suggest, and the while the Conservatives got shut out of a couple of provinces, they still have sizable support there. This kind of outcome is fairly typical for first-past-the-post systems, where the representation in parliament does not match the overall population well, especially when looking by province."
  },
  {
    "objectID": "posts/2021-09-25-elections-fun-2021-edition/index.html#upshot",
    "href": "posts/2021-09-25-elections-fun-2021-edition/index.html#upshot",
    "title": "Elections fun - 2021 edition",
    "section": "Upshot",
    "text": "Upshot\nThere is endless fun to be had with elections data. As usual, the code for this post, including the pre-processing for the animation, is available on GitHub for anyone to adapt and dig deeper into elections data."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "",
    "text": "BC now shares data on the vaccination status of cases and hospitalizations in their weekly Data Reports. This is progress, although calling it “data” is reaching. What is shared is graphs that need manual scraping to be turned into (approximate) data.\nThe numbers themselves aren’t particularly meaningful. Vaccines aren’t 100% effective in preventing symptomatic COVID (approximated by “cases” in BC) or hospitalizations. This means that as more people get vaccinated, there will be more cases and hospitalizations among the vaccinated population. If everyone were vaccinated then all cases and hospitalizations would be necessarily among vaccinated people, this is not very helpful information.\nTo turn this data into useful information we need to specify what question we are interested in: By how much do vaccines reduce our risk of symptomatic COVID and of severe outcomes?. Unfortunately, in BC we don’t have the data we need for robust vaccine effectiveness estimates. But there is a lot of public interest to dig into this question, and with some data being released now it might be a good idea to take a more detailed look at what is needed for vaccine effectiveness estimates, what can go wrong, and what data is missing.\nDespite all this uncertainty, one thing is clear: Vaccines are effective and everyone should get vaccinated as soon as possible. We are aiming to transition to “endemic mode”, which means, for better or worse (I have opinions), we are planning on COVID spreading through the population and everyone should expect to be either vaccinated, or contract COVID, or both."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#vaccine-effectiveness-and-efficacy",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#vaccine-effectiveness-and-efficacy",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "Vaccine effectiveness (and efficacy)",
    "text": "Vaccine effectiveness (and efficacy)\nFirst we need to clear up some terminology. Generally we are interested in the causal effect of vaccines, that is by how much the act of getting vaccinated causally lowers the risk of symptomatic COVID or hospitalization or death. The easiest way to ascertain this is through randomized controlled trials, as has been done as part of the vaccine accreditation process. Formally, vaccine efficacy against symptomatic COVID (or hospitalization) \\(Y\\in\\{0,1\\}\\), where \\(Y=1\\) means being a case or being hospitalized, depending on vaccine status \\(V\\in\\{v_0,v_1,v_2\\}\\), where \\(V=v_i\\) means being vaccinated with \\(i\\) doses, is given by the risk ratio\n\\[\nVE_i = 1-\\frac{P(Y=1|do(V=v_i))}{P(Y=1|do(V=v_0))},\\qquad v_i\\in\\{v_1,v_2\\}.\n\\] A similar and more intuitive measure is the factor by which vaccinations offer protection from symptomatic covid or hosptializations \\[\nPF_i=\\frac{1}{1-VE_i}=\\frac{P(Y=1|do(V=v_0))}{P(Y=1|do(V=v_i))},\\qquad v_i\\in\\{v_1,v_2\\}.\n\\] We will generally present results in terms of the protective factor \\(PF\\) instead of \\(VE\\).\nIn contrast, vaccine effectiveness is a much more vague concept. Generally it is defined as follows:\n\nVaccine effectiveness is a measure of how well vaccines work in the real world.\n\nUnpacking this, vaccine effectiveness includes “side effects” of getting vaccinated like risk compensation, that can reduce or otherwise alter the benefits of vaccines, as well as problems with vaccine administration, can lead to reduced benefits of vaccines in the real world. Moreover, the sample of participants in vaccine trials may not reflect the risk profile of the general population of interest.\nIn theory, vaccine effectiveness is also a causal concept that aims to quantify by how much vaccines reduce risk in real world scenarios where vaccine administration is imperfect and allowing for confounding by side effects like risk compensation. In practice in the medical literature vaccine effectiveness estimates are often treated as statistical rather than causal estimates.\nIn general the logistics of proper vaccine refrigeration and administration is fairly well solved, so we don’t expect large effects because of that.\nRisk compensation can seriously confound vaccine effectiveness estimates as vaccinated people may engage in more risky behaviour, partially negating the protectice effects of vaccines against infection. After all, risk compensation is part of the point, vaccines enable us to go back to normal and engage in more risky behaviour.\nSo why do we bother with vaccine effectiveness estimates at all, other than concerns about the trail sample under-representing important subgroups of the population?."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#variants-of-concern",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#variants-of-concern",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "Variants of concern",
    "text": "Variants of concern\nVariants of concern complicate our initial vaccine efficacy estimates. What we are interested in now is how vaccines behave against the variants currently under circulation, instead of the variants the vaccines were tested on in randomized controlled trials that by now don’t play much of a role in transmissions any more. Vaccine effectiveness studies can help fill that gap to understand how effective vaccines are against new variants.\nWe could do this by benchmarking vaccine effectiveness for one variant against the known vaccine effectiveness of another variant, assuming quasi-random assignments of variants in exposure events. We don’t have publicly available data on variants, cases and vaccination status in BC, so we can’t do this. Another way is to make assumptions about factors influencing vaccinations, cases, and hospitalizations, and try and utilize the tools of causal inference to estimate vaccine effectiveness that way.\nTo do this it’s best to take it slowly and specify a causal model. Inter-jurisdictional differences in how vaccines are rolled out, and how case and hospitalization data is collected, mean it’s best to be clear about the (assumed) underlying causal model and how it relates to local peculiarities.\n\nThis model acknowledges the effect of the vaccine rollout in BC, where vaccination status isn’t assigned randomly but is strongly influenced by age due to the mostly age-based rollout. At the same time the vaccine rollout is impacted by socio-economic status (SES) in several ways, directly by prioritizing certain groups like medical staff but also to some extent essential workers, and indirectly through vaccine hesitancy and ability to access vaccines correlating with various SES variables.\nThere are other possible omitted factors, for example it is not clear how various NPI affect the these pathways, for example universal mask wearing may universally lower the initial dose and alter the severity of infections, which may impact vaccine effectiveness against hospitalization. We believe the above model is a decent approximation for estimating vaccine effectiveness in BC.\nWe expect vaccination status to causally effect the risk of getting infected, the risk of becoming symptomatic even when accounting for getting infected, and the risk of hospitalization even when accounting for being symptomatic.\nWe similarly expect age and SES to affect risk of infection, of becoming symptomatic, and hospitalized via comorbidities.\nIn BC (and in most other places) we don’t observe infection, and we can’t estimate the effect of vaccination status on risk of infection. This also means we can only observe the total effect of vaccination of becoming symptomatic and can’t estimate the direct or indirect effects as mediated through infection. We will remove this step from the graph.\n\nIn BC age is an observed variable, and we (in principle) know the vaccination status, case status and hospitalization status by age. Unfortunately, SES is unobserved in BC. Early on BC decided not to collect SES variables relating to COVID cases, hospitalizations or vaccinations, despite PHAC explicitly asking this data be collected. In principle some of the effect of SES on the vaccination rollout may be knowable if BC kept track of who received a vaccination via the age-based rollout and who through medical staff or essential work or other SES-based mechanisms. But this information, if it exists, is not public.\nThis means that if we believe that either the impact of SES on hospitalization risk, or the impact on vaccination status and on the vaccination rollout are not negligible in BC, the effect of vaccination status on hospitalization is not identifiable in BC. The best we can do is substitute in a range of assumptions about the magnitude of these effects and use this to understand the range of corresponding vaccine effectiveness estimates.\nDue to the unobserved nature of SES variables in BC we will assume the following causal model as a first step, and later try to estimate hypothetical impacts of the unobserved SES variables.\n\nWe will refer to this as the naive model."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#estimating-vaccine-effectiveness-in-the-naive-model",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#estimating-vaccine-effectiveness-in-the-naive-model",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "Estimating vaccine effectiveness in the naive model",
    "text": "Estimating vaccine effectiveness in the naive model\nUnder the assumptions of this model we can estimate several effects of interest: The effect of of vaccines on cases, the total effect of vaccines on hospitalizations, as well as the direct and indirect effects due to hospitalizations being mediated through cases. Formally we estimate the total vaccine effectiveness against symptomatic COVID or hospitalization \\(Y=1\\) using\n\\[\nP(Y=1|do(V=v_i)) = \\sum_a P(Y=1|V=v_i,A=a)P(A=a).\n\\]\nLet’s get to work using BC data. As mentioned up top, BC does not make the required data easily available, but some approximate version of it can be scraped from the weekly data reports. Thanks to BC’s abysmal data practices our 11 year old has quite a bit of practice scraping data out of BCCDC graphs, which he deposited in a google doc in exchange for a gift certificate for fancy ice cream.\nThere are several issues beyond just imprecisions when scraping data out of graphs, for example consecutive Data Reports have the vaccination rate of 80+ year olds drop from one week to the next, which is nonsensical and speaks to BC’s fundamental and ongoing data challenges.\nFurther, we only have access to aggregate data and thus are constrained in the timing of our input data. For hospitalizations we are using data from the August 19 data report that are pegged to admissions between July 17 and August 17. We use the age-based vaccination status from the July 26th data report that reflects the vaccination status as of July 22. The Data Report suggests that vaccination status is reported as 3 weeks post first dose or two weeks post second dose, so that rougly lines up with the vaccination data we use. We use case data from the same report, ignoring the lag between cases and hospitalizations that we generally observe. Moreover, cases have been rising during the given timeframe, which may further skew the analysis.\nThese data challenges make it difficult to have confidence in the subsequent estimates. We will continue to carry them out regardless, mostly just to demonstrate the adjustments that are needed to interpret the BCCDC vaccination data if one were able to access proper and accurate data. We remove the age group 0-11 year olds because the BCCDC vaccination data graph shows zero children in this age group being vaccinated, which is directly contradicted by experience (my 11yo is fully vaccinated) as well as PHAC vaccination data as the BC eligibility cutoff is based on birth year (2009) instead of age. This means that our estimates are for effectiveness of 12+ years olds only, which is not a big restriction.\nTo get a feel for the data we first compute simple case and hospitalization rates based on age and vaccination status.\n\nAs expected this shows that the frequency of cases and hospital admissions strongly depends on both age and vaccination status. We can interpret each bar in this table as giving \\(P(Y=1|V=v_i,A=a)\\), where \\(Y\\) denotes the outcome of either cases or hospitalizations. Dividing the expressions for vaccination status \\(V=v_1\\) and \\(V=v_2\\) (1 or two dose) by the one for vaccination status \\(V=v_0\\) (no vaccination) gives the risk ratio in each category.\nWhat we are interested in is the causal (given our model) total effect of vaccinations on cases and hospitalizations. We can show the vaccine effectiveness for each age group separately, and also estimate the overall vaccine effectiveness by utilizing the adjustment formulas from above.\nGiven the fairly small sample of one month of BC data, together with inaccuracies when scraping the data out of the graphs, we are running 1,000 bootstrap samples of the (simulated) underlying line list data (including healthy individuals) derived from the scraped graphs.\n\nOverall this confirms that vaccines are highly effective against becoming symptomatic (“case”) and even more against becoming hospitalized! It also stresses the importance of people getting their second shot, even though there is some caution as the data we have does not break out the timing between vaccines and having contracted COVID. Getting the second shot increases the vaccine protection by a factor 4, resulting in a 12-fold protection against symptomatic COVID and a 29-fold protection against hospitalizations.\nWe can zoom into the bottom right quadrant, total vaccine effectiveness against hospitalizations for fully vaccinated people. This is the estimate we care about most.\n\nThis result is also consistent with older people, especially the 80+ population, not getting the same level of protection from 2 doses compared to younger populations."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#why-adjust-for-age",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#why-adjust-for-age",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "Why adjust for Age?",
    "text": "Why adjust for Age?\nBut did we really have to do all this work of adjusting for age? We can compare our age-adjusted effectiveness to the unadjusted naive estimate when ignoring age.\n\nThis demonstrates the power of age as a confounder when trying to estimate vaccine effectiveness, especially when it comes to effectiveness against hospitalizations."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#missing-ses-and-causal-vaccine-effectiveness-estimates",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#missing-ses-and-causal-vaccine-effectiveness-estimates",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "Missing SES and causal vaccine effectiveness estimates",
    "text": "Missing SES and causal vaccine effectiveness estimates\nAfter seeing how large the effect of adjusting for age is in our vaccine effectiveness estimates we can appreciate the difference between causal estimates and simple statistical estimates. In BC we can’t account for SES factors because this data was not collected, but it seems reasonable to question if at this point the unvaccinated population shares the same relevant SES features as the vaccinated population.\nUnlike age, the impacts of SES are hard to sign, their single causal pathways in our first DAG up top should be viewed as the combined effect of a collection of mechanisms, many of which come in with opposite signs. However, we should not assume that these effects cancel out.\nFor example, looking at the difference in first vs second dose effectiveness against “cases” in 80+ year olds we notice a significant increase in risk ratio, so a decrease in vaccine effectiveness. This seems implausible and points to the population of 80+ year olds with only one dose differing significantly from the those with two doses. We don’t see the same difference in hospitalizations, which might be explained by the population of 80+ year olds with two doses, which includes people in long term care, being more likely to get tested.\nEstimating the uncertainty due to unobserved SES variables requires adding in simulated SES effects, but this is beyond the scope of this post."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#direct-and-indirect-vaccination-effects-on-hospitalizations-mediated-through-cases",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#direct-and-indirect-vaccination-effects-on-hospitalizations-mediated-through-cases",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "Direct and indirect vaccination effects on hospitalizations mediated through cases",
    "text": "Direct and indirect vaccination effects on hospitalizations mediated through cases\nHospitalizations are mediated through cases, there are two causal pathways how vaccines reduce hospitalizations. One is indirect by reducing symptomatic COVID and thus hospitalizations which are a consequence of symptomatic COVID. The other is directly by reducing the chance a person with symptomatic COVID will develop symptoms severe enough to require hospitalization.\nTo understand how hospitalizations are mediated through cases we estimate the direct effect of vaccines on hospitalizations for people with symptomatic COVID (cases).\n\\[\nVE_{D(C)}^i = 1-\\frac{P(H=1|do(V=v_i),do(C=1))}{P(H=1|do(V=v_0),do(C=1))}.\n\\]\nThis allows us to ascertain how vaccinations reduce hospitalizations, by reducing symptomatic infections and leave the risk of hospitalization unchanged given symptomatic infection, or if vaccines also reduce the risk of hospitalization given breakthrough symptomatic infection and requires estimation of the quantities\n\\[\nP(H=1|do(V=v_i),do(C=1)) = \\sum_a P(H=1|V=v_i,C=1,A=a)P(A=a).\n\\]\nThis gets complicated by not having access to line-level data and having to make assumptions about timing, which we will gloss over.\n\nHere we run into issues with our data, likely because of failure to adjust for SES. The 80+ year old age group with only 1 dose shows an increased risk of getting hospitalized, given they have symptomatic COVID. This seems contradictory to all we know about how COVID and vaccines work, and points to the population of 80+ year olds with a single dose being substantially different from the population of unvaccinated 80+ year olds. Data for the fully vaccinated population looks a little better and gives some indication that vaccines protect us not just by preventing symptomatic COVID, but also by reducing the severity of the disease for people that do exhibit symptoms."
  },
  {
    "objectID": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#upshot",
    "href": "posts/2021-08-30-thoughts-on-vaccine-effectivess-estimates/index.html#upshot",
    "title": "Thoughts on vaccine effectivess estimates",
    "section": "Upshot",
    "text": "Upshot\nVaccine effectiveness estimates are difficult, they require proper accounting of confounders to be meaningful. Age is an important one as age heavily impacts both the likelihood of developing symptomatic COVID as well as the likely severity. Not adjusting for age is likely to substantially bias vaccine effectiveness downward.\nOver the past year we have learned that SES is also an important factor in COVID, it impacts vaccine effectiveness in a variety of ways, from the vaccine rollout and vaccine uptake, to risk to be exposed to COVID, to test seeking behaviour, to likelihood of severe outcomes due to comorbidities. Unfortunately BC does not collect the relevant data which renders vaccine effectiveness estimates difficult to do with BC data. But there is no reason to expect that vaccines behave fundamentally different in BC compared to other jurisdictions, which allows us to have confidence in vaccine effectiveness data from jurisdictions where better estimates are possible.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-08-31 10:32:49 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [34a1425] 2021-08-31: image positions\n## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] googlesheets4_1.0.0 cansim_0.3.9        forcats_0.5.1      \n##  [4] stringr_1.4.0       dplyr_1.0.7         purrr_0.3.4        \n##  [7] readr_2.0.1         tidyr_1.1.3         tibble_3.1.3       \n## [10] ggplot2_3.3.5       tidyverse_1.3.1    \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.7        lubridate_1.7.10  assertthat_0.2.1  digest_0.6.27    \n##  [5] utf8_1.2.2        R6_2.5.1          cellranger_1.1.0  backports_1.2.1  \n##  [9] reprex_2.0.0      evaluate_0.14     httr_1.4.2        blogdown_1.4     \n## [13] pillar_1.6.2      rlang_0.4.11      readxl_1.3.1      rstudioapi_0.13  \n## [17] jquerylib_0.1.4   sanzo_0.1.0       rmarkdown_2.8     googledrive_2.0.0\n## [21] munsell_0.5.0     broom_0.7.6       compiler_4.1.0    modelr_0.1.8     \n## [25] xfun_0.24         pkgconfig_2.0.3   htmltools_0.5.1.1 tidyselect_1.1.1 \n## [29] bookdown_0.22     fansi_0.5.0       crayon_1.4.1      tzdb_0.1.2       \n## [33] dbplyr_2.1.1      withr_2.4.2       grid_4.1.0        jsonlite_1.7.2   \n## [37] gtable_0.3.0      lifecycle_1.0.0   DBI_1.1.1         git2r_0.28.0     \n## [41] magrittr_2.0.1    scales_1.1.1      cli_3.0.1         stringi_1.7.3    \n## [45] fs_1.5.0          xml2_1.3.2        bslib_0.2.5.1     ellipsis_0.3.2   \n## [49] generics_0.1.0    vctrs_0.3.8       tools_4.1.0       glue_1.4.2       \n## [53] hms_1.1.0         yaml_2.2.1        colorspace_2.0-1  gargle_1.2.0     \n## [57] rvest_1.0.1       knitr_1.33        haven_2.4.1       sass_0.4.0"
  },
  {
    "objectID": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html",
    "href": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html",
    "title": "Commodity and Keeping it in the Family",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#tldr",
    "href": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#tldr",
    "title": "Commodity and Keeping it in the Family",
    "section": "TLDR",
    "text": "TLDR\nCommodification of housing: what does it mean? Is it a problem? Can we decommodify housing? Can we establish a baseline for often this occurs in property transactions? Here we draw upon a recent Statistics Canada data release and older Census data to walk through some of these questions."
  },
  {
    "objectID": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#commodification-in-property-transactions",
    "href": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#commodification-in-property-transactions",
    "title": "Commodity and Keeping it in the Family",
    "section": "Commodification in Property Transactions",
    "text": "Commodification in Property Transactions\nThe commodification of housing has been identified as a problem to be resisted by a wide range of analysts and commentators. How do we define it? Usually commodification refers to the degree to which housing is created for and/or distributed through market mechanisms. In some cases, commodification has been tied to specific transformations and sophisticated analyses pointing out potential problems (e.g. Forrest & Williams 1984 (unfortunately paywalled) discussing the contemporary sale of British Council Housing). In other cases, commodification seems to operate simply as shorthand for “tainted by the market,” distinguishing pure untainted non-market housing (good) from impure tainted market housing (bad), regardless of the uses to which the housing is ultimately put (reminder: most of it is not empty!). Rather than wade much further into the details of commodification debates, here we simply want to look at establishing a baseline for discussion. Can we estimate the degree to which market mechanisms dominate the transfer of ownership of housing stock?\nHere we consider market mechanisms as including sales made on open markets, where sellers and buyers operate and set prices at “arm’s length” from one another (with the arms often belonging to real estate agents). Typically agents list properties with specified sales prices, then prospective buyers respond by either making offers (which may lowball, match, or exceed listed prices), or passing to the next property. Possible matches can launch buyers and sellers into several rounds of offers and counter-offers, each bounded by contractual protections. Alternative mechanisms (e.g. auctioning) may also work within open markets. The more buyers and sellers, the more readily markets sort out transactions and assign final sale prices. Imbalances in numbers of buyers and sellers tend to drive prices up or down accordingly. Prospective buyers with more to spend have many more choices available, and are advantaged in bidding relative to those less well off, who may find themselves with no choices at all.\nMarkets are only one of multiple mechanisms by which residential properties can be transferred. Indeed, the basic principle governing most private residential properties is that they are held in “fee simple” land ownership. The term is feudal, descending from “feodum simplex,” and indicating that the owner operates as a landed lord with the least encumbered form of ownership. In particular, “fee simple” means the owner can transfer the land as they like, with the base assumption being that their property is inheritable by their heirs rather than returning to the King or Queen. But crucially, of course, “fee simple” property can also be put up to market. Put simply, we should probably talk about unencumbered privately owned housing as “market-able housing” rather than necessarily “market housing.” In fact, historically the relevant transition was how land became marketable from its former feudal control.\nBut maybe this is a distinction without meaningful difference? How many privately owned dwelling units are transferred without going through open market mechanisms? As it turns out, Statistics Canada just released new data to help us answer this question! Statistics Canada’s CHSP program tracked residential property transactions in 2018, following every property changing ownership in the provinces of BC, NS, and NB.\n\nDepending upon geography, and taking 2018 as a representative year, we can estimate that somewhere between 5%-10% of residential properties change hands in any given year (with Nova Scotia on the low end, Kelowna on the high end). The data also distinguish between market transactions and non-market transactions, and we can even estimate the proportion of transactions taking place outside of an open market context by structure type and a variety of geographies.\n\nExamining the data, it appears that market mechanisms account for the majority of transactions by which properties transfer for nearly all property types. But a substantial minority, ranging from just over 10% (e.g. for condominium apartments in Kelowna) to about 50% (e.g. for mobile homes in the Abbotsford/Mission Metro Area) occur outside of the open market. Interestingly, Vancouver stands out as the CMA with a largest share, 38% of single-detached homes being sold outside of the open market.\nWhat does this mean in the StatCan data?\nAs per StatCan (footnote 5): “A”sale type” of residential property refers to whether a property was sold in a market or non-market sale. “Market sale” refers to an arm’s length transaction where all parties act independently with no influence over the other. “Non-market sale” refers to non-arm’s length transactions, which includes distressed sales, foreclosures, trade and forfeitures, redemptions, sales of part interest, and special interest sales.” We don’t get the full breakdown of non-market sale type in tables, but as described in the StatCan report, most non-market transactions appear to be between related family members. Globe & Mail coverage offers further commentary: “We see that the vast majority of non-market transactions are between relatives,” said Jean-Philippe Deschamps-Laporte, head of the CHSP. “Given the value of those assets, there seems to be a significant shift of wealth between relatives in that segment of the market.”\nHow many of these transactions are simple inheritances? We don’t know, but we can break down transaction type by age of building to perhaps take a peek.\n\nOlder dwellings are far more likely to transfer through non-market means than newer dwellings. Given that older adults don’t move much and often end up in older housing stock, this might provide some evidence that inheritance drives a large portion of non-market transactions. It may also help account for the lower proportion of condominium apartments (typically a newer form of housing) that transfer outside of the market.\nLet’s return to thinking about what this means for the commodification of housing. Non-market transactions seem to defy commodification. Indeed, we can imagine situations where housing was built by the owner (as opposed to built on spec) and passed on to her children, such that the residential property never passed through a market transaction. Similarly, for properties built for sale by developers, inheritance and other non-market transactions would seem to be a route to decommodification, removing their transfer from markets.\nBut inheritance is probably not the route to decommodification most commentators imagine.\nA different route to decommodification arises from removing housing from market distribution. There remains a small public housing sector in Canada, though it’s mostly concentrated in places like Toronto. In places like BC, public housing has largely been sold off, in many cases to non-profit societies, though BC infamously sold off some of its last remaining stock of public housing to Holborn (also the developer of Trump Tower) with the sale of Little Mountain. We can combine public housing together with subsidized non-profit housing to get a sense of the size of the housing stock that has been taken outside of markets (decommodified) in its distribution. This is housing distributed more on the basis of need, as determined by provider. That said, ideally we might need to exclude those with portable subsidies, who might still be understood as engaging in market transactions.\n\nOverall we get figures fitting comfortably between 2% to 5%. This path to decommodifying housing remains pretty small in Canada. Perhaps it’s no coincidence that our subsidized housing comes with some pretty serious waitlists. Comparing two to five per cent of housing removed from the market through public and social housing ownership and distribution mechanisms, we’re left with the vast majority privately held. But private ownership does not mean market distribution or even market transfer. Indeed, even if only a quarter of properties were transferred via non-market mechanisms, the sheer size of the housing stock decommodified via inheritance is much larger than the size decommodified via public or social housing distribution.\nWe could, of course, take the analysis further. Distribution of housing via private rental can also occur outside of the open market, and we could go on to resurrecting our distinction between “market-able rentals” and “market rentals.” Some rentals in “market-able” properties are rented out to non-arm’s length tenancies, often to family members or friends. Parents regularly house their children for free! But many other relationships can also lead to distribution of housing outside of the market. A great example can be found wrapped in the story of the shooting of a prominent Vancouver bike-store owner. In principle we have data on this now in Vancouver via the city Empty Homes Tax, as well as the provincial Speculation and Vacancy Tax, where the arm’s length (or non-arm’s length) nature of tenancies needs to be declared. With luck we will come back to this and examine this in detail at some later point in time."
  },
  {
    "objectID": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#decommodification-or-bust",
    "href": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#decommodification-or-bust",
    "title": "Commodity and Keeping it in the Family",
    "section": "Decommodification or Bust?",
    "text": "Decommodification or Bust?\nOn one hand, housing has always made for an uneasy fit with commodification as theory and rhetoric. After all, most critical theorizing (as with Marx) focuses on the intent of production (for market sale) as defining commodification via the labour process, as considered largely within factory settings. But housing is seldom built in factories, often involves owner labour, and is extremely durable, typically cycling through multiple transactions and owners. How much does the intent of its construction matter when public housing or owner-built housing can become sold via market mechanisms later on, just as housing produced by market-oriented developers can later become social housing or pass between family members? Flipping this logic around, the potential scope of commodification and decommodification related to housing expands enormously when we consider the contexts of all transactions and distribution of housing instead of simply production.\nBut examining transactions tell us something else important: Decommodification can come in multiple flavours.\nThe decommodification of housing can involve transitioning housing into social and public corporations, as described above, and making it available to people on the basis of need. But decommodification can also involve inheritances between family members that we see are strikingly common in private housing. Indeed, far more dwellings likely transfer through inheritance in a given year than become distributed through public and social housing.\nAll this can make for strange bedfellows in the fight to decommodify housing. The child of wealth set to inherit a mansion may be quite happy to line up with the Marxist-inspired activist in decrying the commodification of housing. But the child of wealth may step away from obligations to pay for any additions to the public or social housing stock. Put differently, decommodification rhetoric may seem like a worthy rallying point in the cause of establishing a Right to Housing. But that’s not the only end to which it can be put, and we appear to be far closer to stepping backward toward our history of inheritance between landed lords than forward into a utopia of public housing for all.\nCan we get to a Right to Housing without decommodifying all housing? Probably. An alternative is to push for housing for all, made available in an abundance of forms and through an abundance of transaction types. As argued by careful analysts, we need more “market-able housing” as well as more housing reserved or otherwise made available to people who can’t or don’t wish to compete within markets."
  },
  {
    "objectID": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#next-steps",
    "href": "posts/2021-08-13-commodity-and-keeping-it-in-the-family/index.html#next-steps",
    "title": "Commodity and Keeping it in the Family",
    "section": "Next steps",
    "text": "Next steps\nNext we want to move beyond ownership to get a more comprehensive look at how housing is distributed. The decommodification of “market” ownership properties through non-arm’s length sales transaction, and the commodification of public housing via sales to private ownership have analogies on the rental side. Preliminary evidence suggests a non-trivial portion of rental tenancies are non-arm’s length, often involving family members, and a striking number of tenants in private housing report paying no rent. On the flip side, many examples can be found of market transactions emerging around non-market housing (via key-money or subletting). But we will leave broader ranging coverage of housing distribution to another post.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-08-13 17:24:31 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [b939461] 2021-08-13: tag line\n## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cancensus_0.4.3 cansim_0.3.9    forcats_0.5.1   stringr_1.4.0  \n##  [5] dplyr_1.0.7     purrr_0.3.4     readr_1.4.0     tidyr_1.1.3    \n##  [9] tibble_3.1.2    ggplot2_3.3.3   tidyverse_1.3.1\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.7        lubridate_1.7.10  assertthat_0.2.1  digest_0.6.27    \n##  [5] utf8_1.2.1        R6_2.5.0          cellranger_1.1.0  backports_1.2.1  \n##  [9] reprex_2.0.0      evaluate_0.14     httr_1.4.2        highr_0.9        \n## [13] blogdown_1.4      pillar_1.6.1      rlang_0.4.11      readxl_1.3.1     \n## [17] rstudioapi_0.13   jquerylib_0.1.4   rmarkdown_2.8     labeling_0.4.2   \n## [21] munsell_0.5.0     broom_0.7.6       compiler_4.1.0    modelr_0.1.8     \n## [25] xfun_0.24         pkgconfig_2.0.3   htmltools_0.5.1.1 tidyselect_1.1.1 \n## [29] bookdown_0.22     codetools_0.2-18  fansi_0.5.0       crayon_1.4.1     \n## [33] dbplyr_2.1.1      withr_2.4.2       grid_4.1.0        jsonlite_1.7.2   \n## [37] gtable_0.3.0      lifecycle_1.0.0   DBI_1.1.1         git2r_0.28.0     \n## [41] magrittr_2.0.1    scales_1.1.1      cli_3.0.1         stringi_1.7.3    \n## [45] farver_2.1.0      fs_1.5.0          xml2_1.3.2        bslib_0.2.5.1    \n## [49] ellipsis_0.3.2    generics_0.1.0    vctrs_0.3.8       tools_4.1.0      \n## [53] glue_1.4.2        hms_1.1.0         yaml_2.2.1        colorspace_2.0-1 \n## [57] rvest_1.0.0       knitr_1.33        haven_2.4.1       sass_0.4.0"
  },
  {
    "objectID": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html",
    "href": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html",
    "title": "Basement Confidential: Vancouver’s Informal Housing Stock",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#the-size-of-vancouvers-informal-secondary-suite-market",
    "href": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#the-size-of-vancouvers-informal-secondary-suite-market",
    "title": "Basement Confidential: Vancouver’s Informal Housing Stock",
    "section": "The size of Vancouver’s informal secondary suite market",
    "text": "The size of Vancouver’s informal secondary suite market\nEstimating the size of the informal housing market tends to be difficult precisely because the housing evades regulation, and Vancouver is no different in this regard. There have been a variety of attempts to quantify the number of secondary suites in the city, ranging from using census data, to roll data from BC Assessment, to MLS listings data (see Metro Vancouver’s last round-up estimates here). Other potential sources of information include rental listings data.\nThe Canadian Census has attempted to classify single family homes with secondary suites as two “duplex” units, the main unit and the secondary suite. In cases where the census determined there might be more than one suite the homes have been classified as “apartment, fewer than 5 storeys.” But it’s not always easy for Census workers to find suites or determine if or when they’ve been re-absorbed into the main dwelling. Nor is there a clear mechanism for residents filling out Census forms to make corrections. We can see how the Census has struggled to identify informal dwelling units by observing the shift from “single-detached” dwellings to “duplex” units over time.\n\nThe majority of the observed growth in duplex units is due to re-classification of existing units as the census got better at identifying secondary suites, initiated by the change in census methods between the 2001 and 2006 censuses and again between 2011 and 2016. Half these units were generally understood to be the main units in suited houses, the other half are the secondary suites. The census does not give us a straight-forward way to distinguish between the two. We note that a high share of duplex units register as “unoccupied” in the census, and we strongly suspect that many of these are secondary suites that have been re-absorbed into the main unit.\nUnderstanding homes with more than one secondary suite through the census is hard, one way to get at this is to mix assessment and census data and look for “apartment, fewer than five storey” units in areas which are exclusively covered by “single family” homes (SFH). We have done this in the past, and while this necessarily paints an incomplete picture, it can still reveal a general sense of the prevalence of heavily suited homes and their geographic distribution. We have previously estimated that there are around 7,000 SFH with more than one suite, which come on top of the roughly 30,000 SFH with a single suite. We should probably revisit this estimate at some point with newer data and a more rigorous methodology.\nTo better understand who lives in these units we can again turn to census data, focusing in on duplexes (SFH with a single suite). This is only looking at the occupied units, and within these the units “occupied by usual residents”. Splitting these units by tenure and age of the primary household maintainer, we notice that the maintainers of owner-occupied units skew older while those of rental units, likely mostly our secondary suites, skew younger.\n\nWe also notice a curious pattern in that there are more owner-occupied units than total number of SFH of this type. Given that suites in the City of Vancouver cannot be stratified (and hence easily divided into separately owned properties), we might expect a more even division of owner-occupied to rental suites. We might even expect more rentals, to the extent that some houses are split into suites where both are rented out by landlords who live elsewhere. To find the opposite - that owner-occupied units dominate - can be interpreted multiple ways, speaking in part to the ambiguity of secondary suites.\n\nThe census may be counting suites re-absorbed by owner-occupiers as still separate, but empty. This would boost owner-occupation rates.\nRented secondary suites could simply turn over more often, leaving them temporarily empty around Census time, as when rented out to students.\nOr there may be other reasons those renting secondary suites may be less likely to respond to the long from Census.\nOn the other side of the ledger, owner-occupying families and households may divide themselves up into suited homes and list themselves in the census in complicated ways. Children (or parents or other relatives) living in a secondary suites may not be paying rent or think of themselves as renters, but instead understand themselves to be part of an owner-occupied compound, even when reporting as a separate household from their parents within the Census.\nAlternatively, those spread across multiple suites may still report themselves as part of the same Census household, in some cases leading to the reporting of complex household types.\nA small number of “duplex” units may be stratified and allow for more than one owner.\n\nPoints 1, 2, and 5 only impact the renter/owner share, but can’t explain why there are more owner households than overall buildings. Point 3 may lead to overreporting of owner households, but probably can’t explain the discrepancy on it’s own. Points 4 and 6 likely contribute to this. All of these points highlight the complexity of capturing information on informal housing.\nWe can examine the household types that occupy these units directly within the Census.\n\nWe notice the elevated portion of non-census family households in secondary suites, which are either one-person households or roommate households. The “Complex census family households” are comprised of multi-generational households as well as households of census families with additional people living in the household. The prevalence of these on the ownership side probably speaks to suited homes being used by multi-generational households.\nFor comparison purposes, here is the same breakdown for single-detached homes, so those SFH where the census did not identify a secondary suite. This form still has a sizable portion of “complex” households, but at a lower share than what we see in suites SFH."
  },
  {
    "objectID": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#the-formal-portion-of-secondary-suite-housing",
    "href": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#the-formal-portion-of-secondary-suite-housing",
    "title": "Basement Confidential: Vancouver’s Informal Housing Stock",
    "section": "The formal portion of secondary suite housing",
    "text": "The formal portion of secondary suite housing\nTo understand the formal portion of the secondary suite stock we can turn to City of Vancouver secondary suite rental licensing. There is a broad spectrum here between licensed units and those that have no hope of complying to building code because of e.g. ceilings that are too low. Some units may be easily license-able, but have not obtained a license for other unspecified reasons, including simple ignorance. While the City of Vancouver has a relatively straight-forward webpage ddescribing the licensing process, the necessity of obtaining a license isn’t widely advertised, and some of the categories remain ambiguous. Have a look at the license application form.\n\nHere we’ll stick with secondary suite licenses, which is where landlords are supposed to go to formalize their rental suites. Looking at the number of licenses over time we note that the city has been successful in gradually bringing more secondary suites into the formal housing sector. But comparing to the Census it appears the overall share of formalized licensed suites is still quite low at around 10% of all rented suites."
  },
  {
    "objectID": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#new-suites",
    "href": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#new-suites",
    "title": "Basement Confidential: Vancouver’s Informal Housing Stock",
    "section": "New suites",
    "text": "New suites\nSuites in newly constructed housing will be up to building code, so there won’t be any barriers to integrating them into the formal building stock. But how many new houses have suites? And how many suites get lost to teardowns. The latter question is hard to answer with publicly available data (and still difficult with non-open data sources like BC Assessment roll data and MLS data). But the answer to the former question, how many new homes have suites, can be obtained from building permit data. We have looked into this question a couple of years ago, this is a good opportunity for an update with newer data.\n\nThe addition/alterations data reflects the makeup of the SFH stock being formally altered and pulling a building permit for the work. Here we see that the majority of these homes do not have a suite, but the overall mixture of suited to non-suited homes roughly matches our expectation of the overall building stock. It is not clear how representative homes that take out a building permit for modifications might be. Owners of properties with existing code violations that might be expensive to fix may choose to forego a building permit as to avoid a building inspector visiting the house (and any casual investigation will reveal a lot of informal alterations in Vancouver).\nDemolitions, as well as salvage/abatement permits (most demolitions require both types of permits) give an indication of the suited lots in the redevelopment process, although this may under-estimate the suites that are getting lost as unpermitted suites may not be declared prior to demolition. Homes that get demolished tend to be older. There is no information on how many homes with multiple suites get torn down, which would indicate a higher loss in suites than the data suggests at first sight. The jump in the distribution of units with and without suites between 2017 and 2018 looks suspiciously like a change in accounting methods, it will take more work to look into this.\nOverall it would appear that new buildings tend to come with suites more often then not, and these suites will automatically be up to building code and easily permittable, if the owner wishes to rent them out. This indicates that the City of Vancouver is likely still gaining suites on net, but not at a high rate. Census data suggests that the addition of suites, and laneway houses has barely managed to counteract the loss of population due to shrinking household sizes in many parts of Vancouver.\nBefore wrapping up, let’s quickly look at how the formalized addition of a suite relates to the building value of permitted houses constructed.\n\nThough the mismatch is diminishing, it’s clear that SFH without suites tend to have more expensive building values, and are likely in more expensive parts of town. This is something that we have generally seen in the distribution of suites, they tend to cluster on the East Side of Vancouver, with houses on the West Side remaining more exclusive and exclusionary."
  },
  {
    "objectID": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#upshot",
    "href": "posts/2021-06-08-basement-confidential-vancouver-s-informal-housing-stock/index.html#upshot",
    "title": "Basement Confidential: Vancouver’s Informal Housing Stock",
    "section": "Upshot",
    "text": "Upshot\nSuites are great, and the flexibility of this type of housing adds relatively low-barrier options for households that can’t find other places to live in low vacancy cities like Vancouver. For owners this means the ability to gain rental income to support their mortgage payments. For renters it means finding housing that - while often below building standards - often rents cheap. (The lower rent and sub-standard nature of suites are of course connected.)\nThis indicates that there is indeed a need and a market for housing that falls below the current building standards, where people are willing to trade centrality of location and lower rent for lower-standard dwelling units. Correspondingly, we may want to adapt our building code to:\n\nenable more secondary suites to become permitted and transition into the formal housing market, and\nallow people to make some of these trade-offs in the formal housing market.\n\nOn the other hand, there is a large portion of suites that won’t ever become part of the formal housing market because they still won’t be permittable after reasonable modifications of the building code (and don’t forget the fire code!). These suites exist because of a profound lack of alternatives, and policy should aim to build enough abundant rental options so that these units become unnecessary, as with similar strategies to replace Single-Room Occupancy with more livable, supportive, and affordable forms of housing. In the context of Metro Vancouver, this will likely be a long-term strategy. The housing shortage is too large to envision fully formalizing or replacing our informal housing stock anytime soon.\nAs usual, the code for this post is available on GitHub.\n\n\nReproducibility receipt\n\n## [1] \"2021-07-25 12:55:16 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [7f011b6] 2021-07-14: cap gains link to code and fix typo in empty homes tax expectation post, thanks Natalie\n## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] statcanXtabs_0.1.1 cancensus_0.4.3    VancouvR_0.1.6     forcats_0.5.1     \n##  [5] stringr_1.4.0      dplyr_1.0.7        purrr_0.3.4        readr_1.4.0       \n##  [9] tidyr_1.1.3        tibble_3.1.2       ggplot2_3.3.3      tidyverse_1.3.1   \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.7         lubridate_1.7.10   class_7.3-19       assertthat_0.2.1  \n##  [5] digest_0.6.27      utf8_1.2.1         R6_2.5.0           cellranger_1.1.0  \n##  [9] backports_1.2.1    reprex_2.0.0       evaluate_0.14      e1071_1.7-7       \n## [13] httr_1.4.2         blogdown_1.4       pillar_1.6.1       rlang_0.4.11      \n## [17] readxl_1.3.1       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.8     \n## [21] urltools_1.7.3     triebeard_0.3.0    munsell_0.5.0      proxy_0.4-26      \n## [25] broom_0.7.6        compiler_4.1.0     modelr_0.1.8       xfun_0.24         \n## [29] pkgconfig_2.0.3    htmltools_0.5.1.1  tidyselect_1.1.1   bookdown_0.22     \n## [33] fansi_0.5.0        crayon_1.4.1       dbplyr_2.1.1       withr_2.4.2       \n## [37] sf_1.0-0           grid_4.1.0         jsonlite_1.7.2     gtable_0.3.0      \n## [41] lifecycle_1.0.0    DBI_1.1.1          git2r_0.28.0       magrittr_2.0.1    \n## [45] units_0.7-2        scales_1.1.1       KernSmooth_2.23-20 cli_3.0.1         \n## [49] stringi_1.7.3      fs_1.5.0           xml2_1.3.2         bslib_0.2.5.1     \n## [53] ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.8        tools_4.1.0       \n## [57] glue_1.4.2         hms_1.1.0          yaml_2.2.1         colorspace_2.0-1  \n## [61] classInt_0.4-3     rvest_1.0.0        knitr_1.33         haven_2.4.1       \n## [65] sass_0.4.0"
  },
  {
    "objectID": "posts/2021-03-10-vaxx-vs-vocs/index.html",
    "href": "posts/2021-03-10-vaxx-vs-vocs/index.html",
    "title": "Vaxx vs VOCs",
    "section": "",
    "text": "At this stage in the pandemic there is good news and bad news. The good news is that vaccines are ramping up. And change in dosing schedule means more people are getting some level of protection earlier. The bad news is that variants of concern, or VOCs, are on the rise in BC. We have a decent intuition how each one of these changes our pandemic, but unclear how they interact. Thus, it’s time for some modelling. We will keep the model as simple as possible, yet complex enough to capture the interaction between VOCs and vaccinations.\nBefore we can do modelling we have to assemble the main ingredients."
  },
  {
    "objectID": "posts/2021-03-10-vaxx-vs-vocs/index.html#vaccinations",
    "href": "posts/2021-03-10-vaxx-vs-vocs/index.html#vaccinations",
    "title": "Vaxx vs VOCs",
    "section": "Vaccinations",
    "text": "Vaccinations\nWe need to understand how many vaccine doses we are getting at what point in time. The three vaccines we are getting in BC are about equally effective, we will assume a 90% efficacy in blocking transmissions starting two weeks after getting the first dose. We still don’t have good data on the transmission blocking ability of vaccinations, but early reports are encouraging and we go with the optimistic estimate for this post. And we will assume that we are fairly efficient in getting the vaccines into people’s arms, on average within one week of delivery.\nPHAC has a rough vaccination delivery schedule that we can scrape.\n We are interested in the vaccine we are getting now, so we start counting the patch of BioNTech vaccines we got in March. Cumulatively that’s about 1 million doses.\n\nThat gives us the data we need to estimate roughly how many people we will vaccinate and how much immunity this will yield. In our simple model we will not distinguish people for whom the vaccine did not grant immunity from people that have not gotten the vaccine. It’s not that hard to do, but it does not make a difference to the model since we aren’t looking at outcomes like hospitalizations and deaths where partial immunity still matters. And we aren’t modelling long enough timeframes where it would matter that we can’t pinpoint ahead of time from whom the vaccines will be less effective."
  },
  {
    "objectID": "posts/2021-03-10-vaxx-vs-vocs/index.html#variants-of-concern",
    "href": "posts/2021-03-10-vaxx-vs-vocs/index.html#variants-of-concern",
    "title": "Vaxx vs VOCs",
    "section": "Variants of concern",
    "text": "Variants of concern\nHere the problem starts: unlike Ontario, BC still does not release useful data on variants of concern. That means that the share of cases that are variants of concern right now can only be guessed at. Just last week our PHO said that she believes variants of concern hover around 1%, which we find highly questionable. That may have been the state of things in early February, from the data we do see, we believe a better guess would be about 10% at this point in time. But it may well be only 5% or already 15%. The continued refusal to release daily N501Y screening results to allow painting a decent picture of VOC development in BC is frustrating.\nThe other part we need to understand is the growth rate advantage that variants of concern have. Studies based on UK data have pegged B.1.1.7, the main variant in circulation in BC, to be about 1.5 times more infectious than regular COVID-19. That was based on PCR SGTF as a proxy for B.1.1.7. It’s a pretty decent proxy. Although there is some noise that probably should be dealt with when estimating the growth rate advantage. (Dean Karlen has a cool approach to deal with this, which got me interested in running these kind of estimates.)\nDenmark does a lot of whole genome sequencing and is probably the cleanest data to look at the B.1.1.7 growth rate advantage. To estimate the growth rate advantage we note that over short time periods where behaviour and the susceptible population is relatively constant, the number of cases \\(C_0(t)\\) and \\(C_v(t)\\) for regular and variant cases is well approximated by \\[\nC_0(t) = C_0 e^{r_0t}, \\qquad C_v(t) = C_v e^{(r_a+r_0)t},\n\\] where \\(C_0\\) and \\(C_v\\) are the number of regular and variant COVID-19 cases at time 0, \\(r_0\\) is the base growth rate for regular COVID-19 and \\(r_a\\) is the growth rate advantage of the B.1.1.7 variant. Taking the ratio of variant to non-variant cases\n\\[\n\\kappa = \\frac{C_v(t)}{C_0(t)} = \\frac{C_v}{C_0} e^{r_at}, \\qquad\\qquad \\log(\\kappa) = \\hat c + r_a\\cdot t\n\\] we see that the growth rate advantage is simply the slope of the log of the ratios over time. That’s easy to estimate from the data.\n\nThe fit is really tight, in Denmark B.1.1.7 has been consistently growing at a rate of 0.084 ±0.002 faster than regular COVID-19. Will the same be true in BC? We don’t know for sure, but we can look at the N501Y proxy data from Ontario for some clues on how B.1.1.7 behaves in Canada. N501Y is not the same as B.1.1.7, but should still be a cleaner proxy than SGTF. Let’s take a look.\n\nIt’s definitely more messy, Ontario does less screening than Denmark does whole genome sequencing. But the overall relationship comes out remarkably similar. We will be assuming a transmission advantage of 0.084 for this post. (There are some technical details here that we are glossing over, but they are minor.)\nFor this post we are not interested in the question of more severe outcomes of variants of concern since we only look at case counts. Moreover, we will ignore the question of immune escape of variants because we don’t have good data on this. But these are serious concerns that more rigorous modelling needs to include."
  },
  {
    "objectID": "posts/2021-03-10-vaxx-vs-vocs/index.html#sir-model-with-vaccination",
    "href": "posts/2021-03-10-vaxx-vs-vocs/index.html#sir-model-with-vaccination",
    "title": "Vaxx vs VOCs",
    "section": "SIR Model with Vaccination",
    "text": "SIR Model with Vaccination\nFor this we have to put in slightly more work than in previous posts where we ran a simple exponential model to account for vaccination. Instead of typing out the equations we just show the code for the standard SIR model that we have extended to include vaccinations and also keep track of cumulative infections from which we can recover daily case counts.\nmodel.vaxx &lt;- function(times,yinit,vaccination_rate,paramters){\n  SIR_model &lt;- function(time,yinit,paramters){\n    with(as.list(c(yinit,paramters)), {\n      vr &lt;- vaccination_rate(time)\n      \n      dSusceptible &lt;- -beta*Infected*Susceptible - vr\n      dCumulativeInfected  &lt;- beta*Infected*Susceptible \n      dInfected  &lt;- beta*Infected*Susceptible - gamma*Infected\n      dRecovered &lt;- gamma*Infected\n      dVaccinated &lt;- vr\n      \n      return(list(c(dSusceptible, dInfected, dCumulativeInfected, dRecovered,dVaccinated)))}) \n  }\n\node(func = SIR_model,times = times,y = yinit,parms = paramters) %&gt;%\n      as_tibble() %&gt;%\n      mutate_all(as.numeric)\n}\nKeen eyes will notice that we did not set up separate compartments for variant and regular COVID-19. Instead we will run this as two separate simulations, the difference is that in this setup we allow people in theory to be infected by both, regular COVID-19 and a variant. But that’s going to be very rare and won’t affect the outcomes much as most immunity comes from vaccinations and not from infections. We feel that keeping the model simple is worth the tradeoff. (And we are lazy.)\nTo start let’s look at the effect of our ramped-up vaccination schedule compared to continuing vaccinating only about 2% of the population per month as we have been doing up to now. For now, let’s forget about variants and pretend they did not exist.\n\nThe vaccination effect is clearly visible. There are several assumptions baked into this. One is our initial growth rate, which we set to a very mild increase in cases. The other is how we vaccinate. We know that our age-based vaccination schedule won’t be as good at reducing transmissions as other ways to vaccinate, including vaccinating randomly. This means that the effect of the vaccinations on cases won’t be quite as pronounced as in the above graph. But it will still be strong.\nLet’s see what happens when we add VOC into the mix. Especially for B.1.1.7 we got really good data on the transmissibility advantage as outlined above. In BC we don’t really know what share of current cases are variants, but 10% is probably a decent guess.\n\nThat gives an idea of how bad that added transmissibility really is, and how much the accelerated vaccine schedule is helping. (Although this is likely over-estimating the vaccine effect a bit.) And it also shows that even with the current vaccine schedule, variants of concern are very … concerning. Let’s focus in on our vaccination schedule to get a better image.\n\nFocusing vaccinations on older populations helps keep mortality low in face of higher case counts. But this does not address problems like Long COVID-19. And additionally, we have now good evidence that B.1.1.7 has higher morbidity and mortality than regular COVID-19, which may well outweigh the reduction in mortality achieved through our vaccines. (Someone should model that!)\nBut what about the assumption that we currently have 10% variants of concern? Since BC does not release timely counts with dates and denominators, we are adding graphs for 5% or 15%\n\nThe change in the VOC portion is large, the change in non-VOC cases is largely irrelevant. It shows how important it is to get a decent read on how many variant cases we have right now. That range of assumptions is too large to robustly base policy on.\nThis is a good time to remind readers that these are projections, and they show what will likely happen if there are no changes in regulations or behaviour. If we open up more, cases will likely rise faster. If we add restrictions, case growth will slow down.\nSome people have expressed hopes that warmer weather will bring down case counts. Unfortunately there is little evidence that this will happen to an appreciable degree. And our experience last fall has shown that case growth was quite uniform between July and early November, with the exception of a short period of lower growth around September.\nAs usual, the code is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-03-12 10:08:11 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [363726f] 2021-03-11: vaxx-vs-vocs\n## R version 4.0.3 (2020-10-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.2 broom_0.7.4              \n##  [3] deSolve_1.28              forcats_0.5.0            \n##  [5] stringr_1.4.0             dplyr_1.0.4              \n##  [7] purrr_0.3.4               readr_1.4.0              \n##  [9] tidyr_1.1.3               tibble_3.0.4             \n## [11] ggplot2_3.3.3             tidyverse_1.3.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.0  xfun_0.18         haven_2.3.1       colorspace_2.0-0 \n##  [5] vctrs_0.3.6       generics_0.1.0    htmltools_0.5.0   yaml_2.2.1       \n##  [9] blob_1.2.1        rlang_0.4.9       pillar_1.4.7      glue_1.4.2       \n## [13] withr_2.3.0       DBI_1.1.0         dbplyr_1.4.4      modelr_0.1.8     \n## [17] readxl_1.3.1      lifecycle_0.2.0   munsell_0.5.0     blogdown_0.19    \n## [21] gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6       evaluate_0.14    \n## [25] knitr_1.30        fansi_0.4.1       Rcpp_1.0.5        scales_1.1.1     \n## [29] backports_1.2.0   jsonlite_1.7.2    fs_1.4.1          hms_0.5.3        \n## [33] digest_0.6.27     stringi_1.5.3     bookdown_0.19     grid_4.0.3       \n## [37] cli_2.2.0         tools_4.0.3       magrittr_2.0.1    crayon_1.3.4     \n## [41] pkgconfig_2.0.3   ellipsis_0.3.1    xml2_1.3.2        reprex_0.3.0     \n## [45] lubridate_1.7.9.2 assertthat_0.2.1  rmarkdown_2.7     httr_1.4.2       \n## [49] rstudioapi_0.13   R6_2.5.0          git2r_0.27.1      compiler_4.0.3"
  },
  {
    "objectID": "posts/2021-02-21-on-covid-and-exponential-growth/index.html",
    "href": "posts/2021-02-21-on-covid-and-exponential-growth/index.html",
    "title": "On COVID and exponential growth",
    "section": "",
    "text": "The recent dismissal of PHAC modelling by the head of the BCCDC, coupled with some reactions we have seen on Twitter, have led us realize how hard it is for most people to understand exponential growth. Part of the fault lies with most modellers generally assume too much math literacy in others. In particular, we assume that public health officials or relevant policy makers can understand the models. Even though we have seen time and time again that this assumption is very tenuous. But more importantly, their inability to understand the models will not help them derive at the desirable responses and will not be able to explain to the general public.\nSo how can we explain those models better? I don’t have a good answer, but I have some ideas which I want to try out in this post."
  },
  {
    "objectID": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#how-did-we-get-to-where-we-are",
    "href": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#how-did-we-get-to-where-we-are",
    "title": "On COVID and exponential growth",
    "section": "How did we get to where we are?",
    "text": "How did we get to where we are?\nA good starting point is to look back in time and understand how we got to our current state. Here is the graph I like to show for this, it does some mild cleaning of daily counts to extract a trend line.\n\nThis graph shows major and minor changes in public health measures, coloured by whether they loosen (green) or tighten (red) restrictions. Because of the lag between transmissions and cases getting reported, the graph is showing cases by the date the lab result came back, there will be a lag between a change in public health measures and when we see the effect in the case counts. We should expect that lag to be somewhere between 7 and 14 days.\nThis graph does a good job at showing the dramatic rise in cases in November, but it does not explain well how we got there. Our mind tends to interpret graphs like this as linear. We see a slope that is steepening and we want to say that COVID is spreading faster. But that’s not true. The growth rate of COVID, sometimes expressed in terms of the reproduction number R, has been fairly constant since the beginning of July, only taking an intermittent dip in September.\nTo visualize this we can go to a (semi-) log plot.\nIn the beginning period our testing wasn’t very good, so it does not make much sense to look at case counts before May, when changes in cases were also heavily impacted by changes in testing and not just changes in transmissions.\n\nIn this graph we have segmented the time since the end of March into 6 phases where we have seen fairly constant growth (or decline) of cases, and we draw the corresponding thick purple lines that fit these stages well. The way we divide things up into phases of constant growth is a bit ad-hoc. We could have chosen slightly different change times, but results are similar. The cause why growth rates flattened in September is not clear for me, the other changes line up well with major policy interventions. The slopes of these lines are the fitted growth rates during these time intervals.\n\n\n\nStart\nEnd\nGrowth rate\nR\n\n\n\n\n2020-04-25\n2020-05-25\n-4.9%\n0.73\n\n\n2020-05-25\n2020-07-01\n0.7%\n1.05\n\n\n2020-07-01\n2020-08-20\n3.9%\n1.29\n\n\n2020-08-20\n2020-10-07\n0.8%\n1.05\n\n\n2020-10-07\n2020-11-17\n4.7%\n1.36\n\n\n2020-11-17\n2021-02-18\n-0.7%\n0.96\n\n\n\nWe also added estimates of the effective reproduction number (the average number of transmissions each case causes) using the assumption that the generation interval is \\(\\tau=6.5\\) days, that is \\(R=e^{r\\cdot \\tau}\\) in our simple exponential growth model. We prefer to work with growth rates as this is the intrinsic quantity that captures viral growth. The reproduction number has an intuitive interpretation but is model dependent, so will vary depending on the type of model used to describe the growth. For example, a more realistic model using a distribution of generation intervals will end up with reproduction numbers that are scaled closer to 1.\nAn interesting exercise is to take the same graph with the fitted lines and show this on a linear scale.\n\nWhen looking at it this way we might be misled to think that the July/August growth rate was much less than the October/November growth rate, when in reality they were similar. We might think that our current rate of decline is greater than the one we saw at the end of the spring wave in May, when in fact the current decline is much softer. Both of these graphs have value, but they speak to different aspects of the pandemic.\nAs an aside, the basic observation that our pandemic is very well described by periods of constant growth can be leveraged to create predictions. Unless we change our public heath measures, cases will likely follow the same growth rate.\n\nThe current trajectory suggests that we will reach 100 cases around September 1st. However, projecting things out this far is probably not useful for several reasons. This assumes constant growth, meaning no changes in policy nor voluntary changes in behaviour over this time frame. However, we generally would expect a weakening of compliance with public health measures over time, and thus a flattening of the decline. At the same time, vaccinations are ramping up slowly, and as the share of the vaccinated population increases we will start to see impacts on transmissions. In fact, we are already seeing impacts on the 90+ and 80-89 year old age groups, their rates of cases have decreased dramatically over the past month.\n\nWhile vaccinations are good news, in the short term they won’t do much to overall case growth. While we can see the decline in older age groups when splitting them out, there is no discernible effect visible in overall case numbers. It will take a much higher rate of vaccinations to make a clear dint in the above graph.\nThis shows how fitting exponential curves to case growth runs into limits. It does a good job of explaining high-level behaviour of the virus, which is what we are after here. Better models will refine this and tease out higher level effects. Vaccinations is one of them, waning adherence to public health measures may be another.\nAnd now, we have a third effect that we are watching out for. Variants of Concern, or VOCs. Mutations of our original strain of increased fitness. Either because they are more easily transmissible like B.1.1.7, or because they escape some immunity like is believed to be the case for B.1.351. Both of these strains are now in circulation in BC, and they break our simplistic exponential model in more serious ways than pandemic fatigue or vaccinations do."
  },
  {
    "objectID": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#variants-of-concern",
    "href": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#variants-of-concern",
    "title": "On COVID and exponential growth",
    "section": "Variants of Concern",
    "text": "Variants of Concern\nWe focus on B.1.1.7, the variant first observed in the U.K. because we have very good data on it’s fitness advantage. While there are some uncertainties, it is generally believed to have an advantage that grants it a 1.5 times higher reproduction number. We need to translate this into how that affects our exponential growth rate, which is surprisingly tricky to do for reasons that go beyond the scope of this post. As mentioned above, the reproduction number \\(R\\) is model-dependent, and using the naive estimate of \\(r=\\ln(R)/\\tau\\), where \\(\\tau\\) is the mean generation interval, will under-estimate the growth rate. Fortunately, Dean Karlen recently estimated the daily growth rate advantage of B.1.1.7 in the UK directly from the data, and it comes out to be somewhere between 0.08 and 0.1. Based on this, we assume a (continuous) growth rate advantage of 0.086 for this post, which roughly corresponds to a daily growth rate of 0.09 in the middle of Dean’s range.\nGoing to our table above, that means that under current conditions in BC the B.1.1.7 strain has a growth rate of 0.079. That is higher than what we have seen this fall for regular COVID when we had looser restrictions. (Remember the the time when you could still have small gatherings with friends?) The difference in growth rates between regular COVID and VOC, in particular B.1.1.7, is so large that we can’t model this by fitting exponential growth any more. We need to treat this as two separate pandemics that occur concurrently. With regular COVID acting as a smoke screen for the more dangerous VOC.\nTo gather some intuition we take our current BC growth rate and level of regular COVID and seed a small B.1.1.7 VOC with initially 5 cases. Is this a good initial guess? We don’t know, but it’s something to go with and sharpen our intuition.\n\nWe see the decline of regular COVID in our current environment, it’s exactly the “predicted” curve from above that we get from fitting and exponential decline to case development since the end of November. It’s what we expect to see if everything stays the same. In this environment we seeded a B.1.1.7 variant at a level of 5 daily cases two weeks ago.\nCombining these two different pandemics we get the total number of cases we observe. The B.1.1.7 cases remain hidden in the background of regular COVID cases for quite a while, only in April is the upward trend of the combined cases.\nTo better understand why this is best thought of as two different pandemics, let’s take a look at this using our log scale.\n\nOn each own, regular COVID and B.1.1.7 cases look like lines on the log plot. This is of course by design, that’s the fundamental process of the virus and we modelled it that way. The sum of the two however does not follow this pattern. This might seem overly theoretical, but this also has played out in England.\nFeatures to deal with things like this are built into more complicated models. Not just models that incorporate new variants, in general models have differnet compartments where they vary the transmission rates between age groups, groups of people with different levels of adherence to public health orders (or groups exceeding public health expectations), with complex interactions between these groups. And some models go even further and vary these parameters at the individual person level, like the models we have used before to look at the effect of differnt interventions at schools.\nHowever, projecting things this far out is mostly a theoretical exercise, we will almost certainly see a strong public health reaction at some point down the road."
  },
  {
    "objectID": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#how-about-vaccines",
    "href": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#how-about-vaccines",
    "title": "On COVID and exponential growth",
    "section": "How about vaccines?",
    "text": "How about vaccines?\nWhen projecting out into April we may have to start taking vaccination progress into consideration. In theory it is fairly straightforward to do this within the framework of our simplified exponential growth model, if we were vaccinating randomly we can simply model vaccination by reducing the growth rate by \\(\\ln(1-s_i)/\\tau\\), where \\(s_i\\) is the share of the population that’s immune and \\(\\tau\\) is the generation interval. Here immunity means that they won’t pass on the virus. We know the mRNA vaccines are around 95% effective in preventing severe outcomes, and there are some indication that they will also prevent transmission. This gets further complicated because we don’t really know how good the immunity, especially for transmission, is after only getting one shot.\nTo ballpark the rough effect of vaccines we can ask what percentage of the population needs to be vaccinated so that B.1.1.7 growth rate would be zero (or R for B.1.1.7 would be 1), assuming we don’t have any public health measures and vaccines are 100% effective at blocking transmissions. That’s just given by \\(1-\\frac1R\\) which comes out to 40%. We are quite far from that, even during the given long time window. Moreover, our vaccine schedule is going down by age, which will have a net effect on transmissions that is worse than vaccinating randomly. How to optimize the vaccination rollout to also reduce transmissions, indirectly reducing severe outcomes by reducing spread, is an interesting and ongoing topic of research."
  },
  {
    "objectID": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#the-problem-of-projecting-from-low-numbers",
    "href": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#the-problem-of-projecting-from-low-numbers",
    "title": "On COVID and exponential growth",
    "section": "The problem of projecting from low numbers",
    "text": "The problem of projecting from low numbers\nThere is one more point to talk about. The whole exponential growth approximation works well when case numbers are high. But when case numbers are low, as we have right now for B.1.1.7, things get complicated. COVID is highly over-dispersed, and we should assume the something similar is true for B.1.1.7. What that means in practice is that most people don’t pass on the virus, but a few pass it on to many to make up for it.\nIn practice, this means that having occasional cases does not mean B.1.1.7 will become established. There is a decent chance it will die out. But the more introductions we have, the more likely one of them will kick-start into a superspreader event and take hold for good. We have just seen this in Newfoundland and Labrador, where they are trying really hard to beat that outbreak back into the ground. But Newfoundland and Labrador have the advantage that they have almost no regular COVID, so they go hard against any COVID they see. We don’t have that luxury, we can’t go and automatically trace all contacts of contacts to beat down a new strain, our background COVID means we simply don’t have the resources for this.\nWhat does this mean for our curves? It means that our initial 5 cases could die back down to zero. Or jump up to 40 within a couple of days. This kind of low likelihood but large effect situation is hard to handle. Should we assume we may have already 40 daily cases established right now? In that case, our curve looks a lot scarier. It’s probably more likely we have 5 to 10 at this point, but what should we plan for? The worst case scenario? The best case scenario? It depends on our risk tolerance. The other problem is that we don’t have inter-provincial quarantine and we don’t have good international travel quarantine, we have already seen how leaky it was with the first B.1.1.7 cases we saw in the news. That means we will have continuous seeding of VOC, and more chances for them to take off.\nTo model this properly we would need much better information. Unfortunately the BCCDC does not share much useful information to inform that. We only get cumulative counts of VAC. No sample collection dates, so we don’t know where on our timeline we should place them. We don’t have denominators, how many cases got sequences or what the flow of cases through N501Y screening looks like, so we don’t know how representative the ones we found are. The newest cases that have been announces are from episode onset week 5, so they are two weeks old now. Today there have been some school related B.1.1.7 exposures announced, one of them looks quite recent whereas the other ones are quite old. Data from WGS lags quite a bit, and we don’t get data on N501Y screening in BC at all. Ontario published daily updates on both. In short, it’s a mess in BC and we have next to no useful information how we should seed the model.\nFor good measure, let’s look what scenarios look like where we assume 3 through 10 daily cases (on average) as of two weeks ago (the rough lag that we seem to have for detecting variants). This also gives an indication what would happen if we had an early superspreader event to significantly boost VOCs. We would have to go to more advanced models to better model the probability band of outcomes that incorporate the overdispersed nature of COVID, for now looking at different choices of how to see B.1.1.7 cases will have to do as a rough proxy of potential outcomes.\n\nOf course there are a lot of other uncertainties. There are some indications that our current growth rate might not be negative any more. It’s too early to say for sure, but the tail end of our case trend diverges from our model fit. If we assume we are currently at \\(R=1\\), then our growth estimates would look quite a bit worse.\n\nIf we think that we currently are in a growth regimen, then things would be worse again. On the other hand, without larger changes in public health measures, the changes in growth are mostly due to people slipping in their compliance, which may well reverse as variants of concern become established.\nThe difference in the last two graphs reminds us that growth rates really matter. And how dialling down the growth rate even a little can have large effects two months down the road."
  },
  {
    "objectID": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#bottom-line",
    "href": "posts/2021-02-21-on-covid-and-exponential-growth/index.html#bottom-line",
    "title": "On COVID and exponential growth",
    "section": "Bottom line",
    "text": "Bottom line\nThe big takeaway from this post is our pandemic is well described by sustained periods of constant exponential growth, with changes in growth rate usually initiated by changes in public health measures. The post also demonstrates that we should treat the emergence of new variants as a new pandemic that forms a “hidden wave” obscured by our regular COVID. Our background COVID makes it impossible to pinpoint new variants in a timely fashion, so we can’t effectively fight the new variants without fighting all COVID.\nThe model we used, simple exponential growth, loses some of the detail that more advanced models bring to this. On the upside it’s easy to understand and it (hopefully) brings a clearer understanding to what drives these more advanced models.\nThe last point is these models make it clear that we will have to intervene to reduce the growth of the new variants. Current public health measures simply aren’t anywhere near enough to contain it. The question is when to intervene. Waiting until new variants become established will require much stronger interventions later. We have seen how easy it was for the virus to grow in the fall, and how hard it is now to reverse some of this."
  },
  {
    "objectID": "posts/2021-02-03-bartholomew-s-dot-destiny/index.html",
    "href": "posts/2021-02-03-bartholomew-s-dot-destiny/index.html",
    "title": "Bartholomew’s dot destiny",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\n\nHow did early planners envision Vancouver’s future growth? Fortunately for us, they left a prediction in dot-density map form! Here we compare their prediction to a dot-density map from today. Let’s check out how our dot destiny unfolded!\nVancouver grew rapidly from its incorporation in 1886 right up to the great crash of 1913, followed by WWI and a raging influenza epidemic (which we all know way too much about now). Growth returned through the 1920s, but an appetite for planning also met with a newly professionalized planning profession during this era. The City of Vancouver, in the process of amalgamating with the surrounding municipalities of Point Grey and South Vancouver, initiated a town planning commission, adopted interim zoning by-laws, and hired American planner Harland Bartholomew to consult. Bartholomew’s team kept Vancouver planning in conversation with evolving practice in the USA, where he was a central figure in transforming many municipalities’ explicitly race-based zoning (outlawed by courts) into use-based zoning that would have the same effect (see local planner Stephanie Allen’s award-winning thesis for more). Bartholomew’s report, while not adopted in its entirety, is widely credited as having a profound effect on the shape of the City. Here we want to take a quick peek at his prediction for the City’s future.\nLooking forward from 1929, Bartholomew both suggested and predicted that Vancouver further amalgamate with nearby Burnaby and New Westminster, consolidating the peninsula. The combined population was about 280,000 at the time (reaching 289,681 residents by the 1931 census). Based on a variety of rudimentary forecasts, Bartholomew predicted that the peninsula containing Vancouver, Burnaby, and New Westminster would reach a population of one million residents by 1960. He even plotted out the expected distribution of this population in a lovely density dot-map on p. 94 of his report.\n\nAs it turned out it would take much longer than Bartholomew forecast to reach the one million mark. Indeed, we’ve probably reached it only within the last couple of years. As of 2016, Vancouver, Burnaby, and New Westminster remained unamalgamated, and together with UBC/UNA/UEL and Musqueam 2 (also unamalgamated) they totalled some 952,779 residents. We wanted to see what that distribution actually looks like today, using the same sort of dot-matrix map hand-drawn by Bartholomew’s team. Of course, we’re going to assemble it in R instead of drawing it by hand, allowing anyone to reproduce our work. Here’s what it looks like.\n\nComparing the two maps, a similar overall pattern emerges that reflects, in no small part, the enduring legacy of zoning enacted through the planning process itself. The forecast was that Downtown Vancouver and the West End would remain the most dense, reflecting the least restrictive zoning. The surrounding neighbourhoods would offer a middle density, with apartment buildings going up to three stories. Everywhere else would be dominated by relatively low-density (mostly single-family residential). The big picture today is broadly similar to the forecast from ninety years ago. In particular, all that zoning to protect low-density neighbourhoods remains stubbornly in place! But a few key differences in the map stand out.\nDowntown, Bartholomew’s team forecast a fairly even distribution of high density. The actual distribution is far more variable! We see fewer people than forecast within the Central Business District (CBD) itself, but many more within the high-rise apartment buildings surrounding the CBD. Notably, people also show up along the north side of False Creek, which Bartholomew forecast remaining industrial. Guess he didn’t foresee de-industrialization, Expo 86, and Li Ka-shing!\nOutside of Downtown Vancouver, some areas became more dense than anticipated, while others became less so, and these patterns are pretty interesting! On the more dense than anticipated side, we see regional town centres emerging as hotspots of density in Burnaby and New Westminster, and being linked together through transit-oriented development accompanying SkyTrain lines. We also see Kerrisdale and Marpole showing up as outposts of density. And then, of course, there’s the universities: SFU and UBC and surrounding Endowment Lands. Though large portions of the latter were set aside as Pacific Spirit Park, we see the towers housing an increasing portion of the community, as at Wesbrook Village.\nWhat of where density appears lower than forecast? Select portions of Fairview and Mount Pleasant (as surrounding Jonathan Rogers Park), were re-zoned as industrial land after Bartholomew’s plan, and their population correspondingly failed to grow. More intriguingly, Strathcona, Commercial Drive, and Kits Point also appear far less dense than forecast, due in part to downzonings over the years, making building in these locations increasingly restrictive.\nOf note, other factors also play a role in divergent forecasts. In particular, declines in household size from 4.4 in Bartholomew’s day to 2.4 in 2016 mean it takes significantly more housing now to contain one million people than when Bartholomew made his projection. This helps explain why the low-density, house-oriented portions of the map look even less dense than forecast by Bartholomew’s team.\n\nOverall, it’s a fun exercise to compare ninety year old forecasts in dot-density form to what we see today. And now is the perfect time to do it given we’ve finally matched the predicted population size! This is a great example of a self-fulfilling prophecy where much of the density distribution was enshrined in the zoning. But this exercise should also remind us that we’re still building our cities based on planning decisions about urban form coupled with misguided forecasts made by long-dead men operating in a very different - and more discriminatory - era. We can probably do better.\nAs usual, the code for this post is available on GitHub if others want to reproduce or adapt this for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-02-03 21:52:03 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [fe23106] 2021-02-04: reduce image size, fix link to nathan's blog\n## R version 4.0.3 (2020-10-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] rvest_0.3.6               xml2_1.3.2               \n##  [3] sf_0.9-7                  dotdensity_0.1.0         \n##  [5] mountainmathHelpers_0.1.2 cancensus_0.4.1          \n##  [7] forcats_0.5.0             stringr_1.4.0            \n##  [9] dplyr_1.0.3               purrr_0.3.4              \n## [11] readr_1.4.0               tidyr_1.1.2              \n## [13] tibble_3.0.4              ggplot2_3.3.3            \n## [15] tidyverse_1.3.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.0   xfun_0.18          haven_2.3.1        colorspace_2.0-0  \n##  [5] vctrs_0.3.5        generics_0.1.0     htmltools_0.5.0    yaml_2.2.1        \n##  [9] blob_1.2.1         rlang_0.4.9        e1071_1.7-4        pillar_1.4.7      \n## [13] glue_1.4.2         withr_2.3.0        DBI_1.1.0          dbplyr_1.4.4      \n## [17] modelr_0.1.8       readxl_1.3.1       lifecycle_0.2.0    munsell_0.5.0     \n## [21] blogdown_0.19      gtable_0.3.0       cellranger_1.1.0   codetools_0.2-16  \n## [25] evaluate_0.14      knitr_1.30         class_7.3-17       fansi_0.4.1       \n## [29] broom_0.7.4        Rcpp_1.0.5         KernSmooth_2.23-17 classInt_0.4-3    \n## [33] scales_1.1.1       backports_1.2.0    jsonlite_1.7.2     farver_2.0.3      \n## [37] fs_1.4.1           hms_0.5.3          digest_0.6.27      stringi_1.5.3     \n## [41] bookdown_0.19      grid_4.0.3         cli_2.2.0          tools_4.0.3       \n## [45] magrittr_2.0.1     crayon_1.3.4       pkgconfig_2.0.3    ellipsis_0.3.1    \n## [49] reprex_0.3.0       lubridate_1.7.9.2  assertthat_0.2.1   rmarkdown_2.5     \n## [53] httr_1.4.2         rstudioapi_0.13    R6_2.5.0           git2r_0.27.1      \n## [57] units_0.6-7        compiler_4.0.3\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2021,\n  author = {von Bergmann, Jens and Lauster, Nathan},\n  title = {Bartholomew’s Dot Destiny},\n  date = {2021-02-03},\n  url = {https://doodles.mountainmath.ca/posts/2021-02-03-bartholomew-s-dot-destiny},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von, and Nathan Lauster. 2021. “Bartholomew’s Dot\nDestiny.” MountanDoodles (blog). February 3, 2021. https://doodles.mountainmath.ca/posts/2021-02-03-bartholomew-s-dot-destiny."
  },
  {
    "objectID": "posts/2021-01-25-rethinking-the-foreignness-of-owners-living-abroad/index.html",
    "href": "posts/2021-01-25-rethinking-the-foreignness-of-owners-living-abroad/index.html",
    "title": "Rethinking the “foreignness” of owners living abroad",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nTLDR: Combining our two major sources of data on the “foreignness” of property owners suggests at least half of those owning property in high demand parts of BC but living outside of Canada are Canadian citizens or permanent residents.\nHow Foreign Are You?\nBC housing discussions have often focused on various aspects of “foreignness” – foreign buyers, foreign owners, non-resident owners, foreign capital, home owners with non-anglicized last names, out of province buyers, buyers on 10-year entry program, foreign landlords – the list goes on in bewildering variety, and each category comes with it’s own range of interpretations and definitions. Thanks to the BC’s Speculation and Vacancy Tax (SVT), and Statistics Canada’s attempt to consolidate ownership records through the CHSP dataset, we now have pretty good data on at least two definitions of “foreignness” for multiple years. This is especially great insofar as the latest data allows us to compare and contrast these definitions and possibly take a look at a group that rarely gets talked about in our housing discussions: Canadians who live abroad but still own property in BC.\nLet’s start with our two different definitions of “foreignness” at the property level. “Foreign Owned” properties, as defined via the SVT, are those owned by a person who isn’t a Canadian citizen or permanent resident of Canada. “Non-resident Owned” properties, as defined by the CHSP, are those where the owner is a person whose primary dwelling is outside of Canada. In both cases, where multiple owners exist, definitions can be narrowed (e.g. including properties as foreign owned only where all owners are foreign), expanded (e.g. including properties as foreign owned if any owners are foreign), or differentiated (e.g. setting aside properties where only some owners are foreign as “mixed”) accordingly.\nThe matchup between the two definitions of “foreignness” offered by SVT and CHSP is not perfect. By definition, SVT Foreign Owners includes non-citizen or non-PR holders living in Canada as well as abroad, and the CHSP Non-resident Owner category includes all owners thought to be living outside of Canada, but excludes non-citizen non-PR holders that live in Canada. But this variation is potentially useful! If SVT Foreign Owners are larger than CHSP Non-Resident Owners, we might get a peek at the lower bound for how many Resident Owners are not Citizens or Permanent Residents. By contrast, if CHSP Non-Resident Owners are larger than SVT Foreign Owners, we get a peek at the lower bound for how many Canadians (citizens or PRs) abroad might still own property in BC.\nSo let’s take a peek! But before we get started it’s a good idea to get a clearer picture how these two data sources compare. After all, the two definitions are constructed by two different government agencies drawing upon slightly different (but related) data, and using slightly different inclusion criteria. In particular, properties are excluded from SVT if they’re worth less than $150,000 CA, or located on First Nations land (or, peculiarly, in the Village of Lions Bay). Additionally, in 2018 the SVT excluded residential properties without structures on them. As a result, we might expect CHSP to have more properties. But CHSP is based on assessment rolls, generally assessed as of July 1st, where SVT is levied in January for properties owned based on the prior year’s assessment (from July 1st). As a result, properties added (e.g. via development) between July and January may show up in the SVT database, but not in CHSP, leaving SVT with more properties. So we can start our analysis by comparing the total number of residential properties for each municipality as listed in the two data sources.\nThe graph is done on a log scale so we can more easily compare the view small municipalities like Belcarra and large ones like Vancouver on the same graph. In general, the two data sources agree quite well, but there are some differences. The log scale visually compresses differences, and we can look at the ratio of the estimates from the two sources to get a better picture of the differences.\nWe get a mix, with some municipalities having more CHSP than SVT properties, and other municipalities the opposite. The variations are never especially large, but large enough that we probably shouldn’t treat the two data sources as identical. While the administrative variations in excluded properties and new developments may account for the variations in total properties, we should be careful in interpretation - even moreso given some revisions in SVT data between 2018 and 2019 Technical Reporting. In particular, it seems prudent to avoid deriving new variables by differences counts across datasets, for example subtracting the SVT “Foreign Owner” count from the CHSP properties owned exclusively by “non-resident owners”.\nA more robust strategy would be to compute the shares of each of these properties within their respective universes and compare shares.\nThis gives us a clear way to assess how these levels of “foreignness” compare. Here we can see that CHSP “Non-resident Owners” is a much larger category than SVT “Foreign Owners.” Their shares differ by roughly a factor of 2, generally a little less in 2018, but more (sometimes significantly) in 2019. In other words, it appears that roughly half of BC property owners living outside of Canada are Canadian citizens or Permanent Residents. This may surprise those who’ve taken CHSP “Non-residency” as a straightforward indicator of “Foreignness.”\nOverall, SVT “foreign owned” properties have grown more scarce between 2018 and 2019. Of further note, in both years only a small fraction of “Foreign Owners” are considered “problematic” and then taxed by the SVT. The vast majority are exempt, most likely either renting out their properties to an arm’s length tenant or living in the property as a primary residence (far and away the two most common exemptions, as visible in comparisons across SVT reporting years).\nIf about half of “Non-resident Owners” aren’t showing up as “Foreign Owners,” then where are they showing up in SVT data? That’s a much trickier question to answer. The SVT data establishes a variety of categories, as demonstrated below.\nUnfortunately, from the documentation we have so far, we don’t know whether Canadian citizen and PR property owners abroad get lumped in with “Other Canadians” or end up in “Mixed” or “Other” or even “Satellite Family” categories. Most likely they appear in some combination of these categories, reflecting the complicated assignment of owners to properties.\nRegardless of which categories Canadian citizen and PR property owners abroad get assigned to, we do know that most homes in all categories were deemed to be exempt from the Speculation and Vacancy Tax. Again, far and away most exemptions stem from properties serving as the primary residence for an owner or tenant, though properties can also be exempt for a variety of other reasons. In nearly every municipality, less than one percent of properties paid any Speculation and Vacancy Tax. The standouts differed between years, with Richmond and West Vancouver topping the list of proportionately most taxpaying properties in 2018 (reaching nearly two percent), shifting to Saanich and Belcarra in 2019 (neither of which had enough foreign owners to break the category out).\nOverall the share of properties paying the SVT has almost universally gone down between 2018 and 2019, which should be expected as owners adjust to the new taxes by selling or renting out their property or making other changes to qualify for one of the exemptions.\nCaution in Comparison\nUnfortunately, while we know that the vast majority of “foreign owned” properties are exempt from the Speculation and Vacancy Tax, we don’t know how many are exempt because they live in the property as a principal resident and how many are exempt because they rent out the property (or for some other reason). It sure would be nice if SVT technical reports broke out exemptions by category of property ownership! Anecdotally, it has not been uncommon for those on work permits or student permits (hence not yet permanent residents) to own a residence that they live in as a principal resident while working or studying in BC, suggesting that some portion of “foreign owned” properties likely qualify for the principal residence exemption from the SVT and would likely not be counted as “non-resident” owners within the CHSP data. Following the logic of our calculations above, the larger the proportion of “foreign owned” properties containing their owners as principal residents, the larger our estimate would be of “non-resident” properties owned by Canadian citizens or permanent residents living abroad. In effect, Canadians abroad could account for even more than roughly half of “non-resident” owned properties in the CHSP data.\nOn the other hand, it may be that the CHSP data simply overestimates the number of “non-resident” owned properties by virtue of flaws in their data matching across various administrative sources. We know that matching remains imperfect and the shares may get revised over time as already happened twice with 2018 data. Maybe some of their “non-resident” owners are actually resident, but just not discovered as such by Statistics Canada. In this case there would be a smaller “real” number of properties owned by “non-residents” of Canada than reported by CHSP, approaching something closer to the “foreign” owned properties in the SVT data, and accordingly our estimate of half of properties owned by those abroad being owned by Canadian would be a little high.\nSo our estimate that about half of BC property owners living abroad are actually Canadian citizens or permanent residents might be a little low or a little high. But it’s a reasonable estimate given the combination of SVT and CHSP data and the countervailing sources of possible error.\nFinally, just to round out our exploration, let’s take a look at the CHSP non-resident owner data that also has information on properties that are jointly owned by residents and non-residents. With more careful and detailed breakdowns from the SVT data, we might be able to track how jointly owned “mixed” properties tracked with CHSP “mixed” properties. As it is, we still find a full exploration too tricky.\nThe CHSP non-resident participation categories have remained even more stable than the SVT declarations between 2018-2019. That said, both datasets continue to be subject to revisions. In most cases these adjustments have lead to a reduction in the overall shares of foreign owners and non-resident participants."
  },
  {
    "objectID": "posts/2021-01-25-rethinking-the-foreignness-of-owners-living-abroad/index.html#upshot",
    "href": "posts/2021-01-25-rethinking-the-foreignness-of-owners-living-abroad/index.html#upshot",
    "title": "Rethinking the “foreignness” of owners living abroad",
    "section": "Upshot",
    "text": "Upshot\nDrawing upon both CHSP and SVT data and their differing definitions of “Foreignness,” we combine the two to estimate the size of a new category of interest: Canadians (citizens and PRs) living abroad who still own property in high demand areas of BC. This category appears to account for roughly half of property owners living abroad as estimated by CHSP data for included municipalities. That such a large portion of BC property owners living abroad are likely legally Canadian casts a rather harsh light on the extent to which “foreignness” has played such a strong role in our housing discourse.\nAs usual, the code for the graphs is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-02-03 20:54:36 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [6c670a1] 2021-01-31: typos\n## R version 4.0.3 (2020-10-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cansim_0.3.5              cancensus_0.4.1          \n##  [3] mountainmathHelpers_0.1.2 forcats_0.5.0            \n##  [5] stringr_1.4.0             dplyr_1.0.3              \n##  [7] purrr_0.3.4               readr_1.4.0              \n##  [9] tidyr_1.1.2               tibble_3.0.4             \n## [11] ggplot2_3.3.3             tidyverse_1.3.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.0  xfun_0.18         haven_2.3.1       colorspace_2.0-0 \n##  [5] vctrs_0.3.5       generics_0.1.0    htmltools_0.5.0   yaml_2.2.1       \n##  [9] blob_1.2.1        rlang_0.4.9       pillar_1.4.7      glue_1.4.2       \n## [13] withr_2.3.0       DBI_1.1.0         dbplyr_1.4.4      modelr_0.1.8     \n## [17] readxl_1.3.1      lifecycle_0.2.0   munsell_0.5.0     blogdown_0.19    \n## [21] gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6       evaluate_0.14    \n## [25] knitr_1.30        fansi_0.4.1       broom_0.7.4       Rcpp_1.0.5       \n## [29] scales_1.1.1      backports_1.2.0   jsonlite_1.7.2    fs_1.4.1         \n## [33] hms_0.5.3         digest_0.6.27     stringi_1.5.3     bookdown_0.19    \n## [37] grid_4.0.3        cli_2.2.0         tools_4.0.3       magrittr_2.0.1   \n## [41] crayon_1.3.4      pkgconfig_2.0.3   ellipsis_0.3.1    xml2_1.3.2       \n## [45] reprex_0.3.0      lubridate_1.7.9.2 assertthat_0.2.1  rmarkdown_2.5    \n## [49] httr_1.4.2        rstudioapi_0.13   R6_2.5.0          git2r_0.27.1     \n## [53] compiler_4.0.3"
  },
  {
    "objectID": "posts/2021-01-06-vancouver-s-pandemic-weather/index.html",
    "href": "posts/2021-01-06-vancouver-s-pandemic-weather/index.html",
    "title": "Vancouver’s pandemic weather",
    "section": "",
    "text": "The pandemic changed our lives and behaviours. And our perceptions of things. With physical distancing, various degrees of restrictions and people avoiding the 3Cs: crowded places, close-contact settins, confined and enclosed spaces, people have been focusing on spending time outdoors whenever possible. I certainly pay a lot more attention to the weather than I used to, and Vancouver’s fall and winter has felt especially miserable so far.\nBut has the weather actually been worse or is it just my warped perception? That’s an easy question to check. Environment Canada has historical weather data, we we can see how 2020 has compared to the previous years.\nFirst up, did it rain more than other years. My feeling says Yes, but the data begs to differ.\n\nRainfall has been mostly average. We started out with a wetter-than-usual January, but that was well before we started to take the pandemic seriously. When the pandemic hit in March and April we were aided by exceptionally dry weather. June was wetter than normal and October gave us a bit of relief, but overall the total rainfall was well within the expected range.\nHow about the temperature?\n\nFor most of the year temperatures have been fairly average, but the pandemic winter in Vancouver as been comparably mild. It’s hard to complain about temperature.\nBut how about sunshine? Or maybe the problem is less the total amount of precipitation and more that it was drizzling nonstop? It sure feels like it was. To answer that we turn to hourly data and check the weather categorization. Here are the top 10 categories over the past few years.\n\n\n\nWeather\nShare\n\n\n\n\nCloudy\n22.749%\n\n\nMostly Cloudy\n22.276%\n\n\nMainly Clear\n21.330%\n\n\nClear\n12.849%\n\n\nRain\n11.499%\n\n\nFog\n2.144%\n\n\nRain Showers\n2.061%\n\n\nRain,Fog\n1.739%\n\n\nSnow\n0.616%\n\n\nModerate Rain\n0.464%\n\n\n\nTo make sense of the categories we group Mainly Clear and Clear, and classify anything containing the word Rain as Rain. We keep a separate category for Foggy, Drizzle, Snow and also keep a category for Smoke/Haze. We keep Cloudy and Mostly Cloudy as separate categories. Beggars can’t be choosers and Mostly Cloudy carries the promise of occasional sun rays, which can make all the difference.\n\nOverall it looks like we can’t complain. By Vancouver standards, we had a good amount of clear skies this past year and did not have more rainy times than usual. Yes, there was that smoky period in September. June was more cloudy than usual. But other than that this past year looks pretty good.\nBut maybe that’s because it rained a lot during the daytime this past year, whereas it usually rains at night in Vancouver? Maybe that’s reaching for straws, but that’s easy enough to check. Let’s only count daytime hours, so times between sunset and sunrise.\n\nAnd again, there is not that much to complain about in the data. April and August look spectacularly sunny, overall the fall and winter appear to we well within historical patterns.\nLooks like I grudgingly have to admit that my gripes with Vancouver pandemic weather is mostly my perception and there is no evidence that we have been hit with particularly poor weather this past year.\nAs usual, the code for this post available on GitHub in case anyone wants to reproduce or adapt it to look at other places in Canada (or elsewhere in the world that has similar weather data).\n\n\nReproducibility receipt\n\n## [1] \"2021-01-06 18:25:36 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [51cc423] 2021-01-07: fix twitter/og image\n## R version 4.0.3 (2020-10-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] suncalc_0.5.0    weathercan_0.3.4 forcats_0.5.0    stringr_1.4.0   \n##  [5] dplyr_1.0.2      purrr_0.3.4      readr_1.4.0      tidyr_1.1.2     \n##  [9] tibble_3.0.4     ggplot2_3.3.2    tidyverse_1.3.0 \n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.0  xfun_0.18         haven_2.3.1       colorspace_2.0-0 \n##  [5] vctrs_0.3.5       generics_0.1.0    htmltools_0.5.0   yaml_2.2.1       \n##  [9] blob_1.2.1        rlang_0.4.9       pillar_1.4.7      glue_1.4.2       \n## [13] withr_2.3.0       DBI_1.1.0         dbplyr_1.4.4      modelr_0.1.8     \n## [17] readxl_1.3.1      lifecycle_0.2.0   munsell_0.5.0     blogdown_0.19    \n## [21] gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6       evaluate_0.14    \n## [25] knitr_1.30        fansi_0.4.1       broom_0.7.0       Rcpp_1.0.5       \n## [29] scales_1.1.1      backports_1.2.0   jsonlite_1.7.2    fs_1.4.1         \n## [33] hms_0.5.3         digest_0.6.27     stringi_1.5.3     bookdown_0.19    \n## [37] grid_4.0.3        cli_2.2.0         tools_4.0.3       magrittr_2.0.1   \n## [41] crayon_1.3.4      pkgconfig_2.0.3   ellipsis_0.3.1    data.table_1.13.0\n## [45] xml2_1.3.2        reprex_0.3.0      lubridate_1.7.9.2 assertthat_0.2.1 \n## [49] rmarkdown_2.5     httr_1.4.2        rstudioapi_0.13   R6_2.5.0         \n## [53] git2r_0.27.1      compiler_4.0.3\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2021,\n  author = {von Bergmann, Jens},\n  title = {Vancouver’s Pandemic Weather},\n  date = {2021-01-06},\n  url = {https://doodles.mountainmath.ca/posts/2021-01-06-vancouver-s-pandemic-weather},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2021. “Vancouver’s Pandemic Weather.”\nMountanDoodles (blog). January 6, 2021. https://doodles.mountainmath.ca/posts/2021-01-06-vancouver-s-pandemic-weather."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html",
    "title": "Covid testing data in BC",
    "section": "",
    "text": "What do we know about COVID-19 testing in BC? That’s a surprisingly tricky question, so I decided to do a quick post on this."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html#why-do-we-test",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html#why-do-we-test",
    "title": "Covid testing data in BC",
    "section": "Why do we test?",
    "text": "Why do we test?\nThe main use of testing is diagnostic and to break transmission chains. If we suspect a person has COVID-19 that person will go into self-isolation and seek a test. If the test confirms the suspicion, we contact trace the COVID-positive person and ask close contacts to self-isolate to break transmission chains.\nAs an aside, the timelines of COVID are such that the traditional contact tracing approach described above captures a maximum of about one third of all onward infections and having researchers call for alternative appropaches to traditional contact tracing like borad asymptomatic testing or employing other approaches like tracing contacts of contacts to get ahead of transmissions."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html#test-metrics",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html#test-metrics",
    "title": "Covid testing data in BC",
    "section": "Test metrics",
    "text": "Test metrics\nA natural question is how well our testing is doing in supporting the breaking of transmission chains. There are some obvious metrics to measure the effectiveness of our TTI transmission chain, for example keeping track of what share of confirmed cases were already in self-isolation at the time they (likely) became infectious. Unfortunately we aren’t reporting on metrics like this, and I strongly suspect we aren’t even keeping track of this.\nBut there are some secondary metrics that we are keeping track of. The number of tests we are doing, as well as the positivity rate, so the share of tests that come back positive. The positivity rate tells us if we are testing broadly enough. If the positivity rate gets too high, which people often peg at above 3%, we likely aren’t testing broadly enough to capture all infections and we are starting to miss a lot of cases and the chance the break their transmission chains. In the words of the BCCDC\n\nThis metric [positivity rate] can be used to understand if a jurisdiction is doing sufficient testing relative to the size of its epidemic to catch enough cases to maintain epidemic control. The higher the test positivity, the greater the likelihood that many cases are being missed and there is widespread community transmission, making it more difficult to limit the spread of the virus.\n\nGenerally we try to scale up the number of tests to keep up with the pace of the transmissions, but viral growth is exponential and real world resources generally can’t keep up with that growth. Which leads to rising positivity rates as we increasingly focus our testing on the most likely cases."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html#how-many-tests-are-we-doing-in-bc-and-whats-our-positivity-rate",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html#how-many-tests-are-we-doing-in-bc-and-whats-our-positivity-rate",
    "title": "Covid testing data in BC",
    "section": "How many tests are we doing in BC and what’s our positivity rate?",
    "text": "How many tests are we doing in BC and what’s our positivity rate?\nThe answer to this seems simple, the BCCDC dashboard has data on daily tests, testing turn-around time and positivity rates.\n\nBut here is the rub. This data does not mean what one would naively expect it to mean, at least if “we” means the public that’s doing the testing to diagnose COVID and break transmission chains.\nTo understand what’s going on we need to take a step back. There essentially are two distinct ways we are testing in BC, as the weekly Situation Reports explain:\n\nIn BC, laboratory-based surveillance captures mostly symptom-based diagnostic testing conducted under the Medical Service Plan (MSP) funding scheme, as well as any non-MSP funded screening tests.\n\nTo understand why this matters we keep reading:\n\nScreening tests have a lower likelihood of testing SARS-CoV-2 positive (i.e. percent positivity) than symptom-based diagnostic testing; therefore, including screening specimens will tend to lower the overall percent positivity indicator and the impact of that will be greater when more screening specimens are included.\n\nTo put it simply, in BC we have our regular publicly funded diagnostic testing, based on guidelines given by the BCCDC, that aim to break transmission chains. On the other hand we also have privately funded “screening tests”, most notably those employed by the film industry that regularly tests people detect COVID cases before they can spread and keep their workplaces, where social distancing is not always possible, safe.\nBoth types of tests are great, but private testing is orthogonal to our containment effort, which is what the general public and the BCCDC is focused on. After all, the BCCDC is reporting the number of tests and the positivity rate because it “can be used to understand if a jurisdiction is doing sufficient testing relative to the size of its epidemic to catch enough cases to maintain epidemic control”. The relevant metrics are the number of public tests and the positivity rate of public tests.\nThe dashboard however gives and answer to a question nobody asked: “What are the combined number of private and public tests and what is the blended positivity rate”. If reported separately, this would give us important insight. How is our public testing performing, and what’s the positivity rate of private screening tests – probably the best metric we (in theory) have right now to understand true prevalence in the population. Blending both together as the dashboard does gives us a hard to interpret blend.\nOur PHO got asked about if private tests are included several times, including on October 15th, where the PHO explained that they do monitor what positivity rates look like when just using public testing and that those positivity rates “remain low”. At the time the overall positivity rate for that week from the Dashboard was around 2%, and the one for just public testing was close to 3%. On November 27 the BCCDC finally started to report public testing separately in the weekly Situation Reports. The information is only weekly aggregates, number of tests needs to be back-calculated from the testing rate, and the information only available in image form requiring manual scraping to get the data. The BCCDC has on several occasions refused to provide the data underlying their graphs for the (bizarre) stated reason that the inconvenience of the manual scraping is an essential safeguard of patient privacy.\nConfused yet? You aren’t the only one, things are about to get worse. Apparently the BCCDC is also confused by their own data."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html#bc-covid-19-epidemiology-app",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html#bc-covid-19-epidemiology-app",
    "title": "Covid testing data in BC",
    "section": "BC COVID-19 Epidemiology App",
    "text": "BC COVID-19 Epidemiology App\nIt’s generally a good idea to consume your own data, which the BCCDC did on their new shiny COVID-19 Epidemiology App. There we can see how BC’s testing rate and positivity rate compares to that of other provinces. BCCDC used the testing data from PHAC for this, except for BC. For BC the BCCDC swapped out the PHAC data and spliced in their own data. But not just the data on public testing, but the BCCDC inflated their testing numbers and depressed their positivity rate by adding in private tests. Just for BC. Which defies the whole purpose of making a comparison.\nHow misleading a comparison does the BCCDC COVID-19 Epidemiology App make. That’s easy to see by doing a side-by-side plot using the Dashboard data, which the BCCDC used for BC, vs the PHAC data for BC, which the BCCDC used as comparison for the other provinces.\n\nI am not a fan of this visual, I find it hard to read. So a quick explanation is in order. The grey bar-graph in the background shows the new daily tests (or number of people tested) per 100k population, the line graph shows the number new daily cases per 1M population and the colour of the line graph is given by the test positivity rate.\nIn the two graphs case numbers are (almost) the same, with the only difference given by BC not reporting this data on weekends (which we smoothed out). But the testing rate and the positivity rate are very different.\nThis brings out another issue with the BCCDC COVID-19 Epidemiology App comparison. For BC data is shows number of daily tests, for the other provinces using PHAC data it shows the number of people tested. Similarly, the positivity rate from BC data is number of positive tests by number of tests, whereas the positivity from PHAC data is calculated as number of confirmed cases by number of people tested. It’s never a good idea to compare metrics that have different units.\nFor completeness we can quickly plot what the proper comparison with all the provinces looks like.\n\nUnlike on the BCCDC graph, we now see that BC’s testing rate rate compares unfavourably with many provinces. It’s hard to directly compare the positivity rate off the graph, but we can just plot it separately.\n\nAlberta is in a league of it’s own, but BC, MB, QC and SK also have dangerously elevated levels. Nunavut shot up during their recent outbreak, but thankfully seems to be getting things under control again. One should note here that the positivity rate shown here, as estimated from PHAC data, is given by number of cases divided by number of (unique) people tested.\nThe BCCDC should fix their graphics to insure their comparisons aren’t misleading."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html#bcs-real-number-of-tests-and-positivity-rate",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html#bcs-real-number-of-tests-and-positivity-rate",
    "title": "Covid testing data in BC",
    "section": "BC’s real number of tests and positivity rate",
    "text": "BC’s real number of tests and positivity rate\nAs we have seen, one issue that comes with using PHAC data is that PHAC reports on number of people tested, not the number of tests done. In general the number of people tested, and the positivity rate that’s calculated via cases by number of people tested are the better metrics, but counting the number of tests, so double-counting people when they got multiple tests, is often easier to do.\nThe BCCDC only reports on the number of tests and the positivity rate calculated as positive tests by number of total tests, and after a lot of pressure we now have weekly averages in the Situation Reports. Time for my 11yo to scrape out the data – in exchange for fancy ice cream.\nScraping the data gives us the opportunity to get a better reading on the number of public and private tests done in BC. This is an inexact science, scraping the data out of images will come with small errors, despite the 11yo’s best efforts. And there are no graphs that show the total number of public tests, just the test rates, so we need to divide by the population to untangle this.\n\nWe see that private testing ramped up over time and now takes up almost 20k tests a week, compared to around 60k tests done via public testing. This allows us to also look at the ratio of private tests out of all tests, which helps us better understand how this impacts the positivity rate of the blended data over time.\n\nBC’s overall share of private testing has been between 20% and 30% starting epi week 42, so mid October. Vancouver Coastal’s share of private testing has been significantly higher, Vancouver Island, Interior and Northern’s share has been significantly lower."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html#positivity-rates",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html#positivity-rates",
    "title": "Covid testing data in BC",
    "section": "Positivity rates",
    "text": "Positivity rates\nWhich brings us to the next step, looking at positivity rates. Private routine testing has very low positivity rates, they will be roughly at the levels of population prevalence of COVID-19. (Maybe slightly higher because of occasional false positives.) This means that the more private testing we are doing the lower the overall blended positivity rate that the BCCDC still reports on their dashboard will be.\nThe Situation Reports have the weekly positivity rates, and there we can read off BC’s real positivity rate for public testing. Although the graph they provide is awfully cramped and hard to get a clear idea, but nothing a little ice-cream fuelled scraping can’t fix.\n\nWe can take this down to the Health Region level and compare the real public positivity rate to the one generally reported by the BCCDC on the dashboard. Again, the graph in the Situation Report is not very useful for this, so we scraped and reshaped the data.\n Of note is that the positivity rate of public testing does not always move in sync with the blended positivity rate that includes private testing. Which should make us extra careful when trying to discern trends of the public testing positivity rate that we are principally interested in from trends in the overall blended rate that the BCCDC reports. This graph also answers the question by Tyler Olson if any other Health Region hit the (7-day average) 10% positivity mark before Northern did very recently. Just looking at the dashboard data we might be inclined to say that no Health Region got over 10%. But we are of course interested in public testing positivity rate, and that has been above 10% in Fraser for several weeks."
  },
  {
    "objectID": "posts/2020-12-21-covid-testing-data-in-bc/index.html#upshot",
    "href": "posts/2020-12-21-covid-testing-data-in-bc/index.html#upshot",
    "title": "Covid testing data in BC",
    "section": "Upshot",
    "text": "Upshot\nThe upshot of this post is simple: Report relevant metrics. Don’t report metrics that are likely going to mislead people because they are convoluted and hard to interpret because they don’t address the relevant questions.\nWhen the metrics you report are so confusing that your own team misuses them in very misleading BCCDC graphics, it’s maybe time to stop reporting the blended private/public positivity rate altogether and just report the two separately.\nTesting matters, and testing data matters. Almost a year into the pandemic, and BC main dashboard and public data download is still reporting largely uninterpretable testing data. The only useful testing data we are getting is weekly aggregates that have to be manually scraped out of images.\nIf you are interested in the data, it’s embedded in the code for this post. That was the easiest way to grab it form the image-scraping tool my 11yo used."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html",
    "href": "posts/2020-11-10-tongfen/index.html",
    "title": "TongFen",
    "section": "",
    "text": "The tongfen R package is now on CRAN, so it’s time for an overview post. Tongfen has changed a bit since it’s inception and is now a lot more flexible but slightly more abstract to use."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html#what-is-tongfen",
    "href": "posts/2020-11-10-tongfen/index.html#what-is-tongfen",
    "title": "TongFen",
    "section": "What is tongfen?",
    "text": "What is tongfen?\nTongfen, 通分 in Chinese, generally denotes the process of bringing two fractions onto the least common denominator. This is akin to the problem of making data on different but congruent geographies comparable by finding a least common geography. We will describe the process in detail later, intuitively imagine this as a puzzle game where two different tilings are to be matched up by joining individual areas in each of the tilings until both match. One way to achieve this is to join all areas in each of the two tilings, so it’s always possible to do this. This is analogous to mutiplying the denominators in two fractions to find a common denominator. To be useful, the matching should to be done in a way that minimizes the number of joined individual areas, resulting in as fine a geography as possible, the least common geography.\nIn practice this matching may be done up to a pre-specified tolerance to allow for slight boundary changes that are simply due to improvements of accuracy in delineating geographic regions as is commonly found in e.g. census data."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html#why-tongfen",
    "href": "posts/2020-11-10-tongfen/index.html#why-tongfen",
    "title": "TongFen",
    "section": "Why tongfen?",
    "text": "Why tongfen?\nTongfen is our answer to the problem of needing to compare data on different yet congruent geographies. The most common setting for this is census data across several censuses, where census geographies change. Another setting is polling district data, where polling districts get adjusted over time. These geography changes typically happen via a sequence of fairly localized combine and split operations to get from one geography to another.\nThere are essentially three different approaches on how to deal with the problem of harmonizing data on congruent geographies:\n\nCustom tabulation\nIf possible, for example when working with census data, we can request a custom tabulation of the data on the geography we desire. In some cases this isn’t possible, for example when working with polling district level voting data. But when possible, this is often the best method to harmonize the data.\nHowever, in some cases it won’t lead to satisfactory results. Disclosure cutoffs are coarser for custom tabulations, which may result in suppressed data when working with small areas. With the recognition that traditional privacy measures like random rounding are insufficient to protect user privacy, and e.g. the US census switching over to differential privacy as the release mechanism, custom tabulations don’t just cost time and money, but also eat into the researcher’s allotted privacy budget. This makes alternative methods more attractive.\nThere have been some efforts to create publicly available custom tabulation that span several censuses, for example one for Metro Vancouver and Toronto on 2016 dissemination areas back to 1971 in Canada and the LTDB-DP project for US census data on 2010 census tracts using differential privacy.\n\n\nArea weighted (dasymetric) interpolations\nThese methods try to harmonize the data by computing geographic intersections of the two geometries and estimating values of interest from one geometry to another proportionally to the relative overlap. This is often refined by folding in finer level data, for example block level population counts or by clipping out uninhabited areas. Such an approach was taken e.g. by the Canadian Longitudinal Census Tract Database or the US Neighbourhood Change Database (NCDB/Geolytics), LTDB, or NHGIS.\nThe main advantage of this method is that it is very simple to implement and places no restrictions on the (target) geography to estimate the data on. The tongfen package implements this using the tongfen_estimate method, with the option for dasymetric refinements via the proportional_reaggregate method.\nThe main disadvantage of these methods is that they are only estimates and come with sizeable errors. And the errors are generally correlated with changes in population density or change in other variables that are important to how the geographies are broken down. Additionally, we have not seen a good way to estimate these errors (something we am interested in and might spend some time thinking about in the future). This makes this approach ill-suited for research, although that does not seem to stop a lot of academics from doing exactly that. This is particularly true for the NCDB/Geolytics data because the their methods are opaque and their errors are especially large.\n\n\nTongfen\nIf there is some flexibility in the geography on which the harmonized data should be presented and the geographies in question are sufficiently congruent (as is often the case with census geographies or polling districts) then tongfen can provide exact (at the same precision as the original data) counts on a common geography that is (slightly) coarser than the original geographies.\nThe advantage over custom tabulations is that it is immediately available and does not cost money or privacy budget. The disadvantage is a loss of geographic granularity, the degree of which is dependent on how congruent the original geographies are.\nThe advantage over area weighted interpolation is that the counts are exact and not estimates, and they can thus be used for research purposes. However, some quantities, like medians, can’t be aggregated up this way and can only be estimated. The disadvantage is that tongfen requires flexibility on the target geography and only provides useful results when the geographies are sufficiently congruent.\nAnother advantage of tongfen is that it is not precomputed but carried out live as needed. This makes the process reproducible, auditable, and transparent, and allows the user to fine-tune the process according to their needs."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html#how-does-tongfen-work",
    "href": "posts/2020-11-10-tongfen/index.html#how-does-tongfen-work",
    "title": "TongFen",
    "section": "How does tongfen work?",
    "text": "How does tongfen work?\nAbstractly tongfen works in three steps: 1. Build metadata that makes it possible to aggregate variables of interest. For variables that are additive, like population counts, this is simple and no extra information is needed. For other variables, like average income, we also need a count of the “base population” that the average is based on so it can be properly aggregated. The metadata assembles all required variables and information on how they relate so we can properly aggregate the information when needed. 2. Build a correspondence between the geographies of interest. This is similar to the information provided by e.g. Statistics Canada or the US Census Bureau in their correspondence files. Tongfen implements several methods to do this, one that’s completely agnostic of external information and just operates on the geometries, another that assumes that geographies with identical UID correspond to one another and only matches the remaining geographies, and lastly one that uses external correspondence data. 3. Aggregate up geometries according to the correspondence and data according to the correspondence and metadata.\nWe will showcase a couple of simple examples on how this process works.\n\nTongfen using polling district data\nThe tongfen R package implements several methods to find the least common geography and aggregate up variables. As an example, consider the Canadian federal electoral poll districts from the 2015 and 2019 elections for Vancouver.\n\nTo compare these and compute vote shifts at a granular level we need to get the data on a common geography.\n\nOn the left we show the 2015 boundaries, in the middle the 2019 boundaries and on the right the least common geography based on the former two. This example nicely illustrates the strength and weaknesses of tongfen. On the east side the tongfen process results in a fairly granular tiling, but on the west side the tiling ends up very coarse to the point where it’s almost identical to the entire federal election district of Vancouver-Quadra.\nTo understand how tongfen works in more detail we zoom into the area marked by the red square.\n\nHere we can see our “puzzle game” to find a common tiling at work. Some areas remain unchanged between the years and can simply be adopted without modifications. But other areas don’t match and the puzzle game starts how to best join adjacent areas to build a least common tiling.\nOn close inspection we also notice some insignificant boundary changes where boundaries have been modified over time to better align with features like roads. The tongfen algorithm requires to specify a tolerance up to which boundary changes are consider to be inconsequential. In this particular example the tolerance was set to be 30 metres. This is informed by the observation that polling district boundaries follow the street grid, and a change by less than 30 metres allows for realignments with streets but will detect cases where the boundaries follow a different street.\nThe end process shows the strength and limitations of tongfen. On the west side, where geographies changed a lot, tongfen results in a tiling that is close to the union of all areas. On the east side tongfen is able to maintain a fine geographies.\nOne advantage of not pre-computing the matching but doing it live as needed is that we can refine the computations by incorporating local data. To get a finer tiling we can look at Vancouver land use data and cut the electoral polling districts down to only those areas where people actually live. This can help to further weed out inconsequential boundary changes\n\nMasking the polling district boundaries with the land use area helps us filter out inconsequential boundary changes.\n\nThe TongFen geography is based on the 2019 geographies, with the correspondence built from the geographies that have been intersected with the residential land use. This yields slightly more detail on the west side where geographies had previously combined but the boundary changes were inconsequential where they covered unpopulated areas.\n\nThis only considers votes at the polling station and ignores advance and mail-in ballots. There is a tongfen package vignette that also looks into changes in advance voting patterns, a detailed analysis goes beyond the scope of this overview post."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html#canadian-census-data",
    "href": "posts/2020-11-10-tongfen/index.html#canadian-census-data",
    "title": "TongFen",
    "section": "Canadian census data",
    "text": "Canadian census data\nThe tongfen functionality can be easily extended to facilitate working with particular data sources. Integration with Canadian census data is most advanced, leveraging our cancensus R package that taps into the CensusMapper API and rich metadata for census variables from CensusMapper."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html#t1ff-taxfiler-data",
    "href": "posts/2020-11-10-tongfen/index.html#t1ff-taxfiler-data",
    "title": "TongFen",
    "section": "T1FF taxfiler data",
    "text": "T1FF taxfiler data\nThanks to a CMHC project CensusMapper now also has census tract level T1FF taxfiler data available. T1FF taxfiler data is an extremely rich annual dataset with detailed demographic and income variables, making it very suited for longitudinal analysis at fine geographies.\nWe look at the share of people in low income (LICO-AT) in the City of Toronto between 2005 (the first year our CRA tax data has low income counts) and 2018. The first step is to collect the low income variables, a quick check on the CensusMapper API UI reveals that the relevant internal CensusMapper vectors are v_TX2018_551 for the 2018 percentage of population in low income, with the year string swapped out for earlier years. CensusMapper variable coding is not always consistent through time, but it is for the tax data.\nAs a first step we construct the necessary metadata needed for tongfen. The percentage itself is not enough information to aggregate up the data, we also need to know the base of the percentage – population in this case. CensusMapper metadata knows what variable that is and the meta_for_ca_census_vectors convenience function takes care of assembling any extra variables we need for tongfen. The metadata specifies the variables that are to be considered, and also specifies rules on how they are to be aggregated.\nHere is the metadata just for the variables for the 2018 year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\ndataset\ntype\naggregation\nunits\nrule\nparent\ngeo_dataset\nyear\n\n\n\n\nv_TX2018_551\nlico_2018\nTX2018\nOriginal\nAverage of v_TX2018_546\nPercentage (0-100)\nAverage\nv_TX2018_546\nCA16\n2018\n\n\nv_TX2018_546\nv_TX2018_546\nTX2018\nExtra\nAdditive\nNA\nAdditive\nNA\nCA16\n2018\n\n\n\nArmed with the metadata we need to select the region we are interested in, the City of Toronto in this case, and call the get_tongfen_ca_census wrapper function that downloads the data, harmonizes the geographies, aggregates up the data and hands back the resulting data on a uniform geography.\n\nFor this plot of low income shares over the years we would not have to go though the troubles of harmonizing the geographies, we could have just plotted each dataset on it’s native geography. This would have resulted in a slightly higher geographic resolution for each of the maps. The key advantage of having the data on a unified geography is that we can now easily look at change over time, for example by mapping the percentage point change between the initial and final years."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html#us-census-tract-level-data",
    "href": "posts/2020-11-10-tongfen/index.html#us-census-tract-level-data",
    "title": "TongFen",
    "section": "US census tract level data",
    "text": "US census tract level data\nThe tongfen package also comes with basic support for US census tract level cross-census comparisons. This piggy-backs off of the tidycensus package to ingest census data. As an example we look at the change in household size in the Bay Area. The first step is to build the metadata, which we do by hand.\n\nThe census tracts used here are slightly coarser than the original 2000 and 2010 census tracts, but the resulting data is exact, in the sense in which the 2000 and 2010 census data is exact, and not estimates or noised via differential privacy."
  },
  {
    "objectID": "posts/2020-11-10-tongfen/index.html#upshot",
    "href": "posts/2020-11-10-tongfen/index.html#upshot",
    "title": "TongFen",
    "section": "Upshot",
    "text": "Upshot\nTongFen is not new, people have been assembling regions before to make them comparable. In particular, Statistics Canada as well as the US Census Bureau provide correspondence files that specify how geographic regions relate across censuses. But we have not seen implementations that automate this process and provide the flexibility to run this on arbitrary geographies in a reproducible and transparent way.\nTongfen is designed so it is easy to build dataset-specific wrappers to facilitate working with specific datasets. Currently tongfen has wrappers to work with Canadian and US census data, with the Canadian version being fairly well-developed. We welcome contributions that wrap other datasets or expand the usability or coverage of tongfen on existing datasets.\nAs usual, the code for this post is available on GitHub in case others find it useful. The examples here are similar to the vignettes in the tongfen package.\n\n\nReproducibility receipt\n\n## [1] \"2020-11-11 07:24:37 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [b174751] 2020-11-11: tongfen post\n## R version 4.0.2 (2020-06-22)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Catalina 10.15.7\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.2 sf_0.9-6                 \n##  [3] tongfen_0.3               cancensus_0.3.3          \n##  [5] forcats_0.5.0             stringr_1.4.0            \n##  [7] dplyr_1.0.2               purrr_0.3.4              \n##  [9] readr_1.4.0               tidyr_1.1.2              \n## [11] tibble_3.0.4              ggplot2_3.3.2            \n## [13] tidyverse_1.3.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.0   xfun_0.18          haven_2.3.1        colorspace_1.4-1  \n##  [5] vctrs_0.3.4        generics_0.1.0     htmltools_0.5.0    yaml_2.2.1        \n##  [9] blob_1.2.1         rlang_0.4.8        e1071_1.7-4        pillar_1.4.6      \n## [13] glue_1.4.2         withr_2.3.0        DBI_1.1.0          dbplyr_1.4.4      \n## [17] modelr_0.1.8       readxl_1.3.1       lifecycle_0.2.0    munsell_0.5.0     \n## [21] blogdown_0.19      gtable_0.3.0       cellranger_1.1.0   rvest_0.3.6       \n## [25] evaluate_0.14      knitr_1.30         class_7.3-17       fansi_0.4.1       \n## [29] broom_0.7.0        Rcpp_1.0.5         KernSmooth_2.23-17 classInt_0.4-3    \n## [33] scales_1.1.1       backports_1.1.10   jsonlite_1.7.1     fs_1.4.1          \n## [37] hms_0.5.3          digest_0.6.27      stringi_1.5.3      bookdown_0.19     \n## [41] grid_4.0.2         cli_2.1.0          tools_4.0.2        magrittr_1.5      \n## [45] crayon_1.3.4       pkgconfig_2.0.3    ellipsis_0.3.1     xml2_1.3.2        \n## [49] reprex_0.3.0       lubridate_1.7.9    assertthat_0.2.1   rmarkdown_2.3     \n## [53] httr_1.4.2         rstudioapi_0.11    R6_2.5.0           git2r_0.27.1      \n## [57] units_0.6-7        compiler_4.0.2"
  },
  {
    "objectID": "posts/2020-10-02-chs-core-housing-need/index.html",
    "href": "posts/2020-10-02-chs-core-housing-need/index.html",
    "title": "CHS Core Housing Need",
    "section": "",
    "text": "Today StatCan released four more tables of data from the Canadian Housing Survey, all around the concept of Core Housing Need. Core housing need aims to measure housing stress based on affordability, suitability (crowding) and adequacy (disrepair). It applies to all households with shelter-cost-to-income ratio less than 100%, excluding non-family student-lead households, that aren’t able to afford an adequate and suitable home in their region.\nWe want to give a quick overview what’s in the new data release.\nTo start off, let’s look at core housing need for priority vulnerable groups of the National Housing Strategy. It’s good to see some of this data coming out. In particular it includes data on of first nations, visible minorities, seniors, young adults, immigrants and veterans which should help identify systemic gaps in our support systems. One important note is that the survey does not include First Nations reserves.\n\nUnfortunately the sample isn’t thick enough to give good 95% confidence intervals for some of the categories. Out of th three metrics making up core housing need, housing inadequacy is the least prevalent, although Inuk, and to a lesser extent other First Nation people, stand out. There is a similar spike for Inuk in residential crowding, which is something we have observed before. Crowding is also elevated for recent refugees, West Asian, Black and Filipino populations.\nSeniors living alone stand out for facing affordability challenges, as do several visible minority populations.\nNext up we focus on core housing need by tenure for select metro areas in Canada.\n\nRenters in social and affordable housing stand out, especially in Vancouver and Calgary, which is likely due to rental assistance programs that aren’t counted as income (but as government transfers) with the full rent being reported. A BC senior receiving SAFER rental assistance would be such an example. To check if this is a Vancouver-specific phenomenon we can look how other areas in British Columbia fair.\n\nNext we look at living situation of households, with the case where the reference person in the household has experienced homelessness in their lives broken out separately.\n\nOn the affordability front, one person households have a higher incidence of living in unaffordable housing, women living alone more so than men. Persons that have experienced homelessness before are at elevated risk in any of these categories, in particular for inadequate housing (disrepair) and they have the overall highest share in core housing need.\nOverall, around 2.5% of reference persons in Canadian households have experienced homelessness.\nThat’s it for this post, as usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2020,\n  author = {von Bergmann, Jens},\n  title = {CHS {Core} {Housing} {Need}},\n  date = {2020-10-02},\n  url = {https://doodles.mountainmath.ca/posts/2020-10-02-chs-core-housing-need},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2020. “CHS Core Housing Need.”\nMountanDoodles (blog). October 2, 2020. https://doodles.mountainmath.ca/posts/2020-10-02-chs-core-housing-need."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html",
    "title": "Income mixing and segregation",
    "section": "",
    "text": "At the end of June StatCan released an interesting census tract level metric, dubbed the D-index, measuring how much the income distribution in each census tract differs from the metro-wide distribution, and we decided to take it for a test drive.\nWe are a bit of a sucker for this kind of fine-geography index. Condensing our wealth of information into a single number is an interesting exercise that involves lots of attention to detail. How best to do this depends on the question one is asking. And it’s extremely satisfying to arrive at a single number as a decent measure for a very complex question. But there are risks when developing such indices, once introduced they often develop a life on it’s own and get tortured to answer questions they were not designed for.\nIn this post we want to take a deep dive into the new D-index, as well as a host of related measures."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html#tdlr",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html#tdlr",
    "title": "Income mixing and segregation",
    "section": "TD;LR",
    "text": "TD;LR\nThe D-index is a census tract level measure that is currently only available for 2017 and that measures income-based index of social mixing. It is particularly interesting in that it explicitly considers social mixing withing dwelling types and even individual large apartment buildings, as well as mixing within neighbourhoods compared to aggregate CMA level distributions of income and housing.\nOne of it’s main features is that it can be decomposed into an overall tract-level component, as well as components measuring how sub-distributions across dwelling types and large apartment buildings contribute to income mixing.\nA motivation for including dwelling types and even large apartment buildings is to distinguish if income mixing in a particular area is e.g. driven by an apartment building with lower income residents in a sea of single detached homes with higher income residents. This should help alleviate some MAUP issues, these usually are alleviated by improving model specifications.\nHowever, there are some components of the D-index we find unsatisfactory. * The D-index only considers census families and ignores people not in census families. * The D-index takes family size into consideration when computing the adjusted family income that it uses as a base, but the fundamental unit counted in the index is families, not individuals. When it measures the difference between the metropolitan and tact level distributions it treats a mismatch of a small family the same as the mismatch of a large family. * The D-index ignored that income brackets are ordered categorical data and treats it as if it was unordered categorical data. In particular, it treats a distribution that differs from the reference distribution in two adjacent middle categories the same as one that differs in the two extreme categories, which is contrary to how people or families experience differences in income.\nGiven these quibbles we propose yet another index designed to measure the level of income mixing. The D-index is essentially the HK-divergence based on adjusted family income deciles. It can be trivially extended to be based on persons not families and then extended to unattached individuals to cover the entire population (of taxfiler and dependants), alleviating my first two concerns. To alleviate the third we propose to refine it by considering optimal transport, which naturally leads to replacing the KL-divergence with the Wasserstein metric to define the new W-index.\nHowever, paying attention to the order of the income deciles and moving to the W-index comes at the expense of loosing the additive decomposability that made the D-index easy to refine and interpret when e.g. conditioning on dwelling types and large individual apartment buildings. So unfortunately the W-index won’t be able to serve as universal replacement, but we believe it will be a useful additional tool."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html#what-is-the-d-index",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html#what-is-the-d-index",
    "title": "Income mixing and segregation",
    "section": "What is the D-index",
    "text": "What is the D-index\nThe D-index is modelled on the Kullback–Leibler (KL) divergence of the tract level income distribution relative to the whole CMA. It’s the expectation of the log difference in the tract-level distribution to the CMA level distribution. Additionally, it conditions on dwelling type (apartment vs other) and also conditions on distributions within large apartment buildings, exploiting that the KL-divergence is additively decomposible, so it can easily be refined to the sub-groups of dwelling types or within large apartments.\nIt builds on adjusted family incomes, which is an underused metric that lends itself for comparing income distributions and we have made extensive use of this metric before. It alleviates issues around Simpson’s paradox that e.g. household income can run into or other compositional issues that e.g. plague metrics relying on average individual income.\nThe basic unit in the metric is census families. To compute the census tract level D-index, each census tract is subdivided into * individual “large” apartment buildings, that is buildings with (roughly) 50 or more units and at least 10 census families, * everything else.\nThe census tract level D-index is then the weighted sum of the KL-divergence of the income distribution within each of these sub-categories (large apartment buildings and everything else) relative to the CMA-wide income distribution. The income distributions are quantized to quintiles based on the CMA level distribution, so the CMA-level reference distribution is by definition uniform.\nMore formally, the tract-level D-index \\(D_c\\) for census tract \\(c\\) is given by\n\\(D_c=\\sum_{b\\in B}\\frac{n_b}{n_c} ~ D_{KL}(\\pi_b~|~\\pi_m)\\)\nwhere \\(B\\) is the set of large apartment buildings and the “everything else” category, \\(n_b\\) is number of census families in the building (category) \\(b\\), \\(n_c\\) is the number of families in census tract \\(c\\), \\(\\pi_b\\) is the discretized income distribution in building (category) \\(b\\) and \\(\\pi_m\\) denotes the metro-wide (by definition uniform) discretized income distribution.\nAs a reminder, the (discrete) KL-divergence is defined by\n\\(D_{KL}(\\pi_1~|~\\pi_2)=\\sum_{q\\in Q}\\pi_1(q) ~ \\log\\left(\\frac{\\pi_1(q)}{\\pi_2(q)}\\right)\\)\nWhere the sum runs over all income brackets \\(Q\\).\nThe authors point out that \\(D_c\\) can be (additively) decomposed into components separating out the contributions of variation across dwelling types and the contribution across large apartment buildings. In formulas\n\\(D_c=D_{KL}(\\pi_c~|~\\pi_m) + \\sum_{d\\in T}\\frac{n_d}{n_c} ~ D_{KL}(\\pi_d~|~\\pi_c) + \\sum_{b\\in A}\\frac{n_b}{n_A} ~ D_{KL}(\\pi_b~|~\\pi_A)\\)\nwhere \\(T\\) is the set of building types consisting of two elements (large apartments vs all others) and \\(A\\) is the set of large apartments. (So \\(B\\) is the union of \\(A\\) with the single element of all non-large apartment buildings.) While such a decomposition can generate useful insights, the individual components of \\(D_c\\) were unfortunately not made available in the dataset.\nLet’s take the D-index for a drive and see what this looks like.\n\nVancouver looks (expectedly) quite homogeneous. Downtown Eastside lights up, but the other census tracts have income distributions, including within and across dwellings types and large apartments, match the Metro Vancouver distributions quite well. Some other tracts show up, North Campus and some of the North Shore for example.\n\nMontréal has a much larger overall variance, and shows some stark contrasts. Westmount stands out. Mount Royal is visible too, as is a part of East Montreal near the airport. These latter two stand out for very different reason, Mount Royal has a high share of high-income population whereas the area adjacent to the airport has a high share of lower income population.\n\nToronto’s range is somewhere between Vancouver and Montreal, showing some interesting variation."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html#adjusted-family-income-decile-kl-divergence",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html#adjusted-family-income-decile-kl-divergence",
    "title": "Income mixing and segregation",
    "section": "Adjusted family income decile KL-divergence",
    "text": "Adjusted family income decile KL-divergence\nThe interesting part about the StatCan D-index is that they conditioned on dwellings. Unfortunately we can’t separate out the dwelling component from the overall index that was released, but we can build a related metric. The census has tract-level adjusted family income decile data. However, this data differs from the data used in the D-index in two critical ways that preclude us from making direct comparisons:\n\nThe universe of the census adjusted family income deciles is persons, whereas the D-index is built on census families. This means that an income divergence measures built on the census adjusted family income deciles will weight a large family more than a small family, even if members in both have the same adjusted family income.\nThe universe of the census adjusted family income deciles includes non-census family persons, so an index based on this metric counts considerably more people.\n\nAdditional smaller differences are\n\nThe census adjusted family income data is based on economic families whereas the D-index income is adjusted by the size of the census family.\nThe D-index and the census data is that the census income data is based on the 2015 tax year, whereas the D-index was built from the 2017 taxfiler data.\n\nLooking at these differences, the census adjusted family income seems overall (with the exception of it being based on older data) like a preferable metric to measure income mixing, but the D-index was developed with a focus on dwelling type and within large apartment building income mixing, where the focus on family units may have merit.\nDespite these differences, it is interesting to compare the D-index to the tract-level KL-divergence based on adjusted family income.\n\nAs expected, the result looks remarkably similar to the StatCan D-index. The overall scale is different, most likely due to the inclusion on non-family individuals.\nIn darker areas, the adjusted family income distribution matches the Metro-wide one, in lighter areas it is different. But from this map we don’t know how they differ. With some local knowledge we can deduce that the Downtown Eastside is different because of a high share of lower income people, whereas West Vancouver, or Arbutus Ridge, or Southlands differ because of a higher share of higher income people.\nAnother reason for the difference is that we were working with finer (decile) income data than the quintiles that were used in the D-index, and the additive decomposability outlined above implies that a coarser decomposition can’t be larger and will turn out to have smaller values in practice, as can easily be checked.\n\nThe effect of refining or condensing the income distribution categories is probably better highlighted in a scatter plot.\n\nWe clearly see how the KL-divergence based on deciles is never smaller than the quintile-based version, and sometimes is considerably larger.\nNext we can compare the tract-level D-index to our quintile-based KL-divergence.\n\nThe quintile-based KL-divergence is predominantly larger than the D-index, as discussed before this is likely due to the D-index only considering census families. We also notice that the D-index has been rounded to two decimal places below 0.1 and one decimal place above that. The rounding below 0.1 is probably adequate given the measurement errors inherent in the data, the rounding above 0.1 seems excessive. It looks like the intent is for this to guarantee higher levels of privacy, I’d have to think longer about this to see how large the privacy risk actually is. (It’s high time to implement a more comprehensive privacy strategy like e.g. differential privacy instead of relying on ad-hoc rules for rounding to do the job.)\nTo get a better understanding let’s take a detailed look at the census tract in the Downtown Eastside.\n\nFor more context, we consider a selection of census variables, as well as T1FF taxfiler data for the DTES tract. First let’s consider the time difference. The T1FF recorded median individual income in 2015 was $17,400 vs $18,580 in 2017, which is a modest increase. At the same time, the 2016 census recorded a 2015 median individual income of $15,381, which is quite a bit lower. This hints as some geographic mismatch or under/over coverage in the taxfiler data.\nFurther, median family income was $38,910 for 2017 census families from T1FF data, whereas the census reports $33,920 for economic families, with an average family size of 2.4. This gives us a (very rough) estimate of median adjusted economic family income, only considering persons in economic families and ignoring unattached individuals, of $21,895, compared to a considerably lower median income of unattached individuals of $13,414. This will cause our KL-divergence, that also considers unattached individuals, to be considerably higher than the D-index that only considers census families and likely accounts for much of the difference."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html#h-index",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html#h-index",
    "title": "Income mixing and segregation",
    "section": "H-index",
    "text": "H-index\nAnother popular index to measure mixing is the Theil’s information theory index, also called H-index, that measures the relative entropy of the census tract distribution to the overall metro-wide distribution.\n\nOn the map the H-index appears similar to the other two metrics we considered, although the scale hides some of the detail. Let’s run a quick comparison to the KL-divergence.\n\nThe two are generally highly correlated, but the shape of the pattern is interesting in that the data seems to cluster around a conic while avoiding the centre part. Calgary stands out with a broad shape, whereas Vancouver appears quite narrow. This bifurcated nature of the H-index is best understood by considering a simple example. If the reference metro-level distribution is uniform (so it has maximal entropy), then the KL-divergence becomes proportional to the H-index. Vancouver income deciles match the Canadian (by definition uniform) deciles quite well, resulting in a good fit. The Calgary income deciles however differ quite strongly, having much larger higher income brackets. Montreal on the other hand has much slimmer high-income brackets and fatter middle (and low) income brackets.\n\n\n\nAdjusted family income deciles\n\n\nHaving a non-uniform distribution as background gives opportunity for the H-index to differ from the KL-divergence. The DL-divergence compares the shares in each corresponding bucket (and does not detect if the order of the buckets are changed in a consistent way for both distributions), whereas the H-index only cares about the distributions separately, the results won’t change if the order of the buckets are changed independently for the two distributions. So in Montreal, the H-index has a hard time to distinguish a census tract with a fatter high-income distribution from one with a fatter low-income distribution, but the KL-divergence will see these differently because the background distribution distinguishes them.\nWe can visualize this by broadly colouring each region by whether the lower three deciles, the middle four deciles or the upper three deciles have the highest number of people in each region.\n\nThis offers some insight into how the entropy-based H-index differs from the KL-divergence. The KL-divergence benefits from distinguishing when different permutations are carried out on each of the income distributions, whereas the H-index does not."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html#w-index",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html#w-index",
    "title": "Income mixing and segregation",
    "section": "W-index",
    "text": "W-index\nDo we really need another index? We believe that we do. The previous example highlighted again that the order of the income brackets matter. The KL-divergence, and thus the D-index, don’t change if we consistently relabel the income brackets and e.g. exchange the bottom bracket with one of the middle income brackets.\nTo see how this might cause problems let’s look at a motivating example. We will use a uniform distribution as the background and consider the following two cases.\n\nThe two distributions differ only in exchanging some of the bins. Remembering that we take the background distribution to be uniform we can compute the D-index and H-index for these two scenarios.\n\nThe math-minded people will have guessed this result, the two indices are identical across the two scenarios. They can’t distinguish them. Neither of them encode the order of the bins, form an information theory point of view those two scenarios are equivalent That’s why these indices are best suited for categorical data. But income deciles also come with an order. But that order matters to us.\nGoing back to the two distributions, most people would say that scenario 1 is closer to a uniform distribution than scenario 2. An intuitive way to understand that is to consider what would happen if we aggregated the data to income quintiles. In that case, scenario 1 turns into the uniform distribution, while scenario 2 does not. The order of the bins matters.\nMath or stats folks will know where this is headed. An elegant way to consider order is to fold in optimal transport, which naturally extends the KL-divergence to the Wasserstein metric. Without going into detail, the Wasserstein metric encodes how much “work” it is to convert those two distributions into a uniform distribution. For scenario 1 it’s not that much work, all we need to do is shift some people over from the 5th to the 6th decile.\nFor scenario 2 it’s a lot more work, we need to shift people over from the bottom all the way to the top decile. In our particular application we can also re-interpret that as a measure of the level of taxation that’s required to re-distribute income to get the background income distribution, in this example the uniform distribution or in our census data driven case the metro-wide distribution.\nMore technically, we need to define a “work” or “cost” function, that is the cost that’s associated with moving a person from one bin into another. We won’t go into details and just count the number of deciles we traverse, so the cost to move a person from the 3rd decile to the 7th decile would be 4.\nLet’s take a look how the Wasserstein metric (of order 1) performs on our scenarios.\n\nAs expected, the W-index distinguishes the two scenarios, moving people from the bottom to the top bracket is a lot more expensive than moving people from the 5th to the 6th. Time to try this out on some real data!\n\nWe see a lot of similarities to the KL-divergence (and the D-index) and the H-index, but there are also clear differences. let’s take a closer look.\n\nGenerally the measures correlate well, but let’s take a look at some examples where they differ, so examples that have the same KL-divergence but different W-indices and vice versa.\n\nWe highlighted a three pairs of regions with (almost) the same KL-divergence but different W-indices to take a closer look why the metrics differ.\n\nWe picked 6 regions so that each row has the same D-index, the W-index in the left column is significantly lower than that in the right column. For the each row we have that from an information theory point of view where order of the brackets does not matter, these look quite similar. But when order matters, we see more mixing on the left than on the right. In the first row, shuffling people from the middle over to the sides is cheaper than moving people all the way from the top to the bottom. Similarly in the second and third row, moving people from the extremes toward the middle is cheaper than moving people from one extreme all the over to the other side.\nAnother way to think of this is that in left column, re-grouping the deciles in quintiles generally makes them more equal by evening things out a bit. Doing the same on the right does not really help much to make things more equal.\nWe can similar look at pairs of regions with (almost) identical W-index but different D-index.\n\nAgain, we take a closer look to understand what accounts for the differences.\n\nIf we had to rank these based on which ones are more “mixed” we would probably pick the left one in each case. So the KL-divergence definitely has some redeeming qualities.\nTo round this off, let’s plot the StatCan D-index against the W-index.\n\nOverall, we feel that both the KL-divergence and the W-index both have their strength and weaknesses. We probably tend to favour the W-index, but also like the KL-divergence. It might also be interesting to use a mixture of the two.\nThe big disadvantage of this approach is that the W-index doesn’t allow for a straight-forward decomposition when we refine the regions like the D-index does. There may be a more abstract formulation involving Barycenters that allows for a decomposition, but it’s not immediately obvious to us. And may well not exist."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html#other-quantizations",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html#other-quantizations",
    "title": "Income mixing and segregation",
    "section": "Other quantizations",
    "text": "Other quantizations\nIncome mixing and segregation is an important metric to monitor across space and time. Since we mostly work with geography of residence, this has important implications on housing policy. (It would be interesting to contrast this to income mixing and segregation by geography of work!)\nNoting if income mixing differs from the metro-wide background income distribution is useful, but it immediately raises the question how the distribution differs. We generally feel very differently if an area stands out because it has a lot more low-income persons vs a lot more high income (or middle-income) persons. And in as far as we view income segregation as a problem that may require policy intervention, the interventions will be quite different.\nOne way people have tried to address this is by categorizing neighbourhoods into e.g. “high”, “middle” and “low” income areas relative to their respective metropolitan areas. A prominent example is a location quotient of the average individual income popularized by the Neighbourhood Change Project. Unfortunately this metric mostly ignores income mixing altogether, and suffers from compositional issues, which makes it hard to properly interpret and compromizes its usefulness to informing policy.\nWe have introduced an alternative approach to broadly categorize neighbourhoods by income level that also explicitly models income mixing, we include an example using finer categories for convenience.\n\n\n\nincome mixing\n\n\nThis metric also uses adjusted family income deciles (regrouped into three bins) to account for varying family size (as well as single person households), and in this example we expanded the categories for a finer view. Still, the City of Vancouver shows a high level of income mixing, as can be seen on the map on the left as well as on the scatterplot in the map legend to the right. Grey areas have an income distribution broadly similar to the metro wide average, yellow/brown areas differ by having a higher share of low-income persons, blueish areas differ by having a higher share of middle-income persons and pinkish areas differ by having a higher share of high-income persons. The visualization employs colour mixing to interpolate between the extreme states. The green-grey area on the east side is characterized by still being fairly close to the metropolitan distribution, but with fewer high-income persons made up for equally by lower and middle income persons.\nWe have previously spent time looking into more detail how this income mixing metric differs from e.g. average individual incomes and why explicitly modelling income mixing is important.\nThese kind of categorizations are useful to understand in more detail how neighbourhood income distributions differ from a reference distribution, but they are also harder to work with as they encode more information. The D-index (or the W-index) are easier to work with because they condense information into a single number instead of e.g. taking values in a (discretized) simplex like our income mixing metric shown above."
  },
  {
    "objectID": "posts/2020-09-21-income-mixing-and-segregation/index.html#upshot",
    "href": "posts/2020-09-21-income-mixing-and-segregation/index.html#upshot",
    "title": "Income mixing and segregation",
    "section": "Upshot",
    "text": "Upshot\nThere are lots of different ways to understand income mixing and segregation, and there is no single best metric. The choice of metric depends on the question one is asking, and in many applications it can be useful to consider several metrics.\nOne-dimensional indices can be extremely useful when analyzing income mixing, and the new D-index is an innovative and interesting addition. It is particularly useful in the context of housing policy as it explicitly models the known covariates of segregation across dwelling types and segregation across large apartment buildings within the same census tract, so it is designed to detect when e.g. an area shows good overall income mixing, but incomes fail to mix when conditioning on dwelling types with e.g. high incomes segregating into single detached houses and lower incomes into apartments within the same census tract.\nWhile the decomposability of the D-index is a great features, it’s not really useful unless data on the decomposition is made available. Focusing on families and excluding unattached individuals can be useful for some applications, but if only one index is published we would rather have one that covers the entire population.\nWe are not convinced that the advantages of the decomposability of the D-index outweigh the downside that the D-index ignores the ordering of the income brackets. We believe the W-index is a better overall choice for an index to measure income mixing. Either way, we are looking forward to updates on the D-index, either adding data on the decomposition or adding data for other years to create timelines. And possibly by also adding the W-index.\nAlternatively it would be extremely useful if StatCan were to publish adjusted family income deciles at the census tract level based on taxfiler data for all years, this would allow others to construct the W-index or the KL-divergence depending on their needs. Adjusted family income deciles is probably the most useful income variable to inform public policy, and making this widely available may also help marginalize largely uninformative yet ubiquitous metrics like ones based on average individual income.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes."
  },
  {
    "objectID": "posts/2020-08-27-keeping-the-leavers/index.html",
    "href": "posts/2020-08-27-keeping-the-leavers/index.html",
    "title": "Keeping the Leavers",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nDo people select cities from diverse alternatives? Or do cities select residents from diverse flows of people?\nThe answer is pretty much: both.\nPeople can look around and consider where they want to end up. And cities, through municipal policies, can and do work to select their residents. EXCEPT cities can’t do this directly. At least across North America, cities generally aren’t allowed to establish and maintain their own immigration policies. When they try to do so, the courts shoot them down, because both Canada and the USA enshrine the right of people to move within their borders. Cities can’t stop them. But cities have a big role in deciding how much room to make for people. And they also generally get to decide what form any added room should take. Many, for instance, only allow the most expensive forms of new housing, like single-family detached on large lots, selecting for wealthier residents. So that’s how cities select their residents.\nThe fact that it’s a two-way selection process, with both people and cities doing the selecting, makes it quite difficult to forecast something like future housing needed to prepare for a city’s population growth. Yet this is what cities, including Vancouver, are often tasked with doing by way of justifying their policies.\nOne way of going about this is to argue that past population growth is our best estimate to forecast future housing demand. This is a bad argument on many levels as we have explained at length before. In expensive gateway cities, like Vancouver, this often gets accompanied by nativist notions that population growth is driven almost entirely by international migration as net domestic migration is small. But net estimates obscure the actual size of flows, where local and domestic movers predominate and make up the majority of those occupying new housing.\nMore troubling is the implicit logic that elevates domestic in-migrants over international in-migrants, providing only the former a legitimate claim to the place freed up by a domestic out-migrant. So far, freedom for movement in Canada extends to immigrants, as it should. And not all immigrants come from outside of Canada. Increasingly non-permanent residents turn into immigrants (including both of us!) This simply results in a drop in net non-permanent residents and an increase in immigrants in these stats, without anyone actually moving. This speaks to the complexity of how cities select their residents from diverse flows of people. A thought experiment might be helpful to better illuminate how it works in practice."
  },
  {
    "objectID": "posts/2020-08-27-keeping-the-leavers/index.html#creating-room-for-people-to-stay",
    "href": "posts/2020-08-27-keeping-the-leavers/index.html#creating-room-for-people-to-stay",
    "title": "Keeping the Leavers",
    "section": "Creating room for people to stay",
    "text": "Creating room for people to stay\nFirst let’s look at past population growth. BC Stats splits this up neatly into several sub-categories, which we can think of as flows.\n\nNet population growth for Metro Vancouver has hovered around 28k people a year. But it’s not like this is a one-way flow, about 50k people leave Metro Vancouver every year and somewhere around 75k people come. Some people have a really hard time making room for newcomers. But maybe people are more sympathetic to people leaving. Of course many people leave Metro Vancouver for greener pastures, a better job, move for university or other personal reasons. But the “Leaving Vancouver” letters (practically a genre at this point) are testament that not all people moving away think of their moves in positive terms. Many feel squeezed out. People keep talking about friends that left because they could not find adequate housing in Vancouver.\nSo let’s say, for the sake of argument, that one out of five people moving out of Metro Vancouver to elsewhere in Canada really wanted to stay but could not make it work. And, of course, we already know that feeling “forced” to move is strikingly common in Vancouver, even for those who remain. So let’s say we are sympathetic to the people who leave town and would actually like to insure enough room for them to stay. What would that take?\nThat’s easy to check, all we need to do is reduce the size of the inter- and intra-provincial out-migrant buckets in the above graph by 20%.\n\nThe net effect is that fewer people would have been leaving Metro Vancouver, while the same people came. And our population growth went up by about 30%. Which means that we should have built 30% more housing than we did over the years to make that possible.\nNow some readers will argue that that’s not how things work. If we had built 30% more housing, that does not mean that one in five of the people that moved would have gotten to stay. Some of that housing would have been taken up by people that wanted to move to Metro Vancouver but could not find adequate housing, but with more housing they could have made it work and would have out-bid some of those that were hoping to stay.\nAnd with more housing available, some new households might be created that might otherwise not exist. Maybe someone will move out of their parents place earlier and take up one of those new units without adding to population growth at all. And in return one of the 1 in 5 people that had hoped to stay might still end up feeling forced to leave again.\nAnd people arguing that are of course exactly right. That’s the point of this exercise, housing and population growth are endogenous. Which is kind of a fancy way of saying that people select cities from diverse alternatives AND that cities select residents from diverse flows of people."
  },
  {
    "objectID": "posts/2020-08-27-keeping-the-leavers/index.html#empty-homes---the-ultimate-anti-housing-red-herring",
    "href": "posts/2020-08-27-keeping-the-leavers/index.html#empty-homes---the-ultimate-anti-housing-red-herring",
    "title": "Keeping the Leavers",
    "section": "Empty homes - the ultimate anti-housing red herring",
    "text": "Empty homes - the ultimate anti-housing red herring\nHere in Vancouver, those resisting making room for more people to stay and arrive like to point toward a supposed mismatch of housing growth to household growth between 2001 and 2016, supposedly leaving lots of empty homes. This time window is of course chosen deliberately to include the change in census methods 2001-2006, and this talking point mostly goes away when properly accounting for that. To avoid adding homes people will still point to some vague notion of dwellings being left empty, even though we have better data on empty homes than ever before and there are very few problematic cases paying the Empty Homes Tax or Speculation and Vacancy Tax left in the region."
  },
  {
    "objectID": "posts/2020-08-27-keeping-the-leavers/index.html#how-should-we-do-population-projections",
    "href": "posts/2020-08-27-keeping-the-leavers/index.html#how-should-we-do-population-projections",
    "title": "Keeping the Leavers",
    "section": "How should we do population projections?",
    "text": "How should we do population projections?\nSo given the endogeneity issues: how should we be doing future population projections? In high demand areas like Metro Vancouver we should start from housing growth. That’s what cities can control. How many condos will be built? How many rental homes? How many non-market homes? How many infill homes? And given a scenario of housing growth, we can model what population growth might look like. How many people would move here from elsewhere in BC? How many from elsewhere in Canada? How many from outside the country? How many people would move away? It’s not an exact science, but demographers can build decent models once we know how much housing is being built and how cities are trying to select their residents. And the public can look at different scenarios of housing growth and the resulting scenarios of population change and use that to have a more informed discussion about where they want the city to go as well as who they want to enable to stay.\n\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes."
  },
  {
    "objectID": "posts/2020-07-06-canadian-1996-census/index.html",
    "href": "posts/2020-07-06-canadian-1996-census/index.html",
    "title": "Canadian 1996 Census",
    "section": "",
    "text": "Canadian 1996 census data is now avaiable on CensusMapper for anyone to make maps, for API access and via the {cancensus} R package. Yay!\nThe geographic data is not freely available from Statistics Canada, but can be custom ordered (via a small processing fee). Now the data is freely available on CensusMapper. The geographic data is slightly processed, we clipped out water areas and geographies from CSD upward are slightly simplified for better mapping performance as usual on CensusMapper. I threw the raw data from StatCan into an S3 bucket for download in case anyone is interested in the original data and does not want to pay the processing free."
  },
  {
    "objectID": "posts/2020-07-06-canadian-1996-census/index.html#a-little-history",
    "href": "posts/2020-07-06-canadian-1996-census/index.html#a-little-history",
    "title": "Canadian 1996 Census",
    "section": "A little history",
    "text": "A little history\nWe have been slowly expanding the data available via CensusMapper. CensusMapper started out with 2011 Census and NHS data, a flexible tool to map census data that we opened up to the general public for anyone to make their own maps.\nWhile waiting for the 2016 census we added 2006 census data to CensusMapper, our main focus was still on mapping.\nAs 2016 census data started to trickle in we updated CensusMapper with the new data and build a common tiling of 2006 and 2011 census data to directly compare the two censuses on fine geographies.\nIt quickly became clear that just mapping the data was inadequate, maps aren’t analysis. To deal with the increasing mess of our analysis pipeline we expanded CensusMapper to serve data via APIs and built the {cancensus} R package as a standardized starting point for reproducible data analysis. And we started to publish the code for our analysis on GitHub and soon after moved our blog over to blogdown for tighter integration of code and version control of our posts, with a strong emphasis on transparency, reproducibility and adaptability of our analysis.\nAs we shifted away from mapping and toward data analysis, timelines became increasingly important. We added data for the 2001 census and started work on tongfen, a package that automates the process of making census data longitudinally comparable on fine geographies.\nThrough our work we were able to add custom tabulations to CensusMapper and make them publicly available for mapping or via the API. We also added annual census tract based T1FF taxfiler data all the way back to 2000.\nThe 2000 T1FF taxfiler data comes on 1996 census geographies, which gave us the final motivation to add the 1996 census. We have been eyeballing this for a while, but one holdup has been that we have not been able to find the geographic data for the 1996 census on a clean open data licence. The data is available through various portals, but not in a way that we can freely distribute it through CensusMapper with just the regular Statistics Canada Open Licence that underlies all the other data on CensusMapper. And now 1996 data is also available on CensusMapper, complete with metadata."
  },
  {
    "objectID": "posts/2020-07-06-canadian-1996-census/index.html#how-to-use-1996-data",
    "href": "posts/2020-07-06-canadian-1996-census/index.html#how-to-use-1996-data",
    "title": "Canadian 1996 Census",
    "section": "How to use 1996 data",
    "text": "How to use 1996 data\nThe 1996 data follows a different geographical hierarchy pattern than the 2001-2016 censuses. Instead of Dissemination Areas (DA) it has Enumeration Areas (EA) as the finest geography with all census variables, and it does not have Dissemination Block (DB) data.\nThat requires a slight change to the {cancensus} package to allow querying of EA level data, which we will get ready from CRAN in the coming weeks. To simplify things, the CensusMapper API server will translate requests for 1996 DA level data (which does not exists) to 1996 EA level data, even though these geographies are quite different and DA/EA level {tongfen} will produce results that are essentially equivalent to CT level {tongfen}.\nMoreover, the dataset identifier for the 1996 data (CA1996) does not follow the pattern of the 2001-2016 data where we only used the last two digits of the year (CA01, CA06, CA11, CA16). When we started CensusMapper, we did not anticipate going back that far that we want full years in the labelling. In retrospect that’s a mistake, and we will likely migrate to more consistent naming (without breaking backward compatibility).\nKeeping this in mind, we pick a simple census variable, the share of dwelling units in need of major repair, and set up the data import. For each census year we pick out the variables we need via that CensusMapper graphical API interface, specify the geographic region of interest (City of Vancouver in this case), and string the data for the 1996 through 2016 censuses together.\nvariables &lt;- list(\n  CA1996=c(repairs=\"v_CA1996_1687\", all= \"v_CA1996_1678\"),\n  CA01 = c(repairs=\"v_CA01_104\", all= \"v_CA01_96\"),\n  CA06 = c(repairs=\"v_CA06_108\", all= \"v_CA06_105\"),\n  CA11 = c(repairs=\"v_CA11N_2232\", all= \"v_CA11N_2230\"),\n  CA16 = c(repairs=\"v_CA16_4872\", all= \"v_CA16_4870\")\n)\nregions &lt;- list(CSD=\"5915022\")\n\nget_dwelling_condition &lt;- function(regions,variables,level=\"Regions\",geo_format=NA) {\n  names(variables) %&gt;% lapply(function(dataset){\n    year=str_extract(dataset,\"\\\\d+\")\n    if (nchar(year)==2) year=paste0(\"20\",year)\n    get_census(dataset,regions=regions, vectors=variables[[dataset]], \n               level=level, geo_format=geo_format) %&gt;%\n      mutate(Year=year)\n  }) %&gt;%\n    bind_rows() %&gt;%\n    mutate(share=repairs/all) %&gt;%\n    mutate(share_d=pretty_cut(share,c(-Inf,0.05,0.1,0.15,0.2,0.3,0.4,Inf), \n                              format = scales::percent,ordered_result=TRUE))\n}\nFirst we query the city-wide data.\ndata &lt;- get_dwelling_condition(regions,variables)\ncolours &lt;- setNames(viridis::viridis(length(levels(data$share_d))),levels(data$share_d))\n\nggplot(data,aes(x=as.character(Year),y=share)) +\n  geom_bar(stat=\"identity\",fill=\"brown\") +\n  scale_y_continuous(labels=scales::percent) +\n  labs(title=\"City of Vancouver homes in need of major repairs\",\n       y=\"Share of homes in need of major repairs\",\n       x=NULL,\n       caption=\"MountainMath, StatCan Census 1996-2016\")\n\nThe share of dwellings in need of major repairs reached a peak in 2001 and slowly decreased after, possibly an effect of the leaky condo crisis. This kind of data is best checked via a cross tabulation.\n\nWe would expect the share of buildings in need of major repairs to increase with building age, but looking at the data we see that buildings between 1981 and 1995 (and maybe even up to 2001) show a heightened share of buildings in need of major repairs. Using a crude estimate of maybe half of those units been 1951 and 2001 being due to leaky condos, they make an overall contribution of 1.1 percentage points to the share of dwellings in need of major repairs. Which explains much of the jump.\nMapping the share of homes in need of major repairs might help us understand this better. We start out with Census Tract level.\nggplot(get_dwelling_condition(regions,variables,level=\"CT\",geo_format='sf')) +\n  geom_sf(aes(fill=share_d),size=0.1) +\n  scale_fill_manual(values=colours,na.value=\"darkgrey\") +\n  map_theme +\n  labs(title=\"City of Vancouver homes in need of major repairs\")\n\nThere is clear geographic variability, it might be worthwhile to zoom in to DA/EA level data.\nggplot(get_dwelling_condition(regions,variables,\"DA\",'sf')) +\n  geom_sf(aes(fill=share_d),size=0.1) +\n  scale_fill_manual(values=colours,na.value=\"darkgrey\") +\n  map_theme +\n  labs(title=\"City of Vancouver homes in need of major repairs\")\n\nWe notice quite a bit of volatility in the data, both geographically and temporally. We will leave it to another post to explore the relationships of dwellings in disrepair with other variables. People that have been following the leaky condo and other developments with major deficiencies more closely might be able to make sense of these patterns just by looking at the maps."
  },
  {
    "objectID": "posts/2020-07-06-canadian-1996-census/index.html#upshot",
    "href": "posts/2020-07-06-canadian-1996-census/index.html#upshot",
    "title": "Canadian 1996 Census",
    "section": "Upshot",
    "text": "Upshot\nTimelines matter a lot for analysis, and it’s great to have one more census conveniently available for mapping and API access. As usual, the full code for this post is available on GitHub."
  },
  {
    "objectID": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html",
    "href": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html",
    "title": "Projections and self-fulfilling prophecies",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWhen people want to live in your city, how many should you let in? On the one hand, this is a moral question. Do you have an obligation to people who don’t already live here? On the other hand, it’s a moot question. At least in Canada, cities don’t have the power to control migration.\nBUT WAIT! Cities DO have power over how many new dwellings to allow. This actually changes our moral question a bit. Cities can’t keep people out, but because they have power over dwellings, municipalities can control how many people get to remain in. As a result, if you don’t allow any new dwellings when people want to live in your city then rich people will generally outbid poor people for the housing that’s left.\nIt may be the case that municipal politicians are fine with rich folk replacing the poor folk in their cities while their own housing rapidly appreciates in price. Why let any new housing get built? “No thanks, we’re full!” But they can’t always SAY this. Especially in cities full of renters that generally support progressive and inclusive values.\nSo what to do? Two paths are readily available. One: transform the moral question (“isn’t it terrible that developers make money off building housing?”) Two: turn the moral question into a narrow technocratic one instead. Let’s explore this latter option a bit more, because it’s really interesting and sits well within our wheelhouse (mathematician and demographer).\nHere in the City of Vancouver, a new motion was just launched, titled Recalibrating the Vancouver Housing Strategy (RVHS). There are some good initiatives in this motion, but the main thrust and motivation is to turn the moral question of how many people get to remain in Vancouver into the narrow technocratic question of how do we forecast population growth? As any demographer can tell you, this can be tricky, especially when it comes to forecasting for municipalities. But there’s a naive kind of work-around some people use when they don’t follow demographic techniques and concerns very closely and don’t want to think too hard about the question at hand. They simply turn the population forecast into a projection forward from how a city grew in the past.\nThis is a neat trick! Especially if you’re in a city that’s limited new dwellings in the past and thereby kept its population growth to a minimum and you want to keep it that way. “The evidence suggests we haven’t been growing very fast, so we shouldn’t add much more housing.” With a little bit of hand-waving, the number of dwellings allowed by the city is reimagined as something that can be tailored to meet the forecast rather than the central determinative factor of the forecast.\nIs this the kind of thing that could happen in Vancouver? Before we get into the motion, let’s just quickly look at Vancouver’s recent past. We know prices and rents rose rapidly through 2016 (and beyond), which is pretty good evidence that we didn’t add enough housing for the people who wanted to live here all by itself. But how did the City of Vancouver grow relative to the rest of the region? It grew more slowly. (“No thanks! We’re full!”) Did we lose poor people and replace them with rich people as a result? Yap, this is exactly what has happend in the City of Vancouver, which has lost lower and middle income people, and gained high-income people, at a faster pace than the surrounding Metro area."
  },
  {
    "objectID": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#the-motion",
    "href": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#the-motion",
    "title": "Projections and self-fulfilling prophecies",
    "section": "The Motion",
    "text": "The Motion\nNow let’s get back to that RVHS motion, starting with part A:\n\nTHAT Council direct staff to revisit the Housing Vancouver Strategy targets to align with historical and projected population growth based on census data.\n\nThis is a vague statement. There are, of course, many ways to “align” something (Dungeons and Dragons fans may be immediately reminded of the nine different alignments readily found therein). There are also many ways to project population growth. These often rely upon multiple sources of data. Birth rates, death rates, age structure, labour market statistics, and net migration rates serve as typical baseline sources of information for demographers, and are usually gathered from all manner of data (e.g. vital statistics, surveys, policy-based immigration projections, etc.) rather than simply historical census data. So how is the author of this particular motion imagining more specific alignments and projections? The answer can probably be found in the WHEREAS sections 4 and 5:\n\nPopulation growth has been consistent at approximately 1% per annum over the past 20 years according to Statistics Canada census data. Based on this historical trend, a similar growth rate for the coming decade would amount to a population increase of around 66,000. In the City of Vancouver, the average household size is 2.2 individuals per dwelling unit (or “home”);\n\n\nThe target of 72,000 new homes across Vancouver in the next 10 years multiplied by 2.2 would mean a population increase of 158,400 - more than twice the historical rate. A projected historical rate of population growth would imply instead a need for roughly 30,000 new housing units over the coming decade;\n\nWe’ve left the refined techniques of demography behind here, as well as the determinative forces of births, deaths, and moves. Indeed, people pretty much disappear and their dwellings get only scare-quotes as homes. But let’s follow the math we do get and try and understand what projecting past trends means in terms of numbers (leaving aside if we agree that things went splendid and we should just keep going the same way). Let’s try and reproduce the estimation of new housing units assuming we hold the 20 year trends in the two mentioned metrics, population and household size, constant.\nThe 1% annual growth rate roughly checks out, although there have been variations.\n\nAnd population in the City has grown consistently at a lower rate than overall Metro Vancouver population. In fact, if the City of Vancouver had grown at the same rate as Metro Vancouver over those 20 years, Vancouver would have had roughly 60,000 more people within city limits in 2016. But maybe people would just rather live farther out in the surrounding suburbs? Again, there are variations, but overall that is not what the price and rent data tell us.\n\nPeople want to live in Vancouver. But they often settle for living farther out, based on the specifics of what they want and can afford. The competition for the limited number of dwellings in Vancouver drives up prices here relative to surrounding municipalities.\nSo what to make of the close relationship between population growth and dwelling units added? It’s a real relationship.\n\nThe motion, as presented, seems to suggest that this close relationship is evidence that we’re projecting population growth really well, thereby allowing almost perfectly enough new housing to meet population needs. Is this what we’re doing? Well, no. In fact, the amount of new housing allowed sets a cap on population growth that can only be exceeded by increasing household size (which in many cases cities have also made illegal)1 or decreasing the number of empty dwellings.\nThere is broad support for decreasing the number of empty dwellings, and both the City of Vancouver and the Province of British Columbia have put in place taxes on vacant properties and their owners to do just that. Have they succeeded? Quite possibly! But compared to other municipalities, Vancouver’s vacancies (as recorded in the Census) looked relatively normal prior to the new taxes, despite persistent rumours of some mythical oversupply. After the new taxes, administrative data reveals there aren’t many taxable units left vacant at all (~1%).\nWhat about household size? The motion suggests imposing a constant for Vancouver, expecting 2.2 people per household. But household size is not staying constant. It’s falling all across Canada, due to a combination of forces (aging of the population, declining childbearing, changes in partnership, the rise of people living alone). We also know that as people get richer, they tend to occupy more space. And, as pointed out above, Vancouver’s been getting richer.\n\nAs we see, household size in the City of Vancouver has continuously declined over the years, a trend that has significant impact on the relationship between housing and population growth. Sticking with the bad assumption that past population growth should be predictive of future housing needs, we can see that we’re still going to need more housing per person than in the past. Projecting these trends forward, lazily anchored at the 2016 census data, gives an increase in population in private households of about 67,000 and a corresponding increase in 41,000 households (aka occupied dwelling units). And that is not yet accounting for the increase in population in non-private households that Vancouver has experienced, like retirement homes or similar institutional housing.\nSo if the RVHS motion points us toward a bad way to do population projections, then how should one do it? There are lots of models to look at, but given that people want to live in Vancouver, a key ingredient in any model should be how much housing will be allowed. Conditional on allowing a given amount of housing, we can attempt to forecast how many people will come. But this moves us back from narrow technical questions (which we’re more than happy to continue exploring in depth!) toward the central moral question at hand. How many people are we comfortable allowing to live in Vancouver? Because if we allow more housing, more people will come. And if we allow more housing, we’ll also allow more of those currently at risk of feeling unwanted in Vancouver to stay.\nThat begs the question: What would be the problem with allowing more housing? The last WHEREAS of the RVHS motion holds an answer to that.\n\nA revised and more accurate understanding of demographic needs and demand will assist in properly planning for the post COVID-19 reality. Setting excessively high targets will pressure the City of Vancouver to grant significant amounts of density at a low price, in an attempt to induce housing construction approaching the HVS targets. This will cost the City of Vancouver potential revenue, and will mean that the City abandons its commitment to having growth pay for itself.\n\nIn short, housing might get cheaper. Which incidentally is quite in line the goals of the Vancouver Housing Strategy.\nBut there are a couple things here that need a bit more unpacking. First, from the title throughout the motion and showing up here again are mentions of planning for a “post COVID-19 reality.” To put it bluntly, this is odd. These parts of the motion caution us against assuming what comes next will reflect what came before. But, as discussed above, this is exactly the assumption the rest of the motion says we should make, resting as it does upon a very selective reading of Vancouver’s recent population growth. Weird contradiction. But then again, pretty much the same language has been employed way before COVID-19 was on anyone’s radar, suggesting that COVID-19 has just been tacked on for extra effect.\nSecond, the notion that “growth pay for itself” sounds quite reasonable, but it’s not clear what that means in practice. In Vancouver, new housing projects pay a variety of municipal fees, DCLs, CACs and additional engineering fees upfront, and annual property taxes thereafter. How much of the overall cost of living in the city should be charged upfront, and how much should be charged over the lifetime of the housing as property taxes? That’s a political question that Vancouver should have a discussion on.\nCharging high entry fees keeps prices high, not just of new housing but of all housing. It encourages treating housing as an investment, with low holding costs (property taxes) and high barriers to increasing housing even as population pressures keep prices and rents rising.\nCharging a lower entry tax and collecting a higher portion as property taxes later can lower the entry point to housing and spreads the costs out over the lifetime of the dwelling unit. This treats housing as a place to live, lowering the barriers to new housing construction and asking people to pay for city services and amenities over their time living in the city."
  },
  {
    "objectID": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#the-sort-of-good-parts-of-the-motion",
    "href": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#the-sort-of-good-parts-of-the-motion",
    "title": "Projections and self-fulfilling prophecies",
    "section": "The (sort of) good parts of the motion",
    "text": "The (sort of) good parts of the motion\nLet’s end with a few bright notes. There are some good parts to the motion! We like data and Part B asks:\n\nTHAT Council direct staff to provide annual historical data since 2000 on the number of units approved through rezoning, the breakdown of housing types that have been approved, housing starts and net housing completions, and estimated zoned capacity for the City of Vancouver.\n\nThis part of the motion is asking for better data, but it needs refinement. As it is right now it is hard to see what it will accomplish.\nNumber of units approved through rezoning is hard to interpret unless it is accompanied by more detail on how many of these units actually got built. Take the approved first version of the Oakridge development for example. A massive number of units got approved, yet the project died when drilling found an aquifer that precluded the project from going forward as approved. Several years later, a different proposal got approved, for the data on approvals to be useful we need to know what happened to those units.\nMonthly data on housing starts is already easily available, asking the data be reproduced adds zero value and amounts to a waste of staff time.\nNet housing completions is an important number, but very hard to do in Vancouver, given our high reliance on informal housing. It is still worthwhile to try and approximate this, but the motion should be clearer what part staff should focus on beyond the data on completions, demolitions and secondary suite estimates that we already have.\nEstimates of zoned capacity is a great stat to get clarity on. Some vague estimate has been making the rounds for a while after surfacing in a consultant report, with next to no detail how it was derived. Having an estimate with a clear methodology would be a great addition to inform Vancouver housing policies.\nPart B is a good and simple ask:\n\nTHAT Council direct staff to clarify whether the Vancouver Housing Strategy targets refer to net housing completions or gross housing completions.\n\nPart E is mostly redundant:\n\nTHAT Council direct staff to provide detailed inventory data through the Open Data Portal4 of housing starts, development projects anticipated in the pipeline (including form and type of units), and existing zoned capacity (disaggregated by local area) to inform this work.\n\nThe open data portal already has detailed information on housing units in the pipeline. The information could be improved, but this ask is useless unless it specified how. As mentioned before, detailed information on housing starts is already easily available as open data, monthly stats by structural type and intended market, down to the census tract level. It is less helpful than the other parts above and risks directing staff resources away from other project just to replicate what’s already out there."
  },
  {
    "objectID": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#bottom-line",
    "href": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#bottom-line",
    "title": "Projections and self-fulfilling prophecies",
    "section": "Bottom line",
    "text": "Bottom line\nThere’s no way around it. How many dwellings to allow in a city is ultimately a moral question rather than a technocratic one. Given the overwhelming evidence that people want to live in places like Vancouver, population forecasts necessarily reflect first and foremost how many new dwellings we’re willing to allow. In technical terms, it’s silly to imagine we’re meeting the needs of population growth when we’re in fact setting a hard cap on population growth. In moral terms, we come back to the central question: Are we planning for kicking poor people out? Or are we open to inviting more people in?\nAs usual, the code underlying the stats and graphs is available on GitHub for anyone to reproduce or appropriate for their own use. And if you want to read (much) more about how to know if you have enough housing, check our simple metrics post."
  },
  {
    "objectID": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#footnotes",
    "href": "posts/2020-05-25-projections-and-self-fulfilling-prophecies/index.html#footnotes",
    "title": "Projections and self-fulfilling prophecies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example the City of of Vancouver only allows at most one kitchen per dwelling unit and limits the number of unrelated individuals sharing a dwelling to 3 (+ 2 boarders or lodgers) to restrict sharing of homes.↩︎"
  },
  {
    "objectID": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html",
    "href": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html",
    "title": "COVID-19 data in Canada, effective reproduction rates",
    "section": "",
    "text": "We have written about the situation of covid-19 data in Canada previously, and the need for good data is becoming more pressing as we are poised to slowly open up some of our restrictions and need to closely monitor how the spread of COVID-19 is responding.\nA key number to watch is the effective reproduction rate, the average number of people an infected person passes on the virus ($R_0$). Our collective social distancing has led to the effective reproduction rate to drop below 1, so the spread of the virus is receding. But social distancing also has adverse effects. For some it is merely an inconvenience, for others it has meant serious hardship.\nNow that case numbers have come down in some parts of Canada we are starting to consider loosening some of the social distancing restrictions. The key is to find the right balance of undoing restrictions that cause the most serious hardships, while at the same time keeping the effective reproduction rate at or below 1. If the reproduction rate creeps up above 1 we will find ourselves in a cycle where we have to again tighten social distancing recommendations to beat the case number back down.\nNext to social distancing there is another method to suppress the effective reproduction rate: Rigorous contact tracing. Rigorous contact tracing means that whenever we confirm a new COVID-19 case, we determine all “close contacts” during the time they may have been infectious and quarantine all “close contacts” for about two weeks. If we are fast enough in detecting cases and tracing contacts, the contacts won’t be infectious before they are quarantined and we stop the growth of the infection. To be practical, contact tracing requires a low case number to start with, and British Columbia’s new case numbers are definitely low enough now for rigorous contact tracing to be effective. But life is messy and contact tracing will be leaky, but it will help reduce the spread and push the effective reproduction rate down."
  },
  {
    "objectID": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html#estimating-the-effective-reproduction-rate",
    "href": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html#estimating-the-effective-reproduction-rate",
    "title": "COVID-19 data in Canada, effective reproduction rates",
    "section": "Estimating the effective reproduction rate",
    "text": "Estimating the effective reproduction rate\nThere are a number of great open analysis packages that can be used for modelling COVID-19 data. But modelling accuracy will suffer as data quality declines. Before we can model we need to talk about dates related to the cases. There are several dates that are important for this.\n\nDate of infection\nDate of onset of symptoms\nDate of testing\nDate test result is reported to Health Authority\nDate test result is reported to the public\n\nFor modelling, the date of infection is the most important one. But it’s also the one that we have the least information on. In some cases we will be able to pinpoint the exact time of infection, in many others we won’t. Onset of symptoms is a bit easier to get, although it will be missing at least for asymptomatic cases. Assuming our testing protocol is set up to find any, in which case we generally substitute the date of testing in for this. The distribution of time between infection and onset of symptoms is fairly well understood by now. It will introduce some noise in our modelling, but we can manage that well in our modelling.\nThe case reporting dates are secondary for modelling. They have value because they relate to important contextual information like changes in testing protocol, and help understand reporting lag. Yet they are the most ubiquitously used dates because more data with more relevant dates often isn’t available.\nOn top of that, we need to distinguish different kinds of exposure to COVID-19. If someone gets infected elsewhere and travels to a different region, gets symptomatic and the case gets confirmed, that person should only count as a source of infection, but not as a newly infected case. Having information on the type of exposure will make the model more robust.\nTo start off, let’s take a look at the data by exposure and date of onset of symptoms for the cases that Ontario reported on yesterday.\n\nThis highlights one of the key problems when modelling effective reproduction rates, we are reporting on cases now while symptoms started a month ago. Thus the modelled effective reproduction rate for the time one or two weeks ago will change when new case data gets released that had an onset of symptoms during that time period.\nThe good news is that the bulk of cases publicly reported on May 13th comes in within the previous 8 or 9 days, modelled effective reproduction rates are likely quite stable before that. Understanding the distribution of these lags between symptom onset dates and reporting dates can help us make guesses about the how the overall distribution of cases by onset date is likely to change in the coming days, and sampling from that distribution can inform robust confidence intervals for bringing the modelling up to just a couple of days prior to the reporting date.\nUnfortunately that requires some extra work, Ontario does not publish a clean dataset with the required dates. At this point, linking the reporting date with onset of symptoms date requires to cache all successive data releases. For the above graph, we took the difference of released data on two consecutive days (delaying the publication of this post by one day). Hopefully Ontario will release a clean dataset in the future to support modelling instead of asking researchers to do the extra work of caching historical release data.\nUnderstanding how these days relate begs the question how modelling the effective reproduction rate depends on the type of dates we use for modelling, and also how accounting for travel exposure affects the modelling. For this purpose, we count Travel-related exposure as imported cases and all others as local transmissions. For modelling, we are using the EpiNow package that we found to be quite good.\nBefore we dive into modelling, we need to understand the value of contextual information that goes beyond just case counts. What we are really interested in is the spread of the virus, case counts is just a proxy for that. But generally we only discover and confirm a fraction of actual COVID-19 cases. If that fraction stays constant over time, then the effective reproduction rate based on case counts will mirror the effective reproduction rate of all cases, discovered and confirmed or not. But the fraction of confirmed cases will change over time, in particular when we change our testing protocol. Incorporating this into the model goes beyond the scope of this post, but this is something to keep in mind when interpreting the model results.\n\nWe see that model fit is best when using the episode onset date and accounting for travel exposure. Travel exposure mattered more in the beginning period and less later on when local transmissions started to dominate. We cut off the effective reproduction rate for graphs based on the onset of symptoms date for the most recent 8 days where we lack confidence in the accuracy due to reporting lag as explained above.\nTo understand the effect of social distancing measures, recall that Ontario ordered schools closures on March 12th, Canada announced international travel restrictions March 16, and Ontario started shutting down on March 17th. We do see a drop in the reproduction rate starting around March 18th when using the symptom onset date, which is consistent with our general understanding of the distribution of the incubation period centred around 4 to 5 days. What is interesting is that the modelled effective reproduction rate was already quite low at the beginning of March, suggesting that many already adapted their behaviour before the officially announced social distancing measures. (The higher modelled effective reproduction rate on the first couple of days is a product of the Bayesian prior of a reproduction rate of 2.6 used to kickstart the model.)\nThe bump in the modelled effective reproduction rate at the end of March could be people infected by infected travellers returning from March 16-20 break.\nIt seems starting around April 12th the social distancing measures in Ontario finally became effective enough to push the effective reproduction rate below 1.\nWhen comparing to the last graph that is based on reporting date we notice about a 6-day lag, which is consistent with our understanding of the distribution of onset date by reporting date.\nWe should be careful not to over-interpret the data without also taking other contextual information into account, in particular when and how testing protocol changed.\nWe can also run the same model on BC data, although unfortunately BC does not make onset of symptoms data available in digital form.\n\nThese two graphs are of the same type as the last Ontario graph, they are based on the date of public release, not the date of the onset of symptoms. We see larger uncertainties which is driven by the overall lower case numbers. No complaints about that! Using the date of when the case was reported to the Health Authority gives a slightly better fit than using dates when cases were reported to the public, which is as expected.\nSocial distancing measures got introduced in BC starting March 12th and became successively more stringent in the week following. It is hard to see where exactly that is reflected in the data by reporting date, taking hints from the Ontario data this may be the cause of the drop in effective reproduction rate by reporting date starting around March 25th. The temporary increase April 21th through 25th is likely due to the change in testing protocol introduced at that time. (Changes in testing protocol tie in nicely with reporting date.)\nEspecially given the larger uncertainties due to lower overall case counts, it would be great if we had data by onset of symptoms date easily available. Having better data will drive down model uncertainty as we have seen in the Ontario data. Alas, BC only publishes this data in image form in their Situation Reports, and it’s too much work to scrape it all out for the purpose of this post."
  },
  {
    "objectID": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html#lag",
    "href": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html#lag",
    "title": "COVID-19 data in Canada, effective reproduction rates",
    "section": "Lag",
    "text": "Lag\nThe lag in reporting is a real impediment in getting robust timely estimates for how we are doing. As things stand, we are looking at roughly a 10-day lag from change in behaviour (due to change in social distancing measures) and the result showing up in our estimates. If we open up and cases start to climb again, we will have two generations of COVID-19 spread before we see the results and can re-adjust.\nWe can’t do anything about the lag from infection to onset of symptoms, but there is little reason for the delay between onset of symptoms to reporting to be 6 days. If we are serious about getting timely actionable data, we need to set up processes that people with symptoms can get tested (and isolated) fast with fast reporting. This lag should get cut down so that we aren’t flying in the dark for such a long period."
  },
  {
    "objectID": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html#what-data-do-we-need",
    "href": "posts/2020-05-13-covid-19-data-in-canada-effective-reproduction-rates/index.html#what-data-do-we-need",
    "title": "COVID-19 data in Canada, effective reproduction rates",
    "section": "What data do we need",
    "text": "What data do we need\nThe bottom line for this post is that we need better data. The minimal dataset that can give us decent estimates has: * date of onset of symptoms * type of exposure (travel vs local) * Health Region geographic granularity * date of reporting * number of cases for each combination of the other variables\nThe date of reporting is not essential for the model, but it gives an indication of how to impute cases with a recent onset date that we still haven’t discovered and thus help drive down the lag. This is important as we want to reduce the lag of our modelled effective reproduction rate as much as possible.\nAnd we need this data for smaller geographies. Having it for Health Regions, like Ontario does, is important because the spread is mostly local, especially with current travel restrictions in place. British Columbia unfortunately does not publish data by Health Region, but only by much coarser Health Authority geography. Data for finer geographic regions will allow a more tailored approach to loosening social distancing restrictions, for example Germany has tied the degree of loosening of social distancing measures to local case growth rates. Here are the effective reproduction rate estimates by onset of symptoms date and accounting for travel exposure for the 5 most affected health regions in Ontario.\n\nOntario publishes this data on their open data portal, although the case report date needs to be obtained by caching daily data and taking differences (as we did for the first graph in this post). BC publishes that data in their daily Situation Reports, but for some reason they don’t make this data publicly available in tabular form. They do make it available to researchers after signing an NDA. Typically one only goes through the trouble of involving an NDA if the data can’t be released publicly over privacy concerns, but that clearly isn’t the case as the data is already released in the Situation Reports, albeit in an inconvenient form.\nNDAs are necessary in some cases, but they amount to tall barriers to research progress. It takes time and effort to put an NDA into place, researchers need to spend time to take extra measures to safeguard the data, and most importantly, research gets siloed as data can’t be shared across research groups, imposing a significant drag on collaboration and research progress.\nBritish Columbia, and other provinces that have not done so, need to release the data in digital form. Ideally the data for all regions across Canada is structured and disseminated in a uniform way to avoid replication of analysis and facilitate a coordinated response. Unnecessarily suppressing data used in decision making goes against one of the fundamental tenants of transparent governing. Whenever possible, analysis that informs public policy decisions should be based on open data and open analysis.\nAs usual, the code for this post is available on GitHub for anyone to reproduce the post, play with the data, or even better, improve on the modelling."
  },
  {
    "objectID": "posts/2020-04-19-covid-deaths-in-context-by-weeks/index.html",
    "href": "posts/2020-04-19-covid-deaths-in-context-by-weeks/index.html",
    "title": "COVID deaths in context by weeks",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\n\nIn our previous post on COVID mortality in context, we tried to place COVID deaths, as recorded so far this year, in the context of expected deaths from previous years. There have been a lot more developments since that post. And unfortunately a lot more deaths too.\nHere we’re providing an update to our previous post, but also expanding on that post by talking a bit more about new mortality analyses and the progression of outbreaks in terms of expected deaths on a weekly basis. First, an update! We previously placed COVID deaths in the context of expected deaths at the national level, starting after the 20th death was recorded. What does that look like now?\n\n As visible in the mortality data, Belgium has moved to the forefront of the COVID outbreak in Europe in terms of COVID deaths relative to expected deaths from years prior. Ireland, the UK, and the US appear to continue to climb. By contrast, Spain and Italy, early centres of the outbreak in Europe, have largely levelled off. Though the USA “leads” in deaths from COVID-19, this doesn’t (yet) show up in the relationship between COVID deaths and expected deaths because the USA is enormous, with a lot of expected deaths every year, and the outbreaks of COVID deaths have been heavily concentrated in a few locales so far. Even so, as the NYTimes suggests, COVID is now arguably the leading cause of death in the USA, ahead of heart disease and cancer.\nOverall, and as mentioned previously, there’s still a lot we don’t know in these comparisons. For instance, we don’t know if we’re actually counting all of the deaths due to COVID. Lots of people don’t get tested, and cause of death is always tricky to determine in the best of times, let alone with an overloaded medical system and coroners’ offices. As a result, revisions to the data can add dramatically to the death toll, as happened recently in New York City. In addition to good COVID death data, we’d also like updated data on mortality overall. We’ve seen recent – and very preliminary – data out of NYC and scattered other locales suggesting that all-cause mortality has risen dramatically in places with severe COVID outbreaks.\nWhere we have it, we can put updated all-cause mortality in conjunction with COVID mortality and expected mortality all together. Putting this on a weekly basis really provides a sense of the progression of outbreaks and how overloaded they leave medical systems in terms of the normal deaths they have to deal with. Given some of the data from NYC, here’s roughly what that looks like.\n\nWe notice a downturn in deaths as recorded by the CDC FluView for the last week they report data (the week ending on 2020-04-12). This is not a REAL downturn. Rather it illustrates the reporting lag for data on deaths. It can take several weeks for the numbers to fill in and stabilize. We added the reported Covid-19 related deaths as assembled by the JHU for reference. JHU data was aggregated up the week ending 2020-04-19, so it’s nominally a week ahead of the FluView data. However, these deaths are coded by date reported, unlike the CDC data that is coded by date of death, which causes the JHU data to lead a bit. Even accounting for a possible time shift in JHU data, it appears that JHU data does not account for the full increase in all-case mortality, hinting at likely under-reporting of Covid-19 deaths in the JHU data.\nUnfortunately we still don’t have updated all-cause mortality on the country level. As suggested by the lag in NYC data, it takes awhile to compile in the best of times (here’s a look at efforts to gather some of the European data). So here we’ll provide a replication of our previous analysis, but breaking out COVID deaths against expected deaths on a weekly basis for countries instead of across the entire length of the outbreak.\n\n\nOverall, weekly COVID deaths as a percentage of expected deaths looks broadly similar to our earlier figure, which charted the rise in COVID deaths as a percentage of expected deaths since outbreak deaths began. But there are a few significant differences. The weekly chart better highlights the evolving overload on hospitals and health systems, as well as coroners’ offices, and this is reflected in the y-axis, demonstrating that COVID deaths in Belgium have more than doubled the expected deaths in the last week for which we have data. The weekly chart also more quickly identifies declines in the relative impact of COVID deaths in places where the worst of the outbreak has passed, like Spain, Italy, and France. It will take a long time for the expected death toll to diminish the impact of the overall death toll of COVID in our figures at the top of the post. But on a weekly basis, we can already see the toll of COVID receding in many places.\nAs we’ve noted previously, it will still take a long time to sort out the overall effects of COVID on mortality. Why? Well, we’re still nowhere near done with the outbreak, and we can expect deaths to continue until we have a vaccine and have reached some level of “herd immunity.” But we’ll also be sorting through the mortality data for years to come. Also important: the toll at national levels, while helpful in assessing cross-national differences, masks the impact at local levels where outbreaks often occur. So it is that the estimate from Belgium, where most recent weekly COVID deaths appear to have more than doubled expected mortality, is dwarfed by the estimate from New York City, where the most recent weekly COVID deaths appear to be more than six times the expected (pre-COVID) mortality.\nAs usual, the code for the post is available on GitHub in case anyone wants to refine or adapt it for their own purposes.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2020,\n  author = {von Bergmann, Jens and Lauster, Nathan},\n  title = {COVID Deaths in Context by Weeks},\n  date = {2020-04-19},\n  url = {https://doodles.mountainmath.ca/posts/2020-04-19-covid-deaths-in-context-by-weeks},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von, and Nathan Lauster. 2020. “COVID Deaths in\nContext by Weeks.” MountanDoodles (blog). April 19,\n2020. https://doodles.mountainmath.ca/posts/2020-04-19-covid-deaths-in-context-by-weeks."
  },
  {
    "objectID": "posts/2020-03-31-context-for-covid-19-mortality-so-far/index.html",
    "href": "posts/2020-03-31-context-for-covid-19-mortality-so-far/index.html",
    "title": "Context for Covid-19 mortality so far",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nUnfortunately, more and more people are dying due to COVID-19. We won’t know the full toll from COVID-19 for quite some time. But we can at least start to get a sense of its impact. One useful way of assessing the impact, of course, is just to plot deaths attributed to COVID-19. This highlights the real loss of human lives associated with outbreaks. But as any demographer can tell you, deaths are a normal part of life. Within a given population, we can reliably expect a certain number of deaths to occur over any given time period. So another way of visualizing COVID-19 deaths is also useful: How many deaths attributed to COVID-19 are occurring as compared to the deaths we would normally expect to occur?\nBelow we follow the rise in deaths attributed to COVID-19 through time relative to the expected number of deaths that likely would have occurred without COVID-19 during the same time.\nThis visualization places deaths reported from COVID-19 in the context of expected deaths overall. This helps establish where we know the mortality toll has already been enormous. As of March 31, the end-point of the animation, Italy leads the overall count in deaths attributed to COVID-19. Here we can also report that in just over a month, Italy’s deaths so far attributed to COVID-19 already add more than 20% to its expected deaths. But Spain’s toll relative to its expected number of deaths is ever higher. In just over three weeks time, we can see that COVID-19 already accounts for more than a 30% rise over the deaths that would’ve been expected without COVID-19.\nUnfortunately, most curves are still rising. So far. Initially curves grow exponentially, until aggressive containment or mitigation strategies flatten them. Curves that stabilize and flatten, or even begin to turn downward, reflect countries where deaths attributed to COVID-19 are being overtaken by deaths that might’ve been expected to occur anyway. Hopefully this reflects an outbreak coming increasingly under control - GOOD NEWS - rather than a data gap.\nBut the possibility for data gaps is very real. It will be quite awhile before we can properly estimate the overall toll from COVID-19. We already have preliminary data on deaths attributed to COVID-19 rolling in. But this data will be messy, excluding cases where COVID-19 was missed as a cause, despite being present, and possibly over-including cases where the cause was actually not COVID-19 (e.g. instead common influenza), or COVID-19 was present but the death should be attributed primarily to a different underlying condition claiming the life. Cause of death data is never clean to begin with. As COVID-19 overwhelms medical systems and coroners’ offices, we should fully expect that data quality will suffer further.\nMore concretely, COVID-19 deaths will show up in the mortality databases with code U07.1 or U07.2 in the current ICD-10 classification system (or RA01.0 and RA01.1 once ICD-11 comes into effect). But many will likely also get classifed as J11, J18 or J22. When the dust settles, we will have to check how these cases have evolved over time and estimate how many cases in 2020 (or late 2019 in the case of China) are likely misclassified COVID-19 cases.\nWe will also eventually get data about overall mortality. We will likely see deaths increase beyond those attributed directly to COVID-19. Deaths will rise both in response to complications introduced by COVID-19 in those with pre-existing conditions, and in response to people dying due to failure of overloaded medical systems to be able to respond to non COVID-19 cases they way they normally would. At the same time, some other non-COVID deaths may go down. This can happen when COVID-19 claims lives that otherwise might’ve been claimed by something else (e.g. an underlying condition). But it can also relate to deaths that don’t occur due to lockdown and the measures related to dealing with COVID-19. For instance, the regular toll of influenza may diminish in response to the lockdown targeted at Coronavirus (making it unclear what the “expected” baseline case count for 2020 should be). Similarly, fewer cars on the road will likely result in fewer deaths from car accidents. For references, see the most common causes of death in Canada in normal years here. A similar discussion of the eventual breakdown we’ll need in mortality data can be found in this demographer thread attempting to summarize some of this complexity via twitter feed.\nThe mortality data coming in bears watching, both in terms of COVID-19 attributed deaths and deaths overall. Some analysts (e.g. in Italy and Spain) as well as some China skeptics, are already drawing upon anecdotal mortality data to suggest that the toll from COVID-19 is far greater than revealed in the official data so far. These kinds of analyses are especially potent when applied to cities and regions as opposed to countries. But ultimately it will take years for demographers to sort this all out. In the meantime, we can at least get a rolling sense of COVID-19’s toll by looking at deaths attributed to Coronavirus relative to deaths otherwise expected based on past data from the same rough period of time.\nAs usual, the code for the post is available on GitHub in case anyone wants to refine or adapt it for their own purposes."
  },
  {
    "objectID": "posts/2020-03-31-context-for-covid-19-mortality-so-far/index.html#update-2020-04-06",
    "href": "posts/2020-03-31-context-for-covid-19-mortality-so-far/index.html#update-2020-04-06",
    "title": "Context for Covid-19 mortality so far",
    "section": "Update (2020-04-06)",
    "text": "Update (2020-04-06)\nIt’s been a week since we posted this, and things are changing fast with covid-19 related deaths increasing exponentially and background mortality estimates only increasing linearly with time. The traces in the animated GIF already highlight this, but here is a quick update of what the graph looks like using data from a week later.\n\nAnd for completeness, here is the static graph with the latest availabel numbers."
  },
  {
    "objectID": "posts/2020-03-09-knock-knock-anybody-home/index.html",
    "href": "posts/2020-03-09-knock-knock-anybody-home/index.html",
    "title": "Knock Knock, Anybody Home?",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nEmpty homes are in the news again in West Vancouver after a West Vancouver council motion asking the province for the power to levy their own Speculation and Vacancy tax.\nWest Vancouver seems interested in the empty homes and not the satellite family component of the SVT, which may well be a wise choice given how messy and problematic a law defined based on spousal relationship can get.\nThe motion is interesting for several reasons, not just because of the focus on vacancy vs satellite families. It sets the stage by naming housing affordability as a key challenge.\nAs evidence the motion rightly points at the low rental vacancy rate. The ownership metric is curious though as it explicitly focuses on “houses”, excluding more affordable multi-family units from consideration. This is likely no accident, as West Vancouver has a solid track record of focusing their energy on the most expensive type of housing by permitting fewer multi-family homes than more expensive single-detached houses to be built, the latter of which often just replace older single-detached homes and do not add to the dwelling stock.\nThe next part reads:\nThis is incorrect, the 2016 census enumerated 1,525 unoccupied dwelling units in West Vancouver, comprising 8.2% of the total dwelling stock. Council is only partially to blame for this misstatement, reporting on this census metric has generally been sub-optimal, to say it politely. The problem is not just about getting the number right, but more importantly understanding what the numbers mean. The census enumerates homes that are empty on census day, and homes can be empty for several reasons. Some of which are mundane and even desirable, just one “whereas” ago it looked like council wanted more unoccupied homes – that are available for rent. There are other categories of unoccupied homes that are important in enabling residential mobility, homes that are rented but not moved in yet, homes that are for sale and unoccupied or bought and not moved in yet. The US ACS tries to track down reasons why homes are unoccupied, it can be instructional to use that as base of comparison when looking at Canadian data as in the following graph based on some of our past joint work.\nBeing unoccupied on a particular day, for example Census day, does not give direct information about homes that might be targeted by an empty homes tax. The list of exemptions in Vancouver’s Empty Homes Tax or the provincial Speculation and Vacancy Tax opens another window into reasons why homes may be empty.\nWe can further break down the unoccupied homes the census found in West Vancouver by structural type.\nIn West Vancouver, most homes registering as unoccupied are single family homes, followed by units in suited single family homes that the census refers to as “Apartment or flat in a duplex”. This is to a large degree due to the building stock that leans heavily on single-detached homes. The two dwelling types have also been responsible for most of the growth in homes classified as unoccupied in the census.\nIt is helpful to also look at shares of homes in each type that registered as unoccupied, and put in context with the Metro Vancouver shares.\nThe shares of unoccupied homes are generally higher in West Vancouver, with the exception of row houses and highrise apartments. The shift in row houses is fairly recent, and should probably not be over-interepreted because of the small overall number of row homes. The difference in rates of unoccupied highrises likely stems from a relatively high share of rental highrises in West Vancouver.\nThe high share of unoccupied “duplex” units stands out. Recall that in Metro Vancouver units classified as “duplex” by the census are mostly suited single family homes. These register with the highest share of unoccupied homes throughout Vancouver, which is driven by empty secondary suites in such houses. Incidentally, secondary suites are exempt from both the City of Vancouver Empty Homes Tax and the provincial SVT.\nIn all of this it is important to remember that census unoccupied counts were taken back in 2016, before these taxes came into effect, and some owners will likely have changed their behaviour because of the tax and rented out or sold their previously empty home. Indeed, we now have a much more recent and much better defined dataset predicting how many problem empties are likely to be taxed by an Empty Homes Tax in West Vancouver. That dataset comes from the Speculation and Vacancy Tax itself. Worth noting: we are still in the pre-audit phase for the SVT and it is not clear how many owners are trying to dodge the tax by declaring incorrectly. But setting aside Satellite Families (where homes aren’t empty), the SVT numbers for the City of Vancouver aren’t very different from the City of Vancouver Empty Homes Tax numbers, where we are now in the third year and already have two years of complete declarations and audit cycles. So far so good.\nBottom line is that a much more reasonable expectation of the number of homes that may be targeted by a West Vancouver empty homes tax at this point is around 221, the number of vacant homes paying the SVT.\nThe next two whereas speak to revenue expectations.\nThe $6.6 million cited as being collected from West Vancouver covers both, vacant homes and homes occupied by satellite families. Only $4.1 million was collected for vacant homes in West Vancouver. The comparison the the City of Vancouver tax is somewhat irrelevant to this discussion, other than stressing again that revenue expectations is an important driver of this motion. One should note here too that the tax rate West Vancouver could charge for vacant homes is limited by a very simple calculus. Once the combined tax rate of municipal and SVT vacancy taxes exceeds the property transfer tax, owners can trigger a sale to e.g. a relative in order to pay the lower property transfer tax and be exempted from the vacancy taxes, with all the revenue accruing to the province. The City of Vancouver has hiked their Empty Homes Tax rate and is slowly approaching this limit."
  },
  {
    "objectID": "posts/2020-03-09-knock-knock-anybody-home/index.html#upshot",
    "href": "posts/2020-03-09-knock-knock-anybody-home/index.html#upshot",
    "title": "Knock Knock, Anybody Home?",
    "section": "Upshot",
    "text": "Upshot\nAn Empty Homes Tax can be useful. It incentivizes better use of property by returning some unproductive properties back into the rental or ownership market. It generates revenue in case people are unwilling to rent out their mostly unoccupied home.\nBut it also comes at a cost, it can be intrusive and there are always edge cases. And it takes a sustained effort to administer fairly.\nWe believe that in the case of the Vancouver region the benefits generally outweigh the costs at this time. We can imagine that we might come to a different conclusion if e.g. the rental vacancy rate climbed up above 3%, but we don’t see a medium-term path leading to that.\nLooking back at the City of Vancouver’s experience it seems prudent to approach an Empty Homes Tax with realistic expectations. In the City of Vancouver our Former Mayor said that the tax could free up as many as 25,000 empty units for rent, an unfortunate statement that raised expectations unreasonably high and is still being brought up when people criticize City staff for their EHT numbers not measuring up to lofty promises\nThe bottom line is that clear and realistic expectations are an important part of a successful implementation. It is good politics, and City staff will thank their politicians for this.\nAs usual, the code for the analysis is available on GitHub."
  },
  {
    "objectID": "posts/2020-02-19-wealth-vs-income/index.html",
    "href": "posts/2020-02-19-wealth-vs-income/index.html",
    "title": "Wealth vs income",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWealth and income are different things. Wealth is measured in terms of assets minus debts at any given point in time. It can accumulate or deplete over a lifetime and across generations. By contrast, income represents some variation of how much money one makes over a given time period (usually a year). Most people get this on some level. But since both income and wealth deal with people and their money, the terms are also often used interchangeably. So it was that the CBC yesterday reported that “B.C. budget 2020 promises new tax on wealthy to help ensure future surpluses” despite the actual new tax being a tax on high-income individuals.\nHere the difference matters for two reasons:\nWith wealth taxes in the news (and in multiple Democrats’ platforms in the US), it’s important to separate out wealth taxes from income taxes. Here in Vancouver, as we’ve noted before, our property taxes actually do a pretty good job of taxing wealth.1\nIn this post we’ll focus on our first point: just how well do wealth and income line up together? Underneath this is also the question of how to measure wealth and what to include as income, we will just go with the standard definitions from StatCan’s Survey of Financial Security to answer this question for family net wealth and family income. The data allows us to divide up the Canadian population into equally sized quintiles (fifths) by net wealth and by income. What overlap do we see? The data also allows us to break out sub-areas of Canada, including the Atlantic provinces, Quebec, Ontario, the Prairie provinces, and British Columbia. So let’s run those too!\nFirst let’s look at how income quintiles break down by wealth quintiles, as assessed all across Canada. How many families in the lowest income bracket fit into each wealth bracket? Are they all the lowest wealth bracket? Nope.\nWe can see a clear relationship between wealth and income. But only about half of lowest income families in Canada fit into the lowest wealth category. The same is true on the other side of the distribution. Only about half of the highest income families fit into the wealthiest category. Moreover, there are wealthy (highest quintile) and poor (lowest quintile) households in each and every income quintile. Counter-intuitive as it may seem, there are clearly poor high income folks and wealthy low income folks. Not very many, but at any given point in time they definitely exist.\nLet’s look at some of the provincial differences, remembering that we’re using Canada-wide quintiles. Looking at raw numbers, it’s quickly evident that some provinces (Quebec and Atlantic Canada) are disproportionately lower-income, while others (the Prairie provinces) tend toward higher income. Ontario and BC are more inbetween. Looking at what percentage of each income quintile fit in each wealth quintile by province, the general pattern of a correlation between wealth and income is evident in all provinces. But looking more carefully, a few differences jump out, especially between BC and the Prairies. In BC, each income quintile has a higher proportion of families in the top wealth quintile than one might expect - including the lowest income quintile: wealthy low income folks. In the Prairies, by contrast, each income quintile looks less wealthy than one might expect. In each case, despite the correlation between wealth and income, there are also people showing up in each category.\nFlipping the chart around, we can look at how many families in the highest wealth bracket fit into each income bracket. Only about half of the wealthiest families in Canada are in the highest income quintile. There’s even greater diversity in BC, where only about 40% of the wealthiest are in the highest income quintile.\nLet’s pull out BC from the rest of Canada and run the numbers matrix style. If there were a perfect correlation between income quintile and wealth quintile, then we’d see a bright diagonal line filled with 20% of families in each of the five diagonal cells, surrounded by twenty cells with 0% of families. If there were NO relationship between income quintile and wealth quintile, we’d see each of our twenty-five cells filled with roughly 4% of families. What we see is somewhere inbetween. For Canada as a whole, we see strong evidence of correlation at the margins (for highest and lowest quintiles), but the middle looks very mushy. For BC, we see a strong relationship between being in the top income quintile and the top wealth quintile. But everything else looks mushier than expected. In effect, BC stands out for its generally limited correspondence between wealth and income.\nWhat throws off the relationship? Many peoples’ wealth represents savings over one or more lifetimes. So age matters, as does inheritance. Immigration can also affect patterns, with different results evidenced by program (e.g. investor), time in Canada, and wealth accrued in country of origin (Vancouver’s far from the only place where rapid escalation in prices have made millionaires of home owners). Asset inflation also matters, and BC’s rapid appreciation in real estate wealth surely plays a role in its weirdness. As a reminder, capital gains accruing to primary residence don’t show up in income statistics, but they definitely represent wealth. We could cap current exemptions on this enormous tax break for home owners, taxing these capital gains more like income. But we could also just levy an overall wealth tax. Returning to a theme, taxing wealth is distinct from taxing income.\nAll of which is to say: wealth and income are not the same thing. And it matters. Especially in BC!\nAs usual, the code for the analysis is available on GitHub."
  },
  {
    "objectID": "posts/2020-02-19-wealth-vs-income/index.html#footnotes",
    "href": "posts/2020-02-19-wealth-vs-income/index.html#footnotes",
    "title": "Wealth vs income",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd our property taxes are still too low!↩︎"
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html",
    "href": "posts/2020-01-27-mythical-oversupply/index.html",
    "title": "Mythical oversupply",
    "section": "",
    "text": "It’s been over two years now since the news media reported on John Rose claiming that Vancouver has a surplus of housing and Rose shared his Working Paper, Version 1 detailing his claims of some mythical oversupply of housing in Vancouver.\nWe have written about this on several occasions, but we were missing a piece of data that can greatly simplify our arguments: Cross-tabulations of structural type by document type (whether a dwelling was occupied by usual residents, or occupied by temporarily present persons, or unoccupied) for the censuses 2001-2016. Recently we needed this for a different project, and were able to make it openly available thanks to CMHC, so here is a supposed-to-be-quick-but-turned-out-lengthy post to on what that means for the “supply myth”.\nThe main claim of Rose’s working paper is that Vancouver has a ‘surplus’ of dwelling units, and that this ‘surplus’ must be attributable to “domestic or foreign speculators, visiting students, temporary workers, or those owning a second home in Canadian cities”. In Table 3 of his report we see that this surplus was almost entirely created between 2001 and 2006 when the share of ‘surplus’ units jumped from 3.5% to 6.2%, while increasing a bit more to 6.5% in 2016."
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html#tldr",
    "href": "posts/2020-01-27-mythical-oversupply/index.html#tldr",
    "title": "Mythical oversupply",
    "section": "TL;DR",
    "text": "TL;DR\nNo, you weren’t hallucinating when everyone was talking about how Vancouver was swimming in ‘surplus’ housing but you felt that there were few available apartments and lots of competition when looking for a new rental. There is no mythical oversupply.\nYes, census numbers indicate there was an uptick in the share of dwellings “not occupied by usual residents” 2001-2016, but at least half of that was due to a change in census methods and goes away when re-calibrating to adjust for that change. And the rest is concentrated in the 2001-2006 timeframe, tying this to “speculation” and the 2015-ish price increases is more than a stretch.\nBasement suites are at the root of this mixup, they are the most empty form of dwelling according to the census, they are the Schrödinger’s cat of dwelling units. Lots of people live in basements, and lots of basements don’t have people living in them or do have people living in them that we, apparently, don’t want to count."
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html#the-long-version",
    "href": "posts/2020-01-27-mythical-oversupply/index.html#the-long-version",
    "title": "Mythical oversupply",
    "section": "The long version",
    "text": "The long version\nRose defines ‘surplus’ dwelling units as units that are either unoccupied, or units that are occupied by temporarily present persons that think of their primary residence being elsewhere. We struggle with the ‘surplus’ terminology and will keep it in quotation marks throughout this post.\nUnfortunately, contrary to Rose’s claim that his “relative supply of housing units” is “based on data collected and represented consistently by Statistics Canada across its censuses”, it is largely due to a change in StatCan methodology between the 2001 and 2006 censuses, with ripple effects for the subsequent censuses. The census dictionary explains:\n\nThe Structural type of dwelling variable is collected by trained enumerators. Improvements to the enumeration process have resulted in a better identification of hard-to-find dwellings such as basement apartments. As a result, structures that may have been classified in previous censuses as single‑detached houses because there was no outside sign of an apartment are more likely be classified as apartments – either in a duplex or a building that has fewer than five storeys, as appropriate.\n\nThis had consequences on how buildings got classified:\n\nComparisons of structural type of dwelling data for Canada between the 2001 and 2006 censuses show a decrease in share for ‘single-detached house’ (-2.1%), an increase in share for ‘apartment or flat in a duplex’ (+1.8%) and an increase in share for ‘apartment in a building that has fewer than five storeys’ (+0.4%).\n\nIn the Vancouver setting, this means that SFH that have been identified as “single-detached” in 2001 may have been classified as “duplex” in 2006 if the enumerator detected the presence of a secondary suite, or as “apartment, fewer than 5 storeys” if the enumerator detected more than one secondary suite. The City of Vancouver formally legalizing basement suites city wide in 2004 likely aided the 2006 census’ effort to discover suites.\nAt first glance it may seem that this only effects the classification of dwelling units, but it also results in an increase in the overall count of dwelling units when a single “single-detached” unit gets reclassified as two “duplex” units or more than two “apartment, fewer than five storeys” units. And this change in dwelling units was only due to re-classification, not to any change in the physical stock.\nAt the same time this will increase the number of households, a.k.a “occupied dwelling units” if the census finds households living in these units. But as we have pointed out previously, secondary suites are the most “empty” type of dwelling unit in Vancouver. And it will increase the number of “temporarily occupied” dwelling units, that is occupied units where the occupants think of their primary residence being elsewhere. For example, students in basement suites that are often counted at their parent’s residence in the census fall under this category. But for this post we will follow Rose and somewhat cynically treat these “temporarily occupied” units as ‘surplus’.\nWhile all of this has been explained in detail before, the arguments were not always as direct as they could have been since nobody, including Rose, bothered to pull a census cross tabulation to look at unoccupied dwellings by structural type for the 2001 and 2006 censuses. But now we got one."
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html#the-numbers",
    "href": "posts/2020-01-27-mythical-oversupply/index.html#the-numbers",
    "title": "Mythical oversupply",
    "section": "The numbers",
    "text": "The numbers\nTo start off we reproduce Rose’s ‘surplus’ dwelling units, also know as “dwellings not occupied by usual residents”. As mentioned above, this includes dwelling units occupied by students and other people that are counted in the census as having their primary residence elsewhere.\n\nThis illustrate the change in ‘surplus’ units being concentrated on the 2001-2006 timeframe, as well as highlighting the difference between unoccupied and dwelling units occupied by temporarily present persons. The drop in units occupied by temporarily present persons 2006-2011 is interesting, it may reflect yet another change in classification or it may be due to real phenomena like for example students increasingly living with parents or thinking of their current dwelling increasingly as a longer term residence.\nAnother quick reality check is to compare completions data to census net new dwelling counts between census periods.\n\nThis makes it clear that a good portion of the change in dwelling units as counted by the census in the 2001-2006 period is not due to new construction, but mostly due to reclassification of existing dwellings. Moreover, the other two periods show that completions don’t all translate into net new dwellings, the main difference being demolitions. This mismatch is especially acute in Vancouver, and is not quite captured by the difference in completions to net new census dwellings in the graph above as the later two inter-census periods also experienced some reclassifications.\nWe can independently estimate the ratio of demolitions to completions using permit data as we did in more detail when looking at single family teardowns.\n\nThe demolitions to completions ratio in Vancouver stands out like a middle finger extended toward the effort to create more housing. And this is not a recent phenomenon, but has been fairly consistent at least since 2007 as documented by CMHC.\nTo better understand the impact of the reclassification we take a look at the change in ‘surplus’ dwellings by structural type."
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html#structural-type-of-dwelling",
    "href": "posts/2020-01-27-mythical-oversupply/index.html#structural-type-of-dwelling",
    "title": "Mythical oversupply",
    "section": "Structural type of dwelling",
    "text": "Structural type of dwelling\nTo understand what happened 2001-2006 it is instructional to look at the change in the number of dwelling units by structural type between the censuses. This yields a mixture of changes due to new construction, demolitions and conversions, as well as changes due to the StatCan change in methods.\nIt’s important to understand how these structural types are defined, so here is a quick overview.\n\n\n\n\n\n\n\n\nLabel\nCensus name\nNotes\n\n\n\n\nSingle-detached\nOccupied by usual residents\nSFH without a suite, single unit only. A laneway house counts as a separate single-detached structure.\n\n\nSemi-detached\nOccupied by foreign residents or temporarily present persons\nA unit in a house with two side-by-side units, in Vancouver commonly referred to as “half-duplex”.\n\n\nRow/Townhouse\nUnoccupied\nA row or townhouse.\n\n\nDuplex\nSingle-detached house\nA unit in a house with two up-down units, in Vancouver usually a suited SFH (main unit + one suite).\n\n\nLowrise\nSingle-detached house – Occupied by usual residents\nA building with fewer than 5 storeys and 3 or more units. This includes an SFH with more than one suite.\n\n\nHighrise\nSingle-detached house – Occupied by foreign residents or temporarily present persons\nA residential building with 5 or more storeys.\n\n\nOther single\nSingle-detached house – Unoccupied\nThings like homes built as an extension of an existing building. Vancouver has very few of these.\n\n\nMovable\nSemi-detached house\nMobile home.\n\n\n\nThe change in methods had clearly observable consequences.\n\nThe 2001-2006 shows a sharp drop in dwelling units classified as single-detached, accompanied by a spike in duplex units, as well as an heightened increase in low-rise units. It is safe to say that the bulk of this is due to the change in methods. The 2011-2016 period again shows a drop in single detached units, together with an increase in duplex units, indicating possibly another change in the collection methods that lead to improvements in discovering secondary suites.\nTo validate these assumptions, we can compare the data on the net new units to the dwelling stock by period of construction. That still misses demolitions, but should still allow us to ballpark if our assumptions are correct. Another wrinkle is that the census only has the period of construction of the occupied (by usual residents) dwelling stock, a.k.a. households. Moreover, the date range for new occupied structures in the census start at the beginning of 2001 instead of after the 2001 census, adding more slight data mismatch.\n\nThe mismatch is stark and confirms our assumptions. New occupied Highrise and Row/Townhomes roughly match net new households for each period in these structure types. Duplex sees a noticeably higher share of net new households than occupied structures added especially in the first and also the last timeframe, pointing to reclassification as an important driver of the growth in dwelling stock of these types. In the 2001-2006 timeframe we also see significantly more net new households in lowrise than there are new lowrise units. At the same time, we see a strong loss of net new households in single-detached dwellings, while registering a significant increase in occupied newly-built single detached homes. The Metro Vancouver Housing Data Book registered roughly 11k single-detached demolitions in each of these periods, suggesting that about half of the new (occupied) single-detached homes are replacing teardowns with the rest adding to the overall stock. Some of the teardowns may have been “duplex” units since the demolition data did not distinguish between single-detached and “duplex” (suited single-detached) units.\nUsing this as a reference we estimate that roughly 82% of the net new duplex units and 39% of the net new lowrise units are old units that got reclassified in the 2001-2016 timeframe.\nWe can compare this to the change in ‘surplus’ dwelling units for each structural type.\n\nAs we add dwelling units we expect the number of ‘surplus’ units to increase too. The addition of 55,195 “duplex” dwelling units 2001-2006 has gone along with 9,750 ‘surplus’ dwelling units, which is an extraordinarily high ratio. As an aside, the noticeable increase in ‘surplus’ “single-detached” dwelling units despite substantial drop in overall single-detached dwelling units is interesting.\nSo how many of the new ‘surplus’ units are attributable to re-classification, and how many are due to other factors? Applying our back-of-the-envelope estimates of how many net new duplex and lowrise units are due to reclassification, we estimate for the 2001-2006 timeframe that 7,961 surplus duplex units and 3,288 surplus lowrise units are due to re-classification. In total, 11,249 of the 26,395 (43%) are due to reclassification.\nThis is likely a conservative estimate since this does not account for “duplex” units that have been demolished 2001-2006, and it assumes that re-classified “duplex” units are empty at the same rate as “duplex” units that were already coded as such in the 2001 census, which is quite unlikely given the extreme jump in the share of ‘surplus’ “duplex” units 2001 to 2006.\n\nGraphing the percentage point change in share helps highlight these changes.\n\nIt also shows that in the 2001-2006 period it is not just the duplex and lowrise, but also the share of ‘surplus’ units of the other housing types grew.\nWe can use this estimate to re-calibrate our data by artificially re-classifying the existing duplex and lowrise units we estimate got re-classified in each period, adding these to the total housing stock and adding the estimated corresponding ‘surplus’ units to the starting year ‘surplus’ counts of each respective period. That allows us to get a “re-calibrated” change in share of ‘surplus’ units.\n\nThe re-calibrated view has a smaller initial jump in share of ‘surplus’ units, followed by an essentially flat trend for the next two periods. In particular we note that the change in the 2011-2016 period is negligible, especially in light of other factors like number of units completed close to census day as we will see later on. Which by itself is interesting.\nThat changes the change in ‘surplus’ share 2001-2006 from 2.7% to 1.6% after re-calibrating.\nIf we take a less cynical approach to students and other people in more temporary living conditions and don’t count dwellings occupied by temporarily present persons as ‘surplus’, we get two census periods of increases in unoccupied dwellings followed by one period of essentially no change, after accounting for re-classification. The 2001-2006 change in share of unoccupied units drops from 1.9% to 0.8% after re-calibrating.\nTo elaborate on the importance of suites in this we quickly graph the number of ‘surplus’ dwelling units by structural type in Metro Vancouver.\n\nThis shows that the 2016 census counted more ‘surplus’ duplex units (mostly units in suited SFH) than ‘surplus’ units in any other structural type broken out in the census, including highrise (apartments, five or more storeys).\nAt this point it is useful to also look at unoccupied units instead of ‘surplus’ dwelling units.\n\nUnoccupied units shows a more gradual increase in Metro Vancouver, with again duplex (i.e. suited SFH) units accounting for most of the growth 2011-2016 and making up 33% of all unoccupied units, the largest contingent of unoccupied units by structural type in 2016. And secondary suites are excluded from the City of Vancouver Empty Homes Tax, as well as the vacancy tax component of the provincial Speculation and Vacancy Tax."
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html#other-factors-contributing-to-surplus-dwelling-units",
    "href": "posts/2020-01-27-mythical-oversupply/index.html#other-factors-contributing-to-surplus-dwelling-units",
    "title": "Mythical oversupply",
    "section": "Other factors contributing to ‘surplus’ dwelling units",
    "text": "Other factors contributing to ‘surplus’ dwelling units\nAs mentioned in the introduction, Rose suggests the ‘surplus’ dwelling units are due to “domestic or foreign speculators, visiting students, temporary workers, or those owning a second home in Canadian cities”. And while some ‘surplus’ dwelling units are indeed used in such a way, we know that it’s not the majority. This is a good opportunity to share the infographic we made based on a previous post with Nathan Lauster for Maclean’s 2020 charts to watch, and include the code in this post or added transparency or in case others want to play with it.\n\nPutting data on the Canadian ‘surplus’ dwelling units in context with US data, where the ACS collects information about why the dwelling unit was unoccupied or occupied by temporary residents, helps us distinguish “good vacancies”, like unoccupied homes for sale or for rent or sold and rented homes that have not been moved in yet that are needed to enable residential mobility, from “potentially bad vacancies” like vacation homes or otherwise empty homes”.\nThus changes in residential mobility can lead to changes in ‘surplus’ dwelling units. Rental vacancy rates have been fairly flat over our time period, the variation is unlikely to create more than 500 vacancies. Variations in number of residential listings and sales can be larger, and while only a portion of for sale units are empty, they can have noticeable effect the count of ‘surplus’ units. Units vacant for these reasons tend to be geographically spread out, so they blend in as background noise in maps that break out the share of ‘surplus’ units on fine geographies.\nThe same can not be said when apartment buildings complete close to census day and all units in a large building are ready to get occupied all at once. New apartment buildings take time to fill in. With ever-increasing planning and construction times in Vancouver, it is hard for buyers of new apartments to time their move-in to completion time. Even in the case of resale transactions it can take some time for people to move into their new unit and selling their old one, leaving one of their units vacant in the meantime. These vacancies are fairly short-lived and a normal part of residential mobility. But when larger buildings complete in the months before the census, this can lead to large local distortions in ‘surplus’ units that have tripped up the reporters in the past and lead to false claims of problematic ‘surplus’ units in media reports.\nSimilar effects can occur at the metropolitan level if the number of completions close to the census varies from census year to census year. CMHC keeps track of completions data, although they can’t distinguish between lowrise and highrise completions.\n\nIt would take more time to develop a robust model how exactly timing of completions effect, which will have to wait for a later time and incidentally is part of the reason why we have obtained the more detailed cross-tabulation on unoccupied units used in this post. The above graph suggests that we should expect to see a increase in ‘surplus’ apartment units 2001-2006 and 2011-2016, and a slight drop 2006-2011 based on apartment completions close to the census. Another metric that can be used to estimate variation in the size of mobility-related vacancies are the number of for sale listings and sold properties as well as rental vacancy rates around census time, the latter of which did not vary much in Vancouver over our time frame."
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html#upshot",
    "href": "posts/2020-01-27-mythical-oversupply/index.html#upshot",
    "title": "Mythical oversupply",
    "section": "Upshot",
    "text": "Upshot\nIn this post we focused on the change in ‘surplus’ dwelling units as defined by Rose and showed that a substantial portion of which are directly attributable to a change in census methods that lead to reclassification of existing dwelling units. Moreover, change in ‘surplus’ dwelling units is largely constrained to the 2001 to 2006 timeframe.\nAs usual, the code for the analysis is available on GitHub, including access to the data on unoccupied and ‘surplus’ units by structural type via the CensusMapper API, which was obtained through CMHC funding for an unrelated project we am working on and is now available under the usual StatCan open data licence.\nThat’s it for the main part of the post. We looked at some related questions that aren’t essential for the post, but might be of interest to some people. Instead of burying them on our computer we decided to throw them into an appendix."
  },
  {
    "objectID": "posts/2020-01-27-mythical-oversupply/index.html#mapping-neighbourhood-level-recalibration-estimates",
    "href": "posts/2020-01-27-mythical-oversupply/index.html#mapping-neighbourhood-level-recalibration-estimates",
    "title": "Mythical oversupply",
    "section": "Mapping neighbourhood level recalibration estimates",
    "text": "Mapping neighbourhood level recalibration estimates\nTo round things off we map the change in single-detached dwellings 2001-2006 to show where single-detached homes got added, and where they got re-classified.\n\nTo home in on the re-classification we can add half of the change in the number of duplex units, to validate that one single-detached turned into two duplex units.\n\nThat evens out the areas where we lost single-detached units considerably. We can take this one step further and add in one third of the change in lowrise units – if that change has been positive, as well as the change in (half of the) semi-detached dwelling units.\n\nThat further smooths out the areas that saw large losses in single-detached homes, confirming the theory that some single-detached got classified into “lowrise” buildings with 3 or more units. At the same time it throws the measure off in areas where new lowrise buildings got built. It also highlights areas where lowrise buildings have been re-classified to highrise buildings, and vice versa.\nBut it still shows a significant shortage in dwelling units in many areas dominated by SFH, where we would expect this calculus to come out as close to even. The prevalence of this suggests that prior to the 2001-2006 change in methodology some SFH were likely double-counted, with households in the main unit and households in a secondary suite both being counted as living in a single-detached house.\n\nTenure and sampling\nIn our first post we looked at tenure as a way to estimate unoccupied and temporarily occupied secondary suites. The argument was that almost all “duplex” units in Metro Vancouver are single family homes with secondary suites, only very few units classified as “duplex” in the census are stratified. In particular, there can be at most one owner household per structure containing two duplex units. But we found that the 2006 census counted about 26k more owner than renter households in duplex units, which, by the pigeon hole principle, leads us to estimate that there must be (at least) around 26k empty duplex units.\nIn our follow-up post we noted, using a 2011 NHS cross-tab, that this method likely significantly over-estimates the number of duplex units not occupied by usual residents. We can now extent that check using 2006 and 2016 data.\nThe 2006 census only counted 13,305 duplex units not occupied by usual residents. In other words, using our tenure calculus over-estimates the number of duplex units not occupied by usual residents in 2006 by roughly a factor of 2.\nThat’s a huge discrepancy, which points to three possible directions already discussed in our previous post. One is that there may be a substantial difference in long form response rates by renters in suites compared to renters in other structural types, that has not been adjusted for in census post-processing. The other is the question of how extended families in suited SFH identify in the census. Some may identify as a single household, with the suite showing up as empty in the census. More important to our question at hand, some may identify as two separate households, one in the main unit and one in the suite, with both identifying as owner households. Which would break our assumption that there can be at most one owner household per structure containing two (non-stratified) duplex units. Lastly, there is the possibility that condominium status of duplex units suffers from significant under-reporting.\nThe same issue shows up in 2016 data, although to a lesser degree. Using 2016 data has the added benefit that we can separate out (self-identified) stratified units, so let’s take a quick look.\n\nThe observation here is that the 2016 census counted 26,965 more owner than renter households in occupied non-stratified duplex units (27,950 when also including the stratified duplex units), but only 20,430 duplex units not occupied by usual residents.\nIn summary, this points to another complication on how to interpret census data of (occupied) duplex units. Secondary suites is the rabbit hole gift that keeps on giving."
  },
  {
    "objectID": "posts/2020-01-24-keep-on-moving/index.html",
    "href": "posts/2020-01-24-keep-on-moving/index.html",
    "title": "Keep on moving",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\n\nMore results from the new Canadian Housing Survey dropped earlier this week! And they provide new insights into why Canadians move.\nLast time we only got provincial results. Now we can break down reasons for the last move by metro area and current tenure, but this time around we looking at the last move no matter when it happend, as opposed to only considering moves in the past five years as in the previous data release. So the stats aren’t directly comparable to the numbers from the previous release. But as we’ll show, the trends are pretty similar.\nFirst to the question guide. Lots of good stuff here, but we’re interested in the questions about peoples’ previous residence: “People move for a variety of reasons, either voluntary or non-voluntary. Why did you move from your previous dwelling?” Importantly, respondents are allowed to choose more than one, and only the respondent (rather than other household members) counts. Let’s look at the proportion of people selecting each reason for their last move by metro and by current tenure.\n\nOverall the reasons for moving is fairly uniform across major metro areas, with generally positive housing moves explaining most moves, as we’ve noted before. Hence people move to “upgrade” their dwelling in size or quality; to “become a homeowner”; and to “be in a more desirable neighbourhood.” More ambiguous housing moves, including those to “reduce housing costs”, vie with family-related moves (“change in family size”; “form own household”; “be closer to family”) and work-related moves (“new job”; “reduce commute”) as explanations.\nSeparating by current tenure (did people move into a place they rent or a place they own), the stories are still pretty similar. The first big takeaway is that mobility is pretty normal and common, and most people move for positive reasons. But there are a couple of notable differences. Moving “to reduce housing cost” or “to reduce commute time” factor more into renter’s than into homeowner’s decisions to move.\nFinally, there’s are two reasons for moving that seem unambiguously negative for those involved, reflecting “forced moves.” One set of “forced moves” occur due to “natural disasters and fires.” The other comes down to social causes: “Because you were forced to move by a landlord, a bank or other financial institution or the government.” This happens far more often to renters and far more often in Metro Vancouver.\nThis brings us to the second big takeaway. In terms of forced moves, Vancouver sticks out like a sore thumb.\n\nWhile Vancouver stands out, the other CMAs and rural areas in BC follow closely behind. Exposure to socially forced moves (e.g. evictions) seems to reflect something province-wide. Like our provincial protections for renters (Residential Tenancies Act) and how they’re enforced (or not) by the RTB. Or like our profound lack of rental options overall (low vacancy rates coupled with sometimes predatory landlords). Or like our heavy reliance upon the least secure kinds of rental stock (basement suites and condominium rentals) within secondary rental markets and subject to landlords reclaiming for their own use.\nThe results we have so far may reflect past conditions rather than the present. After all, we’re looking at peoples’ last moves here, many of which occurred more than five years ago. But we’ve got lots to follow up on in future analyses. And hopefully further releases from the CHS will clarify just what mechanisms are at work driving outsized displacement in Metro Vancouver.\nAs usual, the code for the post is available on GitHub for anyone interested.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{lauster2020,\n  author = {Lauster, Nathan and von Bergmann, Jens},\n  title = {Keep on Moving},\n  date = {2020-01-24},\n  url = {https://doodles.mountainmath.ca/posts/2020-01-24-keep-on-moving},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLauster, Nathan, and Jens von Bergmann. 2020. “Keep on\nMoving.” MountanDoodles (blog). January 24, 2020. https://doodles.mountainmath.ca/posts/2020-01-24-keep-on-moving."
  },
  {
    "objectID": "posts/2020-01-06-flow-maps/index.html",
    "href": "posts/2020-01-06-flow-maps/index.html",
    "title": "Flow Maps",
    "section": "",
    "text": "Just came across this excellent flow map tool that takes a google sheet and turns it into an interactive flow map. It’s super-easy to use, here is a quick demo.\nWe are using the commuting flow data between census subdivisions from the 2016 census. First we load the required libraries\nlibrary(tidyverse)\nlibrary(cancensus)\n#remotes::install_github(\"mountainmath/statcanXtabs\")\nlibrary(statcanXtabs)\nlibrary(sf)\nlibrary(googlesheets4)\nNext we create the google sheet for our flow map. It comes with three sheets, one defining overall properties, one defining the locations and one for the flows.\n\nProperties\nmy_properties &lt;- c(\n  \"title\"=\"Canadian animated commuter flow\",\n  \"description\"=\"Canadian animated commuter flow\",\n  \"source.name\"=\"Statistics Canada Census 2016\",\n  \"source.url\"=\"https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/dt-td/Rp-eng.cfm?LANG=E&APATH=3&DETAIL=0&DIM=0&FL=A&FREE=0&GC=0&GID=0&GK=0&GRP=1&PID=111332&PRID=10&PTYPE=109445&S=0&SHOWALL=0&SUB=0&Temporal=2017&THEME=125&VID=0&VNAMEE=&VNAMEF=\",\n  \"createdBy.name\"=\"Jens von Bergmann\",\n  \"createdBy.email\"=\"jens@mountainmath.ca\",\n  \"createdBy.url\"=\"https://doodles.mountainmath.ca/posts/2020-01-06-flow_maps\"   ,\n  \"mapbox.mapStyle\"=NA,\n  \"colors.scheme\"=\"Default\",\n  \"colors.darkMode\"=\"yes\",\n  \"animate.flows\"=\"yes\",\n  \"clustering\"=\"yes\"\n)\n\nproperties &lt;- tibble(property=names(my_properties)) %&gt;%\n  mutate(value=my_properties[property])\n\n\nLocations\nFor the locations data we use the centroids of the census CSDs, where we prettify the names and fine-tune some special locations like Electoral A in Vancouver, which we hide behind the clean_location_data() function call.\nlocations &lt;- get_census(\"CA16\",regions=list(C=\"01\"),level=\"CSD\",geo_format = \"sf\") %&gt;%\n  st_centroid(of_largest_polygon = TRUE) %&gt;%\n  cbind(st_coordinates(.)) %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  clean_location_data() %&gt;%\n  select(id=GeoUID,name=Name,lat=Y,lon=X)\n\n\nFlows\nFor the flows we take the commute flow cross tabulation at the CSD level.\nflows &lt;- get_sqlite_xtab(\"98-400-X2016325\",\"https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/dt-td/CompDataDownload.cfm?LANG=E&PID=111332&OFT=CSV\") %&gt;%\n  select(origin=`GEO_CODE (POR)`,dest=`GEO_CODE (POW)`,count=`Dim: Sex (3): Member ID: [1]: Total - Sex`) %&gt;%\n  collect()\n\n\nPutting it together\nThe only thing left to do is to upload the data to google sheets.\n#my_new_sheet &lt;- sheets_create(name=\"Animated commuter flow\",sheets=c(\"properties\",\"locations\",\"flows\"))\nmy_new_sheet &lt;- sheets_get(\"13Q-xsfL59XXPw7-3ue_9G9xab9FtYiL3S9vOZp-yJyY\")\nwrite_sheet(properties,my_new_sheet,\"properties\")\nwrite_sheet(locations,my_new_sheet,\"locations\")\nwrite_sheet(flows,my_new_sheet,\"flows\")\nThat’s it. We can read off the sheet id and embed our flow map.\n\n\nIt clusters locations by default, so at low zoom levels it only shows high-level flows. Zoom in to see finer commute flows for your region of interest.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2020,\n  author = {von Bergmann, Jens},\n  title = {Flow {Maps}},\n  date = {2020-01-06},\n  url = {https://doodles.mountainmath.ca/posts/2020-01-06-flow-maps},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2020. “Flow Maps.”\nMountanDoodles (blog). January 6, 2020. https://doodles.mountainmath.ca/posts/2020-01-06-flow-maps."
  },
  {
    "objectID": "posts/2019-11-26-property-tax-snacks/index.html",
    "href": "posts/2019-11-26-property-tax-snacks/index.html",
    "title": "Property tax snacks",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2019-11-26-property-tax-snacks/index.html#property-tax-snacks",
    "href": "posts/2019-11-26-property-tax-snacks/index.html#property-tax-snacks",
    "title": "Property tax snacks",
    "section": "Property Tax Snacks",
    "text": "Property Tax Snacks\nResidential Property Taxes have been rising in Vancouver. As always, we’re seeing a lot of sturm and drang about the rise. But we think it’s ultimately a good thing. Why? Here’s three perspectives. From a fiscal perspective, property taxes pool our resources to enable our government to pursue projects and provide for the common good. They’re a big component of how we take care of each other and set priorities. From a social equity perspective, property taxes are directed at wealth, which is highly unequal in its distribution. Property taxes are also - at least around here - mostly a tax on land value, the rise in which is socially produced and largely unearned by any landowner. We should definitely be looking to redirect the massive gains in real estate wealth in this province toward the common good (Henry George for the win!) Finally, from a financial perspective, higher property taxes increase the carrying cost of treating housing like any other investment. They also work to stabilize the market to the extent they counterbalance the weight of shifts in interest rates. In this sense, property taxes and prices are endogenous.\nAlso worth noting: Vancouver’s property taxes are very, very low. Measured as the “mill rate” - or the rate of taxes owing per $1,000 in property value - the City of Vancouver’s rate is far below most other municipalities in BC (and further afield), especially outside the Lower Mainland.\n\nWithin municipalities, property taxes hit real estate wealth, but they’re basically “flat taxes”, set at the same proportion to property values regardless of underlying disparities. What’s more, looking across municipalities, there’s a perverse regressivity to property taxes. The wealthy people (e.g. living in Vancouver or West Vancouver) pay lower tax rates on their properties than those generally less well-off (e.g. living in Nanaimo, Port Alberni, or Prince Rupert). Measures like the School Tax, progressively applied to properties over $3 million, only partially counteracts this underlying regressivity at the Provincial scale. Still, we should be looking at more ways to bend property taxes in a progressive direction, and perhaps even use them to provide relief for income taxes. In short, we can definitely make property taxes a better tool for promoting a more fair BC.\nThe comparison between places like Vancouver and places like Prince Rupert also helps demonstrate the endogeneity of property taxes and prices. Someone owning a $1M property in both municipalities pays different tax rates. The present value of that tax break the property in Vancouver gets above the property in Prince Rupert, assuming the spread stays constant, is $229k. That serves to inflate property values in Vancouver. Which in turn serves to depress the mill rate in Vancouver. Rinse and repeat.\nLet’s briefly touch on property taxes in terms of fairness between the City’s renters and property owners. The city has been working on making itself more fair to renters, who make up the majority of its population but find their options for remaining in the city increasingly constrained. Here we want to provide a simple comparison of property owners to renters in terms of rising costs they face. What’s risen faster, rents or taxes? We also don’t want to forget about rising asset prices too! After all, most property owners have reaped enormous gains in wealth that haven’t been available to renters. Here we’ll set aside other benefits available only to owners (including homeowner grants reducing property taxes, the complete absence of capital gains taxation on sales of principal residence, and even the lack of taxation on the imputed rents home owners pay to themselves) and just look at the rise in property taxes paid and gains in property values relative to median rents over the last few years. What’s that look like?\n\n\n\nProperty taxes to property values to rents\n\n\nHere we’ve drawn upon a representative sample of detached property and apartment condo and used their actual property taxes paid for the property tax data, and used repeat-sales HPI for single family and apartment condo within the boundaries of the City of Vancouver. The rise in property taxes paid by owners of detached properties slightly exceeds, but otherwise more or less matches the rise in median rents over recent years. The property taxes paid by apartment condo owners has had a more complicated journey, ultimately remaining below the rise in median rents (and remember, many of those condos are being rented out!) Overall, property taxes and rents have pretty much kept pace with one another. Property values, on the other hand, are through the roof! Up until very recently, we saw especially strong rise in the value of detached houses. Rapid price appreciation in the detached market (2010-2016) pushed property tax growth higher for detached houses than for condos, who are only recently catching up. The expansion in municipal budgets has driven recent property tax growth, but it remains in line with the increase in rents being paid by representative residents of the City.\nGiven our low vacancy rates, there is little doubt that rents would’ve risen much quicker without provincial rent control. But regardless, rents have still kept pace with rising property taxes. We still have lots of room to raise our property taxes on all of the grounds mentioned above. We could also use more progressivity in our property tax rates, working to counteract their regressive tendencies. Unlike for renters and rising rents, the research indicates that property tax increases seldom result in displacement of home owners. That said, if property owners feel their budgets squeezed too tight, the province also provides a wealth of opportunities for deferring payments. That’s yet another benefit that’s just not available to renters. But if the province wants to start supporting tenants who need a break to catch up on their rent payments, it might help put a big dent in the sky-high proportion of BC’s residents who feel forced to move.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes."
  },
  {
    "objectID": "posts/2019-10-29-commuter-growth/index.html",
    "href": "posts/2019-10-29-commuter-growth/index.html",
    "title": "Commuter growth",
    "section": "",
    "text": "Metro Vancouver is growing, both in terms of population and jobs. That means the number of people commuting to work is growing and putting a strain on our transportation system. The nature of that strain depends to a large extent on how people are getting to and from work. The Canadian census started collecting data on how people get to work in 1996, which allows us to see how commuters and commute choice have changed over time."
  },
  {
    "objectID": "posts/2019-10-29-commuter-growth/index.html#tldr",
    "href": "posts/2019-10-29-commuter-growth/index.html#tldr",
    "title": "Commuter growth",
    "section": "TL;DR",
    "text": "TL;DR\nThere are large regional differences in how net new commuters get to work, with net new commuters in central regions driving much less than those in outlying regions. Yet our regional growth plan calls for outlying regions to grow significantly faster than the centre."
  },
  {
    "objectID": "posts/2019-10-29-commuter-growth/index.html#metro-vancouver-commuters",
    "href": "posts/2019-10-29-commuter-growth/index.html#metro-vancouver-commuters",
    "title": "Commuter growth",
    "section": "Metro Vancouver commuters",
    "text": "Metro Vancouver commuters\nThe evolution of the overall mode share in Metro Vancouver shows a gradually changing transportation landscape that this growth brings with it.\n\nWhile driving dominates, the share of people driving to work is on a decline. Road space is mostly inelastic, forcing trips into more space-efficient modes, in particular transit. At first sight the data for 2001 does not seem to fit the pattern, but we should remember that Vancouver’s 2001 Transit strike overlapped the census period. (Something I did not know about and had to look up when I was wondering what caused the 2001 numbers.)\nTo understand the impacts on congestion we should look at total numbers of cars on the road instead of the mode share.\n\nThe total number of drivers has been growing steadily, causing increased congestion as people navigate the choices they have on where to live and how to get to work.\n\nOver our 20 year timeframe transit and driving account for the bulk of the net new commuters. But this pattern has not been uniform across the region. Commuters living in more central areas or near rapid transit have more options in how to get to work than people living in further out parts of the region, and this is reflected in the municipal breakdown."
  },
  {
    "objectID": "posts/2019-10-29-commuter-growth/index.html#city-level-breakdown",
    "href": "posts/2019-10-29-commuter-growth/index.html#city-level-breakdown",
    "title": "Commuter growth",
    "section": "City level breakdown",
    "text": "City level breakdown\n\nThe differences across municipalities is quite remarkable. All municipalities except West Vancouver gained commuters over this time period. Driving saw the largest gains among the modes in Surrey, the Langleys, Maple Ridge and Port Moody, with transit winning out in the other larger cities. The District of North Vancouver, West Vancouver, and Delta saw a drop in drivers.\nThe change in drivers is the most important component in understanding increase in congestion, we can highlight at the individual contributions of all the municipalities in terms of drivers and non-drivers.\n\nThe range is enormous, with Surrey seeing an increase in 75,360 drivers and West Vancouver a drop of 2,650 drivers. Change in non-drivers ranges from a gain of 65,530 in the City of Vancouver to a loss of 25 non-drivers in West Vancouver. Looking at cities that added at least 2,000 commuters in this timeframe, we can look at the share of non-drivers among net new commuters as an indicator of how sustainably (in terms of congestion) each city grew their commuters.\n\nThis share in net new commuters affects how driver mode share in each of the municipalities has evolved since 1996.\n\nThis shows that all regions decreased their drive to work mode share. We also plot the intermediate years, and 2011 stands out as giving lower values than 2016 for some of the smaller areas, which might be due to NHS non-return bias.\n\nAll municipalities have reduced their mode share, with central municipalities making larger gains. We greyed out areas with fewer than 200 commuters in 2016."
  },
  {
    "objectID": "posts/2019-10-29-commuter-growth/index.html#neighbourhood-level-breakdown",
    "href": "posts/2019-10-29-commuter-growth/index.html#neighbourhood-level-breakdown",
    "title": "Commuter growth",
    "section": "Neighbourhood level breakdown",
    "text": "Neighbourhood level breakdown\nTo better understand the drivers behind this, we can plot this on finer geographies. To start off we look at the total change in the number of commuters between 1996 and 2016 on the 2016 census tract geography on the custom tabulation that we have used before.\n\nIt’s remarkable how all CTs in West Vancouver and large portion of the District of North Vancouver, as well as the West Side of the City of Vancouver have lost commuters. We see similar patterns in the low-density areas of Richmond and Delta, White Rock and the south-western edge of Surrey, as well as other pockets throughout the region. This can only partially explained by population loss that we have also seen in some of these regions, but is mainly a function of changing demographics.\nLooking at the change in drivers, all areas that lost commuters also lost drivers or only had minimal gains.\n\nBut there are many other areas that gained commuters and still lost drivers, for example large parts of Mount Pleasant area in the City of Vancouver and ares in the Cambie Corridor.\nTurning to the the change in non-drivers, almost all areas posted gains.\n\nNaturally areas that saw a lot of development in that time period stand out, especially those near skytrain. We can summarize these two maps by looking at the difference in the change in non-drivers vs the change in drivers. This is a simple metric that looks at where we have added more drivers than non-drivers and vice versa.\n\nWe can also see where the driver to work mode share dropped and where it increased.\n\nIt’s worthwhile to try and understand what made these regions special and dig a little further. Or even better, run a more detailed analysis on the factors that impact the changes in commuters, drivers and non-drivers. But that will have to wait for another post."
  },
  {
    "objectID": "posts/2019-10-29-commuter-growth/index.html#commute-times",
    "href": "posts/2019-10-29-commuter-growth/index.html#commute-times",
    "title": "Commuter growth",
    "section": "Commute times",
    "text": "Commute times\nAnother way to look at commutes, and the possible congestion they cause, is commute times. Congestion is not just a function of the number of cars that commute, but also the time they spend on the road.\n\nThere is surprisingly little variation in drive times across the municipalities in the share of people driving less than 30 minutes one way, with about 25% of drivers commuting less than 15 minutes, and bout 60% commuting less than half an hour. There is more variation in long commute times of at least 45 minutes, with the central areas, led by the City of Vancouver, clocking in with the lowest portion of drivers commuting longer than 45 minutes. Bowen Island is a curious example with both, the highest share of commuters commuting less than 15 minutes and those commuting over an hour, cleanly separating commutes within the island from those that leave the island.\nWe can extend the same graph to all commuters not just drivers. \nThis paints a very similar picture. Commute times are also reflected in how early people are leaving their house for work.\n\nHere we are again looking at commute times of all commutes, not just drivers. Ordered by the share of people that leave the house before 8am we again see the similar pattern of commutes further out from the centre getting up earlier, and people in more central communities getting to sleep in and leave the house after 8am.\nAnother way to look at the data is by distance to work.\n\nAgain a similar pattern emerges, where we ordered the municipalities by the share of commuters that travel less than 10km to work. Taken together, commute times, time leaving for work, and commute distance hint at the life quality impacts of commuting that are independent of the well documented health impacts of commuting by private motor vehicle."
  },
  {
    "objectID": "posts/2019-10-29-commuter-growth/index.html#upshot",
    "href": "posts/2019-10-29-commuter-growth/index.html#upshot",
    "title": "Commuter growth",
    "section": "Upshot",
    "text": "Upshot\nAs our region grows, we should pay more attention to the interplay between location and transportation. Our current regional planning dictates that twice as many people should move to Surrey than to the City of Vancouver, which is very likely to significantly boost the population driving to work compared to e.g. a scenario where Vancouver and Surrey grow proportionally to their respective populations. Allowing growth to happen in a way that gives people convenient choices of how to travel to work and other places should receive more attention when we talk about how we grow our region.\nAdding housing in central parts of the region and along rapid transit networks can help reduce driving and overall congestion in our region. Yet people regularly oppose development projects in central locations because they are afraid of the (car) traffic they fear the new project will bring.\nAs usual, the code for this post is available on GitHub for people wanting to play with the code. Unfortunately, the data going back to 1996 that we used is only available for Vancouver and Toronto CMA, people hoping to extend to the code to other regions of Canada will have to make due with the 2001 to 2016 timeframe or find other ways to add in the 1996 data."
  },
  {
    "objectID": "posts/2019-10-17-rents-and-incomes/index.html",
    "href": "posts/2019-10-17-rents-and-incomes/index.html",
    "title": "Rents and incomes",
    "section": "",
    "text": "Following up on our previous post on rents and vacancy rates there is another rental stat originating from City of Vancouver documents that is making the rounds and that is misleading. Again, our housing crisis is fundamentally a rental crisis, so it’s important to keep the numbers straight so that we can better focus our energy and resources. This one is a bit more serious, but still has been making the rounds quite broadly on social media.\nThe graphs purports to show that average rent increased by 75% while median income increased only by 18%. It originates from page 14 of the Housing Vancouver Strategy, but it comes with the caveat that the income change has been adjusted for inflation, while the rent increase is reported as nominal. The graph is misleading as most people won’t understand the fine print and assume the rent change and income change numbers are directly comparable. Which is not the case. A better way to graph the data is to either adjust both income and rents, or none.\nThere are some other issues too. The choice of time frame is odd. The data is using census data, which only has incomes for 2000, 2005, 2010, and 2015, but reports these based on where people live in the respective year after (the census years). We would probably be better off using CRA T1 data that is available on an annual basis, we will do that further down. For now, let’s just see what happens when we follow the Housing Vancouver Strategy and use census income numbers. Moreover, it probably makes more sense to use median rents instead of averages. (Interestingly, median rents increased more than average rents.) Separating out bedroom types is not that important when looking at the rent change index, although three or more bedroom rents rose noticeably slower than the rest. Another choice is how to adjust for inflation. It probably makes most sense to use the Metro Vancouver consumer price index, although other choices, e.g. the national CPI, will only change the absolute value of the index and not the relationship between the rent and income index. Lastly, the document is not clear on what income are being used in the graph. It talks about “local incomes”, which is unhelpful in understanding the definition used. We are hypothesizing that the report used household income, as other parts of the report mentioned household income and households directly map to dwelling units.\nWe can also express this in inflation adjusted terms.\nEither way, median rents and median household incomes in the City of Vancouver have both risen roughly in unison, with rents slightly outpacing incomes.\nIt is worthwhile to separate out the changes by bedroom type.\nIt is interesting to see that real rents for three or more bedrooms did decrease at times, although the small sample size tells us we should not put too much weight into this. Moreover, the Total rent was higher than any of the individual rents in 2015, which points to a change in composition, probably driven by a mixture of location effects as well as by City of Vancouver requirements for 2 and 3 bedroom units, which generally fetch higher rents, having an impact on the unit mix and pulling up the total rent change.\nCensus data is only available every five years, using census data for incomes makes it hard to track the relationship of rents and incomes in a timely fashion. Rather than relying on census data for incomes, we can also measure changes in incomes using the annual T1 taxfiler data for census families and unattached individuals. (Alternatively we could use income of economic families.) Right now we don’t have a cross tab at the city level available, but this will likely change in the not too distant future. For now, we will investigate changes in rents and incomes at the metro level.\nThe graph is somewhat messy with the large number of timelines, but it paints a very similar picture to the previous one in that incomes of couple families tracks rents quite well. The same can’t be said for persons not in census families. It’s worthwhile to investigate this further. Persons not in census families contains a rage of of subgroups that don’t expect to see changes in income, for example students, which will pull down changes in median income. Persons not in census families also don’t map well to households, especially in Vancouver with their high share of complex households, which makes it difficult to directly compare their relationship to rents with that of census families. We again notice the volatility in the three or more bedroom rent estimate, which is less robust due to the relatively low number of such units.\nFor completeness we also show the change in real terms by adjusting income and rent changes by overall CMA level inflation.\nWe also note how T1 taxfiler data lags the rent data, as tax declarations always lag one year, with further processing by the CRA and StatCan delaying the release of the data even further."
  },
  {
    "objectID": "posts/2019-10-17-rents-and-incomes/index.html#upshot",
    "href": "posts/2019-10-17-rents-and-incomes/index.html#upshot",
    "title": "Rents and incomes",
    "section": "Upshot",
    "text": "Upshot\nMedian rents have tracked median household incomes quite well in the City of Vancouver. Similarly, Metro Vancouver median rents have tracked median family incomes. This on it’s own does by no means mean that all is well in the rental market. With our anemic vacancy rates this is likely the result of sorting by income, with lower income people getting pushed out and higher income people staying. We have noticed clear signs of exactly this sorting by income happening in the City of Vancouver.\nRenters are also facing a loss in mobility, with turnover rents being significantly higher than (rent controlled) stock rents.\nAnother point of caution is that we looked at overall income changes, not specifically at income for renters only. It might be worthwhile to make the effort to dig into this further, look into the full income and rent distributions, and also split off market rentals from subsidized rentals and treat student households separately. But that would require a custom tabulation and is beyond the scope of a blog post. From other data we know that shares of renter households with shelter-cost-to-income above the 30% or 50% thresholds have not changed much or even decreased over the time period, indicating that this may not yield much different results.\nRents tracking income changes also means that things have not gotten better. Renters have been struggling for a long time with high rents and a larger than comfortable share of their income going toward shelter costs.\nDigging deeper into the data helps us to better understand the issues renters are struggling with. Instead of casting this as an issue of “local incomes” diverging from rents, which is not the case as we have seen, we should focus on reducing the sorting pressure by creating more homes for renters. Below-market rentals will be most effective in this, and we should mobilize all the federal, provincial and local resources we can get to make that happen. But adding housing in general, which has mostly been market housing, has also generally helped low income populations to maintain or even grow their size.\nAs usual, the code for this post is available on GitHub for anyone to reproduce and adapt for their own purposes."
  },
  {
    "objectID": "posts/2019-10-07-spatial-autocorrelation-co/index.html",
    "href": "posts/2019-10-07-spatial-autocorrelation-co/index.html",
    "title": "Spatial autocorrelation & co",
    "section": "",
    "text": "These days I run a fair bit of spatial analysis. And there are three problems that regularly come up:\nNone of these problems is insurmountable, but they are all annoying to various degrees. Often I might ignore them on my first analysis run, but these problems need to be dealt with sooner or later. Which can eat up significant amounts of time. Sometimes the analysis results don’t change much after properly dealing with these issues. Sometimes they change significantly. Sometimes they change dramatically to the extent of reversing the overall interpretation.\nThere are other issues that come up in analysis too, but these I find regularly in my particular workflows, and these are usually the most time consuming to deal with.\nAll these problems are fairly well understood, and a lot of ink has already been spilled on each of these. Not all of these problems come up in every setting, but at least one or two of them come up in pretty much any research dealing with spatial data. And despite these problems being well-known, I am surprised how many papers don’t adequately deal with them, or even completely ignore them.\nFor the first problem we have written the tongfen package, that focuses on Canadian census data and greatly simplifies the common task of making census data comparable through time, and also allows to estimate data on unrelated custom geographies.\nThe ecological fallacy comes into play when trying to extract information on individual level behaviour from ecological level correlations. This comes up when individual level cross-tabs aren’t available, for example when mixing data from different sources. There are a number of approaches to deal with this, in particular Gary King’s elegant methods. Dealing with this generally requires a lot of hand-holding, we have not found implementations that are easy to use and automate the error detection and iterations that we found are usually required. We will leave this for a separate future blog post.\nThis leaves us with the last issue on our list, spatial autocorrelation, and I want to take this opportunity to walk through a detailed example myself, complete with code for reproducibility."
  },
  {
    "objectID": "posts/2019-10-07-spatial-autocorrelation-co/index.html#spatial-autocorrelation",
    "href": "posts/2019-10-07-spatial-autocorrelation-co/index.html#spatial-autocorrelation",
    "title": "Spatial autocorrelation & co",
    "section": "Spatial autocorrelation",
    "text": "Spatial autocorrelation\nWhen fitting models, we need to check if the assumptions of the model are satisfied. When working with spatial data, we impose an additional assumption on most models. That the residuals aren’t spatially autocorrelated.\nSpatial autocorrelation boils down to the fact that nearby things tend to be more related than far away things. When residuals of a fitted model exhibit spatial autocorrelation the model is facing two problems. The individual observations that fed the model can’t be treated as statistically independent. This can inflate p-values in linear models, and bias model validation done via standard random train/test data split. Moreover, it can introduce sizable (and statistically highly significant) spurious correlations.\nAnd the most annoying part is that when building models from real life spatial processes, autocorrelation in residuals tends to be the norm rather than the exception.\nTo better understand why I obsess so much about this these days, let’s consider an example. This is reproducing part of an excellent post by Morgan Kelly where he explores the likelihood that improperly handled spatial autocorrelation invalidates results of selected published economics research.\nFollowing Morgan Kelly, we take a 100 by 100 square, and assign values to each cell via two independent random spatial processes. The word spatial here means that the process behaves like spatial processes we see in nature. Say a temperature distribution. Or soil contamination. Or how demographic variables like median income, tenure or mode of transportation to work tend to distribute. That is, a random process with spatial autocorrelation.\n\nThese pictures show the values of our two random spatial processes, which we called sim1 and sim2. They are modelled as a constant model with spatial noise, we refer those interested in details to the code. We think of sim1 and sim2 taking values over the same square.\nNext we will sample the square at 80 randomly chosen points, with locations shown in white below, and correlate the values of sim1 at these points to those of sim2 at these points.\n\nOne can think of this as correlating e.g. air contamination with density of factories. Or income with home values. Except using two independent random spatial processes instead of real data. Since the two processes sim1 and sim2 are independent random processes, we would expect their values at our sample points to be uncorrelated.\n\nVisual inspection shows this not to be true. More precisely we get a strong and highly significant correlation between these variables, with low but significant adjusted \\(R^2\\) of 0.15.\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-0.13\n0.05\n-2.82\n6e-03\n\n\nsim2\n-0.53\n0.14\n-3.82\n3e-04\n\n\n\nWe can proceed with standard checks to see how the residuals are distributed.\n\nThings looks reasonably normal on this front. To understand what goes wrong here we plot the associated variogram.\n\nThis shows high autocorrelation at least up to a distance of about 25. Guided by this we can produce neighbour weights up to a distance of 25 scaled by inverse distance and check Moran’s I statistic, which clocks in with a statistic of 0.71 and a p-value at or below 1e-04, the resolution of our Monte Carlo simulation of 10,000 runs.\nAs expected, the model residuals are highly autocorrelated. Had we not checked this, we might have concluded that sim1 and sim2 are highly correlated, when in fact the correlation is entirely due to spatial autocorrelation.\nOf course this is not news to researchers and has been well documented before, which is why I am so surprised to keep coming across research papers in economics, planning, or geography that does not even check from spatial autocorrelation. Morgan Kelly runs through a couple of such examples in his post."
  },
  {
    "objectID": "posts/2019-10-07-spatial-autocorrelation-co/index.html#how-to-deal-with-spatial-autocorrelation",
    "href": "posts/2019-10-07-spatial-autocorrelation-co/index.html#how-to-deal-with-spatial-autocorrelation",
    "title": "Spatial autocorrelation & co",
    "section": "How to deal with spatial autocorrelation",
    "text": "How to deal with spatial autocorrelation\nJust because regression residuals exhibit significant autocorrelation does not mean that the analysis is doomed. There are ways to deal with this problem. Probably the best way is to identify a non-random spatial process that can explain the autocorrelation. For example, if you are fitting a model to housing prices and you see strong spatial autocorrelation in residuals, you may be missing some important neighbourhood-level features. Maybe school catchment areas, or patterns in street trees, areas with ground slope that favours views vs areas that don’t have views, or proximity to amenities. Plotting the residuals can help reveal these patterns. If this approach is successful it’s a double win. It reduces autocorrelation and strengthens the model by adding missing variables with explanatory power.\nBut often this strategy does not work. And if it does it usually only reduces autocorrelation, but does not remove it to the extent that it can be ignored.\nAggregating data based on the range of the autocorrelation will reduce the autocorrelation, but it also reduces the data we want to use to build our model from. Generally a better strategy is to select a model that explicitly deals with autocorrelation. Typical choices are spatial lag or spatial error models. These are linear models that add a spatially lagged term to a linear regression model. The spatialreg package in R implements several choices, but they only work for regular linear models, possibly with weights. For more complex models, even just lasso or ridge regression, one needs to custom build this by hand. Having more flexible ready-made processes for building such models would be helpful and might help increase adoption.\nFor our case at hand, we choose a spatial error model of the form, \\[\ny = X \\beta + u, \\hspace{1cm} u = \\lambda W u + \\epsilon\n\\] where \\(w\\) is the spatial weights term. The model picks up \\(\\lambda\\) of 0.68 and an overall superior (AIC) fit compared to our previous naive linear model with pseudo \\(R^2\\) of 0.58. The coefficients however aren’t statistically different from zero.\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n-0.03\n0.07\n-0.4\n0.69\n\n\nsim2\n-0.03\n0.13\n-0.2\n0.84\n\n\n\nIn summary, the spatial error model successfully picks out the spatial autocorrelation and finds no evidence for correlation between our two random spatial processes sim1 and sim2. Checking Moran’s I we have a statistic of 0.0092 and a p-value of 0.43, providing no evidence of remaining spatial autocorrelation.\nChecking in on the residuals we notice a slight increase in performance too, with residuals better fitting the normality assumption.\n\nOur spatial error model was fully successful in picking up the spatial autocorrelation and removing the spurious correlation. This is the best-case scenario, in real life things usually aren’t that clean."
  },
  {
    "objectID": "posts/2019-10-07-spatial-autocorrelation-co/index.html#the-nitty-gritty",
    "href": "posts/2019-10-07-spatial-autocorrelation-co/index.html#the-nitty-gritty",
    "title": "Spatial autocorrelation & co",
    "section": "The nitty-gritty",
    "text": "The nitty-gritty\nThings don’t always go this smoothly as in this toy example. In practice, things tend to get more messy and it can be at times challenging to properly identify the correct patterns of spatial autocorrelation and properly deal with them.\nIt should be said that spatial autocorrelation does not always negatively affect naive regression results. It’s the nature of random processes that they can sometimes produce positive spurious correlation, sometimes negative spurious correlations, and sometimes no significant correlation.\nTo exemplify this, we can re-run our initial random spatial processes for sim1 and sim2 a number of times and compare the results of running a naive regression.\n\n\n\nRun\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\nr squared\n\n\n\n\n1\n-0.53\n0.14\n-3.82\n0.00027\n0.15\n\n\n2\n0.30\n0.08\n3.89\n0.00021\n0.15\n\n\n3\n0.08\n0.12\n0.62\n0.53756\n-0.01\n\n\n4\n0.01\n0.07\n0.16\n0.87241\n-0.01\n\n\n5\n0.10\n0.10\n1.00\n0.32031\n0.00\n\n\n6\n0.35\n0.09\n3.73\n0.00036\n0.14\n\n\n7\n-0.18\n0.08\n-2.15\n0.03462\n0.04\n\n\n8\n0.07\n0.11\n0.64\n0.52476\n-0.01\n\n\n9\n0.25\n0.14\n1.72\n0.08867\n0.02\n\n\n10\n0.14\n0.09\n1.57\n0.11995\n0.02\n\n\n11\n-0.54\n0.13\n-4.01\n0.00014\n0.16\n\n\n12\n-0.55\n0.12\n-4.70\n0.00001\n0.21\n\n\n13\n0.58\n0.11\n5.03\n0.00000\n0.24\n\n\n14\n0.23\n0.11\n2.11\n0.03791\n0.04\n\n\n15\n0.34\n0.13\n2.55\n0.01259\n0.07\n\n\n16\n-0.02\n0.17\n-0.09\n0.93039\n-0.01\n\n\n17\n0.07\n0.09\n0.75\n0.45801\n-0.01\n\n\n18\n0.00\n0.14\n-0.03\n0.97316\n-0.01\n\n\n19\n0.05\n0.08\n0.60\n0.55069\n-0.01\n\n\n20\n-0.25\n0.06\n-4.25\n0.00006\n0.18\n\n\n\nWe see that 10 of our 20 runs give statistically significant correlations at at least the 0.05 level. In that sense, our initial example was somewhat engineered in that we chose processes that did yield significant autocorrelation of the residuals. But we did not have to try very hard to get one of these.\nThe spatial process we used to generate the data used an exponential variogram model with range of 25, which our diagnostic variogram we plotted above correctly identified. Changing the range will change the frequency with which random processes will produce statistically significant spurious correlations. But even in cases where spatial autocorrelation does not negatively impact model coefficients, accounting for spatial autocorrelation will still increase overall model performance."
  },
  {
    "objectID": "posts/2019-10-07-spatial-autocorrelation-co/index.html#validation-splits",
    "href": "posts/2019-10-07-spatial-autocorrelation-co/index.html#validation-splits",
    "title": "Spatial autocorrelation & co",
    "section": "Validation splits",
    "text": "Validation splits\nA related issue with spatial data comes up when splitting data into test and training data. Typically one just splits the dataset randomly, but this is problematic when dealing with spatial data. The idea behind splitting data is that the two subsets are uncorrelated, so if one is used for model training the other can give an unbiased measure of model performance. But in the presence of spatial autocorrelation, a random split into training and test data will not result in uncorrelated subsets. In particular, if we select a random split in our above example, the test data will confirm the spurious correlation we find using the training data.\nTo better show this we generate 500 random points to evaluate our original model at, and we take an 80:20 split into training and test data. We then evaluate a naive linear model trained on the training data on both the training and test data and compare the results.\n\n\n\nMetric\nTrain\nTest\n\n\n\n\nrmse\n0.29\n0.30\n\n\nrsq\n0.26\n0.28\n\n\nmae\n0.24\n0.24\n\n\n\nThe testing data does not pick up on the spurious correlation. The problem is that our testing and training data are correlated via spatial autocorrelation. To pick up statistically independent test data we can’t rely on simple random sampling but should pick up random blocks of data based on the range of the autocorrelation. There are several R packages available for data splitting adapted to spatial data, for example the mlr package. To understand the difference we visualize the splits of the data using spatial and non-spatial splitting methods.\n\nUsing 4-fold spatial partitioning we can look at the performance of linear models trained on our training data by evaluating it on the test data on each fold.\n\n\n\nMetric\nFold 1\nFold 2\nFold 3\nFold 4\n\n\n\n\nrmse\n0.25\n0.27\n0.41\n0.42\n\n\nrsq\n0.24\n-0.25\n-0.80\n-1.54\n\n\nmae\n0.19\n0.22\n0.35\n0.37\n\n\nslope\n-0.58\n-0.72\n-0.68\n-0.26\n\n\n\nWe notice significantly higher errors and lower (pseudo) \\(R^2\\) in all but one of the runs. There is a sizable variation in the correlation slope across the runs. When using spatial partitioning of train and test data, the spatial autocorrelation manifests itself in form of increased discrepancy between train and test data, and gives a more accurate picture of overall model performance. In our case, the pseudo \\(R^2\\) tells us that the models generated from the test data are non-informative at best."
  },
  {
    "objectID": "posts/2019-10-07-spatial-autocorrelation-co/index.html#upshot",
    "href": "posts/2019-10-07-spatial-autocorrelation-co/index.html#upshot",
    "title": "Spatial autocorrelation & co",
    "section": "Upshot",
    "text": "Upshot\nIt is difficult to evaluate the validity of analysis based on spatial data without checking for spatial autocorrelation, and properly dealing with it if necessary. Despite this, I keep coming across studies in economics, planning, geography and other social sciences that don’t check for spatial autocorrelation despite relying heavily on spatial data.\nThese disciplines started out doing spatial analysis at times when awareness and understanding of spatial autocorrelation was low, so it is understandable when older research has ignored this issue. But I frequently come across this in recent research too. I suspect this is a case of collective complacency.\nThe change to include spatial autocorrelation as a necessary check for any published work is bound to happen eventually. This is also sure to trigger a replication effort that checks how results in key papers hold up when spatial autocorrelation is accounted for.\nAs usual, the code for this post is available on GitHub in case people want to further investigate spatial autocorrelation."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html",
    "title": "Low income vs new dwellings",
    "section": "",
    "text": "Canada’s metropolitan areas are growing, which means we need to add housing. But adding housing often faces stiff oppositions. There are many reasons people don’t like to add housing, this post is trying to look at one particular one. That adding housing causes displacement of the low-income population.\nAdding new housing to a neighbourhood has two opposing effects.\nThe gentrification effect starts from the observation that new housing is more expensive than old housing (all else being equal). This means that people moving into new housing are generally more affluent. This influx will, through various means, increase the amenity value of the neighbourhood and draw in more higher-income people. This process makes it harder for low-income people to live in the neighbourhood, increasing pressure on low-income people to move away and adding barriers for low-income people to move in. The net effect is a decrease in the low income population in the neighbourhood. (The term gentrification gets used in a variety of different ways in the literature. Here we use it strictly to refer to net displacement of low-income population.)\nThe supply effect starts from the observation that as the region is growing more people compete for housing. If we don’t add housing, more affluent people will out-bid lower income people, displace them, and renovate up their housing. Adding new housing takes pressure off of the existing stock.\nThe question is centred on the relative strength of these two effects.\nOn a regional level it is fairly uncontroversial that the net effect of adding housing is to put downward pressure on prices and ease displacement pressures. But is the same true on a neighbourhood level?"
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#tldr",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#tldr",
    "title": "Low income vs new dwellings",
    "section": "TL;DR",
    "text": "TL;DR\nThe claim that in general “increasing density causes displacement of low-income population” is simply not supported by the data. In fact, in general the opposite is true, increasing density serves to increase the low income population. However, there may be cases where increasing density does correlate with a drop in the low-income population, but this is the exception and not the rule and driven by specific factors not present (or dominating) in general.\nOverall there is a strong positive relationship between the change in the size of the Lico-AT population and the change in dwellings in a neighbourhood.\n\n\n\nModel"
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#southern-california",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#southern-california",
    "title": "Low income vs new dwellings",
    "section": "Southern California",
    "text": "Southern California\nThis insight is nothing new, there are numerous studies that look at various aspects of this question from a variety of angles. One example is the a study by Legislative Analyst’s Office (LAO) in California on low-income housing affordability. In the part that is most relevant to our question they categorized low-income census tracts in the Bay Area by whether they experienced high or low market-rate housing construction, and checked how many of these experienced displacement of low-income households."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#canadian-data",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#canadian-data",
    "title": "Low income vs new dwellings",
    "section": "Canadian data",
    "text": "Canadian data\nWe can look at Canadian data and use similar methods to understand the dynamics between adding housing and displacement of people in low-income. For this, we set up the problem very simply and look at the relationship between the change in the number of dwelling units and the change in the number of people in the Lico-AT low-income measure between 2006 and 2016 censuses for the four largest metropolitan areas (in 2006 boundaries).\n\nAll metro areas have added housing in this timeframe, while the number of low income people has increased in some and decreased in others. To bring this out better we can focus just on the change in Dwellings and population in Lico-AT.\n\nAt the metro level we don’t see any relationship between those two variables. But that’s not surprising, at the metro level there are many factors impacting the people in Lico-AT, from increased economic opportunity to displacement outside of the metro area. The question that we are interested in here is how adding housing in a given neighbourhood within a metro area effects the low income population in that neighbourhood. But it is good to keep these overall trends in mind when interpreting the results below.\nFollowing the LAO study mentioned above, we base our neighbourhoods on take census tracts."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#analytical-setup",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#analytical-setup",
    "title": "Low income vs new dwellings",
    "section": "Analytical setup",
    "text": "Analytical setup\nThere are a number of analytical choices we are making. We TongFen the data to common geography for the 2006 and 2016 censuses. This process builds the common geography up from Dissemination Areas, which leads to a courser geography than simply adjusting to 2006 census tract boundaries but ensure maximal geographic integrity across the two censuses.\nTwo challenges we have to deal with that prevent us from running a naive linear model of low income vs change in dwelling units are heteroscedacity and non-normality of residuals, as well as significant spatial autocorrelation of the residuals. To deal with the latter we re-specified this as a spatial autoregressive error model. This also softens the other issues, but some degree of non-normality still persists. We won’t resolve this completely in this post, for the Vancouver region we show that some of this can be traced down to re-classification between collective and private dwellings, with the low income data from the census only counting the population in private dwellings.\nThere are other ways to deal with these analytical challenges, but we felt that this was the best way to approach our question. We refer people that want more information on the exact model specifications to the code."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#absolute-change-in-low-income-population",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#absolute-change-in-low-income-population",
    "title": "Low income vs new dwellings",
    "section": "Absolute change in low income population",
    "text": "Absolute change in low income population\nThe simplest way to pose our question is to look at the relationship between the change in the number of people (in private dwellings) in Lico-AT and the change in the number of private dwellings in each census tract.\nA naive linear model correlating change in the population in Lico-AT y to the change in dwelling counts x yields the following highly significant results.\n\n\n\n\n\n\n\n\n\n\n\nMetro\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nMontréal\n(Intercept)\n-154.00\n6.738\n-22.86\n0\n\n\nMontréal\nx\n0.16\n0.014\n11.36\n0\n\n\nToronto\n(Intercept)\n-72.58\n7.618\n-9.53\n0\n\n\nToronto\nx\n0.28\n0.006\n48.16\n0\n\n\nCalgary\n(Intercept)\n-87.29\n16.717\n-5.22\n0\n\n\nCalgary\nx\n0.16\n0.008\n19.46\n0\n\n\nVancouver\n(Intercept)\n-138.95\n12.956\n-10.72\n0\n\n\nVancouver\nx\n0.28\n0.013\n21.25\n0\n\n\n\nHowever, the residuals are highly auto-correlated as a quick Moran I test reveals.\n\n\n\nMoran I\nStatistic\np-value\n\n\n\n\nMontréal\n0.26\n1e-04\n\n\nToronto\n0.19\n1e-04\n\n\nCalgary\n0.17\n1e-04\n\n\nVancouver\n0.20\n1e-04\n\n\n\nThe (pseudo) p-values are cut off by our choice of 10,000 Monte Carlo simulations, true p-values are likely even lower. To address this issue we are turning toward an autoregressive error model.\n\nHere \\(\\lambda\\) is the coefficient of the autoregressive term in our autoregressive error model. As expected, there is no evidence of spatial autocorrelation in the residuals.\n\n\n\nMoran I\nStatistic\np-value\n\n\n\n\nMontréal\n0.00\n0.45\n\n\nToronto\n-0.01\n0.72\n\n\nCalgary\n-0.01\n0.54\n\n\nVancouver\n-0.02\n0.65\n\n\n\nVisual inspection of the residuals confirms this.\n\nThe map indicates that at the residuals behave nicely in the middle ranges, but we see more extreme values than we would normally expect.\n\nThe Q-Q plots confirm this especially for Toronto and Montreal, but also in Vancouver. We should take a look at the largest outliers.\n\nIn Vancouver, the census tracts at UBC, the Downtown Eastside, part of North Central Coquitlam, and the area at the south corner of Burquitlam show deviations from the model greater than 750 people in Lico-AT in 2016. Individual regions can deviate for many reasons. For the Downtown Eastside we need to keep in mind that many SROs got re-classified from private housing to collective housing, which would lead to a drop in our metric as the census numbers only capture the low income population in private dwellings. During the given time period the population in collective dwellings grew by 1,370 people, which may account for a good part of the drop of 1,333 people in Lico-AT in private households. UBC saw some reclassification of student housing to private housing, which would have the opposite effect, and student population captured in nearby private housing there could also impact the estimates.\nThe regions have undergone different rates of dwelling growth, ranging from a slight loss of dwellings in the Downtown Eastside, to moderate growth in Southwest Burquitlam and strong growth at UBC and North Coquitlam.\n\nArmed with a better understanding of two of these outliers we are (for now) content with the non-normality in our residuals in Vancouver."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#focus-on-medium-growth-areas",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#focus-on-medium-growth-areas",
    "title": "Low income vs new dwellings",
    "section": "Focus on “medium growth” areas",
    "text": "Focus on “medium growth” areas\nTo understand the effect of ares with high dwelling growth, as well as the stagnant areas, we can re-run the analysis only for areas where private dwellings increased or dropped by between 10% and 90%, removing very high growth areas as well as stagnant areas.\n\nThe results are quite similar, although with much lower explanatory power."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#refining-the-estimates",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#refining-the-estimates",
    "title": "Low income vs new dwellings",
    "section": "Refining the estimates",
    "text": "Refining the estimates\nThere are a number of ways to refine the estimates. One add in a variable z given by the initial (2006) population in Lico-AT to account for home-by-home gentrification, so the process of low-income people getting pushed out without adding any dwelling units simply by the process of getting out-bid by more affluent households that may also renovate up the dwellings.\nIn summary, we are working with the following variables.\n\ny – the change in the number of people in Lico-AT\nx – the change in the number of dwelling units\nz – the initial (2006) number of people in Lico-AT.\n\nBut this is also where things get tricky, the number of dwelling units that get added in a neighbourhood is not independent of the number of low-income people at the beginning of the period, a point that we will return to at then end of the post. But the colinearity is not too concerning. As we will see, the effect of adding in the z variable will only have a small effect on the change in dwelling coefficient, although it will absorb essentially all of the intercept.\n\nThe intercept is now not meaningfully different from zero with the effect being much better captured by our home-by-home gentrification term, and the coefficients for the dwelling growth have adjusted upward a bit, with Vancouver seeing the strongest upward adjustment from 0.28 to 0.32.\nWe can also plot the dependence on our initial population in Lico-AT.\n\nThese results are again fairly stable when restricting to the “medium growth” areas."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#change-in-share-of-people-in-lico-at",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#change-in-share-of-people-in-lico-at",
    "title": "Low income vs new dwellings",
    "section": "Change in share of people in Lico-AT",
    "text": "Change in share of people in Lico-AT\nA related interesting question is to look at the percentage point change in share of people in Lico-AT, so looking at more at neighbourhood change than at displacement. This sets a much higher bar on the effect of adding new dwellings, one might expect that while the total low income population does not decrease when new dwellings are added, the share of the low income population may well drop. As dependent variable we then choose the relative change in dwellings, as well as the initial (2006) share of low-income people, but weight the analysis based on the number of private dwellings in 2006 so the analysis does not get skewed too much by low denominators.\n\nThe intercept looks unreasonably high in these graphs, but this is offset from the effect of term for the initial Lico-AT population. In Montréal we do indeed now see a drop in the share of low-income people of the order of roughly 1.8 percentage points for each percent increase in dwelling stock. Calgary comes out essentially flat and Toronto and Vancouver show a significant increase in share of low income people with dwelling growth.\n\nThis model further comes out with a significant baseline increase in share of low income people, offset by fairly high rates of attrition in existing share of low income people. For example in the case of zero change in dwellings in Vancouver, this model predicts a net drop of share of population in Lico-AT in areas with 7.7% or higher initial share of population in Lico-AT. For reference, the initial overall share of the population in Lico-AT in Metro Vancouver in 2006 was 17%.\nThe distribution of the residuals is generally better than when we looked at the total change.\n\n\nThis model again under-estimates the low income population in the Downtown-Eastside, and over-estimates the low-income population in North/Central Coquitlam and UBC, but also in Hastings-Sunrise, Burnaby Mountain/SFU, the west side of the Cambie Corridor and parts of Central Surrey.\nThe model fit for Toronto and Vancouver is significantly improved when using the relative change of the Lico-AT population instead of the percentage point change. But this also increases the issues around heteroscedacity and non-normality of the residuals, but it might be worthwhile to investigate this further."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#vancouvers-larger-municipalities",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#vancouvers-larger-municipalities",
    "title": "Low income vs new dwellings",
    "section": "Vancouver’s larger municipalities",
    "text": "Vancouver’s larger municipalities\nParticular development patterns, and decisions on what kind of development gets approved, are done at the municipal level. In particular inclusionary zoning requirements and share of purpose-built rental developments differ significantly across municipalities, and this may well lead to hidden variable bias. We can look at the four largest municipalities in Metro Vancouver to investigate this. Census tracts don’t line up perfectly with municipal boundaries, but this should not be a major concern for us.\n\nWe see that the dependency on dwelling growth holds up well across these municipalities, with Burnaby and Richmond seeing a significantly higher dependence than the overall metro model. The much higher baseline attrition of Lico-AT population in Vancouver compared to Surrey, as indicated by intercept, jumps out.\nWe can again improve the overall model fit by adding in the size of the initial (2006) population in Lico-AT.\n\n\nWe see that the change in dwelling is still associated with a strong increase in population in Lico-AT. Interestingly, Burnaby is showing the largest rate of increase. Going back up to the map of residuals for the metro-wide model we see that Metrotown does not come in far off from the model predictions, which we found somewhat surprising. Again, the intercept looses statistical significance under this model specification."
  },
  {
    "objectID": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#upshot-and-next-steps",
    "href": "posts/2019-09-02-low-income-vs-new-dwellings/index.html#upshot-and-next-steps",
    "title": "Low income vs new dwellings",
    "section": "Upshot and next steps",
    "text": "Upshot and next steps\nI had run the initial correlations of dwelling growth to population in Lico-AT quite some time ago, but never taken the time to properly deal with the significant spatial autocorrelation, refine the model, and investigate reasons for non-normality of the residuals. But the question on displacement of low-income people due to new construction keeps coming up, and I was curious to see if the relationships I initially observed still hold up when doing a more rigorous investigation. And they do.\nAs usual, there are a couple of caveats.\n\nThe entire analysis is done at the ecological level. In particular, it says nothing about displacement of individual low-income people, or the impact of adding dwellings on low income people at the individual level as the recent Federal Reserve study did. This analysis looks at the net effect on the overall population in Lico-AT, so the of net in and outflows.\nWe have not done a proper causal analysis, that would require developing a strong theoretical framework of causal relationships that drive low-income population. However, this analysis clearly shows that the claim that “increasing density gentrifies the neighbourhood”, where gentrification is understood as involving a net displacement of the low-income population, does not hold in general. In fact, this analysis adds to the evidence that in general the supply effects outweigh the gentrification effects, even in fairly small geographic neighbourhoods.\nThis analysis does not say that increasing density will always increase the population in Lico-AT, it’s a statistical analysis that simply says this is the norm, but displacement may well happen in particular areas. In particular, the analysis says little about neighbourhoods with high initial shares of people in Lico-AT as explained below.\n\nOver half of the neighbourhoods in our data have initial shares of population in Lico-AT below 20%.\n\nWith very few neighbourhoods with higher share of Lico-AT in our sample we should not assume our results holds there. The absolute change in dwelling units also biases strongly toward low initial population in Lico-AT.\n\nHowever, when looking at the relative dwelling growth we get a different picture.\n\nCalgary and Toronto see a spike in relative change in dwellings at the high end, something that we also see in Montréal and Vancouver, except that areas with very high rates of low income population we see a net loss in dwelling units. It would be worthwhile to investigate this pattern further, in our context this means that we should be careful when interpreting our results for areas with share of low-income population above 40%.\nThe code for this post is available on GitHub, anyone interested in more details on the analysis, or wanting to improve the models, is welcome to grab the code."
  },
  {
    "objectID": "posts/2019-08-12-mirhpp-tradeoffs/index.html",
    "href": "posts/2019-08-12-mirhpp-tradeoffs/index.html",
    "title": "MIRHPP tradeoffs",
    "section": "",
    "text": "The City of Vancouver has introduced the Moderate Income Rental Housing Pilot Project, with density bonusing in exchange for 20% of the units renting at about 35% below market."
  },
  {
    "objectID": "posts/2019-08-12-mirhpp-tradeoffs/index.html#tldr",
    "href": "posts/2019-08-12-mirhpp-tradeoffs/index.html#tldr",
    "title": "MIRHPP tradeoffs",
    "section": "TL;DR",
    "text": "TL;DR\n\nMIRHPP is a win-win, it manages to create both, new units that rent significantly below market, as well as market rentals. Both of which are badly needed, paid for with additional density.\nThe allocation mechanism for deciding who gets to rent one of the sub-market units is problematic. Delegating this to the developer/operator of the building creates conflicts of interest and lacks transparency. The City should take charge of this process and maintain a central waitlist, check tenant eligibility criteria, and conduct the lottery.\nSub-market MIRPP units are financed through artificially high market rents. It is a pilot program and capped at 20 submissions. If it is extended, the inclusionary zoning requirements should be very closely monitored and adjusted to allow for market rents to fall. In the long run, a tax on market rents is an inefficient way to finance sub-market housing."
  },
  {
    "objectID": "posts/2019-08-12-mirhpp-tradeoffs/index.html#mirpp",
    "href": "posts/2019-08-12-mirhpp-tradeoffs/index.html#mirpp",
    "title": "MIRHPP tradeoffs",
    "section": "MIRPP",
    "text": "MIRPP\nThe program works with inclusionary zoning. It allows for substantial density increases in exchange for 20% of the units to be rented at around 35% below market value. More precisely, rents for the sub-market units are capped at\n\n\n\nBedroom type\nMonthly rent\n\n\n\n\nStudio\n$950\n\n\n1 Bedroom\n$1,200\n\n\n2 Bedroom\n$1,600\n\n\n3 Bedroom\n$2,000\n\n\n\nin the initial MIRHPP year, with the caps increasing at the provincial rent increase limit (recently changed from inflation + 2% to just inflation), regardless of turnover. That’s roughly 35% below current rents people are paying in newer (2005 or later) purpose-built units, and even further below market rents at turnover. The sub-market units are limited to people in specific income ranges, the building operator is in charge of selecting tenants and checking the eligibility requirements. In short, new tenants are eligible if rent payments require at least 25% of their income, and existing tenants become ineligible if they are paying less than 20% of their income on rent, witch eligibility checks every 5 years."
  },
  {
    "objectID": "posts/2019-08-12-mirhpp-tradeoffs/index.html#tradeoffs",
    "href": "posts/2019-08-12-mirhpp-tradeoffs/index.html#tradeoffs",
    "title": "MIRHPP tradeoffs",
    "section": "Tradeoffs",
    "text": "Tradeoffs\nIn essence the MIRHPP manages to build below-market units, paid for by density bonusing. But density bonusing with inclusionary zoning has tradeoffs, and it’s worthwhile to examine them in more detail.\n\nBenefits\nIt’s easy to understand the benefits of this program. MIRHPP manages to build new below-market units, something that is generally quite expensive to do. In this case, it is paid for by density bonusing, so there is little direct cost. The program builds those units in desirable locations, and it achieves income mixing at the building level.\nA secondary benefit is that it might make it politically easier to add more housing by directly addressing people at the lower end of the income spectrum, who face huge pressures in our housing market. This is especially true in an environment where councillors block market rental housing based on the premise that the rental market is heavily segmented.\n\n\nCosts\nThe cost are more abstract, and they depend on the counter-factual chosen. The most obvious one is the cost that’s associated with how the units are distributed.\nTo get a rough idea how many households in CoV would be eligible for these units we can look at 2016 census data (using 2015 incomes).\n\nThere were 27,515 one person households with income $25,000 to $49,999 that would roughly fit the studio or 1 bedroom requirements, plus 10,700 couple without children households in that income range that would fit the 1 bedroom requirements. For the 2 and more bedroom households there were 13,910 couple without children, couple with children or lone parent households with income $50,000 to $74,999. Additionally there are 7,760 households in the $50,000 to $75,999 income range consisting of roommates or other “complex households” like families or couples with additional unrelated persons or multi-family households for a total of 59,885 eligible households. These estimates are quite rough, the number of eligible households currently in Vancouver is likely higher, not to mention households that would happily move to Vancouver if they got the change to occupy one of these units.\nWe can also tighten the eligibility requirements to exclude owner households, as well as households currently in subsidized housing. That cuts the total number of eligible households down to 33,670.\nAn even narrower category would entail (market) renter households in our income range that currently spend more than 30% to less than 100% of their income on housing, which gets the total number of eligible households down to 17,615. Cutting off at 100% of income spent at housing better aligns this with the CMHC concept of Core Housing Need.\nWith the MIRPP program currently capped at 20 buildings, it is safe to say that there are two orders of magnitude more eligible households than anticipated sub-market MIRHPP units. This leaves us with a distribution problem, the cost of which accrues to the people that are eligible but won’t be able to find a unit, as well as people that narrowly miss out on the eligibility requirements, while a lucky few obtain tax-free benefits between $6,000 and $13,000 per year, depending on the particular unit.\nThe second fundamental welfare theorem tells us that we should be able to mitigate that cost. For example, the city could extract the rent benefits as a continuous cash stream and redistribute the proceeds to help households pay market rent in a way that aims to maximize social welfare.\n\n\nAdministration\nCurrently MIRHPP eligibility criteria and waitlists are managed by the developer/operator. This part of the program could probably be improved if the city would take charge of the waitlist and eligibility checking, delegating this to developers lacks transparency and opens up numerous conflict of interest scenarios. Moreover, it would be better to have a softer way to deal with tenants that become ineligible, other than kicking them out of their unit. (I am unsure if and how the Residential Tenancy Act applies to the sub-market units to make make eviction in case of lapsing eligibility, or other softer changes like e.g. hiking rents up to 20% of income, are dealt with.)\nThe cost of delegating this to developers/operators is in lack of transparency and public trust in the program, as well as lack of data on waitlists.\n\n\nSecondary costs - who pays for MIRHPP units?\nThere are secondary costs that accrue when the MIRHPP is extended beyond the pilot stage. MIRHPP, like all inclusionary zoning programs, require the cost of market housing (market rents in the case of MIRHPP) to be artificially high. To understand this, we need to examine in more detail how the sub-market MIRHPP units are paid for. At first sight, it’s the developer that pays for them. But the developer is not loosing any money, otherwise they would not build those units. In effect, it’s the rents in the market rental units that are subsidizing the sub-market units.\nWe can quantify that subsidy. The total subsidy for 20% of the units renting at 35% below market is \\(0.2 \\cdot 0.35=0.07\\), so 7% of the total rental revenue. In other words, the developer could also be charging everyone 7% less rent and maintain the same gross rental income, showing that market rents are 7% above what it costs to profitably build and operate a market rental building. So the inclusionary zoning is paid for by market rents being (at least) 7% higher than what current land values and construction costs dictate. MIRHPP works because market rents are inflated.\nMore than that, MIRHPP requires market rents to remain elevated to continue to function. It’s effectively a tax on market rents. Which in principle is not a bad thing, inclusionary zoning is supposed to be re-distribute in nature to help lower income people to stay in the region. But a tax on market rents is a poor way to do this for two reasons.\n\nRenters, including those able to pay market rents, are on average lower down the income spectrum compared to owners. And even further down when measuring wealth. They are a poor target for a redistributive tax.\nThis is a very inefficient tax. The cost of the MIRHPP could be covered by taxing only the market rentals in the MIRHPP building, but the tax applies to all market renters, no matter what building they are in with the remaining benefits accruing to landlords of non-MIRHPP buildings."
  },
  {
    "objectID": "posts/2019-08-12-mirhpp-tradeoffs/index.html#possible-fixes",
    "href": "posts/2019-08-12-mirhpp-tradeoffs/index.html#possible-fixes",
    "title": "MIRHPP tradeoffs",
    "section": "Possible fixes",
    "text": "Possible fixes\nThere are a number of ways MIRHPP could be improved by building on it’s strengths and reducing the downsides.\n\nThe City should take charge of the waitlist and income verification process.\nThe City should conduct a robust discussion on the policy goals and evaluate if there are better ways to increase social welfare than tying rental benefits to rental units.\nMIRHPP should be re-structured so that it does not compete with the parallel policy goal of lowering market rents. At the very least the inclusionary zoning requirements should be continuously re-evaluated and adjusted to allow for market rents to fall. A better solution would be to find a different funding mechanism that improves the redistributive fairness and is more efficient. For example, funding the MIRHPP sub-market units through property taxes would be a clear improvement on both fronts.\n\n\nLowering market rents\nTaking the MIRHPP logic that affordability gains trump the externalities of added density, we should take a closer look at the mechanism and benefits of leveraging this insight to lower market rents.\nThe underlying observation is that, as explained above, the feasibility of MIRPP projects shows that current market rents are at least 7% higher than they need to be given current land and construction costs. If added affordability trumps the externalities of higher density, then we should ask ourselves if we should allow higher density if it serves to lower market rents.\nThe difficulty with this line of reasoning is that the ability to deliver profitable market housing at 7% lower rents does not necessarily translate into actual rents being lower. In our tight rental market renters pay those higher rents for lack of alternatives. We need significantly more rental supply, and with it higher vacancy rates to see a drop in market rents. Achieving this is no easy feat, given our large housing shortfall in the regions, and it would require a concerted regional effort to get there. Localizing this effort to just the City of Vancouver will make it very hard to eliminate our housing shortage and bring vacancy rates up to healthy level, let alone into a range where rents will drop. The argument of aiding affordability by allowing more density for market rentals is much less direct than the on of inclusionary zoning, and hinges on the ability to do this at scale and region-wide.\nOn the upside, the benefits to lowering market rents are enormous as it accrues to all renters, not just the ones in MIRHPP buildings. In our rent controlled environment, the effects will only be directly felt by those whose current rental contract started recently, but households with rents frozen further below current market rents also benefit by reducing their penalty to move. We should note that renter mobility is still fairly high, 26.9% of renters in market rental in 2016 lived in their unit for less than one year. That’s at a minimum 35,030 renter households in market rental housing in the City of Vancouver that are paying close to market rents, with many more that moved into their unit in the preceding years and that would still directly benefit from a 7% drop in market rents.\nAll of the above is not yet factoring in that lowering market rents will also exert downward pressure on land values, leading to further room to lower market rents. And that lowering rents will also lower home values.\nLowering market rents should be part of any housing discussion, just like aggressively increasing the supply of sub-market housing is essential. Looking at the number of households stretched to pay their rent, as well as the current political climate, it is unrealistic to expect to be able to build enough non-market housing to fill our current shortfall. We need to keep pushing hard for funding to build more non-market housing, and at the same time put downward pressure on market rents. That narrows the gap between those lucky enough to secure a spot in non-market housing and those affluent enough to comfortably live in market housing from both sides. Only working on lowering the entry to market housing will fail the most vulnerable in our increasingly unequal society. Only working on non-market housing will keep the pool of those vying for spots in non-market housing unnecessarily large and is very unlikely to produce enough new housing units to be able to fill our current housing shortfall."
  },
  {
    "objectID": "posts/2019-07-14-taxing-toxic-demand-early-results/index.html",
    "href": "posts/2019-07-14-taxing-toxic-demand-early-results/index.html",
    "title": "Taxing Toxic Demand: Early Results",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\n\nThe province has released (via press release) the first data on its Speculation and Vacancy Tax (SVT)! Huzzah!\nPreviously, we’ve speculated on what this data would show. In particular, we estimated that around 8,800 dwellings would show up as empty in a way likely to be taxed by the speculation tax. How close were we? Well, the speculation tax has so far identified 8,738 owners of empty properties. Hot damn! We’re on a roll!\nBut wait! No celebrating yet. It’s early days, and two issues remain to be resolved:\n\nThe province seems to identify owners owing taxes in its press release rather than properties owing taxes. There can be more than one owner per property! (And more than one property per owner…). On average, CHSP data suggests that there appear to be about 1.58 owners per property in major metro areas covered by the tax (e.g. family members co-owning properties, investors, etc.). That may mean that the 8,800 dwellings we thought would appear empty should correspond to 13,904 owners - and so far we’ve only found 8,738, so we’re still short!\nThere are around 23,000 undeclared taxfilers out there, so figures for owners of empty properties may rise. We really don’t know anything about these undeclared filers. The province, so far, has not identified them as likely speculators. Instead the press release goes out of its way to reassure those who haven’t filed that they’ll be contacted by the Province about applying for an exemption. It’s possible that late declarations are late for reasons unrelated to the tax (e.g. forgetfulness, hospitalization, death), in which case undeclared filings will probably come in similar to declared filings, with maybe another 180 taxed owners added. Similarly, it’s possible that late declarations reflect overlapping ownership claims and property owners’ assumptions that someone else had already declared on their behalf. Or it could be that late filings disproportionately reflect owners with limited ties to the province, boosting the number of vacancies likely to be discovered. It would appear that over one-fifth of late filers of the City of Vancouver’s Empty Homes Tax, for instance, ultimately ended up paying the tax (a much higher proportion than for those who filed on time). This kind of ratio, of course, could add another 4,500+ owners of empty properties if applied to the late filers. In other words, we’d end up pretty close to the 13,904 owners we initially projected (based on 8,800 dwellings showing up empty).\n\nSo we’ve learned that our estimates are at least going to end up in the ballpark in terms of vacant properties declared to the province. In addition, we’ve got data on where owners of vacant properties appear to reside as citizens (within BC, elsewhere in Canada, or outside of Canada) and we get our first look at declared satellite families. To date, we’ve had very few ways of estimating the size of the latter population. The tax defines satellite families as those earning more than half of their combined spousal incomes outside of Canada (hence undeclared on Canadian income taxes). Rather than attempting to estimate this directly, we mostly played around with the kinds of situations (mismatches between incomes and property values) likely to trigger audits in case people didn’t file as satellite families. As we discuss in our earlier post, there are many reasons why people may end up in satellite family arrangements. It is probably more productive to think of the component targeting satellite families as complementing federal tax law that is quite ineffective in taxing worldwide income of residents, although the SVT can only capture homeowners and determining residency for transnational families is inherently complicated.\nThe data that we’ve got so far may change, of course, both as remaining undeclared owners file and as audit systems begin to look through cases. But to put the data in context, let’s plot our preliminary declarations data against what we know about properties overall in the areas covered by the tax. Here we compare CHSP data on property ownership, residency of owner, and owner-occupation with the declarations from the Speculation Tax so far. Note that CHSP data and SVT data have different bases, the former is based on properties, while the latter is based on declarations, so property cross owners. This means we can look at all of the properties that are owner-occupied in the taxable region and compare them to the number of owner-occupiers declaring themselves part of satellite families, as in the first two columns below. You have to squint to see that second column, because compared to all owner-occupied properties in the region (800,000+), the number of declared satellite family owner-occupiers is very small (3,241).\n\nWe can make the same basic comparison for the number of properties that are investor owned (which we use as a catch-all for any non-owner-occupied property). Through the CHSP data, the non-resident (“overseas”) investor-owners can be distinguished from those residing in Canada. They’re much smaller in number, but they’re definitely part of the mix. We can compare the number of investor-owned properties to the number of owners declaring a vacant property subject to the Speculation and Vacancy Tax. Comparing, it would appear that the vast majority of investor-owned properties are not left vacant for the length of time needed to trigger the tax. Instead almost all appear to be rented out, making up a sizeable proportion of rental stock.\nAs we also discussed in our Speculation post, we didn’t know the overlap between properties left “empty” and those deemed “foreign-owned.” Now we do! It’s hard to see it in the figure above because the declaration numbers are so tiny, but owners declaring vacancies that show up in the Speculation Tax data look like they’re just over half foreign (see also figure below). Some owners may be holding for purely speculative reasons, some may be running short-term rentals, some affluent investors may have second (or third) vacation homes in the area. Other owners may be stuck in transitions of various kinds not covered under the Speculation Tax exemptions. What’s clear from the figures is that owners of vacant properties are few in number compared to investor-owned properties overall. Of course, some properties may have been rented out as a means of avoiding the tax, and as with the City of Vancouver’s Empty Homes Tax, we’d suggest that this aspect of the tax is worth supporting, even if the overall numbers of people paying remain small. As a bonus, the proceeds from the tax are earmarked for affordable housing!\nLet’s return to our fudge factor to put properties and owners are the same footing. If we assume, based on estimates from CHSP data (detailed below) that there are 1.58 owners per property, then we get a total of 1,655,243 owners overall required to make declarations, using 1,048,290 residential properties from the CHSP data as a base. That probably over-estimates to total number of declarations required, as not all residential properties are required to declare their SVT status. Still, this matches reasonably well with the “1.6 million” letters on how to apply for exemptions it appears the government expected to send out back in January. Using this figure as our base, we can estimate the percentage of all owners who’ve so far declared themselves as members of satellite families or owners of a vacant property. What’s that look like?\n\nDeclared satellite family members make up less than a quarter percentage point of owners overall. Owners of vacant properties make up just over half a percentage point. Together, taxed owners are less than one percent of owners overall. While these numbers might still change, depending on the late declarations (&lt;1.5% of owners) and possible audits, as well as variations in number of owners per property in each sub-category, the findings so far demonstrate a much broader point: The situations subject to the Speculation and Vacancy Tax probably are rare, and probably aren’t contributing a great deal to BC’s housing crises.\nIt would appear that “toxic demand” in the form of Satellite-Family-Foreign-Owned-Empty-Dwellings just aren’t all that big a thing, and we should probably stop blaming foreigners and transnational families for our housing woes (especially given the toxicity such blame spreads to discussions of race and immigration in Vancouver). As always, there remain caveats to our assessment. The data isn’t final yet. And there may be some geographic clustering, or clustering by property types, so the impact may be somewhat bigger in very specific sub-markets. Single family homes on the west side of Vancouver, or in West Vancouver, have been identified as especially subject to “toxic demand” before. Once we get better numbers we will have a clearer picture of this, but these sub-markets that soak up most of the attention aren’t the main battle grounds of our affordability crisis, but rather speak to a crisis of certain professionals’ sense of entitlement. Until we learn more, let’s keep our vacancy tax. But let’s also keep our eyes on the prize of achieving broad regional affordability across a diverse housing stock, moving forward to provide serious answers to the questions of how we should make room, meet housing needs, and build enough housing to promote a more inclusive BC for everyone.\n\nAppendix\nTo get our fudge factor we look at the differences between CHSP data on owners and data on properties. On average across our CMAs there are about 1.58 owners per residential property, which may be a slight under-estimate as the data does not provide details for properties with more than three owners on title.\n\nA quick check across metro areas affected by the SVT confirms that there is little geographic bias. In summary, there are no significant differences in how many owners are on title across CMAs or residency participation.\n\nThere also seems to be little variation across residency types, except that properties owned purely by non-resident owners have fewer owners on title, while properties owned by mixed resident and non-resident owners have more. But that’s expect. The share for non-resident participation properties confirms that the differences from the average are almost entirely due to conditional bias. Thus there should be little issue with applying the same fudge factor across the board.\nAs usual, the code for the analysis is available on GitHub.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{lauster2019,\n  author = {Lauster, Nathan and von Bergmann, Jens},\n  title = {Taxing {Toxic} {Demand:} {Early} {Results}},\n  date = {2019-07-14},\n  url = {https://doodles.mountainmath.ca/posts/2019-07-14-taxing-toxic-demand-early-results},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLauster, Nathan, and Jens von Bergmann. 2019. “Taxing Toxic\nDemand: Early Results.” MountanDoodles (blog). July 14,\n2019. https://doodles.mountainmath.ca/posts/2019-07-14-taxing-toxic-demand-early-results."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "",
    "text": "Another “working paper” on Vancouver’s real estate woes came out, this one by Josh Gordon. We have been contemplating for a week now if it is worth responding to, but after seeing one too many obviously false statements about what the working paper supposedly shows making the rounds, we felt the benefits of addressing this might outweigh the costs of further entrenching the camps in Vancouver’s real estate debates with this post.\nFor those that don’t have the patience to read the whole post, feel free to jump around between the following fairly self-contained sections."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#affordability-measure",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#affordability-measure",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "Affordability measure",
    "text": "Affordability measure\nThe author measures “affordability” using the price to income ratio, defined as “average [detached] house price to average household income”, taken at the municipal aggregation level. This is a curious choice of metric. Why average household incomes? Why include renters in the income metric? Why single family homes and exclude other types of homes? Why aggregate at the municipal level before taking the ratios? We are left to guess, but the underlying assumption that would justify this metric is that every households should be able to buy a single-detached home in the municipality they currently live in. Unfortunately that only makes sense in some fantasy universe, but it does make for catchy price to income ratios."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#numbers",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#numbers",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "The numbers",
    "text": "The numbers\nIn Figure 1 of the working paper we are shown how this supposedly played out in 2016. Household income is a concept that’s only available in the census, so the graph likely made some adjustments to extrapolate from the average household income reported in 2015 to 2016. One way to do this is to scale the 2015 incomes by the Metro Vancouver growth in median family income of 2.85% between 2015 and 2016. While not a perfect substitute, this should give a reasonably good measure.\n\nIt’s immediately apparent that these numbers differ dramatically from those in the working paper. Looking at West Vancouver the working paper underestimates the average household income by almost a factor two, and that’s still not counting some sources of income like capital gains income, which averaged $27k per household in 2010. (We don’t have 2015 numbers handy.) Or RRSP withdrawals which likely are above average in communities like West Vancouver, which has the highest share of seniors in Metro Vancouver at 28% of the population. For some other municipalities, for example Maple ridge, the average household income estimate in the working paper still comes out short, but only by less than 10%.\nIn case this needs saying explicitly, any analysis that’s based on comparing metrics based on such wildly incorrect income numbers is bound to produce wrong results..\nThere probably are some income numbers of some type that match those used in the analysis, and the income numbers are defined and labelled incorrectly throughout the text. Who knows, either way the metric is constructed extremely carelessly. But the main issues is actually not that the input metrics of the analysis are grotesquely off, that part is reasonably easy to fix by just substituting in better estimates. The main issue is that the methods of the type of analysis performed with the metric is flawed, as we will explain in the following sections."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#mismatch",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#mismatch",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "Mismatch of metrics",
    "text": "Mismatch of metrics\nThe main metric employed in the working paper suffers from a severe mismatch of populations the aggregate is taken over, as we already noted above. The income metric is taken over all households, whereas the price metric is taken over all detached dwellings (as defined by the real estate boards). These two metrics match up in the case of an owner-occupied detached dwelling, while mismatches occurs for non-owner occupied detached dwellings on the dwelling side, and renter households as well as owner households in attached dwellings. This makes the price to income ratio as defined in the working paper very hard to interpret, we have written about this problem before.\nTo appreciate the effect of this mismatch, we can look at the average income of owner households in single detached housing (where some smaller municipalities are aggregated into groups) using a custom tabulation we have worked with before.\n\nAs expected, the numbers come out significantly higher than overall household incomes, but the respective ranking is also different. When just looking at owner households in single detached housing, such households in for example the City of Vancouver have higher income than those in Delta, but overall average household income is higher in Delta than in the City of Vancouver.\nThe reason for this is of course that detached homeowners come from a different portion of the income distribution in different municipalities, a textbook example of the Simpson’s Paradox that the working paper falls prey to. There are several other income metrics one may want to choose, for example one can exclude senior households, or external migrants within the previous year where income numbers are uninterpretable. Or use medians instead of averages, or use adjusted family income, or work with the full income distribution. Any of these choices will lead to different overall numbers, as well as differences in the ranking.\nThe bottom line for the working paper is that the correlations at the aggregate level are quite sensitive to the choice of input metric. In particular, the metric chosen in the working paper, is prone to lead to analysis skewed by household composition, with e.g. 38% of households in the City of Vancouver being 1 person households, which tend to have significantly lower incomes than family households, compared to 20% in Delta. On the dwelling side, in the City of Vancouver roughly 28% of dwelling units are detached, compared to 72% in Delta. Looking at tenure, 46% of households in the City of Vancouver are owner households, compared to 78% in Delta. These differences alone explain a good portion of why the rank of the two municipalities switches depending on the choice of income metric. And they illustrate how the analysis in the working paper is heavily confounded by the differing demographic composition of the municipalities.\nIt is very unfortunate that the report completely ignores these issues, with the exception of the case where the report argues to remove an the City of Toronto as an “outlier” because “pooling many lower income renters (who typically live in apartments) with higher income detached homeowners [has the effect of] boosting the price to income ratio”, and because of “relative rarity of detached houses” with “rezoning potential” – arguments which all apply to the City of Vancouver to a higher degree. Unfortunately, these kind of selectively applied case-by-case arguments are endemic in the report, underlining the complete lack of methodological framework."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#real-world",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#real-world",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "The real world",
    "text": "The real world\nSo how about those correlations between incomes and home prices? Josh Gordon writes\n\n[W]e should see a strong positive correlation between housing prices and household incomes by sub-area in an urban region. Yet in Vancouver, the correlation was virtually non-existent.\n\nHis analysis shows that that statement is true in Josh Gordon’s fantasy world where average household income numbers are vastly different from reality, and where every household should be able to buy a single family home in the municipality they currently live in.\nMeanwhile, let’s check how things play out in the real world, where people live in different kinds of housing and different tenures, we can compare incomes and home values of owners living in their homes. We can do that for all owner-occupiers, or just for owner-occupiers of single detached homes. Or for those in apartment condominiums.\n\nAnd indeed, we see a strong positive correlation between average households incomes and average home values of owner-occupiers in our 12 regions. The average dwelling values here are based on self-estimated values, which will likely have some variance for individual households, but in aggregate are fairly close to average sale prices around that time. Just to check this, we give a labelled graph of the average dwelling values.\n\nThe dwelling values come out close to the values used in the working paper, accounting for the grouping of municipalities in our dataset.\nWe can also check the price-to-income ratios.\n\nThose are very high price to income ratios, but nowhere near the ones the paper reports for the fantasy universe. As noted in the working paper, wealth likely plays a significant role for people purchasing homes, as will Canadian income sources not accounted for in Census income numbers we already mentioned above, as well as gifts, inheritance, and also unreported income earned abroad."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#ecological-fallacy",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#ecological-fallacy",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "Ecological fallacy",
    "text": "Ecological fallacy\nThe report proceeds to introduce a second metric, the share of non-resident owners in each municipality and correlates them with the price to income metric. The obvious issues arising from the mismatch between CHSP’s “single detached” and the real estate board’s “detached” categories go completely unmentioned.\nThe report then attributes the strong correlation these metrics exhibit to the buying behaviour of non-resident owners, which is a textbook example of the ecological fallacy. The report takes no steps to mitigate this despite this fallacy being well-known for often producing wrong results to the degree that it can even reverse the sign of correlations run on ecological vs individual level data. In short, the conclusions drawn from the correlational analysis in the working paper are well-known to be highly problematic, in particular this renders the paper’s attempts to establish causation to nothing but wishful thinking in the hopes that the ecological fallacy does not apply. Let alone that the other variable in the correlation, the price to income metric, is incorrectly calculated and has little meaning outside of the author’s fantasy universe of single family abundance.\nTo round this off we quickly check how the ecological correlations with non-resident single-detached owners play out in the real world.\n\nThis gives still a reasonably strong correlation, although with a much lower R2 that depends to a significant degree on the choice of non-resident owner metric chosen. In particular this shows that the lazy argument in the working paper asserting that the choice does not make a difference is not correct. And this is not a product of our grouping, correlating the grouped majority vs any non-resident participation gives an R2 of 0.947, not much different from the 0.959 in the ungrouped data.\nWhile we have reduced some of the unaccounted confounders by moving the price-to-income ratios from the fantasy world to the real world, it would be foolish to assume that this was the only confounder at play, let alone not having dealt with the ecological inference problem. The regions with large residuals give an indication of missing confounders, with the commute distance to the central business district being a strong contender in fitting the general model of how people make trade-offs between housing and transportation costs."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#how-to-fix",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#how-to-fix",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "How to fix this?",
    "text": "How to fix this?\nIncreasing the overlap between the populations for the price and the income metric is a simple starting point, and above we have outlined how that plays out in the real world. Increasing the geographic granularity is another, municipalities is too coarse a geography with very high demographic variability to be useful for aggregate metrics in this kind of analysis. We did both in a report back in 2016. But this only reduces issues around Simpson’s paradox, it does not completely remove them. And this path still bumps into the ecological fallacy if one wants to use this to attribute variations in the price to income ratio to behaviour of individuals within those regions, and thus can’t provide much insight into causation. There are ways to mitigate this problem, but for the particular argument made in the working paper the metric on share of non-resident owners is only available at the municipal level. This does not give enough regions to have much hope in removing the underlying issue, the aggregate metrics simply don’t carry enough information for such inferences.\nA much better and more straight-forward route is to take individual level price to income ratios, and analyse how these vary across a variety of other parameters. Care needs to be taken to do this properly, we have some examples of how one could do this. On a descriptive level, we can simply check who lives in high-value homes that the report is so focused on. We have done a descriptive analysis detailing several variables of owner-occupiers of high-value homes, including price-to-income ratios. One can take this further and add more variables to understand owner households in very high price to income situations, along the lines Nathan Lauster and I have done to understand households that might get caught in speculation tax audit triggers based on price to inocome ratios as well as for how long they have lived in the home. For this we used PUMF data, which provides great flexibility to explore and fine-tune metrics at the expense of higher uncertainty and coarse geographies, but this can of course be remedied by pulling a custom tabulation once one has a good understanding of the metrics and variables one is interested in. This approach has the added advantage of allowing to loosely separate out recent buyers.\nOne disadvantage of this approach is that it does not give information on buyers that don’t owner-occupy their place. At this point, the only thing we know about these places is if the owner lives in Canada or overseas using CHSP data, or if the place is rented or vacant using census data. But we can’t even crosstab these metrics although CHSP is working on developing measures of how the non owner-occupied homes are used."
  },
  {
    "objectID": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#the-upshot",
    "href": "posts/2019-06-25-how-not-to-analyze-the-roots-of-the-affordability-crisis/index.html#the-upshot",
    "title": "How not to analyze the roots of the affordability crisis",
    "section": "The upshot",
    "text": "The upshot\nTaking a step back, the working paper tried to offer insight into what drove the housing market in Vancouver (and Toronto). I am looking forward to reading work that will add insight to this question, using rigorous methodology. I am afraid Josh Gordon’s working paper does not do this.\nIt should also be pointed out that this post should not be read as an analysis in it’s own right. In particular, the scatter plots presented still contain numerous unaccounted confounding factors. This post is not aimed at offering an alternative analysis, but it simply serves to illustrate the methodological failings in Josh Gordon’s working paper. For insight into mismatches between housing and income metrics we refer the reader to the posts linked in the previous section that build on individual level price-to-income and shelter-cost-to-income metrics.\nAs usual, the code for the analysis is available on GitHub in case someone wants to check the details, or adapt the analysis for their own purposes."
  },
  {
    "objectID": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html",
    "href": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html",
    "title": "Some notes on investor and corporate ownership of residential properties",
    "section": "",
    "text": "One week ago the new batch of CHSP data on ownership of residential properties in British Columbia, Ontario and Nova Scocia came out, and I tweeted some quick graphs. While there has been reporting on some aspect of the numbers in the news a couple of days after, it struck me that this did not really hit all the questions that are on the public mind that the data can address.\nThus a quick post serving two purposes, putting a couple more graphs and data points into the public eye and providing the code to reproduce the visuals and serve as a launching point in case others want to further explore different angles."
  },
  {
    "objectID": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#multiple-ownership",
    "href": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#multiple-ownership",
    "title": "Some notes on investor and corporate ownership of residential properties",
    "section": "Multiple ownership",
    "text": "Multiple ownership\nThe first question the data answers is how many individual owners own more than one property.\n\nThe answer is quite simple, it’s about 15% in British Columbia and Ontario, and about 22% in Nova Scotia."
  },
  {
    "objectID": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#individual-investors",
    "href": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#individual-investors",
    "title": "Some notes on investor and corporate ownership of residential properties",
    "section": "Individual investors",
    "text": "Individual investors\nNext up a question that Nathan Lauster and I looked at a while ago using different methods, that is how many investment properties, i.e. properties owned by individuals but not occpied by them, are owned by local investors, so investors residing in Canada, vs by overseas investors that is investors residing outside of Canada.\n\nThis confirms the estimates of our previous work that about 4 in 5 investment properties are owned by local investors, a result that attracted some heated (and uniformed) emails when we first put it out. It also adds context to the news stories that focused on the ownership rates and implied that investors were mostly a product of owners living overseas, which is directly contradicted by the very data the news stories were based on.\nThe above were the two images I already tweeted out this week, but there were several other tables in the CHSP release that I want to briefly take a look at."
  },
  {
    "objectID": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#non-individuals-owners",
    "href": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#non-individuals-owners",
    "title": "Some notes on investor and corporate ownership of residential properties",
    "section": "Non-individuals owners",
    "text": "Non-individuals owners\nNext up is non-individual owners, so owners of property that are corporations, governemnt agencies, trusts and such. We already saw that the portion of properties owned by non-individual owners is quite small, with 9.85% of properties in British Columbia owned by non-individuals, dropping down to 5.61% for Metro Vancouver.\nThe new data gives information on the owners of these properties. \nThe vast majority of non-individual owners are corporations, with Government taking the second spot and, as one would expect, tend to own a large number of properties. But how about bare trusts that get a lot of attention these days?\nBare trusts aren’t split out, but are a sub-category of Other and multiple legal types. And looking at the overall number of these in British Columbia, there aren’t many.\n\n\n\nNumber of proerties owned\nNumber of owners\n\n\n\n\nTotal, all number of properties owned categories\n140\n\n\nOne property\n105\n\n\nTwo properties\n15\n\n\n3 to 9 properties\n10\n\n\n10 to 99 properties\n10\n\n\n\nSo 140 different owners are trusts, 105 of which own one property only and the remaining owning multiple properties. Again, this category includes bare trusts among other legal structures, so it is an upper bound on the number of bare trusts owning property in BC. This number may surprise some, given how much attention bare trusts get in the public mind.\nIt also helps re-focus the discussion about bare trusts on what is at it’s core: Transparency. Or the lack thereof. Just because the number of trusts is very small does not mean we should ignore it. But given how small it is one does wonder why not just get rid of ownership by bare trusts, that seems like a much simpler solution than continuing with bare trust owbership and trying to fix the transparency problem. Someone better informed about the uses of bare trusts may have good arguments why trusts can’t just be replaced by company ownership, but so far I have not seen any good reasons."
  },
  {
    "objectID": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#multiple-ownership-of-immigrants",
    "href": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#multiple-ownership-of-immigrants",
    "title": "Some notes on investor and corporate ownership of residential properties",
    "section": "Multiple ownership of immigrants",
    "text": "Multiple ownership of immigrants\nThe data also has tables on immigrant owners. Unfortunately it does not include totals for the number of all people considered, or all immigrants, or all recent immigrants. It only has numbers of property owners, I would be curious to look at ownership rates, although some of this has been reported on before using IMDB data.\nWhat we can look at is multiple ownership rates, that is how many owners in each immigration category own multiple properties.\n\nWhile this does not speak to the overall ownership rates of each category, it does tell use the the share of owners that own multiple properties for immigrants is on par with that for non-immigrants in British Columbia and Ontario, with recent immigrants showing lower rates of multiple ownership."
  },
  {
    "objectID": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#upshot",
    "href": "posts/2019-06-18-some-notes-on-investor-and-corporate-ownership-of-residential-properties/index.html#upshot",
    "title": "Some notes on investor and corporate ownership of residential properties",
    "section": "Upshot",
    "text": "Upshot\nThat’s a wrap for now. There are still some tables that we haven’t explored in this post, but we encourage anyone interested to grab the code and have a go at it."
  },
  {
    "objectID": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html",
    "href": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html",
    "title": "Simple Metrics for Deciding if You Have Enough Housing",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWhat are the best metrics for understanding if a given place has enough housing, just the right amount, or too much? Whether you’re a potential renter or buyer or an analyst or policymaker, the answer really depends on what you’re looking for.\nFor potential renters and buyers, if you can’t find what you’re looking for and/or it’s not in your price range, then there’s not enough housing. If you can find it, then there’s just the right amount. When is there too much housing? Mostly if you’re already comfortably housed, but concerned about changes to your neighbourhood and/or you’re looking to maximize the price you can get for selling your housing. So we can root a set of foundational answers to questions about housing supply in peoples’ direct experiences interacting with the housing market. We can also extend this to non-market housing. If there are people on the waitlist, there’s not enough non-market housing (note: there are ALWAYS people on the waitlist and we definitely need more non-market housing).\nBut decisions about whether we have enough housing aren’t actually left to people interacting directly with housing markets. Most people can’t add much to the supply of housing by themselves. Housing has become exceptionally technical, and a vast slew of regulations now prevent most self-building except in informal sectors (in Vancouver most notably the subdivision of existing dwellings into suites, only a minority of which comply with building codes and have a permit). Instead most decisions about how much housing we have are produced via a combination of developers working through their financial models in conjunction with planners, regulators, and politicians working with tight existing constraints on what can be built where. Interestingly, both the comfortably housed and those looking to maximize their prices for selling housing DO get a voice. Why? They tend to be the ones electing (and speaking directly to) local politicians. This group notably includes local developers, who are both actively engaged in maximizing the prices they can get for selling housing and actively engaged in local politics (if you think market developers are unambiguously pro-supply, think again).\nSo how do we know if we have enough housing in a given place? Or, since the answer always depends upon the perspective, how do we hear from potential residents (including renters and buyers) about whether THEY have enough housing? Their voices are the ones that tend to get left out of debates. Usually, to the extent their voices are heard at all, it’s through some set of metrics informing decision-makers. So let’s return to metrics, because different metrics tell us different things!\nIdeally decision-makers consider metrics with specific goals in mind: do we have enough housing in a given place for what purpose? Are we interested in enough housing to meet demand, preserve affordability, or address need? Enough to promote the right kind of growth? Enough to support transit, reduce greenhouse gas emissions, promote urban vitality? Or perhaps we’re worried about too much housing to support our preferred sales price, keep out the wrong kind of people, preserve our favourite aesthetic, maintain green space, or just generally keep our neighbourhood the way we like it? Being clear about these goals is helpful, insofar as they set the criteria for which metrics can provide meaningful answers. If we can decide on our criteria, then we still have to figure out the right metric. Let’s start by looking at the four common elements that make up most metrics:\nThese are the things we tend to track with our metrics for whether or not we have enough housing, just the right amount, or too much. Dwellings are housing. If we want to figure out if we have enough, then we definitely need to keep track of dwellings. Of note, dwellings can also be differentiated by square footage, number of bedrooms, and related characteristics. Money is an expression of desire, weighted by wealth and/or income (and hence also inherently unequal). People are bodies, variously disposed to live together and share space. Both money and people move around, unlike most dwellings, which are fixed in place. Land is how we fix dwellings in place, and can support various numbers of dwellings. By virtue of fixing dwellings in place, land also defines various kinds of places we might be concerned about: e.g. neighbourhoods, cities, and metropolitan areas. Places are connected to one another: what happens Downtown has an impact on nearby neighbourhoods (e.g. Kitsilano), just as what happens in the City of Vancouver has effects on what happens in the City of Surrey. As a result, metrics should pay careful attention both to place of interest and interconnection between places. In the background, fitting these elements together, we also want to keep in mind that time matters to how we construct metrics.\nThe key metrics we tend to track often involve just two of the elements above, measured at varying scales of aggregation, places, and times. We can provide a quick and dirty guide to the different questions answered by the key metrics we use to measure if we have enough housing or too much as well as the underlying logistical mechanism guiding our inquiries.\nHow we define elements matters to how the metrics work, as does how we incorporate time and the level of aggregation (individuals, households, census tracts, cities, metro areas). We’ll keep coming back to these throughout, often with reference to examples from Vancouver, the metro area we know best, but it’s helpful to start by keeping things simple."
  },
  {
    "objectID": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#money-per-dwelling-a.k.a.-price",
    "href": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#money-per-dwelling-a.k.a.-price",
    "title": "Simple Metrics for Deciding if You Have Enough Housing",
    "section": "Money per Dwelling (a.k.a. price)",
    "text": "Money per Dwelling (a.k.a. price)\nPerhaps the most obvious way to bring these elements together is by asking how much dwellings cost. Given the persistence of market allocation for housing, there will always be enough housing to meet demand… at some price. That’s because the price mechanism sets prices at where demand curves and supply curves meet. Put differently, the demand for $1 dwellings is practically limitless. The demand for $100 million dwellings is practically zero (so far). In between, there’s a demand curve specifying how many dwellings would sell at what price. On the supply side, self-interested owners would rarely sell dwellings if they could only sell them for $1. But they’d probably sell as many as they could get away with if they could sell them for $100 million. In between there’s a supply curve specifying how many dwellings will be sold at what price. The market pricing mechanism moves prices toward equilibrium where demand and supply curves meet. This is the stuff of basic economic analysis (brought to you by a mathematician and a sociologist).\nHow about if you don’t just want to meet demand, but you want to meet it at a particular price? Maybe you want the market to meet a certain affordability threshold for a certain kind of dwelling? Let’s define this better: do we have enough housing if we want the average two bedroom dwelling priced at $250,000? In some places (e.g. Edmonton), this isn’t far off the mark. There’s enough housing there relative to demand that two bedroom dwellings sell for about $250,000. In other places (e.g. Vancouver), there’s not enough two bedroom dwellings to go around to everyone who might want them at that price, so they’re bid up to a far higher price. It would take the addition of a lot more dwellings to bring prices down to $250,000. So if that’s where you want prices to go, then there is definitely not enough housing.\nThe same general dynamics apply to the market pricing mechanism for apartment rents. Landlords respond to their understanding of local supply and demand when setting their asking rents. The longer their apartments stay on the market without being rented, the more likely they are to lower their asking rents accordingly. Vacancy rates measure the supply of apartments for rent. Correspondingly, the negative correlation between vacancy rates and rent change is very strong. As vacancy rates go up, rents come down. Here’s a comparison by metropolitan area in Canada.\n\nSay you want to ensure average rents for two bedroom apartments are affordable, at about $1,200/mo (again, around the rent level of Edmonton, vacancy rate around 5%). The takeaway from the above would appear to be that if you want to lower rents to this level in a market like Metro Vancouver (average rent @ $1,650, asking rents much higher, vacancy rate around 1%), then you need to ensure that a lot more two bedroom apartments come on the market to rent. In short, you don’t have enough housing.\nExactly how many two bedroom apartments would you need to add to bring average two bedroom rents down to $1,200/mo in Vancouver? This is a tricky (and worthy) question to answer. It would require knowing the shape of the demand curve (made up by knowing how many apartments would be rented at each rent from, say $1/mo to $1 million/mo). It would be difficult to figure this out, even if we could ask everyone in Vancouver what rent they’d be willing to pay for a two bedroom apartment. Why? Two reasons: 1) at lower rent points, some people might be willing to pay for multiple two bedroom apartments (rich people do all kinds of odd things, and when we use price as our metric, the whims of the wealthy matter more than the needs of the poor); 2) we should almost certainly assume that there are a lot of people living outside of Vancouver (including former residents) who would love to move here if they could find a two bedroom apartment for $1,200/mo. They only get a vote in how much housing gets built through their influence on the demand curve. Otherwise they don’t get heard at all. So it’s difficult to tell just how many two bedroom apartments we would need to add to bring Metro Vancouver rents down to $1,200/mo.\nAnother way to set a metric is to set an ideal vacancy rate instead of a specific rent. Vacancy rate targeting was explicitly mentioned by several candidates in the last City of Vancouver civic election. Inflation-adjusted rents tend to fall when vacancy rates rise above 3%. Setting a vacancy rate target of 4% or 5% will work to deflate rents.\nIn general, if your goal in asking if a place has enough housing is to preserve the affordability of market housing, then prices (or rental vacancy rates) should be your metric. If prices are higher (or lower) then you want them to be, then you should work to add to (or reduce) the supply of housing accordingly.\nBut how do we add to the supply of housing? Generally the most important way to add supply is to build more housing. It’s what builders do. But it’s worth noting that if they want to build more housing, builders get stuck in the middle of even more demand and supply curves. Labour, materials, and (most variably) land all influence the costs of constructing new housing. Just like buyers and sellers in the housing market, builders also watch price signals, and they tend to build when they think they can sell the housing they construct for a significantly higher price than they pay to purchase labour, materials, and land, with the difference equal to profit. The Minimum Profitable Production Cost (MPPC), or the minimum cost to bring a new unit to market, sets a hard cap on when builders have any incentive at all to try and add housing. As a result, it also provides a lower bound on the price of new market housing. And this minimum cost rises as density increases and construction becomes more involved and expensive (the minimum profitable production cost of new rental housing in Vancouver is currently too high for market developers to offer new two bedroom apartments at $1,200 market rents). Not surprisingly, holding other characteristics constant, new housing always tends to be more expensive than old housing. As a result, when you compare new housing to old housing, it might seem like new housing is doing nothing at all to bring down prices. But when you consider that building new housing is the primary way of adding more dwellings to the market overall then you get how new housing might “soak up” some of the demand in a given market, thereby lowering the prices of older housing from where they’d otherwise be and bringing down prices overall. Of course, building new housing only adds to the total housing market to the extent that you build more new housing than you demolish, a point to which we’ll return below.\nAside from demolitions, how would one reduce the supply of housing? Generally speaking, we seldom see demolitions exceed new construction, so this doesn’t happen much. But there are a few examples we can talk through, perhaps most prominently AirBnB. In response to new profit-making incentives of AirBnB, many property owners have removed dwellings from the long-term rental market into the short-term, hotel-style market (these markets once weren’t so distinct, but they have become so over time with the passage of laws like BC’s Residential Tenancy Act). As dwellings get removed from the long-term rental market, it drives down vacancy rates and correspondingly drives up asking rents for those units remaining.\nWhat else matters? Location, location, location. Additions and subtractions from the supply of dwellings for sale or rent don’t just have local effects. Their effects spill over into places near and far, tied together by their fixture to land and to transportation networks. For instance, the effects of building and renting out a bunch of new housing in Downtown Vancouver may be felt in asking rents in suburban Surrey. The degree to which additions of housing in one place affect rents in another is heavily dependent upon how long it takes and how much it costs to travel between them as well as to job centres and amenities. That said, some observers suggest that hyper-local “induced demand” may come in to play, meaning that new construction in Downtown Vancouver could potentially drop asking rents in suburban Surrey more than asking rents Downtown. The evidence gathered to date suggests this likely doesn’t happen much, but certainly the scale of the metric matters when thinking about how the addition of new supply affects prices and rents.\nSo far we’re also talking strictly about dwelling characteristics like bedrooms and size, but not about the structural type of dwellings. We can’t add more single family homes in the inner municipalities in Vancouver, so market mechanisms are constrained in terms of reducing the rent or price when we restrict ourselves to single family homes in the inner municipalities. Being very picky on location can have similar effects. Adding condos or rental properties in the downtown peninsula is more expensive than adding them in e.g. Dunbar. Adding housing in downtown requires concrete high-rise, which is substantially more expensive than 4 or 6 storey low rise which can still add significant housing in Dunbar. Providing amenities like public spaces and libraries for a growing population is also more expensive in areas that are already denser. Given demand and various constraints, it’s quite possible that the market won’t ever be able to supply rental housing at a cost that can push rents down into the $1,200/month range (or push the sale price into the $250,000 range) for a 2 bedroom apartment in Downtown Vancouver. But Surrey seems possible. Regardless, if we want to try we have clear price signals that we’d need to add a lot more 2 bedroom apartments than we have now.\nConsidered as a class metrics for Money per Dwelling, including prices (per dwelling, per sq ft, etc.), rents (per BR, etc.), rental vacancy rates, and sales listings, represents transactional data reflecting market pricing mechanisms. Inequality is built into these measures as a reflection of how market allocation weighs the whims of the wealthy of greater importance than the desperate desires of the poor. Correspondingly, reductions in inequality make for more egalitarian housing outcomes. Given market allocation of housing, this is the class of metrics people should turn to if they’re interested in achieving or preserving affordability. They provide the clearest path for identifying if there’s enough (or too much) housing when affordability is the criteria of interest. Of course, these metrics don’t resolve the debate between those who want prices and rents to rise (home sellers and landlords) and those who want them to come down (home buyers and renters), but at least they provide a common empirical grounding."
  },
  {
    "objectID": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#people-per-dwelling-a.k.a.-residential-crowding",
    "href": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#people-per-dwelling-a.k.a.-residential-crowding",
    "title": "Simple Metrics for Deciding if You Have Enough Housing",
    "section": "People per Dwelling (a.k.a. residential crowding)",
    "text": "People per Dwelling (a.k.a. residential crowding)\nPeople per dwelling provides a different class of metrics for thinking about whether there’s enough housing, focused on residential crowding. Fundamentally these metrics ask if there are there enough dwellings to “fit” the number of people we have in a given place. Of course, this is only a potential measure of fit when houses are mostly distributed by the market. Wealthy people probably take up way more room (and rooms) than they need, while poor people more often end up stuffed together. There are two solutions to this situation: one is to ration housing, so that extra rooms are shared around. We see this only for the small proportion of our housing stock that’s non-market housing. Market housing isn’t at all rationed according to need, but instead doled out by wealth-weighted desire (money). The other solution, far more common across North America, is to outlaw too much residential crowding via maximum occupancy codes and sharing rules. This is very common, and in the absence of rationing housing according to need this tends to lead to the exclusion of poor people altogether.\nAcross most of Canada residential crowding remains low. This is especially true of those places with strong municipal regulations against crowding (e.g. fire codes and occupancy standards) and market distribution of housing. Non-urban, non-market housing, especially on First Nations reserves and in Nunavut, where rationing is more common, tends to be where we see the greatest number of people per dwelling. Here we see a real failure of investment in non-market housing to match occupancy standards observed elsewhere, though differences in family sizes and cultural openness to different rules for living together also play a role.\n\nWhile crude aggregate crowding metrics can help reveal the lack of housing across reservations and Northern territories, they don’t tell us much about differences between metropolitan areas, which stick together in a relatively narrow range between two to three people per dwelling. The narrow range reflects how crowding is both generally outlawed and also discouraged by market mechanisms distributing the vast majority of housing (above). We also know residential crowding is on the decline in most places, resulting from long-term declines in childbearing, family size, and tolerance for living together combined with the general rise of affluence, occupancy standards and enforcement. Correspondingly, crude aggregate crowding metrics should probably not be used to answer questions about whether metros or municipalities have enough housing. They don’t tell us much.\nDespite their problematic nature, people per dwelling metrics are commonly used to answer questions for which they’re not suited. Several municipal planners and even a couple of academics have used new persons (or new households) per new dwelling as a metric for whether a place is adding enough housing. Given constraints on crowding and market mechanisms, this is equivalent to asking whether housing supply is meeting demand (as above). Of course it is! By definition, local housing is ALWAYS meeting demand (at some price). Similarly, by definition if you count all of the housed people added and all of the new housing added in a given location, there will always appear to be enough housing added to house everyone (at some level of crowding). After all, only housed people are counted, meaning only the net “winners” able to out-compete others for the dwellings being offered by the market. Net “losers” not provided housing by the market don’t get counted at all! Put differently, if price metrics weigh the whims of the wealthy too high relative to the needs of the poor (a valid critique), then crowding metrics ignore everyone without local housing entirely: all the people who want to live in a place but are prevented from finding housing there don’t get a vote.\nContributing to this fundamental problem, net housing additions are also often poorly counted, either because of changing census methods or failure to combine completions data with demolitions data. This has proven a particular problem for analyses that take for granted how people distribute themselves into households and simply compare new households to new dwellings, taking the leftover number of new dwellings as “empty” excess (in this case, the number of net new housed households can never exceed the number of net new dwellings except in cases where there were previous “empty” dwellings). Given the myriad of problems involved, crude aggregate measures of new persons or new household per new dwelling are especially poor metrics for determining if metro areas or municipalities are building enough. The answer they provide, by default, is practically always “yes.” For similar reasons, reinterpreting past census counts into population projections as the basis for how much housing development to allow is backwards. In high demand places, the availability of housing limits population growth rather than the other way around. Planners and academics should stop using metrics that count only local winners as answers to whether we’re building enough housing.\nWhat about more refined measurements of crowding at different levels of analysis? These are often worthwhile to consider. Given a few strong assumptions about the privacy needs of people while they sleep (practically the least interesting activity they undertake), residential crowding can be measured in terms of bedrooms rather than simply dwellings. Measured at the household level, we can get a sense of how many households are living in dwellings that force more than two people to share a bedroom. We can come up with even more elaborate rules, as in the Canadian National Occupancy Standard, where we assume people need one bedroom per sleeper, but we allow couples to share with each other, and kids to share with other kids (below age 6) and other kids of the same gender (below age 18). Applying these rules more clearly demonstrates the residential crowding on First Nations and in Nunavut. But once again, the metric tells us little about most municipal and metropolitan variation.\nWe can also refine measures to explore residential sharing at particular ages. When do children leave home? It might be that adult children remaining living with their parents is a sign of need for more dwellings. This is tenuous as an indicator (some children want to stay home, others do not), but interesting!\n\nWe can also count individuals without dwellings. This is a form of mismatch. Given the current distribution of housing, how many people are going without? Homeless counts offer an important signal about whether there’s enough housing: if we can count people who are homeless, then there is not enough housing. But this is a broader problem with inequality. Bringing more housing to market may not solve the problem, especially since the demand for housing isn’t just local, and the whims of the wealthy will continue to outweigh the needs of the homeless. Homeless counts are an especially good signal of the need for more non-market housing. Of course, another good signal of the need for non-market housing are the waitlists for cooperative, subsidized and supportive housing. Effectively, both homeless counts and non-market housing waitlists register urgent local needs not being met by the market distribution of housing. That said, homeless counts and waitlists suffer some of the same problems as other crowding metrics insofar as they only tend to record housing need that’s already in a given locale. But people fall in and out of need and they also move. The dire needs of refugees in tent camps tens or thousands of miles away do not get considered, even if those refugees might eventually show up in a municipality. As a result, there remain difficulties in determining just how much need to meet: there are probably no ethically satisfactory stopping points. And even if there were, under rationing systems of all sorts, housing waitlists can grow to enormous lengths. As with attempts to preserve market affordability, we can know we need to build a lot more non-market housing without necessarily knowing when (or if) we should stop.\nFinally, returning to the notion of “excess” dwellings, we can also count dwellings without people in them. This is ultimately a bad measure of whether there’s enough housing without a) greater knowledge of the reasons why units appear to be empty and without b) a corresponding will to expropriate “bad” empty units and ration them out according to need. Speaking to the first point, if dwellings register as “vacant” and available to the market (e.g. rental vacancies or unoccupied sales listings), then these dwellings will help reduce prices (see above). If they’re not on the market, they may reflect development processes (pre-demolition or recently constructed dwellings) working toward adding more housing. A variety of other procedural transitions (deaths, inheritances, etc.) may also account for dwellings without people in them before we get to second “vacation” residences (whims of the wealthy, etc.), and alternative uses (AirBnBs, etc.). To the extent these kinds of unoccupied dwellings are rising, they may result in reductions to the market supply of housing, pushing up prices for dwellings that remain. Finally, keeping housing empty and off the market may result from attempts to reduce transaction costs and/or speculatively manipulate market pricing. This is of greatest concern from the standpoint of maintaining market stability and affordability. The diversity of reasons that dwellings might show up as unoccupied means that, by itself, keeping track of unoccupied or empty dwellings is probably a bad measure of whether the market is building enough housing. After all, empty units may be adding to supply or detracting from supply, with varying affects on affordability, depending upon whether they’re on the market. That said, like homeless counts, “empty home” counts can be useful as an indicator of how the market is working to match people to dwellings (given underlying and unmeasured inequality). Moreover, empty homes can be bad in their own right, potentially deadening neighbourhoods. A Lincoln Institute report defines thresholds at which vacancy becomes a problem, with “low” vacancy (a problem for facilitating moves) below 4%, “reasonable” vacancy between 4%-8%, and high vacancies at 8%-20%. “Hypervacancy” (20% or more) poses special problems, especially in the case of declining cities. All major Canadian metro areas fit in the “reasonable range.”\n\nBut in high demand cities, lots of empty homes can point toward the desirability of higher property taxes, potentially including Empty Homes Taxes, which can distinguish between types of vacancies and induce owners of empty units and second homes to more quickly return them to market, boosting supply and lowering prices. This will reduce the profitability of any speculative market manipulation. But of course another response to that kind of manipulation is to add more dwellings and credibly promise to keep adding dwellings, placing pressure on prices and rents to lower over time and make speculation unprofitable."
  },
  {
    "objectID": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#dwellings-per-land-a.k.a.-dwelling-density",
    "href": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#dwellings-per-land-a.k.a.-dwelling-density",
    "title": "Simple Metrics for Deciding if You Have Enough Housing",
    "section": "Dwellings per Land (a.k.a. dwelling density)",
    "text": "Dwellings per Land (a.k.a. dwelling density)\nDwellings per unit of land as a class of metrics measures dwelling density, constituting yet a different aspect of whether there’s enough (or too much) housing in a given place. This class of metrics has important implications for urban dynamism and environmental impact. It also has potential effects on for parking, noise, and the preferred aesthetics of many neighbourhood organizers. Dwellings per unit of land is often measured as dwellings per acre or hectare. Beyond definitional issues, there are tricky aspects to measuring this, insofar as both the areal unit (lot, block, neighbourhood, municipality, metro area) and what gets counted as potential land for dwellings (in the denominator) really matters. If we’re interested in housing density, should one count only land allowing dwellings? What about streets? Or other land uses, like industrial parks? What about recreational parks? Schools? Subtracting out streetscapes makes a big difference, and when other features fall within small areal units, like blocks, they can really affect measures of housing density, making a block with a park look much less than dense than the block next door, even if both are made up of entirely the same kind of housing. Counting only land allowing dwellings constitutes “net housing density” while counting all land and uses constitutes “gross housing density.”\nOverall it’s worth noting that this class of metrics is also a bit of a dodge, since often what we’re really interested is people per unit of land, better known as population density. After all more people in a given place constitute more potential interactants in public spaces, more likely transit riders, more shares of infrastructure, and more possible “eyes on the street.” More people also constitute more potential competition for parking and services. People sharing space are also often understood to be poor and potentially dangerous, bringing down property values. So debates over housing density as a class of metrics are often really about how many people should be encouraged or tolerated in a given place. But the regulatory powers of cities are stronger over buildings than bodies, so the focus often ends up being on dwelling density rather than population density. Aside from population density, dwellings per unit of land can have independent effects on the aesthetic “character” of neighbourhoods, as expressed by many peoples’ aversions to high-rises. As noted above, we can, more or less, substitute between population density and housing density just by dividing population density by average household size. This doesn’t always work, insofar as denser housing tends to hold smaller households, but it still gives us a rough translation. We can even figure in unoccupied dwellings if we want, which would give us an overall standard of about 2.34 people per dwelling in Metro Vancover. Alternatively, instead of measuring dwellings per acre, we could measure bedrooms per acre. Bedrooms relate more closely to population than dwellings, and are often similarly regulated by cities. \nIn terms of impact, housing density (or dwellings per acre) has been linked to urban vitality. Jane Jacobs famously set a few thresholds for what she considered suburban (six or fewer dwellings per acre) and truly urban (one hundred or more dwellings per acre). She considered “in-between densities” as less conducive to the “lively diversity and public life” of the city. Needless to say, the vast majority of the landscape of North American cities fall in Jacobs’ “in-between” ranges, “fit, generally, for nothing but trouble.” Outside of Downtown and a few other scattered census tracts, the same is also true of Metro Vancouver. Where the best threshold for urban vitality might be located remains a matter for debate.\n\nSimilar thresholds have been suggested for what kind of densities can support urban transit. Commonly cited thresholds suggest about 12 dwellings per acre around a large central business district is enough to support a decent urban transit system. Guerra & Cervero provide more careful updates on this estimate, exploring capital costs in conjunction with what can be supported by population and jobs located near stations. Using their estimates, a project like Vancouver’s forthcoming skytrain extension along Broadway, at a capital cost of nearly $500/km2 CAD (nearly $600/sq mile USD), would require over 120 people per acre gross population density to support, or more than 50 dwellings per acre near skytrain stations.\nGenerally speaking, higher dwelling densities enable more transit viability, encourage people to get out of their cars (when coupled with jobs and commercial destinations), promote lower energy useage and generally support transitions to more sustainable cities. But higher dwelling densities also challenge some peoples’ conceptions of what they want their neighbourhoods to look like and how many people they want to compete with for parking. Moreover, higher dwelling densities tend to be forbidden on the vast majority of North America’s urban land base. Why? Zoning.\nMost residential land, including in the City of Vancouver and surrounding suburbs, is zoned to support single-family residential character. At its strictest, single-family zoning insures only one dwelling can be built per lot, and in some cases minimum lot sizes can be enormous. Dwellings are often rationed out according to quite draconian land use rules. Even on the relatively modest 33’ x 122’ standard residential lots that make up a large part of Vancouver’s urban landscape, a single dwelling per lot standard nets only about 10 dwellings per residential acre. Initiatives to add and legalize secondary suites, laneway houses, and most recently duplexes (with secondary suites) means that the actual range of legal dwellings per lot on most single-family zoned land in the City of Vancouver can get all the way up to 40 dwellings per acre. Not bad, but nowhere near the densities supportive of urban vitality or skytrains.\n On the other hand, a 33’ x 122’ lot located within a commercial zone in Vancouver is allowed greater dwelling density and the ability to build out to lot lines. Even under the same broad height restrictions applied to single-family zoning, twelve dwellings can easily be fit into a given lot while retaining a central courtyard, achieving a dwelling density of about 120 dwellings per residential acre, like this low-rise apartment building in a C-2 (where one of the co-authors of this post lived when he first moved to Vancouver). This moves solidly into Jane Jacobs & heavy transit supportive territory, though the difference between net density and gross density suggests we’re still not quite there yet."
  },
  {
    "objectID": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#setting-rules-to-metrics",
    "href": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#setting-rules-to-metrics",
    "title": "Simple Metrics for Deciding if You Have Enough Housing",
    "section": "Setting Rules to Metrics",
    "text": "Setting Rules to Metrics\nA lot of the metrics we describe above are set into rules (e.g. by-laws, policies, etc.) for regulating cities. In particular: zoning by-laws often set hard limits to dwelling density (dwellings per land) and maximum square footage (Floor Space Ratios) for given lots. The metrics embedded in our zoning effectively mean that we’re rationing out how many dwellings we allow per land parcel. Through the sharing rules embedded in our occupancy standards, we’re also disallowing most residential crowding. But after we apply these rationing and sharing rules to structure housing production and occupancy, we switch to the market in terms of how we develop and distribute most housing. In high demand locations, the net result of these general policies is construction for rich people and the gradual exclusion of poor people. Their dire needs in the market weigh as less important than the whims of the wealthy. Since poor people are also prevented from sharing existing dwellings in high concentrations, they can’t even get a foot in the door, and don’t show up in crowding metrics at all.\nWhile some rules set to metrics are built to be responsive and flexible, automatically adjusting to conditions (e.g. setting rent control to inflation, and setting below-market rates at a set discount from market rates), others require lengthy hearings and political debates to change (changing zoning). As presently configured, debates about dwelling density largely exclude everyone not currently living in our cities. Indeed, this is one reason legislators in places like California and Oregon have moved to erode the power of municipalities to exclude development near transit hubs. They want to give potential renters and buyers a bigger say in whether we have enough housing by allowing them to speak through the demand curve, encouraging developers to build more housing in these places. To date the political process hasn’t let them get away with much, which ironically insures that developers profit hansomely from the scarcity of new housing being added to the market. In a high demand place like Vancouver, this means that in the long term, rents and prices tend to just keep going higher (though as we’re learning, in the short term prices can still swing up and down in line with speculative booms and busts, just like anywhere else!)\nIf we’re concerned about the exclusionary effects of high prices, we could reform our zoning regulations to be responsive, automatically adjusting to both transit development and market conditions (just like with rent control or the setting of below-market rents). There seems to be a lot of potential in considering this possibility. One example would be to set affordability thresholds. We could, for instance, automatically enable a rise in the number of dwellings permitted on a lot equal to one for every $250,000 in its assessed value. Once a lot hits three million in value, we could automatically enable up to twelve dwellings, looking something like the building above. Thresholds for non-market housing could be set even lower, enabling non-market developers (including the City) a competitive advantage in securing lots. Cities could also take over the production of non-market dwellings themselves, purchasing low-density lots and using their power over zoning to upzone and redevelop for the higher densities needed to support a more economically diverse population."
  },
  {
    "objectID": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#conclusion-and-preview",
    "href": "posts/2019-06-12-simple-metrics-for-deciding-if-you-have-enough-housing/index.html#conclusion-and-preview",
    "title": "Simple Metrics for Deciding if You Have Enough Housing",
    "section": "Conclusion (and Preview)",
    "text": "Conclusion (and Preview)\nOverall, there’s still lots to think through when asking if we have enough housing! But metrics can establish crucial common ground for providing answers. Stripping down our metrics to their basics helps demonstrate their utility in terms of what answers they can provide and who they give voice. Overall, price (and rent) metrics provide the best indicators of whether we have enough housing to preserve or achieve market affordability. Non-market waitlists and homeless counts provide the best indicators of local non-market housing need (though they still exclude need from elsewhere). By contrast, residential crowding metrics (people per dwelling) don’t generally tell us much in urbanized Canada, and tend to privilege the voices of those already living in a place (e.g. the “winners” in finding housing). Dwelling per land metrics point toward the limits often imposed upon getting to enough housing in a place, and potentially spell out the rewards for getting there in terms of sustainability and urban vitality.\nIn terms of underlying logics, the market distribution of housing tracked by price metrics is problematic insofar as the whims of the wealthy far outweigh the dire needs of the poor. But when we simply wave away price metrics, and pretend we’re rationing out housing by need instead (by only tracking persons per dwelling), then we’re saying we don’t care who wins for the limited amount of housing we’re willing to offer when we ration out dwellings to land. Really addressing housing need is a monumental and important task, and requires a much greater investment in non-market housing. But questions quickly arise as to how non-market housing should be rationed, and advocates should pay more attention to providing answers that don’t assume that no one ever moves.\nIn future posts on housing metrics, we’ll compare across specific measurements within the same class and dig further into more complicated metrics that combine multiple classes (e.g. price to income multiples, core housing needs, shelter & transportation cost to income rations, etc.) So consider this a preview. Rest assured we’ll keep playing around with metrics!"
  },
  {
    "objectID": "posts/2019-06-04-multi-census-tongfen/index.html",
    "href": "posts/2019-06-04-multi-census-tongfen/index.html",
    "title": "Multi-Census Tongfen",
    "section": "",
    "text": "Two days ago I gave an example using the new (to CensusMapper) 2001 census data to mix with 2006 data on a common geography based on dissemination areas. A question came up if this works for several censuses, not just for two. Yes, the TongFen package was built with exactly that in mind. Time for a quick demo.\nFor this we will look at the households spending between 30% and 100% of income on housing in the City of Toronto. To grab the relevant variables we query all available CensusMapper datasets and query each for all census variables with the term “30%” in the description and grab all “parent” and “child” variables (wrt the CensusMapper variable hierarchies) for good measure.\nTo start off, we take a look at the summary statistics for the City of Toronto. No TongFen needed here, the geography has not changed since 2001.\nThe high-level numbers suggest a slight worsening, that is an increasing share of shelter-cost burdened households. If we are interested in a finer breakdown of the geographic distribution, we need to TongFen the data to CTs or DAs."
  },
  {
    "objectID": "posts/2019-06-04-multi-census-tongfen/index.html#ct-level-data",
    "href": "posts/2019-06-04-multi-census-tongfen/index.html#ct-level-data",
    "title": "Multi-Census Tongfen",
    "section": "CT level data",
    "text": "CT level data\nTo get CT level data for all four censues on a common tiling we just need to make the appropriate call, tongfen will take care of the rest.\ndata_ct &lt;- get_tongfen_census_ct(regions=regions,vectors = all_vectors$vector,geo_format = 'sf') \nWith the data at hand, we can easily take a look at the geographic distribution. Here we have encapsulated the computation of the shares in the compute_shares function call, details are in the code.\nplot_data &lt;- data_ct %&gt;%\n  compute_shares\n\nggplot(plot_data) + \n  geom_sf(aes(fill=share_d),size=0.1) +\n  facet_wrap(\"Year\") +\n  share_theme +\n  labs(title=\"Spending 30% to 100% of income on shelter\")\n\nThis gives us a good overview over where shelter-cost burdened households were located in Toronto in each year. We can also see some changes over time, but to really understand the changes it is easier to plot them directly.\n\nThere is significant variation on how the shares changed during those years. Part of that volatility is possibly an indication that households cluster around the 30% cutoff and thus can easily slide across that line. Another issue is data quality for the 2011 NHS, one should probably view changes involving 2011 data with a bit of caution. We have observed before that while CT level income estimates were better than their reputation, this did not translate to good individual level income to shelter cost estimates. Imputation of aggregate income numbers using CRA taxfiler data is reasonably easy, but imputation of individual level incomes to join with shelter cost data is hard. The long-term 2001 to 2016 estimate in the bottom right is probably the one that should receive most of the attention."
  },
  {
    "objectID": "posts/2019-06-04-multi-census-tongfen/index.html#da-level-data",
    "href": "posts/2019-06-04-multi-census-tongfen/index.html#da-level-data",
    "title": "Multi-Census Tongfen",
    "section": "DA level data",
    "text": "DA level data\nIf we want even finer resolution we can take this down to dissemination area level. Here we pass an optional “transform” function to transform the initial census data before the TongFen process. This allows us to convert NA values for areas without households or population to zeros, eliminating areas for which we can’t report data just because they contain subareas without population.\ndata_da &lt;- get_tongfen_census_da(regions=regions,vectors = all_vectors$vector,\n                                 geo_format = 'sf',na.rm=FALSE,\n                                 census_data_transform=census_transform)\n\nplot_data &lt;- data_da %&gt;%\n  compute_shares\n\nggplot(plot_data) + \n  geom_sf(aes(fill=share_d),size=0.01) +\n  share_theme +\n  facet_wrap(\"Year\") +\n  labs(title=\"Spending 30% to 100% of income on shelter\")\n\nAnd of course, we can also compute the changes at the DA level."
  },
  {
    "objectID": "posts/2019-06-04-multi-census-tongfen/index.html#uncertainty",
    "href": "posts/2019-06-04-multi-census-tongfen/index.html#uncertainty",
    "title": "Multi-Census Tongfen",
    "section": "Uncertainty",
    "text": "Uncertainty\nHowever, here we start to run into problems. Changes across small geographic areas are prone to errors. Statistical rounding and biases in the census data really start to matter for some of the areas. In the past we have dealt with this using “Surprise maps” that make a model assumption, for example “no change in share of shelter-cost burdened households”, and colour regions based on their (signed) statistical evidence against our model assumption. Areas with low household counts would provide little statistical evidence against our model assumption, similar to areas with little change but higher numbers.\nBut it can also be instructional to separate these two factors with a bivariate scale, where we fade out the colours as we lose confidence in our estimates.\n\nThe expected error is the expected percentage point error of the estimate due to statistical rounding. One can refine this by e.g. folding in non-return rates to estimate the uncertainty due to this, and one can tune the colour scale to the specific purpose of the map."
  },
  {
    "objectID": "posts/2019-06-04-multi-census-tongfen/index.html#summary",
    "href": "posts/2019-06-04-multi-census-tongfen/index.html#summary",
    "title": "Multi-Census Tongfen",
    "section": "Summary",
    "text": "Summary\nIn summary, with TongFen it’s straight forward to compare census data at the CT or DA level across the four censuses 2001 through 2016 that we have made available via CensusMapper. Especially at the DA level we need to be mindful of limitations to census data as quirks in the data start to distort our results.\nAs usual, the full code for this post is available on GitHub for those interested in replicating or adapt this for their own purposes."
  },
  {
    "objectID": "posts/2019-04-24-population-weighted-densities/index.html",
    "href": "posts/2019-04-24-population-weighted-densities/index.html",
    "title": "Population weighted densities",
    "section": "",
    "text": "We are big fans of measuring different densities, and conceptualizing density in different ways. From tax density, tax density in 3D, plus an animated version, lot level density of single detached homes over time, estimating FSR from LIDAR data, density treemaps, dot-density maps, comparing Vancouver and Vieanna densities, building height profiles, renter density and net dwelling density, city density patterns and city density timelines.\nWhen I saw the following tweet and linked blog post, I of course could not resist to reproduce some of the graphs and explore population-weighted densities.\nIt links to a great post which you should definitely read. In this post we will take up the idea of population weighted densities and again employ the global GHS population grid to to compute these densities."
  },
  {
    "objectID": "posts/2019-04-24-population-weighted-densities/index.html#population-weighted-densities",
    "href": "posts/2019-04-24-population-weighted-densities/index.html#population-weighted-densities",
    "title": "Population weighted densities",
    "section": "Population weighted densities",
    "text": "Population weighted densities\nHaving population density maps is great if one wants a lot of detail about a specific location. But it does not really speak to how people experience density. For that the treemaps we used before can be helpful in that we can use them to visualize what portion of the population lives in what kind of density. But that still makes it hard to compare density patterns across cities. If we want to condense this down into a single number, population-weighted density can be useful. Regular population density computes the average number of people per unit area. Population weighted density shows the density the average (in some sense) person lives in.\nTo keep things simple, we will ignore administrative boundaries and take 30km radii around regional centres to compute this. One advantage of population weighted densities is that sparsely population regions that get caught in the 30km radius barely factor into the total, since only few people live there.\nWe will again use the GHS data, and because we have been using it so much lately we refactored it into a (fairly messy) R package.\n\nThe numbers differ from the ones computed by Charting Transport, partially because we have chosen a different way to delineate cities, using 30km radii instead of administrative boundaries, and because data sources differ somewhat. Also, we used a 250m grid instead of the 1km grid used for some of the cities in the blog post. Population weighted densities do depend on the grid size, especially in cities with stark variations in population density as is the case for example in cities near the ocean where population density drops off from often high number along the shore to zero in the ocean.\nThe GHS data is available for 1975, 1990, 2000 and 2015. The dates are just approximate, with exact dates varying a bit between cities depending on the data sources used to construct the population grid. It should be interesting to see how densities evolved over time, and in particular how this change compares across cities.\nNo better way to do this than with one of these animated bar charts that made the rounds recently.\n\n\n\npopulation_weighted_densities\n\n\nHanoi really stands out in the graph with a huge population drop in the first period right at the end and after the war. Let’s take a look in more detail to understand what is going on, using the (unweighted) density timelines we built before.\n\nThis shows that changes in density can have many reasons, and high-line numbers don’t capture all of them well. But even if we keep the built up area fixed, densities tend to decline. As people get richer, they tend to consume more housing. Both because household sizes fall and because the sizes of dwellings grow. People don’t bunk up as much and family sizes drop. In low density areas people add extensions or build larger houses, in high density ares people combine neighbouring apartments and new apartment buildings tend to have larger units. This can be counter-acted by increasing gross floor area, so the building intensity."
  },
  {
    "objectID": "posts/2019-04-24-population-weighted-densities/index.html#grid-size-sensitivity",
    "href": "posts/2019-04-24-population-weighted-densities/index.html#grid-size-sensitivity",
    "title": "Population weighted densities",
    "section": "Grid size sensitivity",
    "text": "Grid size sensitivity\nAnother note is that population weighted densities can be quite sensitive to the grid size used. We have been using a 250m grid. That means if there is spiky high-density housing nestled in between water, mountains, and commercial areas, it will pick up those very high population densities. If we instead smooth things out a bit, for example by using a 1km grid, that density will wash out. This is particularly visible in the case of Hong Kong, for comparison here is the corresponding graph using a 1km grid.\n\n\n\npopulation_weighted_densities_1k"
  },
  {
    "objectID": "posts/2019-04-24-population-weighted-densities/index.html#us-cities",
    "href": "posts/2019-04-24-population-weighted-densities/index.html#us-cities",
    "title": "Population weighted densities",
    "section": "US Cities",
    "text": "US Cities\nLet’s focus in one one country, the US for example, and see how population weighted density evolved over time. We simply take the 40 most populous cities and turn the crank.\n\n\n\npopulation_weighted_densities\n\n\nThree things immediately stand out. The absolute dominance of New York in terms of density, no other US city comes even close. Looking at the other cities, we see little overall movement, but some changing of ranks. New Orleans changes from number 2 right after New York down to almost the bottom quarter. And Las Vegas rises from second to the bottom almost into the top quarter."
  },
  {
    "objectID": "posts/2019-04-24-population-weighted-densities/index.html#next-steps",
    "href": "posts/2019-04-24-population-weighted-densities/index.html#next-steps",
    "title": "Population weighted densities",
    "section": "Next steps",
    "text": "Next steps\nThat’s it for now, as usual the code is available on GitHub in case anyone is interested in seeing the nuts and bolts or wants to adapt this for their own purposes. Fair warning, the first time the code runs it will download a couple of gigabytes of GHS data."
  },
  {
    "objectID": "posts/2019-04-15-vsb-x-boundary/index.html",
    "href": "posts/2019-04-15-vsb-x-boundary/index.html",
    "title": "VSB X-Boundary",
    "section": "",
    "text": "District wide enrolment in VSB schools has been on a steady decline for over a decade. At the same time there are areas within the VSB that have seen strong growth in children requiring new schools to get built.\nLooking at a couple of time series for the VSB District we can see where the problem lies. The children aged 5-17 living in the VSB District is estimated by BC Stats based on a number of data sources, and the number has been declining over the years. We can see a similar decline in provincial (projection) data, which lists actual numbers for past years. The provincial enrolment data is somewhat lower than the VSB projections that reflect actual enrolment for the current year, we are not sure what causes this. Possible explanations are that provincial enrolment counts don’t count children below school age, and we are not sure if they count international students.\nThe most interesting feature is the very different trends of Ministry of Education and internal VSB enrolment projections, with the former starting to turn from declining to rising around 2019, whereas the internal VSB projections show a steady decline."
  },
  {
    "objectID": "posts/2019-04-15-vsb-x-boundary/index.html#projections",
    "href": "posts/2019-04-15-vsb-x-boundary/index.html#projections",
    "title": "VSB X-Boundary",
    "section": "Projections",
    "text": "Projections\nProjections are hard, and always wrong, some more than others. In cases where different methods give significantly different results, and it is important to get fairly accurate estimates for facility planning, the usual procedure would be to read through the methodology of the diverging models to better understand what causes the discrepancy and make informed decisions. Unfortunately this is not possible in this case. While the Ministry’s projections are based on the BC Stats models, which is described in depth on the BC Stats webesite, VSB internal projections are based on private black box methods by Barager systems.\nThe BC Stats projection are publicly available for 5-year age brackets, and we show the 5-14 year old age group and we can see how the change lines up nicely with the change in enrolment projections done by the Ministry.\nWhile overall enrolment is a primary driver for the enrolment issues faced by VSB, it is not the only one. We also need to pay attention to the shifting distribution of students within VSB, which has lead to some schools overcrowding, and new schools being added, while other schools have been losing students.\nCensus data can give a good indication of the changing spatial patterns.\n\nThis shows quite clearly the problems VSB is facing, which demand for schools building up in some areas, and dropping in others.\nWe see that the number of children increased in Downtown, Olympic Village and Mount Pleasant along the Cambie Corridor, as well as UBC and pockets in Dunbar and Point Gray, and to a lesser extent in Kits. Other areas ranged from stable to very strong decreases, most pronounced in Arbutus ridge. It would be interesting to dig down further into what is driving these geographic patterns. Just taking a cursory look we see that traditional single family neighbourhoods are losing children, which we have observed before.\nWe have overlaid the catchment areas, which gives some indication which schools are struggling to attract students, and which are turning students away. To better understand why VSB had such a hard time dealing with areas experience student growth like Yaletown or Olympic Village, it’s instructional to take a closer look at one example."
  },
  {
    "objectID": "posts/2019-04-15-vsb-x-boundary/index.html#ubc-schools-a-case-study",
    "href": "posts/2019-04-15-vsb-x-boundary/index.html#ubc-schools-a-case-study",
    "title": "VSB X-Boundary",
    "section": "UBC schools, a case study",
    "text": "UBC schools, a case study\nI am most familiar with the schools near UBC, since that’s where my son goes. A little under five years ago Norma Rose Point opened, relieving a lot of the pressures on University Hill Elementary and Secondary schools. At the time, University Hill Elementary was reduced from K-7 to K-5, leading to lots of changes at the school from letting go of teachers to removal of portables that housed an after-school program. This was done based on enrolment projections done at the time. Past summer, four years after the downsizing of the school to K-5, the VSB board voted to change UHill Elementary back to K-7, based on the current enrolment realities. At the same time, the board also voted to postpone plans for the South Campus elementary school for at least a couple of years.\n The enrolment and capacity situation is summed up in the Long Range Facilities Plan on page 64 and following.\nWe see that the combined three schools in the area are operating at capacity, which is not a problem since VSB projects that enrolment will decline. UHill Secondary also has a good portion of international students that could be moved to other schools if the need arises, so there is a little more buffer than the graph suggests. However, anyone familiar with the university neighbourhoods will find the notion that enrolment will decline curious. The problem is that VSB does not take new developments like Block F into consideration, even though the developer did provide projections on the children population based on unit mix of the development and extrapolation from census counts for similar unit mix, coming up with around 170 additional children once built-out. \nAt the same time, development continues at the usual pace in South Campus, with several large multi-family buildings completing every year. This makes it really hard to understand the VSB insistence on working based on the assumption of declining future enrolment for the UBC area schools.\nAt the same time it explains well what went wrong with planning regarding Yaletown and the Olympic Village, VSB is really bad at accounting for new development. These stresses leads to increased cross-boundary traffic. Which can be quite difficult for students, especially in geographically isolated areas like the UBC schools."
  },
  {
    "objectID": "posts/2019-04-15-vsb-x-boundary/index.html#school-enrolment-patterns",
    "href": "posts/2019-04-15-vsb-x-boundary/index.html#school-enrolment-patterns",
    "title": "VSB X-Boundary",
    "section": "School enrolment patterns",
    "text": "School enrolment patterns\nThe VSB has made detailed cross-boundary data available, which gives us the ability to better understand the pressures each school faces and how children and parents work around these constraints.\nThe VSB cross-boundary data allows to split enrolment into regular and “non-regular” programs, which includes district programs and international programs. The Ministry makes data on enrolment by program type available, and mixing the two one can get a good picture of enrolment patterns in schools. One caveat is that the two datasets may not line up perfectly due to timing of when they were pulled and some students dropping out or entering into the VSB program mid-year. In our data there are four schools where the total number of enrolled students differ by one between the two datasets, which gives us some confidence that the discrepancy is not large and it makes sense to merge the data from the two different sources.\n\nThe black bars indicate the capacity of each of the schools, showing most schools operating below capacity, some by a lot.\nWe can similarly explore other within-school metrics.\n\nWhile enrolment has been the main metric employed by the VSB in school closure discussions, in-catchment population is another metric that should be of interest. The idea of neighbourhood schools is still important to many, including myself. This gets complicated by the abundance of district programs, some of which give priority to in-catchment children while others don’t. It also raises the question about Annexes, which might be best treated as extensions of the main catchment schools in terms of their capacity.\n\nLooking at all students by catchment, we see that some of the variance in the rates of attendance in catchment schools (including annexes) is explained by varying proportion of students attending out-of-catchment district programs. Moreover, some catchment schools have in-house district programs, which students in that catchment seem to prefer over travelling to out-of-catchment district programs. This speaks to in-school district programs being quite attractive to students and their parents.\nWe can investigate this further by looking at the share of students in each catchment that attend district programs.\n\nSome of the higher rates of children attending district programs can be explained by district programs in catchment schools, but that still leaves significant variation unexplained."
  },
  {
    "objectID": "posts/2019-04-15-vsb-x-boundary/index.html#cross-boundary",
    "href": "posts/2019-04-15-vsb-x-boundary/index.html#cross-boundary",
    "title": "VSB X-Boundary",
    "section": "Cross-boundary",
    "text": "Cross-boundary\nOf most interest to me are the cross-boundary movements of children. Here it is useful to distinguish movement to district programs like French Immersion from movements to regular programs.\nBefore we get started, it’s useful to first get a better overview of the cross-boundary movements. This interactive map allows one way to explore this, hovering over a school shows the origins and destinations of net flows, with the net flow arcs coloured red at their origins and green at their destinations. Net flows substantially under-state total flows. For example, there are 69 students in the University Hill Elementary catchment at attend Norma Rose Point and 64 children in the Norma Rose Point catchment that attend University Hill Elementary, resulting in a net flow of 5 students from University Hill Elementary catchment attending Norma Rose Point.\nHere we peg the origins at the “catchment schools” coloured in blue on the map, with “annex schools” in black and “district program schools” in green. For “annex schools” we chose to count students with the catchment of the associated school as “in catchment” and won’t show them as flows from the catchment school to the annex. The radio buttons allow to show all flows, or just flows to regular programs or flows to district programs. We put the origin for students coming from out of district a little to the south-west of Vancouver. We don’t have data on students within VSB that attend schools in other districts.\n\n\nView Fullscreen\nArmed with this data, we can start to answer some basic questions about cross-boundary flows. Most importantly, what is the extent of net east-west flows?\nWe use two different ways to quantify east-west flows. One is a simple metric that looks at movements relative to the eastside/westside division that to this day plays a role in Vancouver’s psyche and remains visible in demographic data.\n\nThis crude but psychologically important metric shows that while most movement is contained within each east/west side of the city, we do see stronger flows from eastside to westside than vice versa.\nNext we quantify the general tendency to move east or west, focusing exclusively on the east-west component of moves and labelling moves farther than 2.5km along that component as “far” and moves closer than 1km as “near”.\n\nThis cements the notion that there is indeed an east-to-west migration in the cross-boundary data. It is visible in district programs as well as regular programs, and most pronounced in “slight” east-west movements. Another way to look at this is to take the mean east-west person distance travelled to each school.\n\nThis helps identify which schools see the heaviest east-west traffic, and which schools see the reverse. The schools are ordered from east-most to west-most. The east-most schools can only attract cross-boundary traffic from the west, so their mean westward travel distance has to be negative. It’s remarkable how quickly schools located fairly east start to attract a net influx of westbound traffic.\nOne caveat here is that we don’t know the locations of the students within each catchment, but compute this based on the assumption that students are located at their catchment schools. However we should expect this to roughly average out in most cases."
  },
  {
    "objectID": "posts/2019-04-15-vsb-x-boundary/index.html#next-steps",
    "href": "posts/2019-04-15-vsb-x-boundary/index.html#next-steps",
    "title": "VSB X-Boundary",
    "section": "Next steps",
    "text": "Next steps\nThis post turned out a little more rambling than usual, mostly because it was written in small chunks over a period of several weeks. Big thanks to all the people helping when I tried to make sense of the data, as well as related discussions in this long twitter thread. And sincere apologies to everyone that got tagged into this for having to endure it.\nThere are lot of related questions that one could go after, but this post is already too long. We might pick this up again and drill down into related questions. Also, we only looked at elementary schools, at some point it would be good re-run this for secondary schools.\nAs usual, the code for the analysis is available on GitHub for anyone interested in looking into details or expanding on it."
  },
  {
    "objectID": "posts/2019-03-17-city-density-patterns/index.html",
    "href": "posts/2019-03-17-city-density-patterns/index.html",
    "title": "City density patterns",
    "section": "",
    "text": "I saw the tanaka package fly by on twitter, and in particular liked the application to the world population grid. Cities are interesting beasts, and I like exploring the extent of cities free from political boundaries. I am travelling right now, but I like looking at different ways to calculate and visualize density and could not resist running some inter-city density comparisons.\nFor this, we only show areas with at least 4 people per hectare (or about 1000 people per square mile, the cutoff used by US Census to designate areas as urban), and pick some population density cutoffs above that to show grades of population density. We graph the cities on a 40km radius around the city centre to get an indication of the spatial extent of the functional metropolitan area. We are using the global 250m GHS 2015 population grid and smooth the data with a Guassian kernel with standard deviation 0.5km.\nStarting out with a selection of 9 cities, with selection influenced by cities I like to compare Vancouver to, we notice stark differences in the makeup. North American San Francisco, Toronto and Seattle look quite similar, they appear stretched out and bump with medium density centres.\nThe European Vienna and Athens are compact with uniform density. Hong Kong, Taipei and Singapore have much higher density, and the 40km radius includes other cities, with Shenzhen to the north of Hong Kong, Malaysian and Indonesian cities to the north and south of Singapore, and Taoyuan to the west of Taipei showing up as separate metropolitan areas. In Taipei, the MRT, rail and HSR lines accumulate enough density to show the connection between different cities, drawing lines toward Taoyuan, as well as Tamsui to the north and Keelung to the east."
  },
  {
    "objectID": "posts/2019-03-17-city-density-patterns/index.html#more-cities",
    "href": "posts/2019-03-17-city-density-patterns/index.html#more-cities",
    "title": "City density patterns",
    "section": "More cities",
    "text": "More cities\nThe coverage of the dataset we are using is world wide, so let’s take a look at other cities around the world.\n\nThe images highlight how different the cities are arranged. European cities tend to have fairly uniform density centres, with Paris and maybe Copenhagen (and Malmö across the Øresund) showing density gradually declining away from the centre. Tehran and Casablanca seem quite European this way too. Cairo density highlights how important the Nile is to the region, with a very-high density centre and lower density areas assembled along the river and delta.\nMelbourne, Sydney and New York fit better into the North American cities we have looked at earlier, although the central part of New York does have noticeably higher density.\nThe large Asian cities of Beijing, Shanghai, Chongqing, Chengdu and Seoul have very high density centres supported by high large high density surrounding areas. Tokyo and Jakarta are different in that they are lacking the very high density spikes but have uniform high density throughout the city. Mexico City, as well as the South American cities on our list follow a similar pattern of fairly uniform high density. Cape Town looks almost North American like a smaller version of New York. Lagos has some fairly large very-high density areas, which sets it apart from the Asian giants that typically have smaller very-high density spikes."
  },
  {
    "objectID": "posts/2019-03-17-city-density-patterns/index.html#other-cities",
    "href": "posts/2019-03-17-city-density-patterns/index.html#other-cities",
    "title": "City density patterns",
    "section": "Other cities",
    "text": "Other cities\nWe have to cut somewhere, sorry if a city you are interested in did not appear. But you can always grab the code and trow in other cities you like to see."
  },
  {
    "objectID": "posts/2019-02-27-tax-speculations/index.html",
    "href": "posts/2019-02-27-tax-speculations/index.html",
    "title": "Tax Speculations",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nBC has introduced the Speculation and Vacancy Tax and instructions for filling out the declarations are in the mail. The tax targets homes in major urban centres that are left empty, or that are owned by “foreign and domestic speculators” that “don’t pay [income] taxes” in BC. The tax rate is 0.5% of the assessed value in 2018. From 2019 onward rates increase to 2% for foreigners (not permanent residents nor Canadian citizens) as well as citizens or permanent residents that are deemed members of “satellite families.” A “satellite family” is defined as a family – combining spousal incomes - where less than 50% of total worldwide income is declared (and taxed) in Canada.\nThe portion targeting empty homes follows along similar lines as the City of Vancouver Empty Homes tax, with similar exemptions. Homes are generally exempt from the tax when owner-occupied or rented out for at least half of the year. Importantly, foreign and satellite family owners face additional burdens in renting out homes. Tenants must either be arm’s length, meaning they have no special relationship with the landlord, or, if non-arm’s length, they must be permanent residents or Canadian citizens with Canadian income at last three times the annual fair market value of the rent for the entire residential property.\nThe tax has been reported to affect about 32,000 homes, about 20,000 of which will be British Columbians with the remaining 12,000 foreigners or residents of other provinces, and generate around $200M in revenue. Unfortunately the province has not shared a more detailed breakdown of how many homes are in each of the category the tax targets, the empty homes, the foreign owners, or the satellite families.\nLike everyone, we’re curious how it’s all going to work! Here we want to try and put out some preliminary guesses as to how many a) empty homes and b) foreign owners might get taxed. We also want to think a bit more about satellite families and imagine how possible consumption audits might work. This enables us to make some educated guesses about c) the population at risk of being audited. Surely some of those audited will either have to pay the speculation tax or end up referred for income tax avoidance. Others will have ready explanations for why their property holdings fail to match their reported incomes, likely explaining their lifestyles as products of income volatility or legal gifts (falling beyond combined spousal income). Finally, we want to address the possibility of better rental income reporting as a result of the Speculation and Vacancy Tax. Might there be even more d) revenue gained from better reporting rental income relative to direct Speculation and Vacancy tax revenue? Let’s find out!"
  },
  {
    "objectID": "posts/2019-02-27-tax-speculations/index.html#empty-homes",
    "href": "posts/2019-02-27-tax-speculations/index.html#empty-homes",
    "title": "Tax Speculations",
    "section": "Empty homes",
    "text": "Empty homes\nHow many empty homes will the tax effect? Empty homes are hard to estimate. The City of Vancouver commissioned a study based on BC Hydro data to estimate the number of empty homes in the city in a similar manner to how the tax applies, coming up with 10,800 to 13,500 empty homes in the city. In the first year, 2,538 properties were subject to the tax (roughly half declared themselves so with the rest failing an audit or failing to file or appeal). Another 5,385 were declared exempt (some of the exempt properties were not in the universe of the Ecotagious study). It is unclear how much of the difference is due to previously empty homes getting occupied or evasion. It is difficult to use this to estimate the total number of empty homes affected by the speculation tax, but one very rough estimate would be to take the number homes unoccupied on census day and scale the numbers down by a factor 8.6, roughly the ratio of the 21,820 homes unoccupied on census day in the City of Vancouver to 2,538 empty homes paying the empty homes tax.\n\nOverall there were 75,870 dwelling units that were unoccupied on census day in the regions where the Speculation and Vacancy Tax applies. If we use the Ecotageous study for the City of Vancouver as a guide, we would expect 41,725 empty properties using the definitions from the Speculation Tax, and 8,825 properties that will pay the tax. This might be a low-ball, given that the province has more effective means in checking for evaders entering into “fake rental” agreements and that the tax rate is lower (for the first year, and for permanent residents and Canadian citizens that make up the bulk of the affected owners in the years after) than in the City of Vancouver, and that the tax can be offset against BC income taxes, potentially inducing fewer people to sell or rent out their property in response to the tax."
  },
  {
    "objectID": "posts/2019-02-27-tax-speculations/index.html#foreign-owners",
    "href": "posts/2019-02-27-tax-speculations/index.html#foreign-owners",
    "title": "Tax Speculations",
    "section": "Foreign owners",
    "text": "Foreign owners\nHow many foreign owners will the tax affect? Foreign owners are defined as those owning property without being a citizen or permanent resident in Canada. Keep in mind that foreign owners won’t face any speculation tax so long as they rent out their properties to an arm’s length tenant or so long as the deal seems plausible for a non-arm’s length tenant (right now 37.7% of all secondary market renters in the regions affected wouldn’t meet the specified plausibility requirements, but that’s mostly due to their income being too low and that doesn’t matter for arm’s length tenants). Only if the foreign owner themselves occupies the property, leaves it empty, or keeps family members (like children) housed upon the property will they face the tax.\nEver since instituting the Foreign Buyer Tax in 2016, BC has been tracking data on how many purchases are made by foreign buyers. But for a variety of reasons, this kind of transaction data is a poor reflection of the number of foreign owners at any given point in time. Statistics Canada has sought to better collect data on foreign property ownership through its CHSP program, but the definitions differ from tax policy definitions. For CHSP purposes, it’s the primary residence of owners that matters rather than citizenship or permanent residence status – in other words, do owners live at foreign addresses or Canadian addresses? Some people with overseas primary residences will have Canadian permanent residence or citizenship. Some people with primary residence in Canada will not have Canadian permanent residence or citizenship (this status, for example, covered both authors of this blog post when they first moved to Canada). While imperfect, the measure of primary residence probably isn’t a terrible proxy for who will face taxation.\nExtrapolating from the CHSP data for the region covered by the Speculation and Vacancy Tax suggests that 46,110 “foreign” owned properties might face the tax, in addition to the relatively small number of foreign individuals likely registering their properties through corporate ownership. It’s important to remember that there may be a significant overlap between the empty properties we looked at above and the properties of non-resident owners likely to face the tax, so these are non-exclusive categories. But we don’t yet have any good data on the degree of overlap.\n\nOf note, so far we can report that the impact from the Speculation and Vacancy Tax will vary widely by geography. Many municipalities have very few empty properties or foreign owners. Others, as near UBC (Metro Vancouver A) have a lot. Of course it’s worth noting that the housing around UBC is unusual for many reasons, including its student population (often boosting census unoccupied counts and highly transnational). Moreover, Electoral Area A weirdly extends into the mountains of the North Shore, where a small number of empty cabins complicate the picture, but there aren’t too many, so we don’t show that part on the map."
  },
  {
    "objectID": "posts/2019-02-27-tax-speculations/index.html#satellite-families",
    "href": "posts/2019-02-27-tax-speculations/index.html#satellite-families",
    "title": "Tax Speculations",
    "section": "Satellite families",
    "text": "Satellite families\nWhat about satellite families? Brace yourself for a much longer discussion, necessarily delving into the definition of satellite families, the methods BC may attempt to use to audit those they suspect of being satellite families, and the limits of the information we can gather about satellite families.\nIn common parlance, satellite families refer to families where income earners live and work in one place while children and spouses live in another. Within the family sociology literature, this includes a variety of spousal and spouse-like relationships grouped as Living-Apart-Together (LAT). It also includes adult children being supported by parents who live elsewhere, and minor children who might be living with other caregivers (like grandparents) while receiving parental support. Within the immigration literature, satellite families might also be understood to include a wide variety of ways families work around and across borders, often sending different family members to places where they’re likely to see the most economic opportunities, but involving remittances sent back across borders for the good of the family as a whole. The Philippines and Mexico are perhaps the places most studied where families send workers abroad who return remittances back home, usually with a long term goal of reuniting the family. But other countries, including China and even Canada, have similar traditions. Satellite families create transnational ties, constituted in part through the flow of resources across borders but between family members. Vancouver likely has a lot of satellite family members engaged on both sides of transnational income sharing, both as wage earners supporting those abroad, and family members supported by those abroad.\nVancouver also has a lot of wealthy residents, including both immigrants and non-immigrants. And if there’s one thing we know about wealthy residents, it’s that they often don’t pay their fair share of taxes. In the specific context of BC’s Speculation and Vacancy Tax, the debate over satellite families has often emphasized tax avoidance. Satellite families are frequently suspected of gaming tax systems for their own advantage by deriving their income from another country, leaving it untaxed by Canadian income tax. Income tax, of course, helps to fund many services (e.g. education, healthcare) enjoyed by Canadian permanent residents and citizens. Property tax also funds many services, though as many observers have noted, BC has very low property tax rates. So it’s possible to game tax systems - entirely legally - by one family member working in a location (outside of Canada) where income tax rates are lower than BC’s and paying income taxes there, while other family members buy property and enjoy many of the services of BC, where property taxes are often lower than elsewhere. This is the situation the Speculation and Vacancy Tax is meant to correct, though of course it also potentially creates problems for transnational families who aren’t attempting to game tax systems. It also has no impact on satellite families who rent rather than own. Aside from identifying satellite families, the joining of property data with income data also has the potential to identify tax evasion. As revealed by recent CRA audits, tax evasion among wealthy Vancouverites is probably pretty common.\nHow many satellite families are there? We really don’t know yet. There’s no good data on the issue, especially since families filling out census forms may, or may not, choose to list members regularly working overseas as resident in BC (and census residence is different from tax residence). That said, we have a better sense of who might be at risk for either listing themselves as satellite families or being audited under suspicion of tax avoidance or evasion. But we have to make some guesses about what might trigger audits.\nFirst, let’s remind ourselves of what data is being collected. The declaration form for the Speculation and Vacancy Tax asks about worldwide income for property owners, including the combined worldwide incomes of spouses. This is attached to property tax data from the assessment rolls. So the tax authorities should have declared data on worldwide income, income taxed in Canada, and property values. Recall that owners with less than half of their worldwide income declared in Canada are considered satellite families. Some people will identify themselves as satellite families. But in other cases, they may provide false declarations regarding their worldwide income. This opens up a variety of auditing opportunities for BC and the CRA. How will they decide who to audit for compliance?\nWe already know the CRA has identified lifestyle audits as a lucrative means of tracking tax evasion. We also know they’ve got a rule in place regarding rents deemed legitimate for non-arm’s length tenants. We can build on this to explore cases likely to trigger audits if undeclared as satellite families. Keep in mind there may be many explanations for discrepancies between property value and income, including family income volatility; dramatic appreciation of housing purchased long ago; or living off savings, inheritance, or gifts. But other explanations will identify home owners as “satellite families.” Of note, still other explanations may be referred to the CRA or police authorities when they suggest tax avoidance, tax evasion and/or work in illegal economies. So how many people are at risk of being audited as satellite families?\nWait, just a few more methodological caveats! We will try to estimate the number of households at risk of being audited or labeled as satellite families using the recently released 2016 PUMF data. 2016 data is, of course, now somewhat dated, being collected prior to a number of policy changes of interest to what we’re exploring, including (but not limited to) the imposition of the Foreign Buyer’s Tax in 2016, the imposition of the Empty Homes Tax in the City of Vancouver, and the slow roll-out of the Speculation and Vacancy Tax itself. We also won’t be able to achieve a perfect match with the regions the speculation tax applies in, having to make due with using Census Metropolitan Areas. The largest discrepancy is that this drops the Nanaimo area region. PUMF data is based on a weighted subsample, so estimates based on PUMF data are never counts as when using the census, but ranges based on different weightings. In most cases, actual census counts will be contained in these ranges, so PUMF data adds a conceptual nuance we usually don’t see when using census data. At this point it is good to remind ourselves that what we are really interested in is not the census counts but the actual numbers on the ground that the census is trying to estimate, but as usual we will gloss over this last step and be satisfied with estimating census counts. Here we will use the primary household maintainer as a proxy for the owner, and we will ignore dual or multiple ownership scenarios where owners fall into different categories. The speculation tax puts heavy emphasis on spousal income, which is different from family income or household income. That makes it a bit difficult to use census data to compare, we would need another custom tabulation to extract the income of spouses only. For this post we will gloss over this issue and just use household income instead. While family income may be closer to spousal income, we simply felt that household income is a more appropriate measure in the context of the Speculation Tax. People that favour different preferences are welcome to grab the code and make the appropriate adjustments. A related issue is what counts as a “satellite family”, in particular it is not clear if it applies to individuals who are not married or living common law. While only married (including common-law) tax residents in BC would appear to be at risk of declaring themselves part of satellite families, single individuals could also be flagged for lifestyle audits to determine tax compliance, so we include both, but we separate them. Throughout we will exclude immigrants that came in 2015 or 2016, as their 2015 Canadian income may not correctly reflect their subsequent Canadian income. Moreover, we exclude households that have moved within the preceding year, as well as properties worth less than $150,000, as these are exempt from the tax. Generally we don’t report if a category contained fewer than 30 (unweighted) cases.\n\nHome-value-to-income based triggers\nAssessed home value to income ratios could serve as a trigger for consumption based audits. But what’s a good ratio to use? For a foreigner renting out their property to a non-arm’s length tenant, the tax requires the income of a tenant to be at least three times the (fair market value of the) rent in order for a foreign owner to be exempt from the tax. We take this as a hint we can use this as an implicit definition of a satellite family. A satellite family may be identified as a household with declared income taxed in Canada that is less than 50% of three times the imputed rent. Why? There’s the expectation encoded in the non-arm’s length definition that housing costs will take up no more than one third of income. And if more than 50% of the household’s combined spousal worldwide income is declared outside of Canada, one is considered a satellite family. To estimate imputed rent we use a gross cap rate of 3%. This test is effectively asking that owner households spend at most two-thirds of their total Canadian income on shelter cost based on imputed rent.\nHowever, this will catch quite a few “house-rich but income-poor” people. Take for example a senior that bought their house a long time ago for a lot less money than it would take today. If their house is now worth, say, $2M, then the imputed rent comes out to be $5k a month, or $60k a year, requiring an total annual income of at least $90k to pass our test. Given the fairly large appreciation of property, especially in the years before the census, it seems reasonable to adjust the trigger by how long the property has been held. The province will have the exact time the property was purchased to fine-tune this, but using census data we can only check if the person lived in the same residence one and five years prior. As we are exempting people that moved within the year before the census (analogous to the Speculation Tax exempting properties in the year they transacted), this leaves us with the five year timeframe.\nGiven the explosive rise in property values in the year before the census, we discount the imputed rent by a factor of 0.8 if the household maintainer moved into the property between one and five years before the census, and by a factor of 0.5 if the maintainer moved in more than 5 years before – reflecting the roughly doubling of property values within the five years before the census. We call this the adjusted imputed rent test.\nOf note: our data is top-coded for dwelling-values above $2M, which can lead to some mis-classification for some properties with very high dwelling values, but ultimately different ways of adjusting for this have little big impact on the high-level numbers. We added an additional filter excluding households with household income above $90k, which softens potential issues around top-coded dwelling values.\n\nCombined spousal income determines satellite family status under the Speculation and Vacancy Tax, so we separate out our estimate of those failing our adjusted imputed rent test (and hence at risk of being audited) by marital status. This yields an almost identical number of single vs married or common law households failing the test, combining for around 45,000 in total. While only married (or common-law) people would seem to be at risk of being labeled satellite families given the focus on combined spousal incomes (“gifts” to children and other family members don’t count the same), it’s possible that auditors will still include single people in the pool of those at risk of being audited for tax evasion and failing to accurately report worldwide income. So we’ll keep both singles and marrieds in the analysis, but treat them separately.\n\nHousehold Status\nWe also want to look at other statuses that might matter. Students and seniors come to mind as being particularly vulnerable to audit because of their lower incomes. In addition, seniors may be especially likely to have purchased their homes long in the past, meaning their homes may have done much more than double in value since they’ve lived in them. So let’s see what happens when we separate out these groups.\n\nWe see that in particular single seniors make up a good portion of households at risk of being audited, but the bulk is taken up by working age population that is not attending school. Household type gives a different way to understand the makeup. If satellite families mostly involve an overseas wage earner supporting a spouse and children, do we see a lot of these types of households?\n\nAs it turns out, there are relatively few people who report being married but living as a lone parent who fail our adjusted imputed rent to income test. There are only around 2,400 married or common law household maintainers that show up in lone parent households, making up a small proportion of those failing our test overall. But it’s possible that many respondents filling out census forms still report their spouses as belonging to the household, even if they spend a significant amount of time working overseas, so we shouldn’t count out other married and common-law categories, split between those with and without children, from being considered satellite families.\n\n\nDwellings\nWhat kinds of dwellings are people who fail our test living in? First let’s talk about dwelling values. By our metric, the disjuncture between dwelling value and reported income triggers possible audits. Higher dwelling values have a mechanical effect on increasing the income needed to avoid an audit, so we’d expect households with higher dwelling values to be more likely to fail our test. Is this what we actually see?\n\nAs one would expect, relatively few lower dwelling value homes are impacted. But each half a million dollar value bracket between $500k and $2M seems fairly evenly filled by about 4,000 households, jumping to higher number for homes above $2M, especially those occupied by married or common law household maintainers. Those most at risk of being audited would appear to be those living in the most expensive homes.\nWhat structural sorts of dwellings are the people who fail our adjusted imputed rent to income test living in? Condo apartments? Single detached homes? Both dwelling type and condominium status will be available to government auditors. Our data only has three dwelling types: single detached, apartment and other. In our focus on owner-occupied dwellings and taken together with the condominium variable, we’re mostly separating condominium apartments from single-detached houses, with the latter showing up either as a single-detached house or a non-condominium apartment (i.e., house with a secondary suite or “duplex”). But there will be some other types of non-condo apartments and other types of structure (e.g. rowhouses) showing up as both condos and non-condos.\n\nMost of those at risk of being audited would appear to live in single-detached houses, with or without suites, with condominium apartments taking a distant second. It would appear that not too many other kinds of housing will be targeted, or at least we don’t see enough of them to provide reliable estimates of their frequency. But this analysis by itself is interesting in policy terms. As a reminder, some condominium apartments will be temporarily exempted from the tax if they have restrictions on rentals - an out not available to other dwelling types (and also not available for long!) Detached houses with secondary suites have another potential loophole. Regardless of their status, property owners might be able to avoid paying the Speculation and Vacancy Tax on their house as a whole so long as they rent out one of the suites on the property to an arm’s length tenant, pointing toward the categorical flexibility houses with suites repeatedly demonstrate in policy terms.\n\n\nImmigration\nIn the context of satellite families we often think of immigrant households. These are the households expected to maintain transnational connections, though overseas income earning may diminish with time (and generation). Of course, non-immigrants can also find themselves earning incomes (or partnered to those earning incomes) outside of Canada. Moreover, we know Canadians of many stripes and backgrounds attempt to evade taxes, just as they also have “bad years” where their incomes may drop out of the normal. So let’s look into immigration by period, including non-immigrants in the mix. How does immigration relate to risk of being audited as a satellite family using our adjusted imputed rent test?\n\nHigher numbers of non-immigrants (i.e. Canadian born) fail our test than any ten-year immigration arrival bracket. Non-immigrants especially dominate the set of single people with lower incomes than expected by housing values, but they also appear in great numbers for married people. This is a striking finding, but also reflects the greater overall size of the non-immigrant population. Looking at immigrants by period, we tend to see what we expect: recent immigrants fail the test more often than more established immigrants. Recent immigrants failing our test also tend to be dominated by married couples, unlike what we see for non-immigrants, but this gap diminishes over time as immigrant patterns come to look increasingly like Canadian born patterns.\nLooking at the share of owners failing our test in each immigrant category, as opposed to their total numbers, helps clarify these patterns further.\n\nHere we see that higher proportions of recent immigrant owners fail the adjusted imputed rent test than for non-immigrant owners or more established immigrant owners. Reading shares by period of arrival sideways, the evidence would suggest that more recent arrivals owning homes will likely move toward non-immigrant patterns for home owners the longer they remain in Canada. But culture and wealth of immigrants may vary with period, so there may be other explanations at play as well.\nWhere are those who fail our test coming from? Let’s take a look, using place of birth! Of note, sending countries vary from period to period, meaning the period analysis (above) influences the place of birth analysis (below) and vice-versa. Arrivals from China, in particular, tend to be more recent. We should remind ourselves that place of birth is not necessarily the same as the place people immigrated from. In particular in the of China, sizable portions of immigrants from Taiwan and Hong Kong that were born in China.\n\nHere Chinese born and Canadian born household maintainers contribute the most to owners failing our adjusted imputed rent test. But other sizable contributors to possible audits include those from Hong Kong, other East Asian countries, and the United Kingdom. The United Kingdom may seem unexpected as a group likely to face audits, but we have already seen some of the relevant cases documented in the news. Let’s look at share of owners failing our adjusted imputed rent test by place of birth.\n\nDiving into the share of owners likely to trigger audits, we see in all cases that it’s a minority of owners at risk from each country. The uncertainty ranges are too large to sensibly rank the data by place of birth. We grouped immigrants from birth places with fewer than 30 (unweighted) combined cases into larger groups. Nevertheless there are sizable proportions of owners arriving from China, Hong Kong, and Other Eastern Asian countries at risk of being audited. This likely reflects Canadian immigration programs selecting for wealth, like investor class programs, popular in these countries. Comparing investor immigrants living in the speculation tax regions to all immigrants by place of birth, we notice how the investor program leans heavily toward Pacific Rim countries.\n\nWe know just over 22,000 property owners in Metro Vancouver were identified as investor class immigrants in 2018 CHSP data. We also know that the incomes of the investor class immigrants reported in Canada have tended to be lower than for other streams, as confirmed in the 2016 census data below.\n\nLooking at the adjusted family income deciles, the bottom decile is very strongly represented, with incomes slowly rising the longer the immigrants have been here. While we don’t know how these roughly 62,000 investor immigrants group into households and household types and break up into renter and the 22k owner households, this does provide more circumstantial evidence that a fair number of investor class immigrants will get caught by the adjusted imputed rent audit trigger.\nEast Asian ownership patterns may also reflect price discrepancies that make Vancouver real estate seem especially cheap to immigrants arriving from across the Pacific Rim. Arrival with wealth, whether from the sale of a pricey residence overseas or other sources, enables movers to quickly purchase housing in Vancouver. Once arrived, they could become satellite families by returning income earners to countries of origin where they see stronger job prospects (and less discrimination), or they could simply be living off of their savings as they adjust to life in Canada as homemaking migrants (ungated version). In this, immigrants may constitute a special case of income volatility in the years after their arrival. And of course let’s not forget that where there is wealth, no matter the source, there are likely to be attempts at tax avoidance and evasion!\n\n\nRegional Variation\nLastly we quickly check on how properties likely to be declared as satellite families or audited for lifestyle discrepancies are distributed over the CMAs that we consider. Not surprisingly, in terms of sheer numbers, the Speculation and Vacancy Tax is overwhelmingly going to target Metro Vancouver. Almost all of the properties failing our test are in Metro Vancouver. Which isn’t too surprising since it’s where all the people live.\n\nBut what about in terms of share? Metro Vancouver is also where the highest value homes are located and the area with the most transnational ties. So perhaps it’s not surprising that the share of households likely to declare as satellite families or be audited as such looks highest in Metro Vancouver. But by share it’s more clear that Victoria, Kelowna, and Abbotsford pull at least some weight.\n\n\n\nComparing to Shelter-cost-to-income triggers\nAnother measure that has been in the public discussion regarding satellite families is the shelter-cost-to-income ratio. Instead of (adjusted) imputed rent, we can take the actual shelter cost from census data. This won’t be directly available to the government for audit purposes, but the government could try to approximate this using the mortgage registered against the property from their land title database.\nReplacing our adjusted imputed rent with shelter cost we now can ask that owners have enough income to cover three times their shelter cost. Folding in the Speculation Tax definition of spouses having to declare at least half their joint spousal income in Canada we arrive at a shelter-cost-to-income cutoff of 66.7%. That’s something that’s reasonably easy to check in Census data, just like we did for renters near the top.\nThe total numbers of owner households failing the shelter-cost-to-income test is very similar to those failing our adjusted imputed rent test, but the populations don’t fully overlap as the following graph demonstrates.\n\nThis shows that the tests are quite sensitive to the definitions, and using these kind of tests for audits will not be entirely straight-forward. Provincial auditors will likely be busy, and will require a robust data-driven audit system in order to be effective."
  },
  {
    "objectID": "posts/2019-02-27-tax-speculations/index.html#rental-income-tax-reporting-compliance",
    "href": "posts/2019-02-27-tax-speculations/index.html#rental-income-tax-reporting-compliance",
    "title": "Tax Speculations",
    "section": "Rental Income Tax Reporting Compliance",
    "text": "Rental Income Tax Reporting Compliance\nReading through the requirements one can’t help but think that the BC government will make use of detailed individual tax return data to enforce the regulation. They may be able to use rental income on tax returns to verify the that arm’s length tenancies were correctly declared. At the same time, this should prove a very effective measure to ensure rental income is properly declared by landlords, which in turn forces proper declaration of capital gains taxes in case of a sale of a secondary residence, both of which are suspected to have low compliance. To get a rough idea of the impact, we use census data to estimate the total rent being paid by tenants. The aggregate shelter cost of tenants not in purpose-built, social housing or basement suites in our regions is $3.09B. Here we exclude basement suites because they are affected differently by the speculation tax. Rent is generally a bit lower than shelter costs, because rent may not include utilities. Combined with this, as well as tax write-offs, we assume an effectively 15% of this total is due as tax on rental income. If we take the current compliance rate to be 50%, the compliance rate that was recently estimated for artisinal landlords in London, and assume that the speculation tax increases compliance to 100%, this would generate an additional $232M of tax revenue at the federal and provincial level, which is the same order of magnitude as the projected direct tax revenue from the Speculation Tax. On top of this, declaring rental income makes it harder to evade capital gains tax at the time of sale of as secondary property."
  },
  {
    "objectID": "posts/2019-02-27-tax-speculations/index.html#conclusion",
    "href": "posts/2019-02-27-tax-speculations/index.html#conclusion",
    "title": "Tax Speculations",
    "section": "Conclusion",
    "text": "Conclusion\nThe BC Speculation and Vacancy Tax has been reported to affect about 32,000 homes, about 20,000 of which will be British Columbians with the remaining 12,000 foreigners or residents of other provinces, and generate around $200M in revenue. While we’re not certain where these figures come from, given our estimates above they actually seem pretty reasonable. We’re guessing about 8,800 properties will be considered vacant and non-exempt from the tax, overlapping with 46,000 properties owned by “foreign” owners and subject to the tax if left unattached to a decent rental contract. A sizable 45,000 households may be at risk of being identified (or audited) as satellite families, mostly living in pricey single-family detached (or suited) dwellings. As we note, around a third of these households will be headed by Canadian-born residents, but it’s likely many investor class immigrants will also be hit, and the vast majority affected will be in Metro Vancouver. Finally, the tax will probably generate a lot of revenue indirectly by increasing income tax compliance, quite possibly topping its direct revenue. We’ll be watching to see how it unfolds!\nFor those interested in more details on our methods, or people that would like to make different assumptions and continue to investigate along these lines, the code for the analysis is available on GitHub."
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html",
    "href": "posts/2019-02-15-vancouver-renters/index.html",
    "title": "Vancouver renters",
    "section": "",
    "text": "Apologies up front, this is a bit of a hodge podge of a blog post. I have about half a dozen stubs on rental data and affordability that I looked at at some point while trying to understand some aspect of rental affordability. But it’s a large and complex topic, and I never took the time to distill out coherent storylines. Rather than keep pushing things off I decided to grab a couple of relevant pieces and put them together in a short blog post. Some of this is based on public census release data, other parts are based on a a custom tabulation that I have also used in a previous post and that I am unfortunately not at liberty to share the raw data.\nThere is no clear storyline in the data, rather its a bunch of different aspects that partially intersect and that illuminate some aspects of rental affordability in the City of Vancouver. Hopefully someone will find some of this useful"
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html#disclaimer",
    "href": "posts/2019-02-15-vancouver-renters/index.html#disclaimer",
    "title": "Vancouver renters",
    "section": "",
    "text": "Apologies up front, this is a bit of a hodge podge of a blog post. I have about half a dozen stubs on rental data and affordability that I looked at at some point while trying to understand some aspect of rental affordability. But it’s a large and complex topic, and I never took the time to distill out coherent storylines. Rather than keep pushing things off I decided to grab a couple of relevant pieces and put them together in a short blog post. Some of this is based on public census release data, other parts are based on a a custom tabulation that I have also used in a previous post and that I am unfortunately not at liberty to share the raw data.\nThere is no clear storyline in the data, rather its a bunch of different aspects that partially intersect and that illuminate some aspects of rental affordability in the City of Vancouver. Hopefully someone will find some of this useful"
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html#what-is-affordability",
    "href": "posts/2019-02-15-vancouver-renters/index.html#what-is-affordability",
    "title": "Vancouver renters",
    "section": "What is affordability?",
    "text": "What is affordability?\nIn Vancouver we talk a lot about affordability. Different people have different definitions of what affordability is. And affordability is not the only metric that counts. StatCan says a household is in core housing need if the household does not meet at least one of the adequacy, affordability or suitability standards. A dwelling unit is adequate if it does not need major repairs, affordable if the household spends no more than 30% of total income on shelter costs, and suitable if it is not overcrowded (meets NOS standards). The universe excludes non-family households with at least one maintainer between the ages of 15 to 29 attending school full-time, as well as households with shelter-cost-to-income ratios at 100% or above. Additionally CMHC ran estimates for rents by number of bedrooms (in good repair) in each community, and it does not consider a household in core housing need if such rents would be affordable to a household that currently does not meet the core housing criteria.\nAffordability is part of core housing needs, but there are households in core housing need whose housing is affordable and households that don’t meet the affordability cutoff that are not considered in core housing needs.\nOften we see the affordability metric being used without the nuances that the core housing metrics introduce, and StatCan reports the proportion of households with income greater than zero spending more than 30% of income on shelter in their regular census releases, as well as the proportion of households spending 100% or more of income on shelter.\nThe latter category is difficult to interpret. Sometimes people exclude these when reporting household affordability statistics just like the core housing need definition does, as these are generally dominated by student households and households that otherwise don’t fall into the usual framework of people struggling for housing. In Vancouver, owner households in that category have been interpreted as a sign of undeclared income. Income volatility is at the root of another segment of such households.\nAt other times people include these households when reporting on affordability metrics. As mentioned above, CMHC excludes student households on top of that, which adds an additional filter to separate out households that CMHC considers in ‘transitional’ stages. Another worthwhile approach would be to take the cost of student housing into consideration and model affordability changes of students separately.\nCustom tabulations can separate out renter and owner households, renter households in subsidized housing and market rental, households spending 50% or more of their income on shelter or less than 15% of income on shelter. For specific questions, one can tailor custom tabulations to report different cutoffs and slice by other variables of interest."
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html#affordability-standards-and-city-of-vancouver-affordability",
    "href": "posts/2019-02-15-vancouver-renters/index.html#affordability-standards-and-city-of-vancouver-affordability",
    "title": "Vancouver renters",
    "section": "Affordability standards and City of Vancouver affordability",
    "text": "Affordability standards and City of Vancouver affordability\nCensus data shows that using shelter-cost-to-income ratios, City of Vancouver affordability has slightly improved from the 2006 and 2011 censuses to the the 2016 census. This is of course in stark contrast to some other affordability metrics like median multiples that compare median home values to median incomes. So what’s going on? These metrics measure different things. Shelter-cost-to-income ratios, taken at the individual level, focus on how individual households are handling their housing costs. Ecological level median multiples look take median incomes in an area and compare these to median home prices. There are a number of underlying assumption behind median multiples to ensure this is a useful metric, for example that either everyone does or should buy a home, or that dwelling values are reflected in people’s rent, and that the composition of households in terms of seniors, students and working people is uniform.\nThese two metrics, shelter-cost-to-income ratio and median multiples, tell diverging stories of affordability in Vancouver. While researchers have been struggling with this since October 2017, it is curious that the media has so far been able to largely ignore this. News searches reveal that there are stories about the change in the shelter-cost-to-income ratio in Toronto, but nothing in Vancouver. It is probably no coincidence that affordability according to this metric worsened in Toronto, but improved in Vancouver.\nSo how can we interpret the improving of the shelter-cost-to-income metrics in Vancouver? We already separated out renters and owners, as renter households are far more likely to be stressed for shelter cost than owner households. And the metric still improved for owners and renters when separated out.\nAnother view into this is using a custom tabulation put out by StatCan and CMHC to look specifically at core housing need.\n\nAgain, we see that the share of households in core housing need has slightly decreased in the City of Vancouver. This custom tabulation also allows us to better understand the nuances in the core housing definition, in particular how it is applied and what population is excluded from the universe.\nOverall the share of households in the City of Vancouver considered in core housing need was 19.8% in 2016. This includes households not meeting one of the three core housing need standards, including the affordability standard. But this is significantly lower than the share of shelter-burdened households often pegged at 36.6%.\nIt is instructional to look at the difference between the straight-up 36.6% of households (with income greater than zero) and the portion of them that CMHC deems in core housing need.\n\nThe bottom bar in each of these stacks count the number of households in core housing need not meeting the specified metric. The top bar counts the number of households with (income greater than zero and) shelter cost higher than income that were excluded from the core housing universe, and that don’t meet the specified standards. The middle bar counts households not meeting the standard that could afford a suitable and adequate accommodation in the community, as well as the non-family full-time student households that don’t meet the specified standard but have higher household income than shelter cost.\nThis explains the differences we see in how these metrics are applied. We can slice the data further to better understand the households that are facing affordability challenges in Vancouver. From the high-level numbers we see that renters in particular are having a hard time making ends meet in Vancouver, so for the rest of this post we want to focus on the City of Vancouver and explore the affordability of rental households in more detail."
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html#regional-context",
    "href": "posts/2019-02-15-vancouver-renters/index.html#regional-context",
    "title": "Vancouver renters",
    "section": "Regional context",
    "text": "Regional context\n\nThis highlights how households in unaffordable housing in core housing need are more prevalent in central regions, although the City of Langley is a curious outlier. Typically outlying regions make up for there more affordable housing options with higher transportation costs. We have explored the tradeoff between housing and transportation costs in the past (using 2011 data) at the ecological level, it would be worthwhile to revisit this with 2016 data, as well as computing this at the individual household level via a custom tabulation."
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html#shelter-cost-to-income-ratios-of-renter-households",
    "href": "posts/2019-02-15-vancouver-renters/index.html#shelter-cost-to-income-ratios-of-renter-households",
    "title": "Vancouver renters",
    "section": "Shelter-cost-to-income ratios of renter households",
    "text": "Shelter-cost-to-income ratios of renter households\nRenters are at the core of affordability. Canadians value home ownership, but ownership is not a core housing need. As discussed above, CMHC explicitly excludes shelter-cost burdened homeowners that could afford to rent a suitable dwelling in the community from their core housing need data. But if someone can’t afford to rent in the community, they are getting displaced.\nWhen looking at renter households, we like to distinguish those in subsidized housing from those in market housing. We expect different types of households and different types of rent between these groups, so it is useful to keep them separate. Overall, 13.8% of renter households are in subsidized housing in the City of Vancouver. That includes cases where the unit is subsidized, or where the household receives cash rent subsidies.\n\nWhen looking at the overall affordability picture there are fewer comfortably housed renter households, those paying less than 15% or less than 30% of income on shelter costs, in subsidized housing than in market rentals, which is to be expected. Comfortably housed households tend to be more affluent and won’t qualify for subsidized housing. However at the high end, we see that there is a higher share of severely shelter-cost burdened households, those paying 50% to 100% of their income on housing, than in market housing. As to be expect, the portion of households spending 100% or more of their income on housing is larger in market housing. As we have seen above, these are hard to interpret and CMHC does not consider these in their core housing need categories.\nTo get a better idea of these households, we proceed to slice the data by different variables.\n\nWe see that renter households in the City of Vancouver are overwhelmingly young. Households below the age of 25 have a quite different affordability pattern, a good portion consists likely of student households. Senior renter households also show a declining share being comfortably housed. For now, we will focus on households between the ages of 25 and 64 to approximate the labour force.\n\nAs can be expected, renters with income below $25,000 are severely strained, unless they are in subsidized housing. But even there the vast majority is shelter-cost burdened to severely shelter-cost burdened. It’s surprising to see 955 households with income above $100k in subsidized housing, as well as 21,380 households in market rentals spending less than 15% of income on housing, so being able to afford double their rent and utilities and still being comfortably housed spending less than 30% of income on shelter.\n\nThe ‘apartment’ category here captures apartments, either below 5 storeys or 5 storeys and above, as well as the census category ‘duplex’, which captures main units or suites in suited single family homes. We see that this category dominates the rental dwellings in Vancouver, with condo apartments a distant second place.\nLastly we can check what kind of households are renting.\n\nAlmost half of all renter households are one-person households. To better understand what kind of rentals these different family types are in we can look at the number of bedrooms.\n\nThe couple with children and lone parent households in 1 bedroom units likely feel crowded, and they won’t meet the CMHC suitability metric. Subsidized housing is better at right-sizing, which is usually part of their mandate.\nLastly we can look at household rents, in this case excluding utilities unless utilities were already folded into the rents.\n\nAs expected, rents in subsidized units tend to be substantially lower. Averages out-perform medians, indicating the skewedness of the rent distribution. People that have been renting for a while typically pay significantly lower rents than people that moved into their rental unit more recently. We have explored the moving penalty, the ratio between turnover and in-place rents, before. To gain further insight, we can separate out households that have moved into their rental unit within the year preceding the census from those that did not.\n\nTo bring this out more clearly we can compute the moving penalty, the rent increase that people that did not move in the past year would have experienced if they had moved.\n\nThis highlights to what degree couple with children families in particular are stuck in their current living arrangements, although the moving penalty is quite steep overall, and the overall ratios square quite nicely with our previous work on this using CMHC Rms data, indicating that a similar moving penalty persisted in 2018 two years after the census.\nTo round things off we want to compare condo apartments to non-condo apartment rents.\n\nThis shows how non-condo apartments (including single family with secondary suites) tend to rent for less than condo apartments. However, surveys show that renters are generally prepared to pay more for living in purpose built rental than on the secondary condo market, all else being equal. Unfortunately we don’t have granular enough data to filter by location and building age as proxies for quality to test this.\nWhat we can do here is key out very rough building ages.\n\nThis shows that the condo apartments skew much more recent than non-stratified market rental buildings, confirming our suspicion that the rent differential may be due to dwelling quality. It would be interesting to get finer data on location and building age, as well as separate out secondary suites from non-condo apartment rentals, to better understand what drives the difference in attractiveness of these dwelling units that get reflected in the market rents they are able to fetch."
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html#update-march-4-2018",
    "href": "posts/2019-02-15-vancouver-renters/index.html#update-march-4-2018",
    "title": "Vancouver renters",
    "section": "Update (March 4, 2018)",
    "text": "Update (March 4, 2018)\nI just saw an interesting question about newer vs older rental units, and how affordability, as measured by shelter-cost-to-income ratios, relate to building age.\n\nAnd indeed there is a tendency of older stock having higher shares of households below the 30% shelter-cost-to-income cutoff. Looking at discussions on the moving penalty higher up, we suspect that at least some of this effect is due to newer dwelling units have a higher share of people that moved in recently, and thus not benefiting as much from rent control as renters in older units. We can check this by looking at households that did and did not move in the previous year separately.\n\nThis shows that for people that have moved within the past year the share of people paying less than 30% of income on housing is independent of the age of the building, although the share of people paying very high shares of income on housing is lower in the pre-1996 stock. The picture for households that did not move in the preceding year aligns a little better than before taking out the recent movers, but still shows some variation. It would be good to come back to this again with a cross tabulation using the mobility 5 instead of the mobility 1 variable."
  },
  {
    "objectID": "posts/2019-02-15-vancouver-renters/index.html#next-steps",
    "href": "posts/2019-02-15-vancouver-renters/index.html#next-steps",
    "title": "Vancouver renters",
    "section": "Next steps",
    "text": "Next steps\nThis post opens up more questions than it answers. Hopefully I will find the time to add to this by adding posts on different aspects, or maybe others pick things up and continue from here. As always, the code is available on GitHub, although I am currently not at liberty to share some of the custom tabulations used in this post."
  },
  {
    "objectID": "posts/2019-02-03-there-is-no-brain-drain-but-there-might-be-zombies/index.html",
    "href": "posts/2019-02-03-there-is-no-brain-drain-but-there-might-be-zombies/index.html",
    "title": "There is no Brain Drain, but there might be Zombies",
    "section": "",
    "text": "Zombie attack! Zombies fleeing Vancouver want to eat your brain… drain… or something.\nA couple of weeks ago The Canadian Press reported a story asserting that young professionals were leaving Vancouver because of the high cost of housing. This fits in with a common zombie refrain that we hear from the media. It’s a story that just won’t die, no matter how many times it’s proven wrong: Millennials, or young people, or boomers, or people important for some other reason are leaving Vancouver because of housing. Usually there are supporting anecdotes, and indeed, it’s not too hard to find people leaving Vancouver who will tell you about their frustrations with housing. But here’s the thing: there is almost never supporting data that actually indicates a decline in people worth caring about. Why? Two reasons. First, in growing cities, like Vancouver, when some people leave, even more people come in to replace them. Second, ALL people are worth caring about.\nIf we set aside that ALL people are worth caring about - just for a moment - we can take up some important questions about differences in in-flows and out-flows of people in Vancouver. Maybe there are aspects of in-flows and out-flows that should trouble us. In The Canadian Press story, we’re led to believe Vancouver is experiencing a brain drain, so that all the smartest and best people are somehow leaving and they’re either being replaced with people who are not so smart OR they’re not being replaced at all. As noted above, Vancouver is growing. So we know whoever leaves is being replaced, and then some, by new people coming in. But are the people arriving in Vancouver somehow less brainy than those leaving? We’re both immigrants to Vancouver, and quite frankly we find that a little offensive. Everyone arriving in Vancouver has a brain, so population growth cannot result in a brain drain. But we set aside, for a moment that idea that ALL people were worth caring about. So let’s try putting differences in in-flows and out-flows in slightly less offensive terms by returning to the “young professional” framework. Are people arriving in Vancouver unable to do the same kind of professional work as those who leave? Are we losing out on educational credentials?\nIdeally we could easily access direct information on in-flows and out-flows to Vancouver (and in some places with population registry data, this is easily accomplished). In Canada we work mostly with census data, and the out-flow data, in particular, isn’t generally made public. But as we’ve demonstrated previously, we can compare across censuses to get net migration data broken down by age group. We just age people forward from one census to the next and compare how many we see in the next census to get a sense of how many people - in net terms - must’ve moved in or out over the years in between.\nNow if we’re interested in education then it complicates age-based net migration models. After all, people can and do acquire new educational credentials as they age forward in time. That said, we can probably assume that most people who acquire university degrees and more advanced credentials do so by age 25. We’ll leave out some late achievers, for sure, but if we assume we have a pretty stable division into those with a completed Bachelor’s degree or more, and those without by age 25, then we can get a sense of how those populations change as they age forward in time. So, with apologies to late achievers, that’s what we’re going to do.\nWe’ve got ten year age groupings by education to work with in 2016 data. So let’s go back to 2006 data for comparison. Is it plausible that we lost a bunch of “young professionals,” defined as people with university degrees, who weren’t replaced as they aged forward and left Metro Vancouver between 2006 and 2016? Data says… nope.\nAs a matter of fact, Vancouver added a lot more young university graduates than left. Young people with university degrees continued to arrive in greater numbers than they left well through their thirties and on into their forties (we like to think of forties as young). The age labels here refer to people’s “in between” age, that is the ages they mostly passed through between 2006 and 2016 (i.e., the age range each group was in 2011). It’s only once those with university degrees hit their fifties that we start to see a net flow out of roughly even flows in Vancouver. What’s more, this pattern looks very similar in other major Canadian metro areas. The only exception is Montreal, where people with university degrees really do stop arriving in their forties. But it’s probably not a housing crisis driving them out.\nStrikingly, across the board, young people with university degrees are far more likely, on net, to move into our major metro areas than people without university degrees. In many respects, we should expect this. Professionals, in particular, are often drawn by their economic opportunities. Once they arrive anywhere, they’re often paid well enough that they have an easier time navigating local housing markets than non-professionals. Yes, professionals may also have higher expectations about what kinds of housing they deem acceptable than others, but people adapt. One of us has written a book with that theme. In the same way that professionals may drive gentrification, professionals are actually at LESS risk of displacement out of expensive places, like Vancouver, than are non-professionals.\nLet’s double-check the results for Vancouver by looking at in-flow data. The Census provides information about where people lived five years before arriving at their current destination. Do we really see a lot of professionals moving into Vancouver through their thirties and forties? Yes. In fact, for “Skill Level A Professionals” this is exactly what we see. We don’t know how many are leaving from this data, but we know a lot of professionals are arriving - more so than in other occupational skill-level categories.\nFor mobility data the age group labels refer to people’s age in 2016. For an alternative view we can group non-movers and non-migrants (people that did move but not to a different city) together and show the makeup of each skill level by mobility and age group. Again we see that professionals tend to have higher shares of migrants than other skill levels, especially in our lower two age brackets. Those in occupations requiring only a high school degree or on-the-job-training are actually the least likely to come from afar.\nTakeaway: we do not have to worry about a “brain drain” in growing cities like Vancouver. Moreover, we don’t have to worry about professionals leaving. Due to better pay, professionals are better equipped to deal with a tight housing market than most others. Building more housing would certainly give professionals more options to choose from, and we might want to relax our millionaire zoning to direct professionals toward competing with the independently wealthy rather than the poor and working class. But it’s the poor and working class we should really be worried about losing. More housing can lead to a more equitable city with room for people who aren’t well-paid professionals or independently wealthy. And if we want to prevent displacement, we should focus more on those actually at risk. That suggests both building more and promoting a LOT more non-market and rental housing."
  },
  {
    "objectID": "posts/2019-02-03-there-is-no-brain-drain-but-there-might-be-zombies/index.html#methods",
    "href": "posts/2019-02-03-there-is-no-brain-drain-but-there-might-be-zombies/index.html#methods",
    "title": "There is no Brain Drain, but there might be Zombies",
    "section": "Methods",
    "text": "Methods\nThere are some details to be explained when computing net migration data for professionals. We already noted that professionals might get degrees at some later stage in life, but that tends to bias our estimates toward lower professional in-migration. Furthermore, when computing net migration one needs to kill off an appropriate number of professionals to account for mortality as Nathan has explained in details before. We use BC mortality rates for the appropriate years and age groups for this, but that probably over-estimates mortality as educated people tend to have lower mortality rates. This would bias our estimates toward higher professional in-migration. We could adjust for that by reading into the literature to figure out the appropriate fudge factor, but the effect is so small that we just ignored this. We made some adjustement to how we compute mortality rates and now assume a 20% reduced mortality rate for people with bachelor or above, and according higher mortality rates for people below a bachelor. This is a very rough approximation of the imact of educational attainment on mortality.\nThose interested in even more details we direct to the code for the analysis, where Jens is teaching Nathan how to code with R."
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html",
    "href": "posts/2019-01-09-high-value-homes/index.html",
    "title": "High-value homes",
    "section": "",
    "text": "High-value homes frequently make the news in Vancouver, most recently in the wake of the extra school tax for homes valued over $3M. The province will have looked at the data before introducing the legislation, but none of this seems to have filtered out to the general public. So maybe there is a need to take a closer look using Census data.\nThe census is a couple of years old now, and things have changed a bit since then. Nonetheless, it should give a basic overview of who lives in high-value homes. Throughout we will only look at owner-occupied homes and ignore investors that don’t live in their homes.\nDwelling values in the census are self-reported. That introduces some noise, in particular it is not clear how owners estimate their dwelling value. Some may look at market conditions at the time of the census, especially if they just bought the place. Others will rely on property assessments, which were one year out of date at the time of the census. And that year saw spectacular gains.\nWith that in mind, we call a home high value if it’s self-reported value in the 2016 census was at least $2.5M. To better zoom in on these homes, and to filter by metrics we are interested in, we are employing a custom tabulation. Unfortunately, we are not at liberty to share the raw data, which means this post is not reproducible without obtaining a similar custom tabulation from Statistics Canada. But the code is available as always, which contains all the information needed on what variables the custom tabulation contains. Hopefully we will be able to share the underlying data at a later point.\nTo get started, let’s take an inventory of high-value homes in Metro Vancouver.\nAs a sanity-check the number of self-reported owner-occupied high-value homes in the City of Vancouver in the 2016 Census roughly matches the number of homes above $3M in the City of Vancouver based on July 2017 assessed value. Which means that our $2.5M cutoff we are using is likely a bit too low as we should account for some homes being empty or rented out. But we aren’t off by much.\nSome of the grouping of municipalities is due to the way the custom tabulation was done, for this particular purpose it would probably be better to treat West Vancouver and Electoral A separately instead of mixing them with other municipalities. In particular the grouping of Electoral A with North Vancouver is unfortunate. The bulk of the area of Electoral A is indeed on the North Shore, but very few people live there and the vast majority of the population lives in the UEL and UNA near UBC. Grouping into the “Rest of Metro Vancouver” was informed by trying to avoid running into problems later on when slicing the data further.\nFor this we are focused entirely on owner households, and it is helpful to also understand what proportions of owner-occupied homes are “high-value”.\nWest Vancouver stands out, but The City of Vancouver also has 15.7% of owner-occupied homes that are high-value."
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html#types-of-homes",
    "href": "posts/2019-01-09-high-value-homes/index.html#types-of-homes",
    "title": "High-value homes",
    "section": "Types of homes",
    "text": "Types of homes\n\nAs expected, the majority of high-value homes are single-family. Some condos also fall into this category (including land strata), but the fairly large portion of non-condo apartments is surprising on first view, the data counts 8,060 of these. To understand this we need to unravel the definition of “apartment” used here. This category is the sum of three census categories, Apartment with five or more storeys, Apartment with fewer than five storeys, and Apartment or flat in a duplex. The latter category is essentially units in a suited single family home, so these are owner-occupiers in homes that the census has identified as having a suite. (It does not mean that the suite is rented out, we know that suites are the dwelling types most likely to show up as “empty” in the census.)"
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html#mortgage",
    "href": "posts/2019-01-09-high-value-homes/index.html#mortgage",
    "title": "High-value homes",
    "section": "Mortgage",
    "text": "Mortgage\nNext we look at how many of these homes carry a mortgage or require other regular payments like interest-only payments that are mandated on on many HELOCs.\n\nThis shows that roughly half of high-value homes carry a mortgage, slightly more in Richmond."
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html#shelter-cost",
    "href": "posts/2019-01-09-high-value-homes/index.html#shelter-cost",
    "title": "High-value homes",
    "section": "Shelter-cost",
    "text": "Shelter-cost\nBeing mortgage-free does not necessarily means people are not stretched for shelter cost payments, but it certainly helps.\n\nIndeed, the vast majority of owners without a mortgage can cover their shelter costs very comfortably. The same is not the case for households with a mortgage, and Richmond stands out with over half of households in mortgaged high-value homes spending over half of their income on shelter costs."
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html#age-of-primary-household-maintainer",
    "href": "posts/2019-01-09-high-value-homes/index.html#age-of-primary-household-maintainer",
    "title": "High-value homes",
    "section": "Age of primary household maintainer",
    "text": "Age of primary household maintainer\n\nOlder owners are much more likely to have no mortgage, although 19.9% of seniors do carry a mortgage or HELOC with regular payments."
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html#household-type",
    "href": "posts/2019-01-09-high-value-homes/index.html#household-type",
    "title": "High-value homes",
    "section": "Household Type",
    "text": "Household Type\nTo understand better how these high-value homes are used we look at their household type.\n\n48.1% of these are occupied by families with children, with another 16.8% complex households including multi-generational households. When looking at all owner-occupied homes we see a higher portion of one-person households, but the rest of the makeup is similar."
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html#household-income",
    "href": "posts/2019-01-09-high-value-homes/index.html#household-income",
    "title": "High-value homes",
    "section": "Household income",
    "text": "Household income\nWe have already seen in the income-to-shelter-cost ratios that income must vary considerably. People in a $3M home with a $2M mortgage would look toward monthly mortgage payments around $10k. Add to that property taxes and utilities, we should expect a household income reaching $500k to stay below the fairly comfortable 30% of income on housing cutoff. Unfortunately our data only has $200k as the top bracket cutoff.\n\nOwners with a mortgage have generally higher incomes, but not by much. And curiously this tendancy is reversed in Richmond, where owners with a mortgage tend to have lower household incomes.\nOverall 58.6% of high-value owner-occupiers have household income over $100k, and 9.75% have less than $25k, with Richmond high-value owner-occupiers leaning heavier on the low side when it comes to income.\nAt this point we should fold in age groups of the primary household maintainer.\n\nWe dropped the under 25 year old group as it was not large enough to give robust results. The top income bracket is the largest one in all age groups except for 25 to 34 year olds, where the bottom bracket is the largest. At the same time, all age groups have fairly large segments of income that is low in comparison to the high value of the home. Some of that may be explained by income and shelter data being offset by a year, by income volatility and by income sources like capital gains that don’t show up in census income numbers. Metro Vancouver residents declared $6.56 billion in capital gains income in 2016. This interpretation jives with data from the 2011 NHS showing areas with high value homes registering high captial gains income, but these possibilities are unlikely to explain all of the low-income households."
  },
  {
    "objectID": "posts/2019-01-09-high-value-homes/index.html#extra-school-tax",
    "href": "posts/2019-01-09-high-value-homes/index.html#extra-school-tax",
    "title": "High-value homes",
    "section": "Extra School Tax",
    "text": "Extra School Tax\nWhen looking specifically at the impact of the Extra School Tax, we have often heard concerns about seniors, in particular seniors that might have a mortgage or HELOC against their home and that are already stretched for shelter costs. So let’s consider seniors with a mortgage that pay 30% or more of their income on shelter before the extra school tax.\n\nIn total we have 1,200, seniors in high-value homes in Metro Vancouver that are shelter-cost burdened. That’s 10.1% of all senior owner-occupiers in high-value homes and 1.56% of all high-value homes. Those proportions are small, but not trivial. These seniors might have to make use of the property tax deferral program.\nIn this context it is good to keep in mind that there are 27,665 seniors in Metro Vancouver that are renting and spending more than 50% (not just more than 30%) of income on shelter, that’s 23 times the number of seniors owning high value housing with a mortgage that pay 30% or more of income on shelter. And they have no option to defer rent increases or sell and cash out of their home.\nThat rounds off the overview of high-value owner-occupiers. As mentioned at the beginning, unfortunately I am not able to share the data for this post. The code is available as usual in case people have question on details or want more information on the variables used. Hopefully we will find time to take a look into other aspects of the data sometime in the near future."
  },
  {
    "objectID": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html",
    "href": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html",
    "title": "Vacancy rate and rent change",
    "section": "",
    "text": "Today the new CMHC Rental Market Survey data came out, which is a good opportunity to refine my musings on the rental vacancy rate and rent increases. I view this as the renter version of the relationship between months of inventory and changes in resale prices in the for sale market.\nCMHC surveys purpose-built (market) rental apartments every October and reports on a variety of metrics, including statistics about the total stock, median and average rents, vacancy rates, and fixed-sample average rent change among others. I have grown quite font of the fixed-sample rent change metric, because it naturally lends itself to use in analysis. Fixed sample rent change keeps the rental buildings used for comparison fixed from one year to the next, so it is not polluted by changes in composition when a new building comes online or an old one gets torn down. In that sense, it’s the Case-Shiller metric for rents, with the caveat that in rent-controlled environments the change in rent may be capped, although in Canada rents are usually constrained at turnover (vacancy decontrol).\nA point that sometimes get forgotten is that the CMHC purpose-built stock only includes market rentals (modulo messy details on data collection and classification), so subsidized rentals are generally not included in this."
  },
  {
    "objectID": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html#purpose-built-market-rental-stock",
    "href": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html#purpose-built-market-rental-stock",
    "title": "Vacancy rate and rent change",
    "section": "Purpose-built (market) rental stock",
    "text": "Purpose-built (market) rental stock\nTo start off, let’s look at the purpose-built rental stock in Canada.\n\nThe list of CMAs with the largest purpose-built rental stock is naturally dominated by larger cities, but we immediately notice that the ordering does not correspond to population. Nor to the number or share of renter households in each city. The following graph relates the size of the purpose-built rental stock to the number of renters in each area.\n\nSliced this way, we notice a lot of smaller CMAs dominating the list, but some larger CMAs, in particular Montréal, also feature prominently. For comparison, here is how the shares look for the CMAs with the largest stock."
  },
  {
    "objectID": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html#vacancy-rate-vs-rent-change",
    "href": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html#vacancy-rate-vs-rent-change",
    "title": "Vacancy rate and rent change",
    "section": "Vacancy rate vs rent change",
    "text": "Vacancy rate vs rent change\nWhat we are really interested in is the vacancy rate and rent change. The purpose-built market is only one portion of the rental market, the other two are the subsidized rental market and the individual (“artisanal”) rental market. The subsidized market functions very differently, but the purpose-built and artisanal markets are somewhat fungible.\n\nWith a renewed focus on renters and public discussions about rent control, this graph highlights the relationship of the vacancy rate to inflation adjusted fixed-sample rent changes in market purpose-built rental units.\nThe graphs validate the rule of thumb of a three percent vacancy rate as being considered as healthy. When the vacancy rate climbs above three percent, inflation-adjusted rents generally flattens and go negative. When the vacancy rate drops below three percent, inflation-adjusted rents tends to climb.\nThe natural vacancy rate, the rate when inflation-adjusted rent stays flat, varies somewhat across cities and possibly across time. Vancouver appears to have a lower natural vacancy rate, with inflation-adjusted rent dipping in the negative while the vacancy rate maxes out at 2.7%. On the other hand, Sherbrooke, Montréal and Halifax appear to have slightly higher natural vacancy rates."
  },
  {
    "objectID": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html#the-rest-of-the-rental-supply",
    "href": "posts/2018-11-28-vacancy-rate-and-rent-change/index.html#the-rest-of-the-rental-supply",
    "title": "Vacancy rate and rent change",
    "section": "The rest of the rental supply",
    "text": "The rest of the rental supply\nSo far we have only talked about purpose-built market rentals. The main reason for this is that we have longer time lines and better quality data for purpose-built rental, and the idea is that purpose-built and “artisanal rentals” are fungible, so there should not be too much of a different. But in e.g. Vancouver that’s only a small fraction of the rental market. How representative is this really?\nTo understand that, we start by comparing the purpose-built vacancy rate to the condo vacancy rate.\n\nWe have much shorter time lines for the condo vacancy rate, but the two track fairly well. Next, let’s look at rents. For “artisanal” rentals, we don’t have the luxury of a fixed sample rent change series, so we use the year over year change in average rent to stand in.\n\nAnd looking at the data, it’s a real mess. The high volatility in the graphs suggest poor data quality, and the quality indicators provided by CMHC. In Vancouver for example, only the latest two points in the Series come with an uncertainty below 2 percentage points. Probably not too much useful can be learned from this.\nOne could pull census data to try and understand rents in “artisanal” setting better, but they are only available every 5 years. And the census only allows to filter by condominium units since 2011.\nAs always, the code for the analysis is available on GitHub for anyone to reproduce, adapt or appropriate for their own purposes."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html",
    "title": "Understanding income distributions across geographies and time",
    "section": "",
    "text": "When trying to understand the income makeup of regions in Canada we need to take the income distribution and simplify in a way that is accessible. This is no easy task. Simplification is an essential part of this, but we need to take care not to over-simplify but instead still retain the essential parts."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#how-to-measure-income",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#how-to-measure-income",
    "title": "Understanding income distributions across geographies and time",
    "section": "How to measure income?",
    "text": "How to measure income?\nTo start we have to select an income measure. Partially this is constrained by data availability, but there are choices. If we are interested in the income distribution for large geographic areas we can consult annual tax filer data. There are a couple of choices we need to make.\nMost income data, including in the newest census, comes down to CRA tax filer data. It is available from the CRA or CANSIM on an annual basis for broad geographies and a few demographic indicators. Census data is available every 5 years on fine geographies and rich demographic indicators. With frequency and geography having a major impact on our choices, let’s see what we can get from both of these sources.\n\nCANSIM/CRA data\nIndividual income is recorded for all tax filers, so most people above the age of 14. It counts the teenager with their paper-run income at the same time as the full-time full-year income earner. Lumping these two groups together can lead to very misleading information. One way around this is to look at “family income” (e.g. CANSIM 11-10-0012), and maybe focusing on couple-family incomes only as they are less prone to compositional problems. We can further filter by age group to exclude (or focus on) senior households or households below 25 years old at the very start of their careers.\n\nHere we used a composite link model assuming an underlying Poisson distribution to estimate a continuous income distribution from discrete bins.\nSome CANSIM data is inflation adjusted, but finer income bands are generally not. Including in the source we chose, so we adjusted the incomes manually to 2016 dollars using Canadian CPI. There may be a case for using metro-specific CPIs. The graphs show clearly how the distributions shift upward in real terms over time.\nIt helps to view different geographies in relation to one another, so here is a view that emphasizes this comparison.\n\nWe can see how Calgary, and to a lesser extent Toronto, have fairly fat tails of high income earners. Comparing Vancouver to Toronto we see how Vancouver’s tail has fattened up quite a bit, although it is still not at Toronto levels. At the same time, Vancouver’s main mode used to be lower than Toronto’s, but it came out higher in 2016.\nAnother interesting data source is CANIM tables 11-10-0193, which looks at adjusted income deciles and 11-10-0192, which looks at family income deciles by economic family type. Just like the above data, these offer consistent time series, but are only available at the Provincial and not the metropolitan area geography. Here is an example of an observable notebook visualizing average adjusted after-tax income by decile for BC over time."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#census",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#census",
    "title": "Understanding income distributions across geographies and time",
    "section": "Census",
    "text": "Census\nThe census comes with several income distributions out of the box, others can be obtained from custom tabulations or micro data. Standard census profile tables are individual total and after-tax income distribution, household total and after-tax income distribution and adjusted after-tax family income distribution. Further we have custom tabulations available at the metropolitan level that look at a wide variety of income distributions.\n\nAdjusted (after-tax) family income\nAdjusted after-tax family income probably gives the simplest way to compare income data across time and geographies. After-tax income measures how much income people have available after taxes and government transfers, and thus also accounts for differences in provincial taxes and benefits. Adjusted means that it’s scaled by (the square root of) the number of people in the family unit. It does not lump roommates into the same category like household income does, but treats each non-family unit on it’s own. And the census is much better in telling apart families than the CRA-based CANSIM data that we mentioned above. The concept also does not suffer as much from compositional issues, where for example median household income in the City of Vancouver is lower than that in the City of Toronto solely because Vancouver has a higher share of 1-person households, which leads to the popular misconception that median incomes in the City of Toronto are higher than in the City of Vancouver. One still has to be aware of confounders like a high share of university students, or retired people, in an area, which may make interpreting low-income results difficult.\nMoreover, the unit of adjusted family income is people, not households or family units, which makes the data very comparable. The data comes grouped into income deciles, based on the national distribution, which makes it easy to compare nation wide, as well as relative changes across time.\nThe advantage of using census data is that we have it for small geographies and can explore spatial variations. Standard census releases have this data since 2011, although 2011 income data, especially when mixed with demographic data, becomes somewhat suspect due to the voluntary NHS. We have written about this before and generally the NHS income data has gotten a worse reputation than it deserves, with critics often not properly understanding the post-processing of income data by StatCan, and running afoul the ecological fallacy when trying to pinpoint biases. 2016 income data on the other hand has been automatically linked from tax returns to the vast majority of census respondents, which, together with post-processing, allows us to have fairly high confidence in data quality even at dissemination area geographies. We should still treat individual areas with some caution as one must expect a few outliers based on sampling and matching issues.\nTo get an idea how this works, here are the distributions of adjusted after-tax family income by deciles for the four most populous CMAs in Canada.\n\nWe see that in Calgary the share of the population in Canada’s top two income deciles is about the same as the share in the top three income deciles in Toronto and Vancouver, and the top 4 income deciles in Montréal. Similarly, the share of the population in the bottom four or five income deciles in Calgary is about the same as the share in the bottom three deciles in the other three metropolitan areas.\n\n\nMapping distributions\nFor mapping purposes we need to find a way to code distributions. Medians or averages are simple, but often over-simplify the problem to the extent that the graphical representation can become meaningless. Take average income, there is no way to distinguish a bi-modal distribution where almost nobody earns the average income from a distribution that is tightly bunched around the average income. That’s why I have a hard time extracting meaningful information from labelling a region as “middle income” using the average individual income, especially when using individual income that is heavily confounded by demographics.\nWhat are alternative ways to label neighbourhoods? At CensusMapper we have been a fan of tri-colour mixing to to show relative sizes of a distribution partitioned into three brackets. For adjusted family income we partition the data into top 30%, middle 40% and bottom 30% of the Canadian adjusted after-tax family income distribution and map this Canada-wide at all census geographies. Recently Jonas Schöley built a nice R package that uses a similar approach, but allows for better colour choices with the legend pulling double-duty as a scatter plot.\nWe could proceed and use the same methods as in CensusMapper and map relative sizes of the Canadian adjusted family income distribution. But as we have seen above, the income distribution can vary greatly between CMAs. If we are interested in concepts like income segregation within a CMA, we should choose our groups relative to the CMA-specific income distribution for each CMA instead of using the Canada-wide distribution.\nThis gets a little more complex than just aggregating up deciles, but nothing that can’t be easily dealt with in a couple of lines of code.\nTo understand income segregation within a metropolitan area we bin people into the bottom 30%, middle 40% and top 30% of the adjusted family income distribution, where the percentile cutoffs are chosen relative to the overall CMA income distribution (and not the overall Canadian distribution like we did on CensusMapper). This requires a bit of estimation, but we expect the estimates to be quite robust for our purposes. Check the code if you are interested in the details. In this approach we are not bound to deciles any more and could have chosen other cutoffs, for example bottom third, middle third and top third, but we felt that having a little fatter middle income group probably more accurately reflects the common understanding of what it means to be in the bottom, middle or top portion of the income distribution.\nTo label geographies, we choose to condense the data into 4 distinct labels. If at least 50% of the population falls into one of our three incomes groups, high, middle, or low, we label the geography as high, middle, or low, respectively. If not, we label the area as “mixed income”."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#vancouver",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#vancouver",
    "title": "Understanding income distributions across geographies and time",
    "section": "Vancouver",
    "text": "Vancouver\n\nVancouver distinguishes itself as being mostly “mixed income”, with some high-income areas in the low-density parts of the North Shore and a cluster of middle-income ares in Surrey. Among our four CMAs, it’s the one with the lowest share of areas dominated by low-income people."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#toronto",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#toronto",
    "title": "Understanding income distributions across geographies and time",
    "section": "Toronto",
    "text": "Toronto\n\nToronto paints a different picture. It’s still dominated by mixed-income neighbourhoods, but less so than Vancouver. It has about double the share of low, middle and high income areas compared to Vancouver, so is overall much more segregated. And it has the highest share of areas dominated by middle income people among our four CMAs."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#montréal",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#montréal",
    "title": "Understanding income distributions across geographies and time",
    "section": "Montréal",
    "text": "Montréal\n\nMontréal has the highest share of predominantly low-income areas, although not by a large margin. If we had based this on the overall Canadian income distribution and not rescaled the income bins relative to the overall metropolitan area of Montréal, we would see a pattern dominated by “middle income” areas, with about 1 in 5 areas with over half the population in the middle 40% of the Canadian adjusted after-tax family income distribution."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#calgary",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#calgary",
    "title": "Understanding income distributions across geographies and time",
    "section": "Calgary",
    "text": "Calgary\n\nEven after rescaling to the overall Calgary income distribution, Calgary still has the highest share of areas dominated by high-income people. It also has the lowest share of areas dominated by middle-income people.\nHad we used the overall Canadian income distribution to bin our categories, Calgary would show two in five areas dominated by high-income people and almost no areas that are predominantly low-income.\nTo compare the amount of income segregation across regions we can plot the share of labels on a common graph.\n\nThis shows that the differences between these regions on our scale are not large, but Vancouver stands out as a mostly mixed-income CMA.\nWe can also try to understand how the segregated labels are distribution among municipalities within a CMA. As an example, we look at Vancouver.\n\nThis confirms that most of the regions predominantly middle-income areas are located in Surrey, and the District of North Vancouver picking up the most predominantly high-income areas, and labelling 12 out of it’s 19 census tracks as such."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#refinements",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#refinements",
    "title": "Understanding income distributions across geographies and time",
    "section": "Refinements",
    "text": "Refinements\nThere is a number of way how we could refine this. A key aspect of data visualization is to simplify the data in just the right way. Removing non-essential nuance and focusing on the essential parts. We could look at other ways to slice the income distribution, maybe using different percentile cutoffs, or using more than just three categories.\nAnother way is to refine our “neighbourhoods”, census tracts in the above case. We could use dissemination areas for finer grained neighbourhoods to understand segregation at that level. With income now being part of the 100% data of the census this makes sense for the first time with the 2016 census.\nA third way is to add more labels. Especially in the “mixed” income areas, may want to distinguish if a mixed income neighbourhood is tending toward being a high income or a low income neighbourhood.\nAs an example, we will re-run the analysis for dissemination areas with more labels, but focusing in on the City of Vancouver to reduce overall clutter and make allow us to still process the information. We still stick with Metro Vancouver as a reference on how to normalize our income bins.\nFor reference, we start with just looking at our original four categories at the census tract level.\n\nRefining the geographies to dissemination areas lends more geographic nuance to the representation, it is fine enough to pick up some areas dominated my middle-income people.\n\nThis reveals that West Point Grey was mixed income because it was made up of a high-income area reaching west of Blanca and a low income area around the non-profit housing project by Jericho.\nAlternatively we could have kept the geographic aggregation level fixed and allowed for more variation in the binning of income groups by quadrupling the number of categories.\n\nThis reveals an important category that the broader categorization entirely missed, the olive band from Second Narrows down to Marpole, as well as the West End, indicating an even mixture of middle and low income people. West Point Grey is now more precisely defined as mixing high and low income. We could make out the reason for this when looking at finer geographies, but this remains hidden when aggregating up to census tracts.\nLastly we refine both the geographies as well as the categories.\n\nThis reveals a lot of nuance, the band of Middle/Bottom income mixture ranging from the Second Narrows down to Marpole through East Vancouver, as well as the West End, is still clearly visible, occasionally punctuate by regions pulling more towards lower and middle income. Just to the west of that East Vancouver band we see a band of greyish-purple that sits between the high and middle income bands. The West Side is dominated by pinkish colours, at turning purplish when adding more middle-income people or brownish when adding more low-income people. Low income pockets are strong around the Downtown Eastside, with occasional pockets popping up elsewhere, in particular the triangle at Arbutus and 33rd. The non-profit housing project at Jericho also leaves a visible imprint.\nEach of these representations have value, adding nuance in either geography or categorization adds additional information, but also makes it harder to process it. This points back to CensusMapper, where this can be built as an interactive tool that allows for interactive explorations. Starting for simple categories and geographies one could tweak the parameters and dive into more details in areas one might be interested in. Maybe I will have to sit down and refine my three-colour mixing CensusMapper maps to allow for discrete colour categories and add a scatter-plot to the triangle legend."
  },
  {
    "objectID": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#upshot",
    "href": "posts/2018-10-28-understanding-income-distributions-across-geographies-and-time/index.html#upshot",
    "title": "Understanding income distributions across geographies and time",
    "section": "Upshot",
    "text": "Upshot\nThese explorations make clear that we have been paying far too little attention to the fact that census tracts within our largest CMAs don’t segregate by income, the vast majority of census tracts are not dominated by low, medium or high income people relative to the CMA distribution. In particular in Vancouver, there are relatively few areas with over half of the population falling into one of these three categories. This invites to take more time and refine the analysis using fine geographic areas or finer income categories, or adding in other demographic factors to help understand the landscape of income in Canadian CMAs.\nThe code for the analysis is available on GitHub for anyone to reproduce, adapt or extend this. In particular it only requires minimal adjustments to reproduce this for any other region in Canada."
  },
  {
    "objectID": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html",
    "href": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html",
    "title": "The rise and fall of Vancouver eligible voters",
    "section": "",
    "text": "Now three people have asked me about the purported explosion of Canadians 18 and over in Downtown Vancouver, and in particular the claim that eligible voters in the Downtown and West End grew by a combined 70%. And I had to explain three times that while that population grew strongly, it grew by much less than reported. In fact, the number of Canadian citizens 18 years and older in the downtown peninsula only grew by 17.1%. Or maybe 21%, depending how to count. So maybe it’s time for a super-quick blog post, if nothing else it’s a reference people can quote and a way for people to check my code and see what I did.\nThe census conveniently provides a data field for Canadian citizens 18 and over, so it’s just a simple exercise of grabbing the numbers from 2006 and 2016 and dividing them. This can easily be done in excel or by hand, not sure how one could get this so wrong. However, there are a couple of details to pay attention to.\nThe 2016 data only considers the population in private households, whereas 2006 considers the population not in institutional dwellings (which is a larger category). That makes comparisons somewhat difficult. On top of that, we know that between 2006 and 2016 some housing that used to be classified as private has been re-classified as collective, in particular some SROs and elderly homes. That means straight-up comparing the 2006 to the 2016 citizen data will be biased in areas where housing got re-classified and where we have non-institutional collective dwellings.\nAnother caveat is that Musqueam 2 also gets to vote in the City of Vancouver elections, so we want to throw those into the mix and not disenfranchise them as almost happend before.\nGetting the data for the city neighbourhoods is easy, it’s available on the Vancouver Open Data Catalogue and we have played with it before and have all the input scripts already set up. Conveniently, the neighbourhood data for Dunbar-Southlands already includes Musqueam 2.\nA last caveat is that the 2016 data is already over two years old now, and things will likely have changed a bit in the meantime. Oh, and then there is the issue that Canadian citizens living anywhere in BC that own property in the City of Vancouver also get to vote in Vancouver. Even if they don’t live in Vancouver. Yes, it seems crazy that this is the case, but property ownership gives people lots of privileges, including special voting rights!"
  },
  {
    "objectID": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#straight-up-census-numbers",
    "href": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#straight-up-census-numbers",
    "title": "The rise and fall of Vancouver eligible voters",
    "section": "Straight-up census numbers",
    "text": "Straight-up census numbers\nThe first approach is to take straight-up census data, graphing the change change in citizens 18 and over as reported in the census and ignore issues around changes in definition and re-classification in dwellings for now.\n\nTotal population varies by neighbourhood, so the order changes if we look at the change in total number of citizens 18 and over."
  },
  {
    "objectID": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#estimating-voters",
    "href": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#estimating-voters",
    "title": "The rise and fall of Vancouver eligible voters",
    "section": "Estimating voters",
    "text": "Estimating voters\nAs mentioned above, there are a couple of issues with the naive census numbers. Some we can’t deal with, we don’t know how many Citizens live in BC outside of Vancouver and own property in Vancouver. But we can try to at least estimate the effects of the change in definitions and classifications in the census, although there is no canconical way of doing this. We first take a look at the change in the population without citizenship information to get an idea by how much this could effect the results.\n\nDowntown and Strathcona stand out, and we have already seen this when previously looking at change in renters by neighbourhood. We don’t know how much collective housing was built in this 10 year timeframe. And we don’t know what share of the population in collective housing are citizens. Negative numbers are a lower bound on people leaving collective housing, positive numbers are an upper bound on people moving into, or getting reclassified as, collective housing. What we don’t know is how many of these are canadian citizens 18 or older.\nBut we can use this to derive an upper bound on the growth of citizens above the age of 18 by simply adding the non-negative numbers onto our previous graph.\n\nComparing to the first graph, we notice that Strathcona is in the blue, and downtown has crept up a bit. Combining Downtown and the West end we get an upper bound of a growth of 21.0%, a slightly higher growth rate than naively using census numbers, but still nowhere near the 70% that was reported."
  },
  {
    "objectID": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#citizens-voting-age-permanent-residents-and-non-permanent-residents-and-collective-dwellers",
    "href": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#citizens-voting-age-permanent-residents-and-non-permanent-residents-and-collective-dwellers",
    "title": "The rise and fall of Vancouver eligible voters",
    "section": "Citizens, voting age, permanent residents and non-permanent residents and collective dwellers",
    "text": "Citizens, voting age, permanent residents and non-permanent residents and collective dwellers\nThat leaves us with an interesting question of how the neighbourhoods are made up. With respect to voting, we are interested in Canadian citizens 18 and over, Canadian citizens under 18, permanent residents, non_permanent residents and people not in private households. Citizens under 18 will reach voting age some day, and Vancouver council has voted on allowing permanent residents to vote, at least those 18 and older.\n\nOr as share."
  },
  {
    "objectID": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#the-map",
    "href": "posts/2018-10-17-the-rise-and-fall-of-vancouver-eligible-voters/index.html#the-map",
    "title": "The rise and fall of Vancouver eligible voters",
    "section": "The Map",
    "text": "The Map\nWe love maps here, so let’s end by looking at the geographic distribution of the change in eligible voters, naively using the census numbers as an estimate, in each neighbourhood.\n\nAs always, the code for this post is on GitHub in case anyone wants to check the code, reproduce the post or adapt it for their own purposes."
  },
  {
    "objectID": "posts/2018-10-13-council-candidate-neighbourhoods/index.html",
    "href": "posts/2018-10-13-council-candidate-neighbourhoods/index.html",
    "title": "Council candidate neighbourhoods",
    "section": "",
    "text": "I haven’t taken time yet to dive into the council candidate’s data game, Christopher Porter has been tearing it up with great posts, one on candidate location, several on their position on various housing issues and a compilation of endorsements.\nDmitry Shkolnik has been running some analysis on candidate’s tweets.\nNathan Lauster dove into the urbanist / preservationist divide first crowdsourced by the Cambie report.\nBetter late than never, I decided to jump in and look at candidate’s neighbourhoods. The immediate surrounding of candidates should give some clues about who they interact with on a daily basis, at least for the candidates that are connected with their neighbourhoods in ways other than just driving through them to get to their home.\nFor this we grab census data form the 500m radius around their rough home location, geocoded from the zip codes that Christopher Porter copied from the disclosure documents and shared via a spreadsheet linked in his blog post.\nBetween mayor and council there are lots of candidates to make a really messy map of where they live and the neighbourhoods we consider.\nFrom here we use our standard tools to grab census data for a couple of variables from the 2016 census and use our tongfen package, as well as the down-sampling from out dotdensity package, to estimate the variables in the given neighbourhoods. (Check the code if you need to know the details.)"
  },
  {
    "objectID": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#transportation-and-tenure",
    "href": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#transportation-and-tenure",
    "title": "Council candidate neighbourhoods",
    "section": "Transportation and tenure",
    "text": "Transportation and tenure\nWe start by mapping out the active transportation (walk, bike, transit) to work and share of renter households.\n\nActive transportation correlates strongly with renter households, and this is also reflected in the graphs. We see that most parties cover a broad spectrum, except Coalition Vancouver (and to a lesser extent Vancouver 1st) that seem to bunch in the lower-left quadrant. I was surprised to see that COPE also does not reach out into the upper-right quadrant."
  },
  {
    "objectID": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#dwelling-value-and-household-income",
    "href": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#dwelling-value-and-household-income",
    "title": "Council candidate neighbourhoods",
    "section": "Dwelling value and household income",
    "text": "Dwelling value and household income\nNo post about Vancouver is complete without talking about dwelling values and household income.\n\nCoalition Vancouver, as well as some independents, score high on the dwelling value and the income scale, with NPA nd Vancouver 1st the only other parties to crack the $2M dwelling value mark."
  },
  {
    "objectID": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#adjusted-after-tax-family-income",
    "href": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#adjusted-after-tax-family-income",
    "title": "Council candidate neighbourhoods",
    "section": "Adjusted after-tax family income",
    "text": "Adjusted after-tax family income\nAdjusted after-tax family income is probably a better way to compare income, so here a little graph on how the parties pan out in this scale. Not much surprising here with most bunching in the middle, as income distribution in Vancouver is quite uniform.\nWe divided the income distribution into “Low” (bottom 30%), “Middle” (middle 40%), and “High” (upper 30%). The shares are relative to Canada’s overall adjusted after-tax family income distribution.\n\nWe see that only independents and ProVancouver have candidates in predominantly low-income areas. Both Coalition Vancouver and independents have candidates in predominately high-income areas."
  },
  {
    "objectID": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#housing-types",
    "href": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#housing-types",
    "title": "Council candidate neighbourhoods",
    "section": "Housing types",
    "text": "Housing types\nWhen it comes to housing types, there is a much greater variety. We divided the structural types into “SFH” (single detached and suited single detached), “Missing middle” (Duplex and row/town houses and low-rise (four or fewer storeys)), and “Highrise” (five or more storeys). This is the same split that we have mapped on CensusMapper before."
  },
  {
    "objectID": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#population-change-and-density",
    "href": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#population-change-and-density",
    "title": "Council candidate neighbourhoods",
    "section": "Population change and density",
    "text": "Population change and density\nAnother much talked about metric in this election is population change and density.\n\nThat huge outlier in terms of population change is Justin Caudwell, who apparently lives in the Olympic Village. Coalition Vancouver is quite heavily invested in areas with losing population, with only COPE, IDEA Vancouver, OneCity and YES Vancouver running no candidate living in neighbourhoods that lost population.\nTo focus in on this we graph just the population change."
  },
  {
    "objectID": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#upshot",
    "href": "posts/2018-10-13-council-candidate-neighbourhoods/index.html#upshot",
    "title": "Council candidate neighbourhoods",
    "section": "Upshot",
    "text": "Upshot\nThis adds another perspective to what may inform the candidate’s position. As always, the code is available on GitHub, feel free to grab it and reproduce the post, add different metrics or expand on this in other ways. To compute the population change we made use of an internal CensusMapper tiling that gives 2011 and 2016 data on a common geographies. To access this, you will have to grab the “label-fix” branch on the cancensus package until this is merged into master and the CRAN version gets updated. It also uses a fork of the tricolore package in order to get more control over the triangle plots."
  },
  {
    "objectID": "posts/2018-08-16-gross-migration/index.html",
    "href": "posts/2018-08-16-gross-migration/index.html",
    "title": "Gross migration",
    "section": "",
    "text": "We have spent much digital ink on the myth of fleeing Millenials, and related misconceptions around the difference between changing sizes of age groups and net migration. And one of our favourite CensusMapper maps visualizes net-migration across Canada.\nToday we want to take a slightly different angle and take a quick look at gross migration, that is look separately at in- and out-migration. More specifically, we are interested in separating out interprovincial and intraprovincial in- and out-migrants, as well as external in-migrants. Unfortunately the census does not have data on external out-migrants since they don’t live in Canada at the time of the census. Duh.\n\nComponents of gross migration\nHere is a quick view of gross-migration by age group for selected CMAs.\n\nWe see how large a role external migration plays for Canada’s “international arrival” metropolitan areas of Toronto, Vancouver and Montreal, with a more muted role in Calgary and much lower impact in Victoria and Ottawa.\n\n\nComponents of net migration\nTo better understand the role of the “international arrival” CMAs, we can collapse the interprovincial and intraprovincial migration to their respective net migration.\n\nDoing so brings the “international arrival” nature of the large CMAs, where lots of external migrants first settle and then disperse from there. For Ottawa - Gatineau and Victoria we see the “university slingshot” in action, attracting lots of in-migrants (domestic and internationally) in their 20s, and sending them off again across Canada to launch their careers in the 30s.\n\n\nNext steps\nLots of interesting things can be learned by diving further into this kind of data. At some point I might need a “demographics” category for my blog. As usual, the code is available on GitHub for anyone that wants to refine this.\n\n\nUpdate (elder ferry)\nI can’t believe I missed the “elder ferry” effect in Victoria. Here is Nathan to add in this interesting observation.\nIt’a worh pointing out that the net domestic in-migration of older people is inter-provincial, so people from outside of British Columbia moving to Victoria.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Gross Migration},\n  date = {2018-08-16},\n  url = {https://doodles.mountainmath.ca/posts/2018-08-16-gross-migration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Gross Migration.”\nMountanDoodles (blog). August 16, 2018. https://doodles.mountainmath.ca/posts/2018-08-16-gross-migration."
  },
  {
    "objectID": "posts/2018-07-28-active-fire/index.html",
    "href": "posts/2018-07-28-active-fire/index.html",
    "title": "Active Fire",
    "section": "",
    "text": "The other day I saw a link to NASA active fire data fly by on Twitter. It’s a satellite-derived world wide dataset at 375m resolution, where one (or several) polar orbiting satellites scan earth in the infrared band from which fire and fire intensity is computed."
  },
  {
    "objectID": "posts/2018-07-28-active-fire/index.html#redding-ca",
    "href": "posts/2018-07-28-active-fire/index.html#redding-ca",
    "title": "Active Fire",
    "section": "Redding, CA",
    "text": "Redding, CA\nWith the Redding fire in the news I decided to take the data for a test drive. And also try out the gganimate package to watch the fire evolve over time.\n\n\n\nAnimated Fire Map\n\n\nWe highlighted the water features, it is interesting to see the fire jump the river. We also notice what looks like fire activity right on the water, this may be due to hot air and smoke being blown over the water and picked up by the satellite.\nLastly we layer all time slices on top of each other to get an idea of the total burnt area."
  },
  {
    "objectID": "posts/2018-07-28-active-fire/index.html#richmond-bc",
    "href": "posts/2018-07-28-active-fire/index.html#richmond-bc",
    "title": "Active Fire",
    "section": "Richmond, BC",
    "text": "Richmond, BC\nWe have had our own fire in Metro Vancouver in Richmond (and the fire data also registers a couple other spots in Delta), so let’s take a quick look what the cumulative past 48 hours of fire activity looks like.\n\nThe fire does show up, but it gives a stark contrast to the scale and intensity of the fire in Redding.\nAs usual, the code is available on GitHub and should be easily adaptable to other locations."
  },
  {
    "objectID": "posts/2018-07-17-making-room/index.html",
    "href": "posts/2018-07-17-making-room/index.html",
    "title": "Making Room",
    "section": "",
    "text": "Recently the City of Vancouver pivoted their planning for RS (“single family”) and RT (“duplex”) neighbourhoods from downzoning, to slow the pace of teardowns to adding infill as an incentive to to keep older buildings through extensive renovations, to now proposing the Making Room program to allow stratification and higher unit density, and Mayor Robertson adding an amendment to direct staff to look into also allowing multiplexes.\nThis change in policy grew out of a series of consultation processes, and it is quite interesting to browse through them chronologically and observe the shift in how participants talk about low density zoning.\nChange is hard, and public opinion is by no means uniform in their views. Making Room if facing fierce opposition in some quarters, all the while there are those, including myself, that are arguing it does not go far enough.\nToday the first part of Making Room is headed to council. The changes associated with this are small, but have symbolic importance. The RS zoning documents start out with the sentence\n\nThe intent of this Schedule is generally to maintain the single-family residential character of the RS-1 District\n\nand the goal is to change this “to better reflect the form of development in those districts”, i.e. that we already allow multiple dwelling units on all lots in Vancouver.\nThe second important change is to remove the mandate on “poor doors” in RS zoning. Right now, a “single family” home can only have one front door, the door to the secondary suite must not be visible from the street to maintain the “single family character”. This is partially symbolic, but it also paves the way to properly design for two dwelling units within the “single family” home, making secondary suites more livable.\nMore important changes are headed to council later this year, with more to come after the election.\nWe would like to take the opportunity of today’s council meeting to look at the data on how people currently live in low-density zoning.\n\nPopulation growth\nThe essence of Making Room is to allow more diverse housing types. Right now what’s legally allowed to be built in RS zones is a “single family home”, possibly with a suite and a laneway house. Suites can’t have front-facing doors in order to preserve the appearance of a “single family” neighbourhood, and the laneway house is restricted in size to about 700 square feet on regular sized lots. That means we allow a maximum of 3 units per lot, when we check our building stock we see we have on average a little less than half that. When it comes to suites we are close to equilibrium, with permit data indicating that we are building them at rates only slightly above the freqency of suites in the overall stock. We have been building around 400 laneway homes a year. Combined this has been barely enough to keep population in RS zones stable, with the east side of RS in Vancouver slightly gaining and the west side slightly draining population.\nSo when it comes to accommodating the population growth in the City, RS (and RT) are our deadbeat areas, whereas the multi-family areas pick up all the slack and accommodate the entire population growth. With RS, RT and FSD zones taking up 77% of residential land use, we are forcing all growth into the remaining 23% of residential land. Which forces us to build tall. And increasingly tear down smaller multi-family to make room for larger multi-family.\nAt the same time, we are aggressively renewing our single-family building stock without adding (significant) units, which leads to the City of Vancouver’s very ineffective construction process, where we tear down 1 unit for every 5 completed. And this does not only mean that we have to subtract one fifth of units from completion data if we want to talk about net new units, it also leads to unneccessary carbon emissions as we accommodate growth inefficiently.\nMaking Room asks low density zones to make room for our growing population by allowing new buildings to contain more units and accommodate more households. This helps improve the ratio of demolitions to completions and thus also reduce carbon emissions.\n\n\nComplex households\nThere is a lot of talk about the current “single family” building stock accommodating multi-generational households. Children might move out of their parent’s home, but some come back after forming their own families and live on their parent’s property. Either in a secondary suite or a laneway house, or sometimes switching roles with their parents depending on family needs. This can be mutually beneficial, and there is some variation across cultures in it’s prevalence.\nNext to multi-generational households, we have more general multi-family households and other forms of “complex households”, for example couple family (or lone parent) households (with or without children) with additional unrelated persons in the household. For the purpose of this section we want to focus on these “complex” households, while excluding roommate type households (two-or-more person non-census-family households).\nMaking Room asks to also allow other ways to split up homes and allow stratification and designing for multiple separate units. While current zoning allows for three units on a lot, it does not allow for three equal units.\nAn interesting question is how many “complex households” there are in “single family” homes, and how has this evolved over time.\n\nWe see that 16% of households in both single detached, as well as suited single detached (“duplex”) units are “complex households”, which is a sizable share. Again, this includes multi-family households, as well as family households with unrelated persons. Is 16% high enough to justify mandating the current form of single ownership “single family” homes? Making Room would not disallow the use of homes as multi-family homes, and it would not disallow building new homes to fit that model. All it does allow to also build and design for other uses that would prefer to split up homes into smaller owenrship portions.\n\n\nGeography of mutiple-census-family households\nLooking at the geography of multiple-census-family households (skipping census family households with persons not in the census family that we aggregated into “complex households” above) can give us more information on where these households live.\n\nAs expected, multi-family households are more prevalent in the “single-family” areas on the east and south side of Vancouver. We can derive further insight by looking at the change in the share of multi-family households over the past ten years.\nOverall, the share of multi-family households decreased slightly from 2.82% of households in 2006 to 2.53% in 2016. We have seen that multi-family households cluster geographically, and we can check how the geographic distribution has changed over time.\n\nSurprisingly we see that the share of multi-family households generally declined on the east side and and increased on the west side. The change in share can be confounded by an increase in the number of households, we can also look at the change in number of multi-family households to get a different view on the change, greying out areas with fewer than 50 multi-family households in 2006.\n\nThis only gives data on the east and south side areas, but it shows that increasing number of household has masked an actual increase in the number of multi-family households in some areas.\n\n\nEmpty bedrooms\nVancouver has seen lots of discussions on empty homes. We read in the news how “unoccupied or under-utilized” homes should return to the market and be made available for purchase or long-term rental. Which is a good short-term initiative to increase the supply of available housing units, and at the same time ease worries about new buildings remaining empty. Here “under-utilized” refers to the the temporal nature, where for example a vacation home may only be occupied for a short period during the year. But homes can also be “under-utilized” if they have more bedrooms than the household requires. It is virtually impossible to ensure optimal utilization of homes, often considerable time passes after children move out and leave bedrooms empty until parents may (or may never) downsize. People purchase homes with additional bedrooms in anticipation of a growing family. Some people simply like and can afford to have extra bedrooms in their home in case of visitors or for other purposes.\nBut during a housing crisis, where we are comfortable to penalize units that are “under-utilized” temporally, we have to ask ourselves why we mandate “overhousing” through zoning that has the effect of forcing homes with more bedrooms than needed. Just to be clear, Making Room does not disallow overhousing, or restrict the number of bedrooms that could currently be built. But it allows homes to be subdivided (and stratified) into units that are right-sized.\nTo measure the degree of “overhousing” we can count “empty bedrooms”. For this we give each person in a household their own bedroom. We could go a bit further and assume couples share a bedroom like we have done in this interactive map, but for this purpose we want to keep it simple. That means for example that we count a couple household with 1 kid to have no extra bedrooms if they live in a home with three or fewer bedrooms. We count studio apartments as 1 bedrooms here, and the other assumptions around large number of bedrooms or large household size are engineered to under-estimate the count of empty bedrooms. For details check out the code.\nSo how many empty bedrooms are there in low-density zoning? The census can’t quite answer than, but it can answer how many empty bedrooms there are in low-density housing units, the vast majority of which are situated in low-density zoning.\n\nThe result is really quite telling. Out of the 89,435 extra bedrooms, 60,505 of which are in “single family” homes. And this is not counting bedrooms in unoccupied or temporarily occupied homes (or suites).\nAs an aside, we can also split the count of empty bedrooms by tenure. It should not surprise that renter households tend to be overhoused much less frequently with 3.16% of renter households having extra bedrooms versus 15.2% of Owner households across all structural types of dwellings.\nPart of that is confounded by owner households being over-represented in “single family” homes, but even when just looking within single detached (so unsuited single family) homes we have 12.8% of renter households overhoused versus 25.6% of owner households.\n\n\nHousehold size\nA related dimension is that of household size. We can compare how the average household size has changed since 2006 by structural type of dwelling.\n\nThis shows that both Single Detached and Duplex (a.k.a. suited single detached) units have seen a drop in household size, while household size has remained constant in the other dwelling types.\nThis shows that overhousing has been increasing in these dwelling types.\n\n\nChildren\n“Single family” zoning emphasizes “family”, which in people’s minds usually means “family with children”. One question we can ask is what portion of people are children in each dwelling type.\n\nThe results are quite surprising, at least to us. Row/Townhouse and Semi-detached (commonly referred to as “side-by-side duplex”) dwelling units have the highest share of children, followed by suited “single family” homes (“duplex”) and single detached homes. Unsurprisingly, the share of children is much lower in apartments, with low-rise registering with a higher share of children compared to taller apartments.\nIf accommodating families with children is one of the priorities, then Making Room is moving in exactly the right direction in that regard.\n\n\nAffordability\n Ownership in low-density zones has been rapidly moving out of reach for the majority of Vancouver families, which begs the question who is served by maintaining low-density zoning.\nMaking Room proposes to allow stratification of housing units, which in principle gives the option of either buying the traditional “single family home” and renting out a suite, or stratifying the suite as a separate unit (assuming compliance with building codes) and only buying one of the two. Looking at how this plays out in Mount Pleasant RT zones where we have been allowing stratification and multi-plexes in connection with character retention and infill we see that this can substantially lower per-unit cost while only slightly inflating land values.\nMaking Room is bound to lower the bar of entry into low-density neighbourhoods. And reduce debt levels, the best mortgage helper is a lower mortgage (so stratifying the suite and selling it separately instead of taking on more debt and renting out the suite). But we are starting off from really high unafforadbility levels, moving to duplexes will make it still challenging to again be affordable to average families. Median incomes of City of Vancouver families with children was $112k in 2015. Building stock will take time to turn over and adapt to new regulation, most of the stratified duplexes coming out of this process will be new stock and thus more expensive. Moving beyond duplexes to multiplexes will further increase options and probably reach the majority of Vancouver families with children, we will have to do more analysis or wait for the staff report to quantify that.\nSo does Making Room go far enough to restore basic levels of inclusivity in Vancouver? I don’t think so. We have seen how low income residents have gotten pushed out of City of Vancouver proper, with the top income brackets registering very strong growth. At a rate that’s noticeably higher than Metro or Canadian averages. Median incomes in the City of Vancouver used to lag the City of Toronto in 2005, but in 2015 City of Vancouver came out on top of City of Toronto. Inflation-adjusted average City of Vancouver home values have increased 89.1% 2006 to 2016, average inflation-adjusted ownership costs have increased 16.2% and average inflation-adjusted rent has increased 21.3%. At the same time, inflation-adjusted average family incomes have increased 16.3%.\nThis is reflected at the individual household level in that the share of shelter-cost-burdened households, those paying more than 30% of income on housing, has decreased 2006 to 2016. And this still holds true when running this separately for renter and owner households.\nSo affordability actually increased using the CMHC 30% of income on shelter metric, due to strong income growth, self-selection in migration patterns by wealth and income, and tighter distributional matching of incomes to housing costs. The losers in the process are those without access to wealth, high income or both. It will take time to reverse some of this development. This will require a massive investment into public housing, as well as lowering the entry-point to market housing. Making Room works on that latter part, and ideally it can get expanded to higher density of maybe 6 units per lot to reach further down the income distribution. At the same time we need to find ways to address the lower ends of the income distribution that the Vancouver market simply won’t be able to serve, even if the market corrects somewhat.\nThe city has been working of subsidized rental models via inclusionary zoning, and providing land for alternative ownership models. The province and the federal government are starting to take their role in the public housing sector more seriously. But it will take more than what is currently being offered to undo the results of decade-long inaction on the public housing front.\n\n\nUpshot\nThe data of how low-density housing is getting used points toward the Making Room initiative going in the right direction. There are many factors contributing to Vancouver’s affordability problem, the abundance of low density zoning close to the centre of our metropolitan area is an important one. Many Vancouverites look with envy toward Vienna for their affordable housing, but often neglect that, among other important differences, Vienna does not have low density areas so close to the city centre like Vancouver does.\nAs usual, the code that made this post is available on GitHub, including for gathering the required data, generating the graphs and all numbers quoted in the text. Feel free to download it to reproduce or adapt for your purposes.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Making {Room}},\n  date = {2018-07-17},\n  url = {https://doodles.mountainmath.ca/posts/2018-07-17-making-room},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Making Room.”\nMountanDoodles (blog). July 17, 2018. https://doodles.mountainmath.ca/posts/2018-07-17-making-room."
  },
  {
    "objectID": "posts/2018-06-21-skytrain-rents/index.html",
    "href": "posts/2018-06-21-skytrain-rents/index.html",
    "title": "Skytrain rents",
    "section": "",
    "text": "A friend of mine is looking for a new rental, which reminded me that I always wanted to do a quick map of rents near skytrain stations. Should not be too hard.\n\nSkytrain station data\nFirst we need the skytrain stations, which we grab from TransitLand using our transitland R package that we used before to look at transit frequency in Vancouver. \nlibrary(cancensus)\nyvr &lt;- get_census(\"CA16\",regions=list(CMA=\"59933\"),geo_format=\"sf\",level=\"CSD\")\n\nlibrary(transitland)\nroutes &lt;- get_transit_routes(list(bbox=st_bbox(yvr),per_page=1000), get_all = TRUE) %&gt;% \n  filter(operated_by_name==\"British Columbia Rapid Transit Company\")\nstops &lt;- get_transit_stops(list(served_by=unique(routes$operated_by_onestop_id)),get_all = TRUE) %&gt;%\n  st_transform(26910) %&gt;% st_buffer(800) %&gt;% st_transform(4326) %&gt;%\n  filter(!duplicated(gsub(\" PLATFORM \\\\d+$\",\"\",name)))\nHere we decided to go with 800m radius around each station.\n\n\nRental listings data\nNext we need data on rents. CMHC or census rent data give overall stock rents, for people like my friend that are moving we should turn to rental listings data, which we have used before to look at how many square foot you can rent for $1,500/month in Toronto. My friend is interested in an unfurnished one or two bedroom unit near rapid transit, so that’s the data we will grab and compute median rents per bedroom and station.\nlibrary(rental)\nlistings &lt;- get_listings(\"2018-03-01\",\"2018-06-01\",region=st_union(stops),\n                         beds=c(1,2),filter=\"unfurnished\") \nstation_rent_data &lt;- st_join(stops,listings) %&gt;% \n  group_by(onestop_id,beds) %&gt;%\n  summarize(ask=median(price),n=n()) %&gt;%\n  mutate(ask=ifelse(n&lt;10,NA,ask))\nHere we grey out stations with fewer than 10 observations.\nThat’s it, now all that’s left is to grab some background map data and graph the results.\n\n\n1 bedroom rents\n\n\n\n2 bedroom rents\n\n\n\nNext steps\nThere are obvious next steps. We could see how asking rents have evolved over time near the stations. Or through in the B-Lines or other bus routes for comparison. Or get a little more technical and look at rent per square foot and run some more in-depth analysis. But for tonight this will have to do.\n\n\nUpdate\nAnother interesting metric is the availablility of rentals. So we round this off with the number of 1-bedrooms listings per month in each area.\n\nThis shows that especially the Millennium line is under-served when it comes to 1-bedroom rentals, with fewer than 5 rental units becoming available each month along stretches of the line.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Skytrain Rents},\n  date = {2018-06-21},\n  url = {https://doodles.mountainmath.ca/posts/2018-06-21-skytrain-rents},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Skytrain Rents.”\nMountanDoodles (blog). June 21, 2018. https://doodles.mountainmath.ca/posts/2018-06-21-skytrain-rents."
  },
  {
    "objectID": "posts/2018-06-04-vancouver-streets-and-lanes/index.html",
    "href": "posts/2018-06-04-vancouver-streets-and-lanes/index.html",
    "title": "Vancouver Streets and Lanes",
    "section": "",
    "text": "Mitchell Reardon asked me a question about lanes in the City of Vancouver: “Do you happen to have a figure (or quick way to calculate) the number of laneways in Vancouver, and the amount of space they take up?” I have looked at the overall space taken up by roads before using the Metro Vancouver land use dataset, but never looked just at lanes. But that’s easy enough to do thanks to the streets package in Vancouver’s Open Data Catalogue. So I decided to run the numbers. And write up a quick blog post for posterity, in case someone has questions or wants to refine this.\nGiven the sheer mass of land dedicated to streets right-of-ways in Vancouver we should be paying a lot more attention to this. Transportation corridors can be a great resource for public space and even parks as Vancouver’s ratio of park space to people is slowly declining. The Arbutus Corridor is just the latest example of how a transportation corridor can be repurposed, and the proposals on how to shape the individual sections looks amazing.\nYou should also check out Mitchell’s article in Spacing on lanes, the space they take up, and what else could be done with some of that space.\n\nThe Streets\nThe dataset distinguishes three types of streets, “city streets”, “lanes”, and “non city streets”. Let’s take a look what these are.\n\nWe see that non city streets are private strata roads (e.g. Champlain Heights or Tugboat place) or ministry roads like Highway 1 or the roads in Stanley park. We also not that the dataset includes some roads outside of CoV, including the ones in Musqueam 2 that in many cases is included in CoV planning. There is a judgement call to be made if we should include Musqueam in our analysis, we did include it in our previous post. (Of note here is that the Metro Vancouver land use dataset did not include non city roads.) But Mitch’s question just mentions Vancouver, so let’s stick with that for this post.\n\n\n\nRoad ROW widths\nTo understand how much space the roads right of ways take up we need to know how wide each road segment is. Conveniently enough the dataset has information on that. Unfortunately, the individual road segments aren’t labelled, but the widths dataset consists of points at which measurements (or estimates) were taken. We can fold those in to label the segments that we can identify that way. It’s a bit messy as the location of the measurements do not exactly coincide with road segments. We take the approach that each measurement applies to exactly one road segment, the one closest to it.\nThis kind of matching gets messy (and computationally expensive) fast, check the code if you need to know the details of how we wrangled the data.\nThe following gives an overview of the result of our matching process.\n\nIt turns out that we have very good information on lane widths, ok information on regular city streets and poor information on non-city streets. In fact, it seems likely that the city does not have any data on the width of non-city streets and the few we found are due to faulty assignments in our matching algorithm.\nOne way to fill in the gaps is to assign the average width of the roads that we do know the width for to the unknown ones for each street type. Here is what our algorithm found.\n\n\n\n\n\n\n\n\n\n\nType\nUse\nHave Width Data\nTotal Length\nAverage Width\n\n\n\n\ncity street\nArterial\n53%\n222.5 km\n25.2 m\n\n\ncity street\nClosed\n100%\n0.4 km\n20.1 m\n\n\ncity street\nCollector\n50.1%\n41.2 km\n22.3 m\n\n\ncity street\nLeased\n100%\n0.2 km\n20.1 m\n\n\ncity street\nRecreational\n100%\n0.1 km\n15.2 m\n\n\ncity street\nResidential\n49.2%\n1,079.4 km\n19.5 m\n\n\ncity street\nSecondary Arterial\n51.2%\n91.2 km\n22.5 m\n\n\nlane\nlane\n98.8%\n774.6 km\n6 m\n\n\nnon city street\nnon city street\n4.38%\n123.4 km\n17.4 m\n\n\n\n\n\nMapping Widths\nThe last thing to do is to to get a clear handle on Vancouver’s streets based on their width. Before we get there we want to take stock of the road widths data we have derived so far.\n\nWe can also represent this visually on a map.\n\n\n\nArea Covered by Streets\nWith the missing widths estimated by using averages for each type we can now compute the total area each street type takes up, and what ratio of total Vancouver land area that is. We have to be a little mindful how to do this properly. Our street segments connect at the centre lines. If we just multiply the length of each segment by it’s width we will count the intersections double.\nA cleaner way to do this is to turn the street network into a polygon by buffering each segment with half the street widths and bake this into one big polygon in which any overlapping areas will only appear once. That won’t allows us to separate out the area taken up by lanes though, which was the original question. But that’s easy to fix, we just need to do this procedure separately for each street type and then union, intersect or take differences depending on what question we want answered. We won’t distinguish between city and non-city streets for this, but separate out lanes. And we will also refine the total length of lanes calculation by only counting the length of the lanes up to the road right-of-way instead of to the middle of the road.\n\n\n\nType\nLength\nArea\nShare of City Land\n\n\n\n\nLane\n678.6 km\n4.078 km²\n3.5%\n\n\nRegular Street\n1558.3 km\n29.013 km²\n25.2%\n\n\nTotal\n2333 km\n36.583 km²\n31.8%\n\n\n\nOne should emphasize again that this is road right of way, not just the paved surface area. It includes sidewalks, nature strips and encrouchments.\nThe individual street types add up to a total area of 33.091 km² covering 28.8% of City of Vancouver land area. Note that this is a slightly different result from what we got previously using Metro Vancouver land use data, with the difference being explained by Metro Vancouver land use data not counting non-city streets and including Musqueam 2.\nLastly we can map the lane and street polygons to get a visual representation of the actual area taken up by the roads right of way.\n\nAs always, the R Notebook underlying this post lives on GitHub. Download it to reproduce this, or modify it to fit your purposes.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Vancouver {Streets} and {Lanes}},\n  date = {2018-06-04},\n  url = {https://doodles.mountainmath.ca/posts/2018-06-04-vancouver-streets-and-lanes},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Vancouver Streets and Lanes.”\nMountanDoodles (blog). June 4, 2018. https://doodles.mountainmath.ca/posts/2018-06-04-vancouver-streets-and-lanes."
  },
  {
    "objectID": "posts/2018-05-25-short-term-rental-licenses/index.html",
    "href": "posts/2018-05-25-short-term-rental-licenses/index.html",
    "title": "Short Term Rental Licenses",
    "section": "",
    "text": "In the City of Vancouver operators of short term rentals now need to obtain a license from the city to legally operate. There are restrictions on what units can be rented out short term. I won’t get into the nitty-gritty, but here is the main point. Licenses are only available for people that want to rent out their primary residence, that is the place where they reside at least half a year. Secondary suites or laneway houses count as separate residences for this purpose, so e.g. a person living in the main unit of a house can only STR the main unit, but not the secondary suite or the laneway house. However, long term tenants that claim the secondary suite as their primary residence can STR their suite if the landlord permits it.\nMy favourite part about licensing is that we get licence data. Yay. Time for a quick blog post using the data we have so far on the City of Vancouver Open Data Catalogue.\nUp to now we have 868 STR licenses issued, excluding a handful of cancelled licenses. The data comes with license number, name of the holder, issue and expiry dates. Addresses and location data is stripped, but it does identify the neighbourhood the STR is in. Here are the current counts by neighbourhood.\n\nWe can also take a quick look at the issue date, there was a big spike on April 20, that quickly petered off to a steady but low stream.\n\nWe might update the post at some later point in time, but those that want more regular updates are welcome to grab the code on GitHub and run it regularly. It will pull in the latest data and automatically update.\n\nUpdate\nSome asked for a version normalized by dwellings. The neighbourhood level census data provided by the City of Vancouver misses the dwelling variable, we we use the one for occupied dwellings instead. Which probably is just as well, as one cannot get a license for an unoccupied dwelling unit. Here is a quick update for share of STR license among occupied dwelling units.\n\n\n\nEven later update\nI probably should have thought this through earlier, but purpose-built rental buildings are unlikely to get short-term rented by their tenants. The same is true for tenants in secondary suites or laneway houses. To STR these the tenant needs explicit approval from the landlord, who has not much to gain by allowing this. So here is another way to normalize the data, only counting “taxable” dwelling units like we in a previous post on the empty homes tax.\n\nAnd lastly, we can also normalize the STR licenses by owner-occupied dwellings. Realistically speaking, those are the only people that can make the decision to STR their unit without having to ask anyone for permission. An owner household going on a two week vacation abroad can STR their unit to help pay for the trip, a renter household wanting to do the same will first have to ask for permission from their landlord.\n\nI think I got all my bases covered now. If you want to fine-tune this even further, maybe take out strata buildings that have STR restrictions, just go grab the code and hack away. The taxable dwelling unit map requires access to my enriched CoV assessment database though, but if you comment that portion out it should pull in all the other data you need and reproduce everything else. Also, the data will auto-update, so if you run this at a later time it will pull in the STR license data for that point in time.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Short {Term} {Rental} {Licenses}},\n  date = {2018-05-25},\n  url = {https://doodles.mountainmath.ca/posts/2018-05-25-short-term-rental-licenses},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Short Term Rental Licenses.”\nMountanDoodles (blog). May 25, 2018. https://doodles.mountainmath.ca/posts/2018-05-25-short-term-rental-licenses."
  },
  {
    "objectID": "posts/2018-05-11-building-height-profiles/index.html",
    "href": "posts/2018-05-11-building-height-profiles/index.html",
    "title": "Building Height Profiles",
    "section": "",
    "text": "Jim has been using the Copernicus building height data for select European cities to understand the height profiles of cities. Building heights by distance from city centre in London and Paris, from 2012 EU Copernicus data. On average, buildings in Paris are taller throughout. pic.twitter.com/rtGiiBC7pd— Jim Gleeson (@geographyjim) May 11, 2018\n\n\n\nWe thought these were pretty cool. Sadly we don’t have a dataset like this for Canadian metro areas, but we can hack together something similar using LIDAR survey data. Vancouver and Toronto have LIDAR derived building data available. It only covers the respective cities, not cover the whole metro area, although the Vancouver data includes Musqueam 2, the UBC area and Mitchell Island.\nNot much work is needed to get the height profiles, just a tiny bit of data wrangling and the distance computation. We take Granville/Robson and King/Bay as the locations of the respective city centres.\n\nWe should remember that Toronto has about than 4 times the size of Vancouver, and these graphs have a different x-axis scale. While Toronto has higher buildings downtown than Vancouver, on average Vancouver has taller buildings in the core. To better compare the two we can plot them on the same graph.\n\nHere we cut off the axis and added a lot of opacity to better discern how the buildings group together. A point of caution is that the further out we go the more the graph will be affected by being constrained to city limits. In Vancouver we notice a gap and then clear uptick in average building heights, which is due to the UBC area being included in the data. Another caveat is that some buildings might be misclassified in the data. I am somewhat familiar with the Vancouver data that I mapped a couple of years ago, and garages are also captured in the mix. I have not dug into the Toronto data, might be worthwhile to do that at some point.\nIt’s interesting to see that in Vancouver the buildings are on average higher in the core, but lower outside the core, when compared with Toronto.\nAs always, the underlying code is available on GitHub if you want to reproduce or adapt it for your own purposes.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Building {Height} {Profiles}},\n  date = {2018-05-11},\n  url = {https://doodles.mountainmath.ca/posts/2018-05-11-building-height-profiles},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Building Height Profiles.”\nMountanDoodles (blog). May 11, 2018. https://doodles.mountainmath.ca/posts/2018-05-11-building-height-profiles."
  },
  {
    "objectID": "posts/2018-03-28-frontage/index.html",
    "href": "posts/2018-03-28-frontage/index.html",
    "title": "Frontage",
    "section": "",
    "text": "Over the past years several people have asked me questions about street frontage of city properties. When I needed similar data for a work project, and Scot Hein asked me a question about frontages of commercial properties for his Urbanarium debate, I decided to finally pull the numbers. The answer to that question is not as straight forward as it might seem, mostly because properties aren’t necessarily square. There are a couple of algorithm that can solve this problem, but in this case we can keep things reasonably simple as the City of Vancouver has property frontages listed on VanMap and make the data available on their Open Data Portal.\nScot was particularly interested in the frontages in the commercial zones along the arterials. The ingredients we need to answer that question are:\nAll of these can be found on the City of Vancouver open data portal. I’ll spare you the details on how I chose to match things up, if you need to know you can just download the R notebook that made this post and look at the code.\nAs an aside, at this years open data day event I was reminded that there can be high entry barriers to using CoV open data. At this point I have a highly processed database that I usually use for this kind of questions, but with a narrow and clearly defined question at hand I thought it might be a useful exercise to build the data from scratch so that others have the benefit of being able to fully reproduce and adapt the analysis.\nThere will be essentially no commentary in this post, Scot will have done a did a better job adding context than I can. Sadly I am out of the country and will have to watch the recording once it gets posted."
  },
  {
    "objectID": "posts/2018-03-28-frontage/index.html#building-age",
    "href": "posts/2018-03-28-frontage/index.html#building-age",
    "title": "Frontage",
    "section": "Building Age",
    "text": "Building Age\nBuilding age is another important variable in this, as we generally expect higher re-development pressure on older buildings.\n\nWe can view the properties by number or cumulative area."
  },
  {
    "objectID": "posts/2018-03-28-frontage/index.html#relative-building-value",
    "href": "posts/2018-03-28-frontage/index.html#relative-building-value",
    "title": "Frontage",
    "section": "Relative Building Value",
    "text": "Relative Building Value\nThe relative building value may is a more direct measure of development pressure, as we have explained in detail previously."
  },
  {
    "objectID": "posts/2018-02-28-extra-school-tax/index.html",
    "href": "posts/2018-02-28-extra-school-tax/index.html",
    "title": "Extra School Tax",
    "section": "",
    "text": "The new BC provincial budget had lots of interesting changes. One of them is the additional school tax on residential properties, charged at a rate of 2 basis points (0.002) of the assessed value above $3M and an additional 2 basis points of the assessed value above $4M.\nAfter some initial confusion the province clarified that this tax will not to apply to purpose built rental buildings, each of which is typically one single taxable property, which can easily breach the $3M threshold for multi-unit buildings. This would have resulted in a hit of a little over $100 per unit per month for the average rental unit, which would not apply to a similar condo development because the it would be applied to each individual unit, few of which would break the $3M threshold.\nSeparating out all properties the tax applies to is not as trivial as it should, mostly because the data needed to determine this is not public. But we can get a pretty good approximation my merging CoV open data on property parcels, zoning, tax data and Metro Vancouver land use data. I will skip the details, people that will want to know how it’s done most likely already know this.\nNarrowing down to residential land uses, throwing out the purpose built rental buildings and the commercial components of mixed use properties, we see that the new school tax will apply to roughly 21k properties and adds up to around $115M in revenue for properties within the City of Vancouver. There is a slight bit of uncertainty in these estimates as we may not be properly catching the most recent developments, might not properly filter renter building get caught in data problems when merging data sources that aren’t perfectly in sync, but this gives us a good base to understand the impact of this tax.\nTo understand the geographic distribution of these we can aggregate the total tax payable up by physical lot. For single family homes, there is nothing to aggregate, but for condo buildings (or land strata) we want to sum up all the extra tax for each individual condo unit (if any) and visualize the resulting total for the entire building.\n\nOverall the tax mostly hits single family homes, around 20k for a total of $100M. The remaining 2k units hit by the tax are mostly condos, adding up to a total of $15M.\nGeographically, the west-side single family homes dominate. But those aren’t the properties that generate the highest school tax. Some of the high-value condo buildings rack up more as the top 20 list shows:\n\n\n\n\n\n\n\n\nRank\nAddress\nExtra Tax\n\n\n\n\n1\n277 THURLOW ST\n$1,504,478\n\n\n2\n1011 CORDOVA ST W\n$1,255,520\n\n\n3\n1139 CORDOVA ST W\n\\(1,052,272 |\n|    4|1169 CORDOVA ST W    |\\) 750,152\n\n\n5\n1281 CORDOVA ST W\n$ 649,738\n\n\n6\n1233 CORDOVA ST W\n$ 646,782\n\n\n7\n323 JERVIS ST\n$ 592,002\n\n\n8\n428 BEACH CRES\n$ 569,882\n\n\n9\n1328 MARINASIDE CRES\n$ 559,246\n\n\n10\n1560 HOMER MEWS\n$ 525,500\n\n\n11\n181 ATHLETES WAY\n$ 374,896\n\n\n12\n1717 BAYSHORE DR\n$ 353,756\n\n\n13\n3085 POINT GREY RD\n$ 301,348\n\n\n14\n4707 BELMONT AVE\n$ 273,280\n\n\n15\n1060 ALBERNI ST\n$ 198,032\n\n\n16\n1007 MAIN ST\n$ 177,012\n\n\n17\n1155 MELVILLE ST\n$ 174,952\n\n\n18\n4719 BELMONT AVE\n$ 172,736\n\n\n19\n2815 POINT GREY RD\n$ 169,500\n\n\n20\n1465 HASTINGS ST W\n$ 159,744\n\n\n\nAs always, the code that made this post is available on GitHub.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Extra {School} {Tax}},\n  date = {2018-02-28},\n  url = {https://doodles.mountainmath.ca/posts/2018-02-28-extra-school-tax},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Extra School Tax.”\nMountanDoodles (blog). February 28, 2018. https://doodles.mountainmath.ca/posts/2018-02-28-extra-school-tax."
  },
  {
    "objectID": "posts/2018-02-22-building-permits/index.html",
    "href": "posts/2018-02-22-building-permits/index.html",
    "title": "Building Permits",
    "section": "",
    "text": "The City of Vancouver has put up building permit data yesterday, and Aaron Licker swiftly took a look at the data and teased out some interesting bits.\n@VanOpenData @vb_jens @LausterNa Thanks CoV for the new building and demo permit data.  There is clearly a pattern with regards to the pace of residential (re)development (ie demolitions) in the City pic.twitter.com/TMdex1OkIc— Aaron Licker (@LGeospatial) February 22, 2018\n\n\n\nWe have been asking for this data for quite a while, so we had to take a look too. We wrapped the nasty download, geocoding and general data wrangling into convenience functions, so importing the data into a tidy data frame is pretty simple now. The complete code that made this post is, as always, available on GitHub.\n# Vancouver City Outline\nvancouver &lt;- get_census(dataset='CA16', regions=list(CSD=\"5915022\"), labels=\"detailed\", \n                        geo_format='sf', level='Regions')\n# Grab Zoning data and clip to outline\nzoning_data &lt;-get_zoning_data() %&gt;% st_transform(st_crs(vancouver)$epsg) %&gt;% \n  st_intersection(vancouver %&gt;% select())\n# Grab and geocode building permits and join with zoning data\nbuilding_permits &lt;- get_permit_data(2017) %&gt;% st_join(zoning_data)\nTo start off we take a look at what’s in the dataset\nplot_data &lt;- building_permits %&gt;% \n  group_by(`TYPE OF WORK`,`PROPERTY USE`) %&gt;% summarize(Count=n())\nggplot(plot_data, aes(x=`TYPE OF WORK`,y=Count,fill=`PROPERTY USE`)) +\n  bar_theme \n\n\nDeomlitions (and Abatements)\nLooks pretty good. Following Aaron’s footsteps we take a closer look at Demolitions. And throw in Abatements too. Which leads us to our first question. Where are we going with this? What we are really interested in is understanding teardowns, we have had a long fascination with that topic, in fact the first post on this blog was in interactive teardown predictor map, that we have since greatly improved, worked into an interactive data story and further extended by using this to understand the trade offs of embodied vs operating carbon in Vancouver’s Single Family Zoning.\nWhen working on understanding teardowns we tried to obtain demolition data from the City, but were unsuccessful. So we used new buildings as a proxy, while having to be aware that there were a couple of greenfield developments in our time frame (Hello Deering Island!) where new builts may not have been preceded by a teardown. This new dataset adds that missing link, but right now it is only available for 2017 onward.\nBut we have to be careful with this dataset too, it only contains information about demolition permits, but it does not speak to whether the building in question was actually torn down. There have been prominent cases in the news where buildings were spared even though a demolition permit was issued. But buildings that do get torn down will have a demolition permit issues, and if a new building goes up on that lot we can take that as confirmation that the demolition permit was followed by an actual demolition. This dataset on it’s own only contains building permit data, so again we don’t know if the building permitted was actually built, but assessment data is available to confirm a new built.\nIn short, what this dataset adds is that it narrows time frames, gives an earlier indication of what might happen and adds some detail that is otherwise not publicly available, like if a new building or a teardown come with permitted suites and for what lots laneway house permits were issues.\nIn this post we will take the permit data as a proxy for what actually happened, so we treat a demolition permit as a demolition, and a building permit as a new built. One complication is that the same building may have several building permits, or a demolition permit and an abatement permit. So when reporting numbers we will have to make sure we don’t double-count. Our convention for this post is that if a demolition and an abatement happened, we treat that as a single demolition and no abatement. And if a new built and an addition happened, we treat that as a single new built.\n\nThe geographic distributions shows that demolitions are concentrated in the Single Family (RS) neighbourhoods.\nbase_map + \n  geom_sf(data=demo_data, aes(color=`TYPE OF WORK`) ,size=0.3) +\n  labs(title=\"Demolitions and Abatements in 2017\")\n\nWe narrow down our focus to these areas and look at what kind of buildings get torn down or abated.\nplot_data &lt;- demo_data %&gt;% filter(CATEGORY==\"One-Family Dwelling\",`PROPERTY USE`==\"Dwelling Uses\")\nbase_map +\n  geom_sf(data=plot_data, aes(color=`Specific Use`) ,size=0.3) +\n  labs(title=\"Demolitions and Abatements in 2017\",caption=\"CoV Open Data\", color=\"Type\")\n\nTo better understand the change in the building stock we want to look at the overall count in permits in more detail.\n\nWe see that Laneway Houses are a net add to the overall building stock, with 568 permits issued. The total number of demolition permits for Single Family Homes is about the same as the number of demolition permits, but the new builts have a higher proportion of permitted suites at 58.6% compared to 33.3% in demolitions. But we should remember that there are a lot of unpermitted suites in Vancouver.\n\n\nEstimating secondary suites in Vancouver\nEstimating secondary suites relies on exploiting the different ways to count housing units between BC Assessment and the census. BC Assessment counts properties, so for our purposes lots with single family homes on them. For us that means lots with either a single detached house, or ones with a single detached house with a permitted suite on them. There are roughly 75k of these in Vancouver.\nWe have explained the census categories for dwellings before, to match these we start by counting up the single detached houses (so un-suited) houses the census found. There were 41,330 occupied single detached homes, 290 occupied by temporary or foreign residents and 1,955 unoccupied ones, adding up to a total of 43,575 single detached houses. The census counts laneway houses as their own single detached house. The city does not release up-to-date counts on laneway houses, CMHC counted 1,668 completed laneway houses before May 2016. Permit data by the city suggests that this is an under-count, but we will just go with that number for now, partially accounting for the census not finding all of these.\nIn conjunction that means that the census found 41,907 single family lots with no secondary suite, which leaves us with 33,093 single family lots that do have at least one secondary suite. This comes out to be 44% of single family homes having at least one suite. We will stop here for today and leave it for someone else to estimate how many of these have more than one secondary suite.\nThis exercise shows us that we are likely demolishing more suites than the permit data suggests, with the difference being made up by permitted suites. But it also shows that the number of suites is likely still growing in the City as the share of new single family homes with suites is significantly higher. But as we have explained in detail before suites are a strange part of our dwelling stock that end up registering as unoccupied at a higher rate than any other building type in Vancouver. So it takes more work to understand if and how much new dwelling units for people to live in this sizable construction activity in Single Family land actually produces.\nAs Aaron has pointed out before, this construction activity comes with a high cumulative price tag.\n\nIf we look at demolition, abatement and additions for Single Detached and Suited Single Detached throughout the city (not just in RS) in 2017 the total permit value adds up to $673,952,171, while adding very few units to live in.\n\n\nNeighbourhoods\nAnother way to look at the data is to follow Aaron’s lead and compare demolitions to the existing building stock. We will take CoV neighbourhoods as our base geography and only look at Single Family lots. For the census we restrict ourselves to occupied units for convenience since the CoV custom tabulation did not pull information on the total building stock. From that we estimate the number of SFH lots in each region by taking the number of single-detached units plus half the number of duplex units. As we have explained above, that’s only a rough estimate and could be off by a bit in either direction, but it’s good enough for our purposes. We will compare this to SFH demolition permit data.\n\nInterested in taking this further or explore different aspects of the data? The complete code that made this post is available on GitHub, just grab it and modify as you see fit.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Building {Permits}},\n  date = {2018-02-22},\n  url = {https://doodles.mountainmath.ca/posts/2018-02-22-building-permits},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Building Permits.”\nMountanDoodles (blog). February 22, 2018. https://doodles.mountainmath.ca/posts/2018-02-22-building-permits."
  },
  {
    "objectID": "posts/2018-02-01-real-estate-industry/index.html",
    "href": "posts/2018-02-01-real-estate-industry/index.html",
    "title": "Real Estate Industry",
    "section": "",
    "text": "I keep seeing people circulate claims of the type: The Real Estate Industry makes up x% of British Columbia’s GDP, and apparently I am the last person left that doesn’t know what that actually means. So I decided it’s time for me to catch up. Yesterday I saw a chart that came with a source, a good opportunity to sit down and reproduce it to figure out what the “Real Estate Industry” is, or in this case how to measure the “Addiction to Housing Boom”.\nUnfortunately roughly 22% of BC’s economy is dependent on Real Estate. pic.twitter.com/DmuAnpTnHT— Steve Saretsky (@SteveSaretsky) January 31, 2018\n\n\n\nTo be clear, I don’t expect to get a crystal clear answer about this, just a better idea what everyone is talking about. It seems to me that we are interested in economic activity associated with residential housing construction, as well as realtor and broker activity and residential real-estate related financial activity.\n\nCANSIM 379-0028\nTo see what’s in that CANSIM table, we load the data and zoom in the year that we are interested in, remove the grouped categories so that we get a clean split of the individual categories in the dataset. \nThe first thing we notice that the dataset does not have data for Canada overall, and that can’t be reconstructed or computed from the dataset since it only contains shares and not totals. So clearly CANSIM 379-0028 was not the source for this data, or at least not the only source.\nNext, we need to decided what industries are meant by “Housing Construction and Real Estate”. Looking at the options we guess “Construction” and “Real estate and rental and leasing”. \nSo far so good, but the numbers don’t match. Welcome to my world of shitty data forensics, trying to figure out what some graph that got quoted without context is actually showing.\nSo CANSIM 379-0028 was a dead lead, and I am still no smarter. Time to look for data that is better suited for our question.\n\n\nCANSIM 379-0030\nTo get a finer cut at these categories we use CANSIM 379-0030. The first step is to reproduce the data we have already seen. This table has the added advantage of also containing dollar values, so we can also compute the data for Canada overall. And since we have newer data for 2016, let’s use that.\n\nAlways good to see we get the same numbers as from the other table (except we chose to use newer data for this plot). Now let’s dive in a little more and decompose these two categories to better understand what’s captured under “Construction” and “Real estate and rental and leasing”.\n\nWe immediately notice the biggest contributor, “Real Estate”, which we want to understand better.\n\nThe largest chunk of that is “Owner-occupied dwellings”, which is just imputed rent. The number 2 item is rent, and we see the relative sizes of the “Owner-occupied dwellings” and “Lessors of real estate” vary by province according to the proportion of the households that rents, with 80% in Nunavut renting vs only 23% in Newfoundland and Labrador.\nNext up is “Engineering construction”, which is also worth splitting up further.\n\nRental and leasing services captures “Consumer goods rental”, “General rental centres General rental centres”, and “Commercial and industrial machinery and equipment rental and leasing”.\nWe can put all sub-categories into a grand overview.\n\n\n\nResidential Real Estate Industry\nMany of these categories have nothing to do with the residential real estate industry, we need to narrow things down a bit.\n\nThis almost reproduces the numbers from the graph we started with, but even if we select 2015 we still overshoot by a couple percent. I will leave it to someone else to dig deeper into what categories exactly were used, or if there are other possible issues. There may be an argument to remove “Repair construction” from the list, but that won’t help with matching the numbers either.\nIf we want to take a narrower view we can focus in on the core aspects of what people usually associate with the “Residential Real Estate Industry”, construction of residential buildings and agents and brokers. \nWe clearly see that British Columbia stands out with exceeding 5% of GDP attributed to this narrow view on the residential real estate industry.\n\n\nTimelines\nWhile we are at it we might as well look into timelines. To keep things simple we start out with our narrower view of the Real Estate Industry from the last graph.\n\nIf we were interested in the broader category including the big-ticket items of rent and imputed rent we can see how those have evolved over time.\n\n\n\nTakeaway\nThese numbers depend to a high degree on what’s included and what is not. The “Residential Real Estate Industry” is complex and hard to pin down. But we can make some assumptions and get some answer. We can compare across provinces and across time.\nAn important takeaway that the biggest ticket item in the numbers that get ferreted around is imputed rent. Homeowners are certainly a big part of the real estate industrial complex, and the ones that have been the main profiteers from Vancouver’s crazy market.\nRent is the second biggest single ticket item, although Residential construction and real estate agents and brokers combined eclipse it in British Columbia.\nBritish Columbia certainly has a higher share of GDP from residential real estate related activities than the other provinces, no matter if sliced narrowly or more widely. I would be grateful if people making a point about this would link back to what they include and the data they use to that laymen like me know what is actually being talked about. My way of doing this is, as always, providing the code that made the analysis.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Real {Estate} {Industry}},\n  date = {2018-02-01},\n  url = {https://doodles.mountainmath.ca/posts/2018-02-01-real-estate-industry},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Real Estate Industry.”\nMountanDoodles (blog). February 1, 2018. https://doodles.mountainmath.ca/posts/2018-02-01-real-estate-industry."
  },
  {
    "objectID": "posts/2018-01-12-pedestrian-counts-or-when-the-kindergardener-needs-to-use-the-bathroom/index.html",
    "href": "posts/2018-01-12-pedestrian-counts-or-when-the-kindergardener-needs-to-use-the-bathroom/index.html",
    "title": "Pedestrian counts, or when the kindergardener needs to use the bathroom",
    "section": "",
    "text": "Two days ago we took a first look at motor vehicle traffic counts, now it is time to turn to pedestrian lights. Everyone knows the “beg buttons” that pedestrians need to push for the pedestrian signal to turn green. If you forget to push the pedestrian light might stay red even if parallel motor vehicle traffic has a green light, all in the name of efficiency of motor vehicle traffic.\n\nBeg buttons\nThe only good thing about beg buttons is that they leave data traces, and we want to see if we can extract useful information from them. But there are some challenges. Unlike the traffic light induction loops, the beg buttons don’t count pedestrians. They count “begs”. So the kindergardener that really needs to use the bathroom and keeps “begging” in half-second intervals will easy register upward of 100 begs in the system, while the 50 zen-like people lining up at the light where only the first one registered a single beg counts as 1. Some of these issues could be resolved with fine temporal resolution of the data, but Surrey’s data comes aggregated to 15 minute intervals. That still gives us some way to account for the kindergardener hopping from one leg to the other by making some assumptions about signal cycle frequency and capping the counts. So let’s see where this gets us.\n\n\nBegs\nOne good thing about blog posts is that more knowledgeable people than me add information. Chad Skelton added information about school start and end times, and the friendly folks at Surrey Open Data hooked me up with their traffic engineer in charge of the data who explained to me how to extract the beg button data. We will gloss over the details, if you need to know you are probably better served to look at the code than to read through my ramblings.\nUsing the same methods as in our previous post we take begs from Tue, Wed, Thu for three weeks prior and post school start this past summer.\n\nThe difference between the days before and after the start of school is quite pronounced, we average further over the three weekdays to get a cleaner picture.\n\nThe start of school stands out even more in pedestrian begs than in motor vehicle traffic, although the overall numbers are much smaller. The spikes around the school start time, which is between 8:30am to 9am depending on the school, as well as around pickup time, which varies between 2:30pm and 3pm, are very visible.\n\n\nSchools\nJust like before we want to refine this by adding in proximity to schools as another variable.\nWe divide beg buttons into two groups, one within 200m of a school and the other more than 200m away from schools.\n\nArmed with this data we can run our analysis separately for the two types of beg buttons.\n\nFor all weekdays we see that the spike in begs is significantly more pronounced in proximity to schools.\n\nAggregating over all three workdays we also notice a another distinct peak between 11am and 12pm. It is less pronounced than the dropoff and pickup peaks and we have no intuition what might cause this, but maybe someone else knows.\n\n\nFiltering out the urgency\nSo far we have only looked at the total number of begs, it’s time that we alleviate the problem of the kindergardener in need for a bathroom. The maximum begs in a 15 minute window in our dataset is 482 begs. We feel the pain of the kid that pushed that button and sincerely hope they reached the bathroom in time. The data mess this caused is fairly simple to deal with in comparison. NACTO pegs the ideal traffic cycle length in an urban setting at 60 to 90 seconds, so let’s just assume that the signal cycle length is 1 minute. That means if you just missed the green light, you will have to wait a minute for the next green. So our 15 minute window has at most 15 green phases. Then we can transform our unit of “begs” to an estimated number of “pedestrian crossing cycles”, that is cycles where at least one (and possibly more than one) pedestrian is crossing by capping the number of begs to 15. In other words, assuming a 1 minute signal length we are trying to count our 50 zen-like people the same way as the kindergardener in need of urgent relief.\n\nSo even just looking at (an estimate of) pedestrian cycles we notice an uptick during school days, that is more pronounced closer to schools.\n\n\nSummary\nThe great news is that children still walk to school! The number might be lower than we would like, but it’s big enough to show up in data. And the relative effect is larger than the one in motor vehicle traffic, which probably means that generally we drive too much.\nAs always, the code to reproduce this analysis is available on GitHub for anyone interested in reproducing or refining the methods.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Pedestrian Counts, or When the Kindergardener Needs to Use\n    the Bathroom},\n  date = {2018-01-12},\n  url = {https://doodles.mountainmath.ca/posts/2018-01-12-pedestrian-counts-or-when-the-kindergardener-needs-to-use-the-bathroom},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Pedestrian Counts, or When the\nKindergardener Needs to Use the Bathroom.”\nMountanDoodles (blog). January 12, 2018. https://doodles.mountainmath.ca/posts/2018-01-12-pedestrian-counts-or-when-the-kindergardener-needs-to-use-the-bathroom."
  },
  {
    "objectID": "posts/2018-01-09-school-traffic/index.html",
    "href": "posts/2018-01-09-school-traffic/index.html",
    "title": "School Traffic",
    "section": "",
    "text": "School has started, and with it debate about people driving their kids to and from school is flaring up. And again people are questioning how much traffic is caused by this. As someone who bikes to school with his son every day I am keenly aware of the traffic mess around schools. But since I choose not to drive regularly, I don’t have a feeling for broader traffic patterns on non-school days to compare this too.\nBut who needs anecdata when we have actual data on this: counts from traffic light induction loops. While the City of Vancouver has been talking for more than a year to make this data available, Surrey has been at it for about a year now (big thanks!), and we have played with that data a while back. It seems however that the Surrey Open Data API changed since then and the interactive map from last year is not pulling in live data any more. Instead of updating the old map I thought my time is better spent in doing something new.\nThere are still lots of challenges when using loop counts for answering questions about school traffic, but it’s a great resource to ground the discussion on. But it is also a resource that I admittedly have not spent much time with, and that will need some work to better understand the data quirks and issues. But not better way to get started with this than doing an exploratory blog post!\n\nInduction Loops\nMost people will recognized induction loops as the circular (or rectangular) cutouts in the road close to the stop line of traffic lights that detect if cars are waiting to make a turn. The loops emit an RF frequency which will induce a current in nearby metal objects, e.g. a car sitting on top, which in turn will induce a magnetic field that will induce a current in the induction loop. Which enables the induction loop to detect the presence of metal objects: motor vehicles in most cases.\nSurrey publishes (a selection of?) traffic loop counts on their Open Data website, about 1/3 of which have been geocoded so we know where they are.\n\n\n\nCounts\nWe compare traffic counts for a regular work day before and after school starts. To this end we use the start of the summer semester as a test, mostly because we don’t have enough data for the beginning of the spring term and because we don’t have good comparables for non-school days over the winter break.\nOne issue is that the data is surprisingly volatile. To overcome this we average over Tue, Wed, Thu for three weeks prior to school start this summer compared to three weeks after school start. This should give us a first approximation of how traffic changes between school and non-school days, with the understanding that some of the difference may be due to more people being on vacation during non-school days and that there is a fair amount of children being driven to camps instead of schools during non-school days.\n\nThe data still shows substantial volatility even after averaging over three weeks, but there appears to be a consistent uptick in traffic associated with pickup times on school days, as well as increased traffic volumes within an hour before dropoff time. We can further average over the three weekdays to get a cleaner picture.\n\nWithout diving deeper into the changes in traffic patterns that are not associated with the start of the school year it is hard to make definite claims. The spike at pickup time around 3pm seems to correlate very well with the hypothesis of increased school traffic, but that argument seems a little harder for the morning spike that occurs quite a bit earlier than the 9am dropoff time slightly earlier than the Surrey schools dropoff time that varies between 8:30 and 9:00 am as Chad Skelton pointed out, who also notes classes are out already at 2:30pm at some schools.\nWe will have to dig a little deeper to tie this to school traffic.\n\n\nSchools\nIf change in traffic has anything to do with schools it stands to reason that on balance the change will be more pronounced in proximity to schools. So let’s load in the school data.\n\nWith this we can divide traffic loops into two groups, one within 200m of a school and the other more than 200m away from schools. We could choose a softer cutoff strategy or run a more broad analysis using school proximity, and we probably also should account for school enrolment, but that gets complex fast and won’t fit into my evening blog post time budget.\n\nArmed with this data we can run our analysis separately for the two types of traffic loops.\n\nWhat we see here is that while the effect seems more pronounced close to school on Tuesdays and Wednesdays, the reverse appears to be true on Thursdays.\n\nAggregating over all three workdays we see however that the spikes during school days do correlate with proximity to the school. So we do feel that this preliminary analysis verifies with some confidence that there is indeed a measurable uptick in traffic that can be associated to school dropoff traffic. Again, it is hard to quantify this in light of the noise in the data and that our control period likely also contains dropoff and pickup traffic to camps, some of which are also located at schools.\n\n\nSummary\nIt appears that an increase in school-related traffic can be seen in traffic loop data, and it would be worthwhile to refine the methods and explore this more thoroughly. As always, the code to reproduce this analysis is available on GitHub for anyone interested in reproducing or refining the methods.\nAn obvious way to extend this is to compare instructional to non-instructional days, as well as to refine the association of traffic loops with schools and also account for school enrolment.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {School {Traffic}},\n  date = {2018-01-09},\n  url = {https://doodles.mountainmath.ca/posts/2018-01-09-school-traffic},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “School Traffic.”\nMountanDoodles (blog). January 9, 2018. https://doodles.mountainmath.ca/posts/2018-01-09-school-traffic."
  },
  {
    "objectID": "posts/2017-12-01-what-s-a-household/index.html",
    "href": "posts/2017-12-01-what-s-a-household/index.html",
    "title": "What’s a Household?",
    "section": "",
    "text": "Recently there has been a lot of talk about households, and how they match up with dwelling units. So it’s probably a good idea to take a closer look at what a household is.\nApart from better understanding what a household is, I want to also dig deeper into how people live in households. Part of my motivation is to start to understand latent demand in our housing market, some of which may come from suppressed household formation. As a starting point for this I want to better understand “complex” household types that may strive to “simplify”, to put it in language introduced further down. There are many confounding factors for “complex” household types that we would have to account for in order to move from the simple enumeration of household types that we do in this post toward using this to derive estimates for latent demand of suppressed household formation. So this post is more of an inventory of household types and their intersection with dwellings, and does not (yet) make any statements about latent demand.\nBut let’s start of with the census definition of a household. And also touch on what a dwelling unit is. Dwellings are simpler, so we will do that first."
  },
  {
    "objectID": "posts/2017-12-01-what-s-a-household/index.html#types-of-private-households",
    "href": "posts/2017-12-01-what-s-a-household/index.html#types-of-private-households",
    "title": "What’s a Household?",
    "section": "Types of private Households",
    "text": "Types of private Households\nWhich brings us to the question of what a “household” is in terms of the living arrangements of the people in a household. The census distinguishes a range of (private) household types:\n\ncensus family households\n\none census family only households\none census family household with additional persons\nmultiple family households (with or without additional persons)\n\nnon census family households\n\none person non census family households\ntwo or more person non census family households\n\n\nTo understand what that means we need to understand what a census family us. A “census family” consists of a couple (married or common law) with or without children, or a lone parent with children at home. Families can have two generations related by blood or adoption or marriage/common law at most, so if a couple lives in a household with their children and grandchildren or they live with their child with their married or common law spouse also present, that would count as more than one census family. (However, grandparents living with grandchildren without the parents of the grandchildren present also counts as a census family.)\n\nHere we distinguish “simple” household types which are what we generally think of when we hear people say “household”, and “complex” household types, that is household constellations that don’t fit the traditional picture of either a single person living alone, or a family living together. In total there are 139,820 “complex” households in Metro Vancouver. Families living together includes cases where grandparents, children and grandchildren living in the same household, as well as co-housing unrelated families with or without children. One couple census family households without children may be an evolution of a two-or-more person non-census family households (aka roommates) where two people in the household form a common law or married couple.\nTo understand how Vancouver fits in we can compare the distribution with other regions. \nWe see that Calgary and Edmonton have a relative strong share of two-or-more person non-census-family households (several unrelated people sharing a household). In Vancouver and Toronto we see a strong share of one couple census families with other persons in the household without children, as well as multiple census family households. These are household types that are often quite similar to several unrelated people sharing a household, but in a more advanced stage in live where family formation starts to happen and some of the people living together are common law or married couples.\nTo understand this a little better we throw in the age of the household maintainer.\n\nKeying by age shows that Calgary and Edmonton’s higher share of two-or-more persons non-census family households is due to these cities having a higher share of the population in the younger age groups where this household type arrangement is more common.\nWe can re-order the graph to better compare regions for each age group. \nWe clearly see how family formation happens very early in Edmonton and Calgary, who also build up a relatively thick portion of family households with other persons in the household or multi-family households early on. Vancouver and Toronto overtake the two Alberta cities when household maintainers are in their 30s and continue to lead in that segment from that age on.\nTo complete this section we show the percentages of each household type per Metro Region."
  },
  {
    "objectID": "posts/2017-12-01-what-s-a-household/index.html#crowded-housing",
    "href": "posts/2017-12-01-what-s-a-household/index.html#crowded-housing",
    "title": "What’s a Household?",
    "section": "Crowded Housing",
    "text": "Crowded Housing\nWe can further dive into the question of households that may be missing bedrooms by considering the people in the household in more detail. The National Occupancy Standard (NOS) is, for better or worse, a metric that attempts to measure pressure households feel to upgrade their number of bedrooms. A dwelling unit is deemed suitable under NOS if every household member has their own bedroom, with the exception that couples may share a bedroom, as well as two children if they are under the age of 5 or of the same sex and under the age of 18. We should remember that this is a subjective measure, and the pressure experienced by individual households to upgrade their housing will vary greatly depending on their cultural background. Browsing through our interactive housing Suitability map on CensusMapper we recognize some overlap of areas with high proportion of households not meeting NOS with areas that have higher proportions of immigrant households, but there are also counter-examples.\nWe can try to understand the number of households not meeting NOS, and their bedroom shortfalls in order to meet NOS, keyed by household type.\n\nOne-person household of course meet NOS in all cases, couple without children households meet NOS unless they inhabit a studio apartment. We can normalize this as a share of households by household type and compare across metro regions.\n\nIt is interesting to see that Montréal takes the lead of high proportion of NOS households for some household types, but overall Toronto and Vancouver have a higher share of households not meeting NOS compared to Montreal as the following table shows.\n\n\n\n\n\n\n\n\n\nMetro Area\nTotal Households\nHouseholds not meeting NOS\nShare of Households not meeting NOS\n\n\n\n\nCalgary\n519,685\n23,650\n4.6%\n\n\nEdmonton\n502,145\n26,920\n5.4%\n\n\nMontréal\n1,727,300\n97,580\n5.6%\n\n\nOttawa - Gatineau\n535,510\n22,075\n4.1%\n\n\nToronto\n2,135,915\n205,425\n9.6%\n\n\nVancouver\n960,900\n70,370\n7.3%\n\n\n\nLastly we want to take a quick look at how housing suitability wrt NOS is effected by tenure.\n\nAs expected, renter households tend to be more squeezed for housing. We already saw that renter households tend to have fewer spare bedrooms, this confirms that renter households also are more likely to not meet NOS."
  },
  {
    "objectID": "posts/2017-11-28-under-construction/index.html",
    "href": "posts/2017-11-28-under-construction/index.html",
    "title": "Under Construction",
    "section": "",
    "text": "Currently we are at record levels of dwelling units under construction in Metro Vancouver. At the same time, we are also at record timelines from building start to completion. Those two are of course related in that the more projects are being worked on simultaneously, the harder it is to find and coordinate all the labour and materials to finish the projects. There are other reasons too for escalating construction times. We are building taller projects than we used too, more of it is concrete than there used to be. Single detached construction times are probably lengthened by increasingly larger and more elaborate single family homes.\nRecently our current record under construction inventory has received a lot of attention as an indication of future completions. But just looking at under construction can give us misleading messages about what to expect. To get a cleaner idea we should normalize by the average duration construction. For example, if the under construction inventory is 10 times what is normal, but completion times are also 10 times what’s normal, then the expected rate of completions is no different than the normal."
  },
  {
    "objectID": "posts/2017-11-28-under-construction/index.html#under-construction",
    "href": "posts/2017-11-28-under-construction/index.html#under-construction",
    "title": "Under Construction",
    "section": "Under Construction",
    "text": "Under Construction"
  },
  {
    "objectID": "posts/2017-11-28-under-construction/index.html#length-of-construction",
    "href": "posts/2017-11-28-under-construction/index.html#length-of-construction",
    "title": "Under Construction",
    "section": "Length of Construction",
    "text": "Length of Construction\nThe length under construction is the mean duration projects completed in a given month took to complete.\n\nWe right away notice a gap in the data for 1991 and 1992. This won’t bother us too much, but will make some of our estimates later on less accurate for the relevant time frames.\nUp until 2002 it took about 6 months to complete a single family home in Metro Vancouver, by now it takes about a year. Row/Townhouses have traded places with single family homes now as the product that completes fasted these days.\nThe increase in construction times around 2002 coincides with the increase in apartment construction, and the construction time lengthened for all product types in a similar fashion. That suggests that the primary cause of increased construction time is a labour shortage in the construction sector and not just the trend toward concrete and taller buildings."
  },
  {
    "objectID": "posts/2017-11-15-statcan-web-maps/index.html",
    "href": "posts/2017-11-15-statcan-web-maps/index.html",
    "title": "StatCan Web Maps",
    "section": "",
    "text": "Not sure how long this has been live, but this morning fellow cancensus developer Dmitry flagged a new StatCan feature. Interactive thematic web maps. Essentially it enables users to choose from the a selection of 2016 census variables and map them. You can zoom and pan around, and select the aggregation levels to display the data at down to census tracts. And there is a option to download single variables as CSV.\nAfter building and running CensusMapper for over two years now I have some opinions.\n\nThe Good\nThere are a couple of things I like. The map is using a conic projection instead of the ubiquitous web Mercator. That’s a big plus, especially for a country like Canada where web Mercator grossly dilates the northern parts. I probably would have chosen a different base longitude to make the 49th parallel appear more horizontal though. But I am in no position to criticize that choice, CensusMapper is still using web Mercator. Next, they have comparative widgets at the bottom which provide some context.\n\n\nThe Bad\nOn the downside, my first impression was that the tool feels clunky. And it’s essentially useless on mobile. It’s hard to overestimate the importance of mobile these days, getting quick information on the go is a big way of how people consume data these days. 35% of visits on CensusMapper are from mobile (excluding tablets), and the share of mobile usage is growing.\nThe portal only has data for the most recent (2016) census, and only a small subset of variables. Of course some of this might change, but give the choice of ESRI architecture that seems unlikely. The licensing fees StatCan likely pays for this tool in it’s current limited state are likely already quite expensive. There is a reason why CensusMapper shied away from using big corporation technology. These platforms simply don’t allow efficient mapping of census data, whereas there is zero penalty on the CensusMapper platform to make all variables available for users to map.\nThe technology underlying the StatCan map also can’t be adapted to allow for the type of flexible mapping CensusMapper was built for, where a user can type an arbitrary function based on census variables in their browser window and instantly map the result variable. Simple examples of maps are a Gini coefficient or a diversity index or models that aim to estimate transportation costs based on census variables. And more elaborate maps like for example net migration patterns would likely require significant re-design of their platform, if at all possible. The flexibility for general web users to build these kind of more complex maps derived from census data is either very hard or simply not possible with existing big corporation technology. Sadly I have not been able to justify spending the time to beef up the interface to this to make this available to the general public, but there is quite a few people that I have given access to this feature and they have made some amazing maps aimed to inform policy.\nThe “sharing” feature is very limited, it does not even remember the map location and zoom level. Should not be hard to update that though, so this should only be a temporary issue.\n\n\nMy 2c\nI should start my commentary off by stating that I pitched CensusMapper to StatCan about 2 years ago. I built CensusMapper to fill an information gap, where only experts could have access to census data. StatCan was not interested in supporting or absorbing CensusMapper in their quest of making census data more widely available to non-experts. It’s important to stress that CensusMapper is not a polished product. I’d call it a functional demonstration of how census data can be opened up to the general public. And there are obvious ways to expand on it too. The reason why I don’t fix the issues and add functionality is mostly that I simply can’t justify putting in the time and effort into a free product. And I have no interest in closing CensusMapper off and selling the services it offers in order to fund continued improvements. There are already commercially available products with partially overlapping functionality, and it’s not really the thing I enjoy doing.\nIn the meantime I have glanced down the dark abyss of the government procurement process. It’s as drawn out and clunky as the new StatCan map, and while running a small and agile company like MountainMath I simply can’t get myself to participate in the long drawn-out process when I could be doing interesting things instead. There are some people in government that have managed to take the pain out of this process and contacted me directly and helped me navigate this without wasting enormous amounts of time to leverage the agile and cost-effective abilities of a small company like MountainMath to deliver great value at low cost. I am looking forward to sharing some results of such a collaboration in the near future.\nFrom two years ago when I started CensusMapper as a side project to now my focus has shifted considerably. Some of this is reflected in me opening up the CensusMapper APIs to the general public to easily access census data. And the work on cancensus, which is an R wrapper around the CensusMapper API and facilitates fast, transparent and reproducible analysis based on census data.\nThe StatCan data download functions, including the one integrated into the new mapping tool, are a good step forward into the right direction. But they are a far cry away from an API, as all the people that relying on the “Member ID”, the variable identifier StatCan has been using in their census release datasets for 2016, can attest to. StatCan reshuffled the match between Member ID and variables in their last update of 2016 census data. As far as I know, StatCan still plans of providing data APIs. Originally the 2016 census was supposed to be released using the “New Dissemination Model” based on APIs, and I am looking forward to when the API will get done. In the meantime, CensusMapper has APIs for the 2006, 2011 and 2016 censuses that are open and free for anyone to use. They have the disadvantage of not coming directly through an “authoritative” StatCan website, but on the upside they are designed with the needs of analysts in mind, with for example the ability to select data for mixed (by aggregation level) geographies\nIn summary, it’s a great step forward. But I don’t anticipate the need for open tools like CensusMapper and the CensusMapper API to go away anytime soon.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {StatCan {Web} {Maps}},\n  date = {2017-11-15},\n  url = {https://doodles.mountainmath.ca/posts/2017-11-15-statcan-web-maps},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “StatCan Web Maps.”\nMountanDoodles (blog). November 15, 2017. https://doodles.mountainmath.ca/posts/2017-11-15-statcan-web-maps."
  },
  {
    "objectID": "posts/2017-11-02-updated-foreign-buyer-s-data/index.html",
    "href": "posts/2017-11-02-updated-foreign-buyer-s-data/index.html",
    "title": "Updated Foreign Buyers Data",
    "section": "",
    "text": "After the BC government stopped publishing foreign buyer’s data after May this year it reversed course and gave the data out to media outlets earlier this week. It started being released to the general public only earlier today, with the complete data becoming available around noon today.\n\nThe Data\nThere is a number of metrics in the data, the one we will focus on is the share, median dollar value and total dollar volume of foreign buyer purchases. The dataset comes in two parts, one for 2016 data and one for 2017 data. Sadly the two datasets are based on slightly different regions, use different spellings and have some other minor compatibility issues. We show data for all transactions, residential and non-residential.\n\n\nFull timelines\nComparing data through all the datasets is tedious, so we only run the full timelines for data that is easy to compare.\n\n\n\nWhat we see is that the share of foreign transactions briefly spiked in the time frame where the foreign buyer’s tax got announced, likely because sales were being rushed through before the deadline. Sadly we don’t have robust measurements of pre-tax sales volumes, we see that the post tax share and dollar volumes are quite volatile. It is difficult to discern clear trends in the share of foreign buyers, other than 10% appearing to roughly be the cap for Burnaby and Richmond, and 5% for Vancouver and Surrey. There are significant spikes in the total foreign transaction volumes, as well as the relative median transaction price, indicating that at times foreign purchases skew high. Apart from the occasional spikes it appears that median purchase prices of foreign buyers are roughly in line with overall median prices.\n\n\nA closer look at 2017 Data\nWe take a closer look at the top 8 municipalities in terms to total transaction values involving foreign buyers in 2017 thus far. Data is not available for all months due to at times small number of transactions. Next to the municipalities we already looked at above this selection criterion also yields Coquitlam, Victoria, Saanich and the Township of Langley.\n\n\n\nThe municipalities that got added have around 5% share of foreign buyers, with Saanich registering higher. Apart from the spike in relative median transaction values for Surrey that we have already seen above we also notice a spike in high-value foreign buyer transactions in Saanich around February. Other than that, median purchase price involving foreign buyers seems to be in line with overall median purchase prices.\nAs always, the R Notebook that built this post is available on GitHub for those interested in playing with it or looking at different aspects of the data. We plan on occasional update posts as new data becomes available and we notice interesting new trends. We doubt we will be this on a monthly basis, so download the R Notebook if you want more frequent updates.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Updated {Foreign} {Buyers} {Data}},\n  date = {2017-11-02},\n  url = {https://doodles.mountainmath.ca/posts/2017-11-02-updated-foreign-buyer-s-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Updated Foreign Buyers Data.”\nMountanDoodles (blog). November 2, 2017. https://doodles.mountainmath.ca/posts/2017-11-02-updated-foreign-buyer-s-data."
  },
  {
    "objectID": "posts/2017-10-26-a-first-look-at-vancouver-housing-data/index.html",
    "href": "posts/2017-10-26-a-first-look-at-vancouver-housing-data/index.html",
    "title": "A First Look At Vancouver Housing Data",
    "section": "",
    "text": "The 2016 census data on housing got released yesterday. We had the data imported and available on CensusMapper by 10am yesterday morning, after a slight delay due to the 40GB SSD space on the server running out and requiring additional disk space to be mounted for the data import. Lots of people have already used CensusMapper to dive into the data, but I had only just now found some time to take a look myself. This is a descriptive overview as a first step toward understanding what is happening. As always, the code for the analysis and graphs is embedded in this post and the underlying R notebook can be downloaded from GitHub.\n\nThe big question\nOne of the most anticipated parts of the release was data on the shelter cost to income ratio. The key affordability measure in census data. The shelter cost to income ratio is computed at the individual household level, which sets it apart from most other affordability measures that operate on aggregate data. The measure published in the census is the proportion of the households that spends more than 30% of their (pre-tax) income on running shelter costs, and the data can also be split up for owner and tenant households separately.\nAnd the big surprise was that the affordability in the City of Vancouver improved by this measure. Yes, you heard that right, affordability improved when compared to 2006 or 2011. That had a lot of close observers perplexed as it seems to contradict the experience many people have on the ground.\nCases like this exemplify why careful analysis is needed.\n\n\nImproving Affordability\nTo start things off let’s look at the numbers. We will use cancensus to load the data. It’s a little cumbersome since we have to request the data separately for each census year. Sadly don’t have the resources available to enable querying variables across several census releases.\n\nAt first glance it looks like the share of owner and tenant households spending more than 30% of their income on shelter cost has increased between 2006 and 2011, and then dropped off by 2016. But we should remember that the 2011 data comes from the NHS, and unlike when dealing with the NHS income data that performed admireably well for larger municipalities, there is no reliable administrative data for this available that StatCan could have used in their post-processing to counter-act the NHS non-return bias as it was able to do for pure income data by comparing it to aggregate CRA data. It is quite possible that the 2011 NHS numbers somewhat over-estimate the proportion of households not meeting this affordability metric.\nBut even ignoring the 2011 data, there has been a slight drop in both owner and tenant households spending 30% or more of their pre-tax income on running shelter costs. Which is really remarkable.\nAs we dig deeper into this it is important to recall that just looking at median incomes is not sufficient for understanding affordability questions. We have to look at the change in the whole income distributions, we have done in detail a month ago. We have seen that Vancouver lost households in all income groups below $60,000, and almost all gain occured in the $100,000 and over income bracket.\n\n\nA deeper look at 2016 numbers\nTo better understand how to interpret this we first look at some important contextual variables.\n\nBoth owner and tenant households have grown over the years, but the number of tenant households made a jump. This means that the relative composition of owner and tenant households has changed, with proportionally more households renting now. This could skew the income of tenant households higher.\nNext we look at subsidized housing, which is only available for the 2011 and 2016 years. \nWe see that the share of tenant households in subsidized housing went. The overall number of subsidized units grew slightly, but to the number of tenant households grew faster. Overall this is unlikely to have a meaningful impact to explain the improvement in affordability we see in the data.\nLastly we compare the change in the median multiple metric for owner and tenant households combined.\n\nThis development in this graph, which is using a variant of the often cited median multiple metric, seems to contradict the shelter cost to income ratio in the first graph. Although this does not split out owner and tenant households separately, this seems to indicate that the answer to our question lies in a more detailed look at the income and rent distributions that is obscured by only focusing on medians.\n\n\nMetro Vancouver\nTo get more spatial context we look at the region more broadly.\n\nWe see that affordability by this measure hasn’t gotten better in all municipalities in the region. Even if we ignore the 2011 NHS data, affordability decreased significantly in Coquitlam, Burnaby and Richmond.\n\nLooking at the split by tenure we see that the share of tenant households is up everywhere 2006 to 2016 except for Burnaby. The start contrast in share of tenant households in Vancouver compared to the rest of the region also becomes very clear.\nNext we compare the share of tenant households in subsidized housing.\n\nThis seems to indicate that the difference in affordability for tenant households can’t be explained by different ratios of affordable housing.\nLastly we compare the average shelter cost to median income ratio.\n\nAgain, the development of the the median multiple metric is inconsistent with the shelter cost to income ratio computed at the individual level.\n\n\nShelter Cost, Income, Affordability, Tenure and Household Types in Metro Vancouver\nWe want to gain a deeper understanding what affordability looks like when separated out by different variables in Metro Vancouver. Stats Canada has a custom tabulation available that can do exactly that.\nAt the CMA level we can slice the data in many interesting ways. First we stratify the data by income groups and detailed tenure information. \nAs expected, affordability improves with growing income. When keyed by income, tenant households fair better than owner households for incomes above $40,000, although owner households have a larger share falling below the 15% cutoff with the exception of the bracket over $100,000. Owner households are not uniform though, when split by those having a mortgage and those that don’t it becomes apparent that owner households with a mortgage are the most strained group.\nNext we look at the data by shelter cost and tenure.\n\nWe see that affordability generally decreases with rising shelter costs, although this relationship seems less clear for subsidized tenant households.\nLastly we key the data by household type.\n\nWe see a fairly uniform picture, with one-person, lone-parent and non-census-family households struggling a little more and multiple census family households showing higher levels of affordability. This may well be due to coops being generally classified as multi-family households.\n\n\nCity of Vancouver\nLet’s focus back to the municipal level and understand how the makeup of households by tenure and and household type influences affordability. We changed the colours to avoid confusion that the 50% cutoff scale is not available in this data.\n\nWhen split by household type the data very much reflects the general situation in Metro Vancouver.\nComparing across municipalities in Metro Vancouver we can see how affordability breaks down by detailed tenure type.\n\nAgain, the patters are quite similar across the region when split by detailed tenure.\n\n\nTakeaway\nFrom this descriptive overview there are no clear indicators why the affordability metric for Vancouver improved. The increase in tenant households, and with it possibly an improved mix of high income household among tenants may have contributed.\nThe discrepancy when comparing to the median multiple metric seems to indicate that the solution to the paradox lies in a more careful analysis of the development of the income and rent distributions, as well as how these two are matched up.\nIf I get around to it I might try and unpack other aspects of this story. If anyone else would like to pick this up, the R notbook that built this can be found on GitHub.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {A {First} {Look} {At} {Vancouver} {Housing} {Data}},\n  date = {2017-10-26},\n  url = {https://doodles.mountainmath.ca/posts/2017-10-26-a-first-look-at-vancouver-housing-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “A First Look At Vancouver Housing\nData.” MountanDoodles (blog). October 26, 2017. https://doodles.mountainmath.ca/posts/2017-10-26-a-first-look-at-vancouver-housing-data."
  },
  {
    "objectID": "posts/2017-10-21-boomer-exodus/index.html",
    "href": "posts/2017-10-21-boomer-exodus/index.html",
    "title": "Boomer Exodus?",
    "section": "",
    "text": "The dire home affordability in Vancouver weights heavily on everyone’s mind and it seems like we reflexively try to explain everything that is happening, or that we think is happening, though that lens. After we seem to have thankfully left the meme that millennials are fleeing Vancouver behind, an article claiming that Vancouver’s middle-aged population is leaving keeps popping up in my Twitter feed. The data point in the story is that between 2006 to 2016 the City of Vancouver saw a 13% drop in the number of people in the 40 to 44 year age group. And that certainly is a big drop. But contrary to what the article claims this is not evidence that any group of people are “leaving” Vancouver. If this isn’t then what is? And are they leaving or not? (Spoiler alert, the age cohort that was 40 to 44 in 2006 did see about 5.2% of the people leaving by 2016.)\n\nThe Paradox\nI got curious how my (somewhat biased) twitter feed interprets the 13% drop, so I started a poll. So for City of Vancouver the size of 40-44 year old age bracket dropped 13% between 2006-2016. What was the change for Canada overall?— Jens von Bergmann (@vb_jens) October 21, 2017\n\n\nLet’s just say the performance was sub-optimal. The correct answer is that Canada overall saw a 14% (13.6% to be precise) drop in the number of people in the 40 to 44 year age group. And that’s despite the total population of Canada growing by 11% in that time frame. But this gives me a great excuse to explain what’s going on in more detail.\nSo where are those 40 to 44 year olds going? Nowhere, they are just getting older. The 40 to 44 year old age group in 2006 is the tail end of the boomers. In 2016, these people are 50 to 54 year old, and there are fewer people coming after them to fill the 40 to 44 year old bracket in 2016.\nlibrary(cancensus)\nlibrary(cancensusHelpers)\nplot_data &lt;- get_age_data('CA16',list(C=\"01\"))\n\nggplot(plot_data, aes(x = Age, y = Population, fill = Gender)) + \n  geom_bar(stat = \"identity\") +\n  age_pyramid_styling +\n  labs(title=paste0(\"Canada 2016\"))\n\nThere is a reason the boomers are called the “boomers”. We can clearly identify the sharp tail as people that were over 50 years old in 2016. Another way to visualize is to follow the size of the 40 - 44 year old age bracket through time. Timeline data is available at CANSIM, and we utilize the convenient CANSIM2R package for that.\nlibrary(cansim)\ndata &lt;- get_cansim('0510001') %&gt;% normalize_cansim_values %&gt;% \n  filter(GEO==\"Canada\", `Age group`==\"40 to 44 years\", Sex==\"Both sexes\")\nggplot(data,aes(x=Date,y=VALUE)) +\n  geom_line(color=\"blue\") + geom_point(color=\"steelblue\") + \n  age_time_styling\n\nWe can clearly see how the boomers age through this age bracket, cresting in 2005 and dropping off quite sharply and flattening out in 2010.\n\n\nSome basic demographics\nTo understand what’s going on we need to separate several different concepts:\n\nChange in size of a fixed age group\nNet Migration of age cohorts\nMobility by age cohort\n\nThese two concepts are often confused, which leads to wrong interpretation of data. The change in the size of an age groups between two (census) years compares different people to one another. The ones that are 40 to 44 year olds in 2006 are different from the ones that are 40 to 44 years old in 2016. Therefore, this gives no information about people “leaving”, it just gives some indication how demographics are shifting. And even that it only does half-heatedly. To understand how demographics are changing it is often more useful to look at the change in the share that an age group has out of the whole population. That way we can separate out the changes in an age group from overall population growth.\nNet migration on the other hand looks at (net) flows of people. For this, we need to compare 40 to 44 year olds in 2006 to the 50 to 54 year olds in 2016. We’ll want to apply mortality rates along the way, but when subtracting the two we get the “net” migration of the original 40 to 44 year old age group between 2006 and 2016. When we do this for Canada, we see that the 40 to 44 year old cohort from 2006 did not go anywhere (on balance), in 2016 there were 4.6% more people in that cohort than would be expected just based on natural death rates.\nMobility looks at how many people (in a given age group) living in a given region lived in the same region 5 years (or some other time frame) earlier. When combined with net migration, this allows us to not just see the net migration, but also the rate at which people of similar ages moved in and out. This allows us to gauge the size of the in and outflow of people that can happen even if the net migration is zero.\n\n\nVancouver\nHow does all of that fit in with Vancouver? We know that the population in the 40 to 44 year old age group dropped by 13% 2006 to 2016. But how much of this drop is due to net migration, and how much is just because boomers are aging out of this group and there are fewer people in the next age groups further down that are aging into it? This is mostly an academic exercise of aging forward, killing off, and subtracting, but the result is not really relevant. What’s generally more interesting is the two separate phenomena: change in size of a fixed age group, and also net migration by age group.\nFor example, change in size of a fixed age group is very important for the school board, because this directly effects their enrolment. It can inform policy that can try to adapt to these changes.\n Net migration can inform us how populations are moving around by age group. This tells us how people, on balance, are reacting to certain stimuli by “voting with the choice of where to live” by age group.\nThe 40 to 44 year old cohort in 2006 did experience net out-migration by 5.2% by 2016 whereas Metro Vancouver overall saw a 7% net in-migration. So the article did not get it wrong that some of the people that were 40 to 44 year old in 2006 were leaving Vancouver, the proportion is just smaller than the wording implied. So I am somewhat nit-picking here and this should not be read as a direct critisim of what the article is speaking to, but rather a refinement.\nVancouver is quite interesting that way, as the central region in Metro Vancouver it is a strong magnet for young people in their late teens, their 20s, and very early 30s. After that we see a slight net drain in age groups, and a somewhat stronger drain in the under 5 age group. To understand what’s going it is best to explore using our live interactive net migration map, more details on that map can be found in a previous blog post. This map is for the 2011 to 2016 time span, net migration patterns for earlier time frames is very similar. What we can see is, for lack of a better term, the “generational pulse” of our region. At the Metro Vancouver level we see very little net outflow, the only age groups that show a light 1% net drain among the 50 to 60 year olds. But within the region, there is a lot more net migration.\nThese net migration patterns are quite similar to those in other metropolitan regions, the “generational pulse” can easily be observed in Toronto, Calgary or Montreal, although one has to observe this at the census tract geography as these cities have much more expansive “central” municipalities.\nThis inter-metro net migration has interesting implications on the relative size of the age groups of the municipalities in the region. We can compare the age pyramids of different municipalities to visualize the effect.\n\nWe see that each municipality has it’s own demographic fingerprint in terms of the age distribution, the shape of which is formed by net migration patterns, including those introduced by varying birth rates like the baby boomer bump that we can still make out in all of these graphs.\n\n\nTakeaway\nChange in size of age groups and net migration are two different concepts. They can align at times, essentially when the age distribution is uniform, but often don’t. Looking at the Canada-wide change in the 40 to 44 year old age group 2006 to 2016, vs the net migration of the 40 to 44 year old cohort in 2006 compared to 2016 is a great example of that. The size of the age group shrank by 13.6%, but the the age cohort experienced a net in-migration of 4.6%.\nWant to refine this further? As always, the code that ran the numbers and made the graphs is embedded in this blog post. Just grab it from our GitHub repo if you want to play with it.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Boomer {Exodus?}},\n  date = {2017-10-21},\n  url = {https://doodles.mountainmath.ca/posts/2017-10-21-boomer-exodus},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Boomer Exodus?”\nMountanDoodles (blog). October 21, 2017. https://doodles.mountainmath.ca/posts/2017-10-21-boomer-exodus."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "",
    "text": "There was much hand wringing when NHS income data got released. The change in methods were big, most notably the replacement of the mandatory long form census, that was administered to a random 1 in 5 sub sample, by the voluntary NHS that went out to approximately 1 in 3 households. The (design-weighted) response rate for the NHS was 77%, compared to 94% for the long form in 2006. And the rate of households that allowed a direct match with CRA records dropped from 82% to 73%. This lead to an overall drop in response rate for income data from around 80% in 2006 to around 60% in 2011.\nIncome data was never a true random subsample, but the voluntary NHS delivered a serious blow to data quality for this important time series.\nBut not all is lost. Stats Canada has some powerful methods to overcome some of these problems. 2011 still had the basic short form census, and that can be used to benchmark the NHS against. The short form tells us something about who returned the long form - and who did not. And Stats Canada has the ability to link households through time to learn more about biases in which households returned the census and which did not. And it had the ability to fold in other administrative data, like CRA tax returns and immigration data, to adjust aggregate data. And such post-processing was performed on the NHS income data. In fact, such post-processing is performed on all censuses. The following table shows how this has effected the released income numbers.\n\n\n\nImpact of Imputation on Income Data\n\n\nThe question now is, how do the NHS income numbers after post-processing stack up?"
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#nhs-income-data-a-first-retrospective",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#nhs-income-data-a-first-retrospective",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "",
    "text": "There was much hand wringing when NHS income data got released. The change in methods were big, most notably the replacement of the mandatory long form census, that was administered to a random 1 in 5 sub sample, by the voluntary NHS that went out to approximately 1 in 3 households. The (design-weighted) response rate for the NHS was 77%, compared to 94% for the long form in 2006. And the rate of households that allowed a direct match with CRA records dropped from 82% to 73%. This lead to an overall drop in response rate for income data from around 80% in 2006 to around 60% in 2011.\nIncome data was never a true random subsample, but the voluntary NHS delivered a serious blow to data quality for this important time series.\nBut not all is lost. Stats Canada has some powerful methods to overcome some of these problems. 2011 still had the basic short form census, and that can be used to benchmark the NHS against. The short form tells us something about who returned the long form - and who did not. And Stats Canada has the ability to link households through time to learn more about biases in which households returned the census and which did not. And it had the ability to fold in other administrative data, like CRA tax returns and immigration data, to adjust aggregate data. And such post-processing was performed on the NHS income data. In fact, such post-processing is performed on all censuses. The following table shows how this has effected the released income numbers.\n\n\n\nImpact of Imputation on Income Data\n\n\nThe question now is, how do the NHS income numbers after post-processing stack up?"
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#evaluating-nhs-income-data",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#evaluating-nhs-income-data",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Evaluating NHS Income Data",
    "text": "Evaluating NHS Income Data\nThere are a variety of ways to assess the quality of the NHS income data. One it to compare it to T1 taxfiler data. That works reasonably well for individual incomes, although there are a number of caveats. The two are based on a slightly different pool of people and include slightly different types of income. The census only reports on people in private households, whereas the T1 data includes all taxfilers. And T1 data includes RRSP withdrawls but census data doesn’t.\nComparing data from different sources are tricky, but some of these can be mitigated by comparing the change over time, instead of comparing the two datasets for a fixed time. Several groups have carried out such analysis, StatCan included some overview results in their Income Reference Guide and several local groups carried out their own analyses, for example this one for Waterloo.\nWe will not go down this road in this post, mostly because CensusMapper does not have CT level CRA data (which requires a custom tabulation that is outside of the scope of our usual work, and researchers we contacted that have this data weren’t willing to share it for the greater public benefit).\nThe other part of the reason is that we wanted to try out a different kind of analysis."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#sandwiching-the-nhs",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#sandwiching-the-nhs",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Sandwiching the NHS",
    "text": "Sandwiching the NHS\nWe will focus on median household incomes in Metro Vancouver and try and understand how NHS data fits in between the 2006 and 2016 censuses. There are some fundamental issues in our analysis that we will conveniently ignore. Most importantly, the geographic regions have changed between the censuses. So we can’t compare median incomes in all regions. In the 2016 data Stats Canada has stopped reporting average incomes, which makes it impossible to re-aggregate the data to a common tiling across censuses like we have done e.g. for our net migration map. At least without a custom tabulation. The redeeming quality of this is that the areas that change between censuses are generally the fasted growing ones, where we would also expect to see that the models we employ below would perform worse. Still, for these reasons, and because the limit on the time I can justify dedicating to this, this post is just an exploration, not a proper statistical analysis. And, at times, we will invoke the privilege of a blog post and rely on weasel words instead of making precise statistical statements.\nLet’s first look at the median household incomes for all of Metro Vancouver. Here we can be reasonably confident that the numbers reported in all three census, including the 2011 NHS, are robust due to the large population base and the post-processing done by StatCan. We will use cancensus to load the data.\nlibrary(cancensus)\nregions=list(CMA=\"59933\") # 59933 for Metro Vancouver, 35535 for Metro Toronto, 48825 for Calgary, 24462 for Montreal, 505 for Ottawa\nvector_list=list('CA06'=\"v_CA06_2000\",'CA11'=\"v_CA11N_2562\",\"CA16\"=\"v_CA16_2397\") # Median Household Total Income\ninflation_list=list('CA06'=0.8465679,'CA11'=0.9220553,\"CA16\"=1) # Relative CPI to convert to constant 2015 dollars\nlabel_function &lt;- function(x){return(paste0(\"$\",format(round(x/1000)),\"k\"))}\n\n# uncomment this if running analysis on limat percentage instead\n#vector_list=list('CA06'=\"v_CA06_1981\",'CA11'=\"v_CA11N_2606\",\"CA16\"=\"v_CA16_2540\") # LIMAT %\n#inflation_list=list('CA06'=1,'CA11'=1,\"CA16\"=1) # don't adjust LIMAT data\n#label_function &lt;- function(x){return(paste0(x,\"%\"))}\nWith a convenience function defined to load the data into a data frame we grab the overview data for Metro Vancouver.\noverview_data &lt;- get_income_data(names(regions))\noverview_name &lt;- paste0(sub(\" \\\\(.+\\\\)$\",\"\",overview_data$`Region Name`),\" \",names(regions))\nincome_dumbbell(overview_data, overview_name,y=\"name\")\n\nWe see that the 2010 NHS income number (in blue) sits neatly between the 2005 (in red) and 2015 (in green). To be more precise, the adjusted median household income grew 5.3% between 2005 and 2010 and it grew 5.8% between 2010 and 2015. Another way to view this is to say that the 2010 NHS income number is situated at 46.6% up the adjusted income gain between 2005 and 2015."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#modelling-2010-income-data-for-sub-regions",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#modelling-2010-income-data-for-sub-regions",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Modelling 2010 Income Data for Sub-Regions",
    "text": "Modelling 2010 Income Data for Sub-Regions\nFor this post we will use two naive approaches to model 2010 income numbers.\n\nModel 1\nModel 1 assumes that the sub-region income growth occurred uniformly at the metro level rate of 5.3%.\n\n\nModel 2\nModel 2 uses the hindsight of 2015 income data and assumes that changes in income over the time period between 2005 and 2015 were uniform in time in each sub-region, so that the 2010 actual income sat at 46.6% between the 2005 and 2015 numbers. We use this as our “best guess” model.\nThe rationale behind these models is that, to first approximation, change is generally gradual in time and uniform in space. We will formalize this a bit later."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#income-data-for-sub-regions",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#income-data-for-sub-regions",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "2010 Income Data for Sub-Regions",
    "text": "2010 Income Data for Sub-Regions\nWe start off by looking at Metro Vancouver’s medium and large municipalities.\ncsd_data &lt;- get_income_data(\"CSD\") %&gt;%\n  mutate(income_estimate_1_2011= income_estimator_1_2011(income_adj_2006),\n         income_estimate_2_2011=income_estimator_2_2011(income_adj_2006,income_adj_2016),\n         `Model 1 Difference`=income_adj_2011/income_estimate_1_2011-1,\n         `Model 2 Difference`=income_adj_2011/income_estimate_2_2011-1)\nincome_dumbbell(csd_data %&gt;% filter(Households &gt; 10000), paste0(\"Municipalities with at least 10,000 households in \",overview_name),y=\"name\")\n\nThat looks pretty good, in most cases the 2010 data sits in the range between the 2005 and 2015 data where we would expect it to be, with a few exceptions (which should also be expected), but nothing out of the ordinary. Especially the larger cities seem to show a good match, with Richmond registering a drop from 2010 to 2015 that could be a reflection of to the increase in one-person households during that time."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#census-tracts",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#census-tracts",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Census Tracts",
    "text": "Census Tracts\nSo let’s pull in the income data for Metro Vancouver’s census tracts and see how our expectation stacks up against the NHS numbers.\ndata &lt;- get_income_data(\"CT\") %&gt;%\n  mutate(income_estimate_1_2011= income_estimator_1_2011(income_adj_2006),\n         income_estimate_2_2011=income_estimator_2_2011(income_adj_2006,income_adj_2016),\n         `Model 1 Difference`=income_adj_2011/income_estimate_1_2011-1,\n         `Model 2 Difference`=income_adj_2011/income_estimate_2_2011-1)\nincome_dumbbell(data, paste0(\"Census Tracts in \",overview_name))\n\nWe see that in just over half of the tracts the 2010 NHS number lies within the range, but in 27.4% of the cases the NHS number is below the range and in 21.2% above. We would certainly expect it to lie outside the range in some cases, but not in so many. Over the 10 year time frame the income decreased in 11% of the tracts, over the two 5 year time frame it decreased by 29.1% and 22.9%. We would expect the 5 year data to be more volatile, but again not by that much.\nTaking a look at the relative difference of our expectation and the NHS numbers we see that in general the deviation seems balanced,\nggplot(data %&gt;% gather(key=\"Model\", value=\"Relative Difference\", c(\"Model 1 Difference\",\"Model 2 Difference\"))) +\n  geom_density(aes(x=`Relative Difference`, color=Model)) +\n  labs(title=\"Relative Difference of Estimates to NHS (CT level data)\")\n\nskewing slightly low. As should be the case, the NHS data is more consistent with our Model 2 assumptions than with Model 1."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#temporal-correlations",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#temporal-correlations",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Temporal Correlations",
    "text": "Temporal Correlations\nTesting the idea of temporal auto-correlation more formally, we would expect that the median incomes from adjacent censuses correlate higher than the the ones from the 10 year difference. The correlation coefficients are\n\n\n\nComparison\nCT Level Coefficient\n\n\n\n\n2005 to 2015\n0.9444\n\n\n2005 - 2010 NHS\n0.9425\n\n\n2010 NHS - 2015\n0.9449\n\n\n2010 Model 1 - 2015\n0.9444\n\n\n\nWhat we see is that the 2010 NHS income numbers correlate better with the 2015 numbers than both the 2005 numbers and our Model 1, even though the correlation with 2005 numbers is quite poor. This redeems some of the quality concerns we observed earlier, and addresses some of the criticism initially leveraged against the NHS income numbers, namely that they appeared out of line with 2005 incomes.\nThe takeaway here is that the 2010 census tract income numbers add value over both, just having 2010 numbers and our naive model. But serious quality concerns remain."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#spatial-correlations",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#spatial-correlations",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Spatial Correlations",
    "text": "Spatial Correlations\nThe next step is to check for biases in the NHS income data. One simply way to do this, without actually having to check for explicit biases, is to run a spatial auto-correlation of the relative difference of the NHS data to our “best guess” Model 2. If there is bias in the NHS data, it ought to show up in as spatial auto-correlation as pretty much any potential demographic variable linked to non return bias will have spatial auto-correlation.\nAs a first step, let’s visualize the spatial relationships between the relative difference of the NHS median incomes from our Model 2 expectation.\ngeos &lt;- get_census(dataset = \"CA16\",\n                   regions=regions,\n                   level=\"CT\",\n                   geo_format = 'sf') %&gt;% \n  left_join(data, by=\"GeoUID\") %&gt;%\n  mutate(bins=cut(`Model 2 Difference`,c(-Inf,seq(-0.25,0.25,0.1),Inf),c(\"Below -0.25\",\"-0.25 to -0.15\",\"-0.15 to -0.05\",\"-0.05 to 0.05\",\"0.05 to 0.15\",\"0.15 to 0.25\",\"Above 0.25\")))\nggplot(geos) +\n  geom_sf(aes(fill=bins)) +\n  scale_fill_brewer(\"Relative Difference\",palette = 'PiYG',na.value=\"grey80\") +\n  ggtitle(\"Difference of NSH from Expectation\") + \n  theme_void()\n\nTesting formally for spatial auto-correlation\nsp=as(geos %&gt;% select(\"Model 2 Difference\") %&gt;% na.omit,\"Spatial\")\nwr &lt;- poly2nb(sp, row.names=sp$GeoUID, queen=TRUE, snap=0.005)\n#plot(sp, col='gray', border='blue')\n#plot(wr, coordinates(sp), col='red', lwd=0.5, add=TRUE)\nww &lt;-  nb2listw(wr, style='B', zero.policy = TRUE)\nmoran.mc(sp$Model.2.Difference, ww, nsim=500, zero.policy = TRUE)\n## \n##  Monte-Carlo simulation of Moran I\n## \n## data:  sp$Model.2.Difference \n## weights: ww  \n## number of simulations + 1: 501 \n## \n## statistic = -0.00096903, observed rank = 293, p-value = 0.4152\n## alternative hypothesis: greater\nthe results are inconclusive. For visual confirmation, we plot the auto-correlation.\nmoran.plot(sp$Model.2.Difference, ww,  zero.policy = TRUE)\n\nLet’s quickly re-run our tests at the Dissemination Area level. Dissemination Area level is prone to be much more noise than census tract level data, but also much more prone to bias. If the bias dominates we would expect to see stronger non-random spatial patterns, although they may be masked by random noise due to the smaller sample size for DAs.\ndata_da &lt;- get_income_data(\"DA\")\ndata_da &lt;- data_da %&gt;% mutate(\n  income_estimate_1_2011 = income_estimator_1_2011(income_adj_2006),\n  income_estimate_2_2011 = income_estimator_2_2011(income_adj_2006,income_adj_2016),\n  `Model 1 Difference`=income_adj_2011/income_estimate_1_2011-1,\n  `Model 2 Difference`=income_adj_2011/income_estimate_2_2011-1)\nggplot(data_da %&gt;% gather(key=\"Model\", value=\"Relative Difference\", c(\"Model 1 Difference\",\"Model 2 Difference\"))) +\n  geom_density(aes(x=`Relative Difference`, color=Model)) +\n  labs(title=\"Relative Difference of Estimates to NHS (DA level data)\")\n\nAs expected, the DA level data is significantly more noisy. In 35.1% of the cases the NHS number is below the income range set by 2005 and 2015 numbers, and in 31.9% above.\nWe can again check the temporal correlation coefficients, they are\n\n\n\nComparison\nDA Level Coefficient\n\n\n\n\n2005 to 2015\n0.7508\n\n\n2005 - 2010 NHS\n0.6792\n\n\n2010 NHS - 2015\n0.804\n\n\n2010 Model 1 - 2015\n0.7508\n\n\n\nand we again see that the 2010 NHS numbers correlate better with 2015 numbers than either 2005 number or our Model 1.\ngeos_da &lt;- get_census(dataset = \"CA16\",\n                   regions=regions,\n                   level=\"DA\",\n                   geo_format = 'sf') %&gt;% \n  left_join(data_da, by=\"GeoUID\") \nRunning the spatial autocorrelation at the DA level\nsp_da=as(geos_da %&gt;% select(\"Model 2 Difference\") %&gt;% na.omit,\"Spatial\")\nwr &lt;- poly2nb(sp_da, row.names=sp$GeoUID, queen=TRUE, snap=0.001)\n#plot(sp, col='gray', border='blue')\n#plot(wr, coordinates(sp_da), col='red', lwd=0.5, add=TRUE)\nww &lt;-  nb2listw(wr, style='B', zero.policy = TRUE)\nmoran.mc(sp_da$Model.2.Difference, ww, nsim=500, zero.policy = TRUE)\n## \n##  Monte-Carlo simulation of Moran I\n## \n## data:  sp_da$Model.2.Difference \n## weights: ww  \n## number of simulations + 1: 501 \n## \n## statistic = 0.032273, observed rank = 501, p-value = 0.001996\n## alternative hypothesis: greater\nwe see that the spatial distribution of the relative difference exhibits weak clustering at the DA level, and it points to weak systematic biases in the data."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#other-metro-areas",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#other-metro-areas",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Other Metro Areas",
    "text": "Other Metro Areas\nIt is prudent to check how these results compare to other regions in Canada. This simply involves changing the name of the region in the top code snippet. We will spare the details, they are easy enough to check by downloading the R notebook and running it. So we just report summary results.\nToronto yields similar results, with weak non-random variations also appearing at the CT level. Calgary does not show any significant non-random variation, only weakly significant dispersion at the CT level. Montreal shows weak clustering at the CT level that is eaten up by noise at the DA level. Ottawa displays weak dispersion at the CT level that also gets taken over by noise at the DA level.\nMost importantly, in all cases do we see that the strongest correlation in our CT and DA level tables is the one between the 2010 NHS and the 2015 data."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#understanding-the-biases",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#understanding-the-biases",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Understanding the Biases",
    "text": "Understanding the Biases\nThe census has a wealth of variables that we can use to explore this bias, and it is tempting to start to test against variables that have been identified as effecting the non-return rate of the NHS.\nThis is probably a good time to remind ourselves that the NHS income data is the result of post-processing by Stats Canada, so any investigation of that sort will only be able to discover the bias in these post-processed values.\nAnother obstacle is that we will have to investigate the relationships between census variables and biases in NHS income numbers at the aggregate level, which requires powerful statistical tools to make inferences about the relationship at the individual level that we are after.\nWe will leave this for another day and another post."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#the-verdict",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#the-verdict",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "The Verdict",
    "text": "The Verdict\nThe bottom line is that the general caveats for NHS income data remain in place. At higher aggregation levels the NHS post-processing yielded fairly reliable results for standard statistics like medians. At finer levels like census tracts, the data becomes much more noisy and, in some regions, weakly biased.\nKeeping some of the limitations of our analysis in mind, it does point toward some redeeming features of the NHS data. Even though it is quite noise and may exhibit weak biases, it has added a lot of value to analyses. And some of the early criticism, namely the large discrepancy from 2005 results, is partially redeemed by showing much better relation to the 2015 income data."
  },
  {
    "objectID": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#improvements-on-this-analysis",
    "href": "posts/2017-09-29-a-retrospective-look-at-nhs-income-data/index.html#improvements-on-this-analysis",
    "title": "A Retrospective Look at NHS Income Data",
    "section": "Improvements on this Analysis",
    "text": "Improvements on this Analysis\nThere is plenty of rooms for improvement of this analysis. As usual, the entire analysis is embedded in this blog post and can be downloaded from GitHub. Feel free to fork, download, or file and issue or a pull request if you find errors or problems in the analysis. If you may wish to just reproduce the analysis for a different region, or a different variable, that you are interested in, just grab the code and make the appropriate changes."
  },
  {
    "objectID": "posts/2017-09-18-zoned-for-who/index.html",
    "href": "posts/2017-09-18-zoned-for-who/index.html",
    "title": "Zoned for Who?",
    "section": "",
    "text": "The night before the council hearing discussing the character home zoning review and changes to duplex zoning we decided to spend some time understanding for who we keep 67% of residential land zoned as “single family” (RS), and another 2% as quasi single family in First Shaugnessey (FSD) and 9% as “duplex” (RT). Keeping things simple, let’s just look at RS."
  },
  {
    "objectID": "posts/2017-09-18-zoned-for-who/index.html#who-can-afford-to-buy",
    "href": "posts/2017-09-18-zoned-for-who/index.html#who-can-afford-to-buy",
    "title": "Zoned for Who?",
    "section": "Who Can Afford To Buy?",
    "text": "Who Can Afford To Buy?\nThat’s a pretty easy question to answer. The single family property market has been more or less flat year over year, do the latest available assessment data pegged at June 1 2016 is fresh. To figure this out I threw a slider on the map to allow people to select their income level. It then shows which lots these people could afford to buy, assuming they have saved up a 20% down payment and are willing to spend 33% of their pre-tax income on their mortgage. On top of that will go utilities, property taxes and upkeep. That’s a little more than what’s generally considered affordable, buy that’s a floating metric and given the desperate housing situation in Vancouver people tend to stretch the limits a bit.\n\n\nFull screen view\nAt a household income of $125k we start to see the first SFH lots coming into range, at least if the prospective buyer is willing to engage in some “artisanal landlording”, and rent out an existing or newly created suite. That might come bite the owner in the back at the point of sale in terms of capital gains tax, but without the rental income the property won’t be affordable in the first place.\nThere aren’t too many of these homes around at the price range, and chances are that none of these are on the market when we are looking. We could increase our range (and thus our chances) by pretending that rental income is not taxable.\n61,950 households in the City of Vancouver make at least $125,000, that’s almost as many as we have single family homes. The 43,945 households the census found that make over $150,000 have slightly more choices, although still almost nothing is available at $150k sharp without engaging in “artisanal landlording”.\nThe 23,310 households with income above $200,000 have a little more choice, and can even afford a home without renting out a suite. But not that many, being willing to rent out a suite significantly increase the options.\nAt $300,000 homes on the West Side start to come into reach in Marpole. Landlording still helps. But even a tax-cheating artisanal landlording $500,000 earning household still can’t touch the majority of the West Side.\nIf all else fails you can still take artisanal landlording to the next level and try to rationalize why renting out a suite on Airbnb is a good idea to increase your cash flow."
  },
  {
    "objectID": "posts/2017-09-18-zoned-for-who/index.html#who-do-we-zone-this-for",
    "href": "posts/2017-09-18-zoned-for-who/index.html#who-do-we-zone-this-for",
    "title": "Zoned for Who?",
    "section": "Who do we zone this for?",
    "text": "Who do we zone this for?\nHonestly, I don’t know who we zone RS land for. The 23,310 households with income over $200,000? I don’t think people in that income group need special protection for their housing. They will be able to afford a place to live irrespective of zoning. And that still does not answer that question for the West Side. And no, the fraction of households making $500,000 per year is not enough justification to zone most of the West Side single-family.\nRS zoning is dysfunctional. The ridiculous land value gains is a symptom of that, as is our teardown cycle that this is fuelling. At this point, incremental changes aren’t enough to ensure that this land serves the needs of Vancouverites.\nMeanwhile the city is still trying to make incremental changes to somehow nudge RS into affordability. The current proposal is to allow stratified infill on lots with homes that have “character merit”. Tomorrow’s council hearing will discuss incremental changes to select duplex (RT) zones to make it slightly easier to add a little more housing.\nThese changes are good, and I support them. They are necessary steps and go in the right direction. We have already seen how this process produced multiplexes in Mount Pleasant that have significantly lower prices than nearby single family homes. But this kind of gentle density is slow to materialize, the heritage “retention” process makes it more expensive than it should be, and the density is barely enough to open this up as ownership options for larger portions of the population.\nThe focus on “character” is misplaced. We have an affordability crisis, and the city still makes “character” the pivotal point of their changes. The issue with our single family homes is simply that they are single family homes. It’s the wrong typology that we are forcing so close to the metropolitan centre. Let’s rethink what typology is right for the city, and will continue to work for the foreseeable future. Changing the zoning to e.g. allow zero lot-line 16’ row houses, or 3-storey 6-plexes throughout the city won’t change the city over night. But with mindful planning it will set the stage for gradual change that allows more people to live in our city."
  },
  {
    "objectID": "posts/2017-08-24-dot-density/index.html",
    "href": "posts/2017-08-24-dot-density/index.html",
    "title": "dot-density",
    "section": "",
    "text": "I started writing this blog post in December 2015, when CensusMapper quite a bit younger and I hacked together some basic dot-density maps. I never much liked the results and have been slowly improving and thinking about them. I am still not entirely happy with the current implementation, but it is slowly getting there. The final impulse to finsish this post was the work on cancensus, and R wrapper for the CensusMapper API my explorations in multi-category dot density maps in R, now tied up into the new dotdensity package."
  },
  {
    "objectID": "posts/2017-08-24-dot-density/index.html#one-person-at-a-time",
    "href": "posts/2017-08-24-dot-density/index.html#one-person-at-a-time",
    "title": "dot-density",
    "section": "One person at a time",
    "text": "One person at a time\n Dot-density maps look pretty cool. They have been flying around the itnernet lately, so we have been thinking about how to offer them in CensusMapper.\nSo what’s so great about dot-density maps? Essentially two things:\n\nThey are very simple to interpret. One dot = one person is something everyone understands immediately.\nThey can show several variables at once, for example mapping recent immigrants by region of origin like above or mapping ethnic segregation.\n\nBut once one looks closer there are lots of issues that need to be dealt with."
  },
  {
    "objectID": "posts/2017-08-24-dot-density/index.html#the-devil-is-in-the-details",
    "href": "posts/2017-08-24-dot-density/index.html#the-devil-is-in-the-details",
    "title": "dot-density",
    "section": "The devil is in the details",
    "text": "The devil is in the details\nAs simple as the the basic dot-density promise of one dot = one person is, it must fundamentally remain a lie for census maps. We simply don’t have the level of detail that the maps suggest. We don’t know the location of people with the accuracy depicted in the maps, and we certainly don’t know the categories that give the color with the accuracy suggested in the maps.\nThat’s the most serious drawback of dot-density maps, they often suggest a level of detail that simply isn’t there. Does that outweigh the advantages? For that it is useful to take a quick look at the alternative that we have been using at CensusMapper"
  },
  {
    "objectID": "posts/2017-08-24-dot-density/index.html#choropleth-maps",
    "href": "posts/2017-08-24-dot-density/index.html#choropleth-maps",
    "title": "dot-density",
    "section": "Choropleth Maps",
    "text": "Choropleth Maps\nChoropleth maps are the staple of census maps. Every census region gets colored depending on a value of a census variable (or function derived from census variables). While this is also quite simple, in practive there are a number of problems with that:\n\nLow population bias. Let’s look at an example to see how this works. Take a map of the percentage of children living in poverty. We immediately see where the percentage of child poverty is high. And child poverty is a problem. But what we don’t see is how many poor children live in each area. So while we see the relative magnitude, we don’t see the absolute magnitude of the problem. So for example the DA north-west of Broadway and Fir sports 66.7% of children living in poverty, but there are only 30 children in the area, and the expected error due to statistical rounding is 10%, let alone errors introduced by non-return bias or simple sampling error. The DA north-east of Arbutus and 33rd has a similar rate of 69.1%, but there are 345 children in that area. So when trying to understand child poverty in the west side of Vancouver one should focus on the latter not the former, but on the map they appear identical. Dot-density maps do a much better job at representing this properly as they would simply draw a dot for each child in poverty, contrasted by a dot for each child not in poverty as you can see here. Another way to deal with this issue is via a surprise map which we have explained in a previous post.\nChoropleth maps are difficult to understand. If you read this far you probably have dealt with a fair share of maps and won’t appreciate how some people struggle understanding these. But the amount of time I have spent explaining to journalists what they see in the halloween map of trick or treat density is (torturous) testament of the difficulties people have with these kind of maps. I don’t think I would have gotten any questions if I had simply use the dot-density version instead. However, this only works for the trick-or-treat density map, not the trick-or-treat onslaught map.\nOne can only show one variable at a time. There are ways to stretch this a little, for example value by alpha maps are one way around this that tackle the population bias. Another way is the RGB maps CensusMapper has. But this does not exactly make it easier to interpret. Dot-density versions of this is certainly easier to interpret plus one is not limited by the three categories of the RGB maps. We can even combine the CensusMapper 3 color mixing with value-by-alpha to also show density."
  },
  {
    "objectID": "posts/2017-08-24-dot-density/index.html#dot-density-maps",
    "href": "posts/2017-08-24-dot-density/index.html#dot-density-maps",
    "title": "dot-density",
    "section": "Dot-Density Maps",
    "text": "Dot-Density Maps\nOne other big challenge with dot-density maps is that they are surprisingly hard to make. Right now we at CensusMapper just have the bare minimum in place to produce these kind of maps: A way to randomly place the required number of dots into each geographic region colored by the given categories. Just when I was about to try myself at dot-density maps I saw a helpful tweet telling me exactly what to pay attention to, so the implementation was quick and painless. – Well, not quite, I still had to deal with issues due to polygons clipped server side and the fact that census areas are often multi-polygons. And one needs to be careful to employ random rounding when scaling to more than one person per dot, standard rounding may introduce systematic errors.\n\nCensusMapper Dot-Density Issues\n 1. Clipping. There are still some minor issues due to clipping that can lead to the number of dots being off by a small proportion. I won’t bore you with the technical details, but the good news is that it can be worked out at the expense of adding some more custom code on the client. 2. Dynamic dot-value scaling. CensusMapper maps allow for zooming from country-level down to street level. The one dot = one person paradigm does not work very well on all scales. Visually as well as computationally. The smallest unit to draw is one pixel (or 1 quarter of a pixel on 2x retina displays), and at some point (at the latest when having to draw 33 million dots randomly within different regions in Canada) your browser performance will tank. To fix this we need to dynamically adjust the value of each dot. Instead of 1 dot = 1 person it will be 1 dot = 10 people at lower zoom levels. And at higher zoom levels at some point one dot will have to start to get larger to be more visible. Dynamically changing scales can be confusing though. As we zoom we keep the size of each dot relative to the map constant, but if we re-scale we change the size of each dot relative to the map scale to make it clearer to the user that wer are rescaling.  3. Non-uniform distribution of population. The current code has the problem of placing the dots randomly in each census geography regardles of where people actually live. This goes back to the fundamental issues that dot-density maps suggest a level of precision that simply is not there. But it definitely is odd to see dots in the Pacific Spirit Park or camping out on Burnaby Mountain. The good news is that there is a partial fix to this. We have population counts at a finer census geometry: Census Blocks. And at the Census Block level we see that nobody lives in the Pacific Spirit Park, or on most of Burnaby Mountain. So to fix this we simply need to shift the way we decide what census geography to display. This is quite difficult to fix within the CensusMapper paradigm of highly dynamic maps where nothing is pre-computed. 4. Visual feedback on hover / select. For choropleth maps we highlight regions on hover so that the user knows what geographic area the variables in the legend and in the popup are for. This is something that is not too difficult to add, but we will have to wait for the next bigger CensusMapper map refresh."
  },
  {
    "objectID": "posts/2017-08-24-dot-density/index.html#static-dot-density-maps",
    "href": "posts/2017-08-24-dot-density/index.html#static-dot-density-maps",
    "title": "dot-density",
    "section": "Static dot-density maps",
    "text": "Static dot-density maps\nStatis maps is one way where the above issues don’t appear. And most importantly, we can fix issue 3. completely by taking the time to weight the placement of dots by census block level data. The new cancensus R package now makes it super easy to import cenusus data into R, and we wrote a dotdensity R package to implement common functions that deal with the usual pitfalls of multi-category dot-density maps. By moving from CensusMapper to R we trade the dynamic nature of CensusMapper for crisper images and improved processing and dot-placement. Often we aren’t interested in Canada-wide maps that are the staple of CensusMapper, but only want to focus on one particular region. Or maybe a couple of regions, and the cancensus and dotdensity packages still make it very easy to change the region and make the same map for a different geographic region. Or make changes to the variables we want to map. In particular in conjunction with the CensusMapper API helper that reduces the selection of geographies and variables to a couple of mouse clicks and let’s you copy and paste the R code to import the data through cancensus.\nThe dot-density package has two main functions that we use\ndot_density.compute_dots takes care of converting geographic shapes with counts for each category into dots. This is fairly straight-forward, but we need to pay attention to two potential pitfalls.\n\nThe order of the dots need to be randomized so we don’t draw all items of one particular category (colour) last, so that these end up on top and appear more prominent than others.\nWhen we scale so that 1 dot represents more than 1 unit in our category count, we need to employ statistical rounding, not just regular rounding, otherwise the overall count of the dots may not represent the overall averages. To see this, suppose we want to map German speakers, and we scale so that 1 dot corresponds to 50 German speakers. If German speakers are uniformly distributed in each area so that there are 20 German speakers in each area, regular rounding will produce a map without any German speakers at all. Statistical rounding will properly reflect the total number of German speakers, but they will be randomly placed in each area. Not ideal, we probably should adjust our scale. But better than random rounding. And adjusting the scale is not always an option, for example there might be one cluster of German speakers and a uniform distribution everywhere else.\n\nThe dot_density.compute_dots takes care of these issues under the hood. (And so does CensusMapper.)\ndot_density.proportional_re_aggregate takes two nested geographies, for example census subdivisions and census tracts. It will compare counts across the geographic levels and adjust the lower-level geography counts with the more accurate higher-level data. If lower level geographic data has been suppressed due to quality or privacy concerns the overall counts at that aggregation level won’t accurately reflect the overall data. The dot_density.proportional_re_aggregate will re-distribute the missing counts proportionately among the lower level geographies, weighted appropriately.\nThe same function can also be used to weight the dots we want to draw by block level data, so this will produce maps that avoid placing dots in parks or other unpopulated areas, spacing them according to population density at the block level."
  },
  {
    "objectID": "posts/2017-08-24-dot-density/index.html#examples",
    "href": "posts/2017-08-24-dot-density/index.html#examples",
    "title": "dot-density",
    "section": "Examples",
    "text": "Examples\nWhen the data is easily accessible and well-organized, it is incredibly easy to run analysis or visualize it. In CensusMapper, we can make dot-density maps within a matter of minutes, and with the cancensus and dotdensity packages it’s no different in R.\nHear are a couple of examples, first from CensusMapper with links to the live maps, and then from the vignettes embedded into the R package.\n\nCensusMapper\nOccupied dwelling units by type \n\n\nR\nRecent immigrants \nCross-border race"
  },
  {
    "objectID": "posts/2017-08-06-millennials-redux/index.html",
    "href": "posts/2017-08-06-millennials-redux/index.html",
    "title": "Millennials Redux",
    "section": "",
    "text": "Catching up with my local news reading last night I stumbled about another new report on millennials.\nThe notion that millennials are fleeing Vancouver is a recurring theme in the Vancouver press, and we have addressed some of the problems in the data used to support that claim before.\n Sadly, this new article’s use of data is no less problematic, and the topic, as well as the data misrepresentations, are serious enough that I felt they need addressing so as not do distract from the actual real problems that millennials are facing. Problems that are quite different from those the 25 to 39 year old age cohort was facing 20 years ago. Groups like Generation Squeeze have done a good job nailing some of that down in the data.\n\nThe Data Rabbit Hole Trap\n\nTo the data-minded person reading the article there are a number of red flags that go off throughout. Many of these can be attributed to today’s typical data-adverse journalism, but typically the actual hard numbers in the article hold up and are just misrepresented to varying degrees. What got me stumbling in this article was the data chart at the bottom claiming that the 25 to 39 year old age cohort in the “UEL” grew by 5% between 1996 and 2016. The UEL of course is a quasi-municipality that sits wedged between the City of Vancouver and UBC, but many people less attuned to data and administrative details use the term to refer to various portions of the region west of the Pacific Spirit Park, sometimes including “Little Australia” which is west of the Park but an actual part of the UEL and sometimes excluding it.\nI took it to mean some version of UEL and UBC/UNA combined, and the 5% number looked suspicious to me. The population in that area more than tripled during that period, one would expect the change in that age cohort to be much larger. So I started to dig into the numbers.\nThe first step was to look up the numbers for the City of Vancouver since there are no issues with administrative boundaries between 1996 and 2016, just to make sure that the data was labelled correctly and it was really representing the percentage change in the number of millennials between 1996 and 2016. But the number I got was different from the one in the article. The article lists a 10% increase, I calculated an 11% increase. 11.2% to be precise, so there was no chance that this was a rounding issue.\nAnd the data rabbit hole opens up, sucks me in and the trap closes.\n\n\nThe Data\nThe data is, for the most part, reasonably straight forward. I just grabbed the 1996 count of 25 to 39 year olds, then the 2016 counts and compared them. One problem is boundary changes. Administrative boundaries don’t stay fixed. And boundary changes don’t always show up when just looking at non-geographic data, names or even the uniqe geographic identifiers don’t necessarily change when census boundaries change. Looking that the geographic data for both censuses one immediately notices that the UEL/UBC/UNA area changed a lot (and also got new geographic identifiers), and Coquitlam changed too. That complicates things a little, the UEL/UBC/UNA part is easy enough to deal with. In 1996 that area was called the “University Endowment Area”, in 2016 that same area can be obtained by adding to census tracts. Coquitlam is a little trickier and I wasn’t interested enough in figuring out the details so I decided to ignore it.\nStep one, trying to reproduce the graph in the newspaper, is below with blue bars, with the graph from the newspaper in red bars for reference.\n\n\n\nNews Story\n\n\n\n\n\n\nActual Data\n\n\n\nThere is definitely a correspondence between the graphs, but the numbers don’t quite match up. I have no idea how the “UEL” numbers were derived for the article. But I have an explanation for the difference in the other municipality’s numbers. Looking at the graphs suggests that a larger denominator was used in the article, and indeed the numbers match up perfectly if I were to divide the difference of population in the 25-39 year old cohort by the 2016 number instead of the 1996 number. An embarrassing data mistake to be sure, but nothing out of the ordinary for today’s news stories.\nI can’t explain why Surrey, the city with the largest gain in millennials, was dropped from the data used for the story.\n\n\nData Representation\nBut these data problems are really only a side show to the real issue. The most important question is what data to use for what purpose. The article chose to use the change in the total number of millennials to support the notion that the 25 to 39 year old cohort are shunning the City of Vancouver for some of the more outlying regions. The obvious issue with that is that that measure is confounded by population growth. If the population is growing, so will the number of millennials, even if the share of millennials in the population did not change. For this story, this is clearly a very poor choice of data representation.\n As a first approximation to understanding where that age cohort settles in 2016 compared to 1996 one can look at the respective shares of the population in those age cohorts. The only problem, the pretext of the story goes away when one represents the data in this way.\nWhat stands out is that the share of 25-39 year olds dropped in all areas. Some of that is just part of the changing makeup of the population in general. And one sees that the City of Vancouver not only has the highest share of 25-39 year olds, it also experienced the lowest drop.\nOne should probably also look at other age cohorts to better understand how the population is changing. And compare this to other regions to try and distinguis Vancouver-specific trends from more general Canada-wide ones.\n\n\nFraming of the Data\nThe other part of the story that irks me is the deep confusion and free mixing of two different concepts. One is that of migration, that is (the same) people moving from one area to another over some time period. The other is that of the number or share of (different) people in a secific age cohort at two distinct points in time. Nathan Lauster has added some very good analysis to this topic, and has followed up with a series of blog posts. And this was picked up in various news articles too.\nThis article not only lacks appreciation for this important distinction by talking about “migration” when really comparing age cohorts, but it takes it to the next level by talking about “millennials” as being 25-39 year old in 1996 (as well as in 2016), which is comically absurd.\nNot sure what to make of the authors assertion that BC Assessment is in the business of enumerating 25 to 39 year olds between 1996 and 2016, I wonder how people get stuff like this past their editors.\nThe larger storyline is still important here, as Vancouver grows up from a city with surrounding suburbs into an integrated metropolitan area. And a new generation, spurred on by new challenges, including housing affordability, accelerating that transformation and re-defining what some of these former suburbs into hip local centres that are tied together by a growing transit system.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Millennials {Redux}},\n  date = {2017-08-06},\n  url = {https://doodles.mountainmath.ca/posts/2017-08-06-millennials-redux},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Millennials Redux.”\nMountanDoodles (blog). August 6, 2017. https://doodles.mountainmath.ca/posts/2017-08-06-millennials-redux."
  },
  {
    "objectID": "posts/2017-04-10-surprise/index.html",
    "href": "posts/2017-04-10-surprise/index.html",
    "title": "Surprise Maps",
    "section": "",
    "text": "At CensusMapper we like building models based on census data. We now have a common tiling for 2011 and 2016 geographies that allows us to easily model changes over time. After building a model we often want to see how well the model performs. An easy way to do this is to simply map the difference of observations and model predictions.\nThose maps are great and it is easy to understand what is mapped. But they are difficult to interpret properly. In many cases a better metric to map is how consistent the observations in each region are with the model. Which brings us to Bayesian surprise maps.\nThere is a great post making the rounds on the web, and when it recently showed up again in my Twitter feed I finally decided to get in on the fun."
  },
  {
    "objectID": "posts/2017-04-10-surprise/index.html#the-problem",
    "href": "posts/2017-04-10-surprise/index.html#the-problem",
    "title": "Surprise Maps",
    "section": "The Problem",
    "text": "The Problem\n To understand why surprise maps can be so useful, let’s look a a concrete example. Suppose we want to understand dwelling units that are not used as primary residences. (And incidentally it seems that everyone currently living in Vancouver or Toronto wants to do this.) To that end we can simply consult the map of these based on the 2016 census. As we try and understand better what is happening in each region and we zoom in more and more we start to run into issues. Consider this example looking at the Marine Gateway neighbourhood in Vancouver. We see two areas coloured in dark blue, indicating a high rate. The left is the site of the MC2 development that got completed just before the census and we discussed earlier in detail. The right one is a large area with exactly one private dwelling unit, which happens not to be a primary residence for anyone. If someone did use it as primary residence it would show up on the other extreme end of our colour spectrum. Either way, it does not really give much useful information, it is mostly a distraction that takes attention away from MC2 that has a more important story to tell.\nMore generally, areas with very low dwelling counts are much more likely to see high variations of rates of non-permanent residence buildings purely for statistical reasons. This results in a “checkerboard” pattern that is mostly due to statistical noise and hides meaningful variations in the data."
  },
  {
    "objectID": "posts/2017-04-10-surprise/index.html#surprise-maps",
    "href": "posts/2017-04-10-surprise/index.html#surprise-maps",
    "title": "Surprise Maps",
    "section": "Surprise maps",
    "text": "Surprise maps\n That’s were surprise maps come in. As a first step, instead of colouring by the rate of non-primary residence dwellings, let’s colour by how this rate differs from our expectations. So we re-interpret our original map as mapping the difference from expectations where the expectation is that all dwellings are used as primary residences. Alternatively we can also take the regional average rate of non-primary residence dwellings and map the deviation from the average. Or we could build a more elaborate model using the 2011 rates and the Canada-wide average rates per net new dwelling unit.\nEither way, what we are doing here is we make a guess what we think the rate of non-primary residence units should be in each area and we compare it with observations. The better our guess is, the stronger the “checkerboard” pattern will become as the residuals will be reduced to statistical noise. This as can easily be seen when comparing the more advanced model to the checkerboard observed using the “zero” model. In practice that means that we have to click into each region to see the number of dwelling units, as well as the rate, to understand how we want to interpret the result. Through this labour intensive process we will then weed out the area with just 1 dwelling unit and ignore it.\nSurprise maps don’t map the actual difference of model and observation, they map how consistent the model is with the observation in each area. In our initial example of an area with only one dwelling unit in it, no matter if that unit is used as primary residence or not, this cannot be taken as strong evidence that our model is wrong. We should assign this a neutral colour. The MC2 development on the other hand contained 570 dwelling units in 2016, a large deviation from the model indicates that our model prediction might have some problems for that region.\nIn surprise maps, we colour each area by the amount of evidence observations in a that area gives against our model. To have good evidence against our model, observations should deviate from model prediction, and we should be able to exclude regular statistical noise as a cause for this.\nFor this example, we chose a mixture of the “base rate” and “de Moivre funnel” models described in this excellent paper where we essentially modify the de Moivre funnel model by allowing an arbitrary model to take the place of the average of our variable. We also keep track of the sign of the evidence we collect against our model, so whether our model underpredicted or overpredicted the rate of non-primary residence dwelling units.\n The result is a map that makes it much easier to sport “surprising” areas, that is areas where observations provide good evidence that our model does not hold well there.\nWe can now go back to check the Marine Gateway area we looked at before, and we see that the only areas that contribute solid evidence against our model are the unexpectedly high rates at the MC2 development, as well as the unexpectedly low rates at the area to the east between Main and Frasier streets."
  },
  {
    "objectID": "posts/2017-04-10-surprise/index.html#surprise-maps-in-censusmapper",
    "href": "posts/2017-04-10-surprise/index.html#surprise-maps-in-censusmapper",
    "title": "Surprise Maps",
    "section": "Surprise Maps in CensusMapper",
    "text": "Surprise Maps in CensusMapper\n We baked these kind of surprise maps into CensusMapper, so now we can easily apply this to any other kind of observable.\nThe CensusMapper surprise maps that are implemented right now require as input a “model” , an “observation” a “base” variable (the number of dwelling units, averaged over the 2011 and 2016 censuses in our case) and a standard deviation of the difference between model and predictions. For now, that standard deviation will have to be entered manually for technical reasons. Moreover, other parameters like estimates of accuracy of census counts as well ask statistical rounding and other operations that may have been performed on the data, can be added in to account for the fact that the rate observed in the census is different from the actual rate. We added the ability to make the standard deviation region dependent, which allows us to account for estimates of the accuracy of the census data based on region dependent variables.\nFor a simple example what this might look like consider child poverty. We have mapped this variable in CensusMapper, but at times it can be hard to distill out the really important areas that have high child poverty rates as well as a high number of children overall. We have added a scatter plot of the rate vs the total number of children in poverty to the map story, so that the user can more easily determine how significant a high child poverty rate in each particular region is.\nReliability of the data can also be tainted by low NHS return rates, so CensusMapper maps automatically shade to regions that have particularly low NHS return rates (and similarly for the full census return rates) to give some indication of reliability, and we display the return rate on hover. But it is cumbersome to keep track of all this information.\nAnother way to deal with these issues is to make a child poverty surprise map. As model we can simply use the assumption that there are zero children in poverty in each area. We scale the standard deviation of the child poverty rate linearly by the return rate to weight down areas with low return rate. The result is a map that colours each region by the amount of evidence they provide against the model assumption of zero children in poverty.\nThis makes it easier to filter out regions that have a low number of children overall, where high rates of child poverty might just be a statistical fluke."
  },
  {
    "objectID": "posts/2017-04-10-surprise/index.html#where-to-go-from-here",
    "href": "posts/2017-04-10-surprise/index.html#where-to-go-from-here",
    "title": "Surprise Maps",
    "section": "Where to go from here",
    "text": "Where to go from here\nShould all maps be surprise maps? No, there is value in just mapping straight up census variables, or mapping plain differences of observations from the model. But there are many good reasons why such maps should be complemented, or in some cases even replaced by, surprise maps.\nSome kind of automation to aid the selection of appropriate standard deviation for the surprise model is needed before we can open this up to a wider user base. Or CensusMapper server has an R server running that ties directly into the database and communicates with the web server, so we need to build the appropriate scripts that can automate this task.\nAnother logical extension is to include proximity data into the Bayesian estimates. If one Census Tract provides good evidence against our model, but that evidence is distributed uniformly across Dissemination Areas within it, then our surprise map will show weaker evidence at the dissemination area level simply because each area has a smaller base population. But we could check if the deviation from the model is localized in just one Dissemination Area, or also present in neighbouring Dissemination Areas, and include that information in our estimates. This approach would also help with the problem if the evidence is concentrated in an area that is split between to Census Tracts and drowned out by outher data in these respective tracts. So it won’t show up at the Census Tract level because of the particular ways the boundaries were drawn (MAUP), and it will get diluted out at the Dissemination Area level because of low base population counts. Adding in proximity measures could recover this evidence at the Dissemination Area level.\nThere are lots of other interesting possibilities of using surprise maps while leveraging the dynamic nature of CensusMapper. One may allow for several models and let relative importance of the models self-adjust with the map view. The output would then be for each map view a linear combination of the models that provides the best fit for the current map view, as well as the evidence each region in the current map view provides against the model.\nWe will keep experimenting with surprise models and at some point open up some of it’s capabilities to a wider group of CensusMapper users."
  },
  {
    "objectID": "posts/2017-03-22-comparing-censuses/index.html",
    "href": "posts/2017-03-22-comparing-censuses/index.html",
    "title": "Comparing Censuses",
    "section": "",
    "text": "It’s great to have fresh census data to play with. Right now we only have three variables, population, dwellings and households. There is still lots of interesting information that can be extracted.\nSo we started exploring in our last post, things get really interesting when looking at change between censuses. But as we noted, there are several technical difficulties that need to be overcome.\nSo we at CensusMapper took that as and invitation to do what we love most: breaking down barriers.\nThe biggest difficulty in comparing censuses is that census geographies change over time to adapt to new demographic realities on the ground. Whenever that happens, there is no easy way to compare data across censuses."
  },
  {
    "objectID": "posts/2017-03-22-comparing-censuses/index.html#custom-tabulations",
    "href": "posts/2017-03-22-comparing-censuses/index.html#custom-tabulations",
    "title": "Comparing Censuses",
    "section": "Custom Tabulations",
    "text": "Custom Tabulations\nThe best way to compare data across censuses is to pull a custom tabulation for data on a common geography, there are a couple of things to keep in mind though:\n\nIt costs money (or at least time if part of a specific research project with access to a StatCan data centre),\nit takes time to complete a custom tabulation, and\ncustom tabulations are typically not available at fine geographic levels.\n\nFor these reasons, comparing data across censuses is usually confined to select university research projects or well-resourced private interest. And to make matters worse, custom tabulations are rarely shared. I have asked university research groups if they were willing to share custom tabulations through CensusMapper, but got turned down every time so far. While the Tri-Agency Open Access Policy on Publications, postulates that research needs to be published in open access journals and the data also needs to be made available, in practice research groups often keep their data locked up long after publication.\nBut some public agencies do make their custom tabulations available, for example the City of Vancouver has custom tabulations for neighbourhood boundaries available on their open data catalogue."
  },
  {
    "objectID": "posts/2017-03-22-comparing-censuses/index.html#comparable-regions",
    "href": "posts/2017-03-22-comparing-censuses/index.html#comparable-regions",
    "title": "Comparing Censuses",
    "section": "Comparable Regions",
    "text": "Comparable Regions\nBut even without custom tabulations we can compare regions if they have not changed across censuses. For example, we can compare how the City of Vancouver or Metro Vancouver have change over time by tracing them through censuses, as their geography has not changed. Census regions are tracked through their unique region identifier, which allows people to establish timelines for regions. When doing so one quickly runs into issues with this approach though.\nJust because a region has the same name and unique geographic identifier does not guarantee that the region has not changed. Simple examples of this are Census Metropolitan Areas that can change quite dramatically over time without this being reflected in the name or region identifier. For example, Ottawa, Montreal or Abbotsford were all expanded, some substantially) from the 2011 to the 2016 census.\nFor example, by just comparing Census Metropolitan Area data, Montreal’s population grew by 7.2% between 2011 and 2016. But a good chunk of that was simply due to the geographic region of the CMA being expanded, taking the 2016 expanded region as a base and including all people that lived in that region in 2011, including the ones that lived outside of the 2011 CMA of Montreal but that are now included in the 2016 CMA, the region’s population only grew by 4.2%.\nOn top of large changes like this, many smaller regions have undergone slight boundary adjustments while keeping their unique identifier, which move some people from one geographic region to another. And as one drills down into finer geographies one can easily find geocoding errors in the data, where people were moved from one region to another between censuses without any region adjustment taking place. These geocoding problems may persists in custom tabulations, but they are likely to even out as one moves to higher aggregation levels."
  },
  {
    "objectID": "posts/2017-03-22-comparing-censuses/index.html#censusmapper-comparable-regions",
    "href": "posts/2017-03-22-comparing-censuses/index.html#censusmapper-comparable-regions",
    "title": "Comparing Censuses",
    "section": "CensusMapper comparable regions",
    "text": "CensusMapper comparable regions\n At CensusMapper we decided that it would be in the general interest to be able to compare standard census variables across all aggregation levels. So we wrote some scripts to create a “least common denominator” tiling for all of Canada for all aggregation levels, ranging from Census Blocks to all of Canada. The idea is to keep regions that have not changed and, for regions that did change, join regions together in a way that the joins match again across censuses. While this sounds simple in principle, there are lots of issues and pitfalls along the way. Hopefully we have properly dealt with all of them now. We have done some testing, and there are still have 285 regions (36 CTs, 245 CSDs and 4 CMAs) where the adjusted and computed 2011 population counts don’t quite add up. For example, 348 people were moved between Coquitlam and Port Coquitlam in this fashion, most of it due to an adjustment between CTs 9330287.01 and 9330291.01 that is not due to a change in geographic boundary. In the example of Airdrie trading 705 people with Rocky view this is associated with a change of the census geography, but no change in name or geographic identifier. In most cases the adjustment is much smaller than this. Even more extreme than Airdrie is the case of CTs 5350512.00 and 5350511.01 in Mississauga that swapped 1074 people without any apparent boundary change.\nWe have a map that highlights all these difference at or above the CT zoom level, and in all maps we shade areas with adjusted population counts and we indicate the actual population adjustment on hover."
  },
  {
    "objectID": "posts/2017-03-22-comparing-censuses/index.html#the-good-the-bad-and-the-ugly",
    "href": "posts/2017-03-22-comparing-censuses/index.html#the-good-the-bad-and-the-ugly",
    "title": "Comparing Censuses",
    "section": "The Good, the Bad and the Ugly",
    "text": "The Good, the Bad and the Ugly\n So what does all of this buy us? For now, we get comprehensive maps comparing population, dwellings and households across the 2011 and 2016 censuses at all geographic levels, including Dissemination Blocks. As new census data rolls in, we will be able to compare data down to the Dissemination Area level. That’s pretty cool, we think.\nWhat could be bad about this? Census data is far from perfect. And mapping differences tends to bring out the problems in Census Data. Incorrectly enumerated dwellings, households or population, geocoding issues, as well as changes in geographies.\nAnd things start to get really ugly when the rest of the census data comes in where statistical rounding was applied. Even if everything was perfectly enumerated and geocoded, statistical rounding means that inevitably some areas will have census data that has been rounded up in one year and rounded down in the next. And taking differences will bring this out very strongly, especially at the Dissemination Area level. It is hard to overstate the effect of this, statistical rounding means that when mapping change at small aggregation levels we will map lots of statistical noise.\nThere is however still value in showing and working with differences of census variables at small aggregation levels, but the results are much harder to interpret. One very simple workflow that we use regularly is to generally use Census Tract level data, but zoom in to Dissemination Areas when we want more context for particular Census Tract values. Doing this, salted and peppered with a good dose of caution and ideally some ground truthing, we can greatly refine our insights in what is going on in that Census Tract.\nFor this reason we are decided to, at this point, keep fine grained data publicly available. We are aware that this will lead to some people misinterpreting census data, but this is already happening at any aggregation level and is generally not a sufficient argument to lock up data in our view. We may add more visual aids to warn the user of unreliable results, like adding pattern shading to results that could be dramatically effected by small difference errors due to statistical rounding or sampling/enumeration issues."
  },
  {
    "objectID": "posts/2017-03-22-comparing-censuses/index.html#maps",
    "href": "posts/2017-03-22-comparing-censuses/index.html#maps",
    "title": "Comparing Censuses",
    "section": "Maps",
    "text": "Maps\nWe opened up a number of maps on CensusMapper that explore changes between the 2011 and 2016 censuses down to the block level.\n\nPopulation Change\nDwelling Change\nChange in Non-Primary Residence Dwellings\nChange in Household Size\nComponents of Population Change"
  },
  {
    "objectID": "posts/2017-03-01-transit-explorer/index.html",
    "href": "posts/2017-03-01-transit-explorer/index.html",
    "title": "Transit Explorer",
    "section": "",
    "text": "I have played with Mapzen’s Isochrone serivce in the past with a simple visualization of walksheds.\nRecently Mazen updated the isochrone API to allow for a more fine-grained selection of exactly what transit services to include or exclude in transit routing, and they created an amazing mobility explorer based on that.\nPartially motivated by chatting with two TransLink planners I decided to riff off of that and take a look at how well TransLink serves different parts of Vancouver. At different times of day. And how susceptible TransLink’s network is to Skytrain service disruptions.\nTo do this I decided to allow users to drag a location pin around that sets the start location, allow to change the time of day, and call Mapzen’s API to compute transit isochrones to visualize what areas can be reached from the start location in 15, 30, 45 and 60 minutes.\nTo add some fun I made the Skytrain stations clickable, allowing the user to toggle the station status from open to closed, so users can explore how mobility options change if a station is closed for boarding and no Skytrains pass through any more. Essentially this cuts the transit network.\nThis does neglect bottlenecks that will emerge when alternative routes become overcrowded in the event of a skytrain failure, and it does not take into account countermeasures by TransLink to deploy parallel buses, but it nonetheless gives interesting conclusions about how crucial certain nodes are to the overall network.\nDo you feel that your area is not served well enough by transit? Or under served in the evenings? Or are you worried about what happens if the Skytrain breaks down somewhere? Just launch the Vancouver Transit Explorer and play around to see how transit serves your needs.\nDon’t live in Vancouver and want to explore transit in your region? Not a problem. Use the search bar to jump to whatever city you are interested in and click on the map to move the start location there. Then drag it around to explore that region.\nIf transit services in your city are already part of the TransitLand feed registry that is. If not, this visualization won’t do you much good right now. If you are keen to use it to explore transit in your city, just help TransitLand add your local transit agency to their feed registry."
  },
  {
    "objectID": "posts/2017-03-01-transit-explorer/index.html#details",
    "href": "posts/2017-03-01-transit-explorer/index.html#details",
    "title": "Transit Explorer",
    "section": "Details",
    "text": "Details\nI used settings that assume we have some happy walkers that are willing to walk quite a bit to get to and from transit, as well as walking between stations. It seemed to me that the visualization is already overloaded with options that I did not want to throw in another leaver.\nThe Mapzen Isochrone API also allows for routes or operators to be excluded from the calculations, so one can build more complex “what if” type simulations.\nAnd the service does not include bike share, which really is another piece in the whole mobility puzzle that can significantly shorten travel time (or increase travel distance)."
  },
  {
    "objectID": "posts/2017-03-01-transit-explorer/index.html#issues-and-caveats",
    "href": "posts/2017-03-01-transit-explorer/index.html#issues-and-caveats",
    "title": "Transit Explorer",
    "section": "Issues and Caveats",
    "text": "Issues and Caveats\nTimes after midnight may run into some issues, in some places, like e.g. Vancouver or Toronto, the early morning hour isochrones won’t work properly using this visualization. The technical reason seems to be that some GTFS used times past 24 hours, so 25:01 for one minute past 1AM the next day. And that breaks things somewhere. The good news is it’s just a matter of time for this to get fixed one way or another. But for now it’s broken. :-(\nAlso, the tools this is built on are quite fresh. So there might be some glitches and opportunities to improve. Exciting times when services like the isochrone API by Mapzen become publicly, and freely, available."
  },
  {
    "objectID": "posts/2017-03-01-transit-explorer/index.html#pm-transit-sheds-around-the-world",
    "href": "posts/2017-03-01-transit-explorer/index.html#pm-transit-sheds-around-the-world",
    "title": "Transit Explorer",
    "section": "5pm transit sheds around the world",
    "text": "5pm transit sheds around the world"
  },
  {
    "objectID": "posts/2017-02-10-2016-census-data/index.html",
    "href": "posts/2017-02-10-2016-census-data/index.html",
    "title": "2016 Census Data - Part 1",
    "section": "",
    "text": "Finally the first batch of 2016 census data has arrived on Tuesday AM and CensusMapper was updated with the new census numbers by mid-morning.\nDissemination Block data was a little harder to find, but with the help of some friendly StatCan people I finally managed to locate the data and add that too this afternoon.\nTime for writing up some observations. I am hoping to find time to do this regularly as more data gets released.\nThe first batch of data only comes with three variables: Population, dwellings and households. And two more, a data quality flag indicating areas with incomplete enumeration and the area of each geographic region. Area itself is more tricky than it seems at first, one has to make decisions what to count. Lakes? Rivers? National or Regional Parks?\nNext to these variables, we also got the corresponding 2011 population for all geographic regions at the CT/CSD level and above to aid the comparison, but these were not available at the DA or DB level.\nThe variables seem straight forward, but nothing is simple when it comes to big projects like a census, so it might we worthwhile to spend a little bit of time looking at them. ## Population  The quintessential census variable counts the number of people in each census region. With the exception of people that have a primary residence elsewhere in Canada or abroad. There are several reasons for this, the simples one is to avoid the double-counting of people."
  },
  {
    "objectID": "posts/2017-02-10-2016-census-data/index.html#dwellings",
    "href": "posts/2017-02-10-2016-census-data/index.html#dwellings",
    "title": "2016 Census Data - Part 1",
    "section": "Dwellings",
    "text": "Dwellings\n“Dwellings” is short for “private dwellings”, this does not count collective dwellings like prisons, student dorms, hospitals or nursery homes or even some coops. This can lead to interesting situation where there are zero private dwellings (and zero households) but non-zero population. Metro Vancouver has 45 such dissemination blocks. But these people are only counted if they don’t have a primary residence somewhere else, which often is the case for students in dorms. But not for all students, as the Dissemination Block on UBC campus that’s wedged between NW Marine, University, Lower Mall and Agronomy shows. It has zero dwellings, zero households, but a population of 890 people."
  },
  {
    "objectID": "posts/2017-02-10-2016-census-data/index.html#households",
    "href": "posts/2017-02-10-2016-census-data/index.html#households",
    "title": "2016 Census Data - Part 1",
    "section": "Households",
    "text": "Households\nThe “dwellings, occupied by usual residents” is also called “households”. It refers to a private dwelling that is used as a primary residence, and the inhabitants make up a household. If inhabitants of a dwelling only live there temporarily and/or has a primary residence somewhere else in Canada or abroad, then they are not counted in the population nor the household counts."
  },
  {
    "objectID": "posts/2017-02-10-2016-census-data/index.html#unoccupied-dwellings",
    "href": "posts/2017-02-10-2016-census-data/index.html#unoccupied-dwellings",
    "title": "2016 Census Data - Part 1",
    "section": "Unoccupied Dwellings",
    "text": "Unoccupied Dwellings\n The difference between dwellings and households is “dwellings, not occupied by usual residents”. Essentially, that’s dwellings that are not used as primary residences. It’s a fun variable to look at, and it is available at the very fine Census Block level. In some cases, a Census Block can contain just one apartment building."
  },
  {
    "objectID": "posts/2017-02-10-2016-census-data/index.html#mixing-variables",
    "href": "posts/2017-02-10-2016-census-data/index.html#mixing-variables",
    "title": "2016 Census Data - Part 1",
    "section": "Mixing Variables",
    "text": "Mixing Variables\nThe fun starts when we mix these variables. And compare them to the previous censuses. And there are lots of ways to do that, here are a few:\n\nPopulation Change\n The most immediate variable to look at is population change. We have complete Canada-wide data at the CT level or above, but if we are really interested we can also view this data for DAs and DBs that stayed the same across the census years. The issue with doing that is that some regions will have not data, and one has to be very careful not to read too much into an incomplete dataset. Which is why I usually don’t like giving out maps that contain incomplete data like this. But this is great for diving into specific regions to get more information.\n\n\nDwelling Change\n This is a great way to see where more dwellings got built. This only measures net changes, so even if a region shows zero dwelling increase, there could have been quite a bit of construction in the area. In particular, if houses with secondary suites get torn down and replaced by houses without secondary suites, it will show up as a decline in the number of dwelling units.\n\n\nNon-primary residence dwellings\n More formally, these are “dwellings not occupied by usual residents”, it’s the difference between dwellings and households and the object of continued scrutiny in Vancouver. And a good portion of these are the target of the upcoming empty home tax to either monetize non-primary resident homes or nudge them into the rental market.\n\n\nHousehold size change\n A big part of population change is the change in household size. Canada wide the trend to smaller households is continuing. That means that if the number of dwellings and the rate of unoccupied dwellings in a region remain unchanged, then the population in the area will decline if it follows the national trend to smaller household size."
  },
  {
    "objectID": "posts/2017-02-10-2016-census-data/index.html#population-change-null-sum-game",
    "href": "posts/2017-02-10-2016-census-data/index.html#population-change-null-sum-game",
    "title": "2016 Census Data - Part 1",
    "section": "Population Change Null Sum Game",
    "text": "Population Change Null Sum Game\nFrom looking at CensusMapper, the variable that got by far the most attention nationally in the past three days is Population Change. It’s great to see where population grew or declined. But almost immediately people wonder why population change was different in one region compared to another.\nThe first step to this is the Population Null Sum Game. You need the\n\nDwelling Change\nChange in Unoccupied Dwellings\nChange in Household size\n\nto play. The game is then to express Population Change in terms of those three, and thus “explaining” population change in terms of these variables. There are of course other ways to split up the Population Change variable, but I find this a useful one.\nNathaniel Lauster kicked the game off with this great post. Let’s follow up his inter-municipal level analysis with some intra-municipal numbers.\nWe will focus on the City of Vancouver only. People are welcome to use CensusMapper to repeat this for whatever region they are interested in. Vancouver has the nice advantage that at the dissemination area geography only on change was made. A 2011 downtown DA got split into two for 2016. That makes it very easy to carry this out at the DA level."
  },
  {
    "objectID": "posts/2017-02-10-2016-census-data/index.html#components-of-population-change",
    "href": "posts/2017-02-10-2016-census-data/index.html#components-of-population-change",
    "title": "2016 Census Data - Part 1",
    "section": "Components of Population Change",
    "text": "Components of Population Change\nFirst we refine our variables a little bit it seems more interesting to use the rate of unoccupied builings as a variable, rather than the number of these. We want to express population change Δpop as a linear combination of\n\nDwelling change Δdw\nChange in ratio of unoccupied dwellings Δur\nChange in household size Δhs\n\nMore formally:\nΔpop = hs₁₁ * (1-ur₁₁) * Δdw - hs₁₁ * dw₁₆ * Δur +  hh₁₆ * Δhs\nwhere hs₁₁=pop₁₁/hh₁₁ is the household size in 2011 as computed by dividing the population by the number of households, ur₁₁ is the rate of unoccupied dwellings in 2011, dw₁₆ is the number of dwellings in 2016 and hh₁₆ is the number of households in 2016.\nIt is simple algebra to check that the identity holds.\n The first term gives the contribution to population growth due to the growth in dwellings, assuming that household size and the rate of unoccupied dwellings are unchanged from 2011.\nThe second term give the population growth due to the change in the rate of unoccupied dwellings and the third term gives the population growth due to a change in household size.\nGreat, all that’s left to do is to type the formula into CensusMapper and graph the relative contribution of each of these terms. To make this work we will have to make due with using only the magnitude of each term, but we can visualize the sign in the bar graph widget when we hover over an area.\nThis makes it very easy to compare different areas and see how the different components contribute to the change in population in each area."
  },
  {
    "objectID": "posts/2017-01-18-bumper-year-for-thumb-twiddlers/index.html",
    "href": "posts/2017-01-18-bumper-year-for-thumb-twiddlers/index.html",
    "title": "Bumper Year for Thumb Twiddlers",
    "section": "",
    "text": "Almost a year has passed since we first noticed how sitting on single family homes and twiddling thumbs generates more income than working. And not just at the level of individual single family households. In the City of Vancouver, the cumulative land value gains of just the single family homes eclipsed the cumulative taxable earnings reported to the CRA for the entire population.\nWith the new assessment data available now, it is time to run the numbers and see how our thumb-twiddlers fared vs workers this year. If you thought last year’s twiddling thumbs returns were crazy high, you better hold onto your hats! \n\nWork\nNot much changed in terms of the income people earned. CANSIM does not make the taxfiler data freely available at the municipal level, we we will again estimate the cumulative income for residents of the City of Vancouver by using 2010 Census data and extrapolating by applying the Metro Vancouver rate of income growth.\nThis way we get a cumulative $22.3bn pre-tax or $18.6bn after-tax income in 2010, which grew around 13% between 2010 and 2014. Extrapolating this for another two years to 2016 we estimate an income growth of around 20% since 2010, which then pegs the cumulative income for the City of Vancouver at $26.8bn pre-tax or $22.3bn after-tax (ignoring financial drag and other issues).\n\n\nTwiddling Thumbs\nLet’s compare this to how much Vancouver home owner households “earned” last year by twiddling thumbs while sitting on their property. To keep things simple and consistent with last year, we again focus on just the single family homes (SFH). And again, we only consider land value changes, after all changing the building value because of renovation or rebuilding certainly does not happen by twiddling thumbs.\nThe median SFH land value increased was $435,000 (the average was $594,005), for a cumulative land value increase of $46.7bn, accounting for about half of the total land value increase of the City of Vancouver or two thirds of the land value increase for residential (and mixed) land uses.\nSo while last year the twiddling thumbs return were still comparable to the cumulative income in the City of Vancouver, this year the thumb twiddlers blow the income earners out of the water. Just by by twiddling their thumbs, the SFH property owners alone earned twice as much as the entire population of the City of Vancouver did by actually working. And in most cases homeowners won’t pay taxes on their thumb-twiddle earnings, although the CRA recently made it harder with their new reporting rules.\nWe are glossing over a couple of details here, for example the numbers are not adjusted for inflation, and costs like property taxes and property transfer tax are not taken into account.\nOf course, comparing income from work to income from twiddling thumbs is not entirely fair. The income from twiddling thumbs is frozen in the property until the owner sells. But gains accumulate over the years, and eventually everyone sells and realizes the gains (or passes them on to the next generation). And it’s always good to remember that “house-rich cash-poor homeowners are one transaction away from simply being rich renters”.\n\n\nThe Long Term\n Most people don’t trade houses like stocks, so what really matters is the long term gains, not the year to year changes. The 11 year timeframe for which we have data roughly matches the average holding time for a single family house. The year over year changes in land value vary dramatically, as the image shows and can be explored further using the interactive version.\nOver 11 years, the single family home land value quadrupled. The median (nominal) single family home land value increase over that timespan was $1,233,000, or $112,000 per year. Broken down further, that’s $2,339,000 ($213,000 per year) for the median west side home and $1,031,000 ($94,000 per year) for the median east side home.\n We mapped the land value gain averaged over 11 years. We can observe that the average yearly increase depends on the square footage of the property as well as the location. The rates are mostly uniform throughout the city (with some notable exceptions), but properties starting out with a higher value will see higher total value increases. It’s probably fair to say that even using the 11 year average, most homeowners “earned” more money twiddling thumbs than pursuing a more traditional employment.\n\n\nHourly Rate for Twiddling Thumbs\nUsing the same methods as last year we can compute the hourly earnings of thumb twiddlers. For the July 2016 to July 2017 timeframe, thumb twiddlers in the City of Vancouver averaged a tidy $239 per hour. Using the 11 year averaged numbers instead of focusing on just the last year we still get a healthy average thumb twiddling rate of $62 per hour.\nAnother bumper year for thumb twiddlers! Considering to change your line work to thumb twiddling? To bad thumb twiddling is so unaffordable!\n\n\nData Dump\nHere is the raw output of the stats run on the single family properties for anyone interested.\n\nSFH Land Value Rise (2016 - 2017)\n\nCity Wide: Average $594,005 Median $435,000, Count: 78648, Total: $46,717,325,799, Hourly: $239\nEastside: Average $373,306 Median $358,000, Count: 47988, Total: $17,914,233,199, Hourly: $150\nWestside: Average $939,435 Median $866,000, Count: 30660, Total: $28,803,092,600, Hourly: $378\n\n\n\nSFH Land Value Rise (2006 - 2017)\n\nCity Wide: Average $153,777 Median $112,090, Count: 77809, Total: $11,965,271,272, Hourly: $62\nEastside: Average $96,892 Median $93,727, Count: 47545, Total: $4,606,748,985, Hourly: $39\nWestside: Average $243,144 Median $212,636, Count: 30264, Total: $7,358,522,287, Hourly: $98\n\nThe total number of units for the 2006 to 2017 analysis are a little lower since not all properties can be traced over that time period without resorting to more tedious analysis.\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Bumper {Year} for {Thumb} {Twiddlers}},\n  date = {2017-01-18},\n  url = {https://doodles.mountainmath.ca/posts/2017-01-18-bumper-year-for-thumb-twiddlers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Bumper Year for Thumb\nTwiddlers.” MountanDoodles (blog). January 18, 2017. https://doodles.mountainmath.ca/posts/2017-01-18-bumper-year-for-thumb-twiddlers."
  },
  {
    "objectID": "posts/2017-01-16-2017-assessment-data/index.html",
    "href": "posts/2017-01-16-2017-assessment-data/index.html",
    "title": "2017 Vancouver Assessment Data",
    "section": "",
    "text": "The friendly folks at Vancouver Open Data just updated their property assessment data with the fresh 2017 property tax assessments. Time to run the script to update the Vancouver Assessment Map with the new data, just like we did last year.\nFor now we just updated our standard visuals and computed some overall statistics. We will take a closer look at the data over the coming days.\n\nMaps\n\n By now we have a variety of maps that highlight different aspects of Vancouver real estate, and after running the import scripts they automatically serve the newest data. Our basic interactive assessment map offers a variety of ways to slice and display the data. It’s mostly focused on functionality, some of which we described in last year’s post, as well as other posts like the one on twiddling thumbs vs working.\nWe also have interactive views focusing on how real estate prices varied over time, for example the houses and dirt map that separates out (inflation adjusted) values of the structures and the land, and also allows to filter by type of housing, to animate changes over time.\nFor people that like simpler maps we also have a plain total (nominal) value over time map that allows to interactively step through the years and see how single family house values in Vancouver changed over time. Here we also added the ability to visualize year-over-year value changes, which also hints at how BC Assessment changed their valuation algorithm over the years.\n\n\nThe Data\nThe data originates with BC Assessment, which estimates land and building values of each property based on recent sales of comparable properties. The values are pegged at July 1 of each year, with the the most recent available now being July 1 2016. The estimates for the values do not reflect changes in the market since then. Moreover, the estimates can be quite off on an individual property level, but are unbiased. That means that any statistics derived from a large subsample should fairly accurately reflect actual market conditions for July 1st. Lastly, the assessed values will still change a bit as some will be successfully appealed.\nCity of Vancouver, as well as the City of Surrey, make this data available for general use through their open data portal, which allows us to create these maps. The format of the data the municipalities are giving out through their open data portal is different, so lazy me is only importing data from City of Vancouver. Sadly, BC Assessment does not make this data generally available province wide for us to make province wide maps.\nWhile BC Assessment makes this data available on their eValue website for browsing individual properties and also provides it in bulk to researchers, the attached license does not allow the thematic mapping of individual properties.\nThe motivation behind the map was to understand the building stock, so in the maps as well as the summary statistics below we filter out parks and some other properties.\nThe new city dataset does not include the 2017 tax levy, so our maps still only show the 2016 tax levies until CoV updated their dataset.\n\n\nHistory\nIn the spirit of last year’s post we ran some quick summary statistics to break down the numbers by neighbourhood. Instead of listing the most recent land and building value increases by neighbourhood we stuck everything into an interactive graph for the entire time span between 2006 and 2017 tax years. Use the dropdown menus to drill down into city neighbourhoods, view values for all properties or just residential properties and display as total value or year-over-year percentage change.\nThe last year again saw an huge increase in property values. For the City of Vancouver land values were up 35% and building values 10%, with the land value increase setting a record for the timeframe for which we have data. The increases become even more pronounced when we zero in on residential property only.\n\n   Total Value Percentage Change \n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {2017 {Vancouver} {Assessment} {Data}},\n  date = {2017-01-16},\n  url = {https://doodles.mountainmath.ca/posts/2017-01-16-2017-assessment-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “2017 Vancouver Assessment Data.”\nMountanDoodles (blog). January 16, 2017. https://doodles.mountainmath.ca/posts/2017-01-16-2017-assessment-data."
  },
  {
    "objectID": "posts/2016-11-26-character-retention/index.html",
    "href": "posts/2016-11-26-character-retention/index.html",
    "title": "Character Retention",
    "section": "",
    "text": "Today I went to the City of Vancouver Character Retention open house. It was quite informative, city staff were helpful and knowledgeable, and the display board and feedback form asked many good questions. But I came away with a couple of points that I think need to be addressed further:"
  },
  {
    "objectID": "posts/2016-11-26-character-retention/index.html#tldr",
    "href": "posts/2016-11-26-character-retention/index.html#tldr",
    "title": "Character Retention",
    "section": "TL;DR",
    "text": "TL;DR\nIt gets a little wonky, so here the very short version:\n\n\nCurrent and proposed “character retention” is hollow, just retains shell. Should be handled in design guidelines.\n\n\nReal character (or heritage) retention should take closer look at underlying economic drivers to become more effective.\n\n\nGentle, ground-oriented density like 4-plexes is desperately needed in RS and RT, should be decoupled from “character retention”."
  },
  {
    "objectID": "posts/2016-11-26-character-retention/index.html#faux-retention",
    "href": "posts/2016-11-26-character-retention/index.html#faux-retention",
    "title": "Character Retention",
    "section": "Faux Retention",
    "text": "Faux Retention\n The existing character retention guidelines in RT, which seem to serve as a model for the expansion of character retention to RS, often don’t “retain” all that much.\nWhat the existing character retention process entails is essentially a “character home” being gutted down to the studs (in most cases), potentially moved to a corner of the property and a foundation added. Then the entire house, including structural elements, gets build up again and infill added to the back.\nThe result of the process is a main house with exterior form resembling a character home, and infill in the back matching the style. This process is almost indistinguishable from a new built to “character design guidelines”. The difference in landfill waste is minimal (and much better addressed through recycing policies) and there is no difference to the eye, as can easily be seen when comparing the new built infill to the “retained” front house, like in the Mt Pleasant 4-plex pictured above."
  },
  {
    "objectID": "posts/2016-11-26-character-retention/index.html#economics",
    "href": "posts/2016-11-26-character-retention/index.html#economics",
    "title": "Character Retention",
    "section": "Economics",
    "text": "Economics\nCharacter retention is hard to do. It takes incentives to make it happen. The more the character retention process can leverage some underlying economic drivers, the more effective it will be.\nA while back we ran some analysis on 11 years of property-level assessment data in the City of Vancouver that was made available through the open data catalogue. The upshot is that the single most important factor that predicts if a building gets torn down is the (assessed) value of the building alone relative to the total value of the property. We call this quotient the “teardown coefficient” and found that if it is below 5%, the building has a 1/6 chance to get torn down within 8 to 10 years. More details and background on this can be found here, a refinement of this analysis is works in progress.\nCurrently, 34% of the SFH building stock in Vancouver fall into that category. Often people assume that the risk of a building getting torn down simply depends on the building age. But the relationship is not that simple as the following graph shows.\n\n\n\n\n\n\n\n\nThe blue denotes properties facing little teardown pressure, the red is for properties with high teardown pressure. We can see that the pre-1920 building stock holds up reasonably well. In fact, 33% of the pre-1920 building stock with high teardown risk. For the 1920 to 1935 stock we have 59% of buildings in teardown territory, and for 1935 to 1965 buildings that number jumps to 78%. This makes the particular choice of the character cutoff year 1940 a bit of a head-scratcher.\n This informs the scale of the teardown pressure the building stock of a particular vintage is facing. We can similarly map the individual properties to understand their geographic distribution. One should note that assessment data, while unbiased in aggregate, can be quite off when looking at specific buildings. So when mapping for example all single family homes with high teardown pressure it might mis-identify some buildings as teardown candidates, or miss others. However, in aggregate the data will be quite robust and one can easily see that there are no obvious geographic biases in teardown risk. To quantify some of this, 31% of eastside SFH vs 39% of westside SFH face high teardown risk.\nUnderstanding the teardown pressure, as well as the geographic distribution and the pressures by vintage, gives an important baseline to the heritage and the character discussions. Houses far above the 5% threshold face little teardown pressure, and they need little, if any, policy protection to retain them. On the other hand, for houses below the teardown cutoff it makes very little economic sense to retain a building, and will require a lot of effort and policy protection to try and retain them. And even with these protections, there is a good chance that the building will eventually go or face “demolition by neglect”.\n One example of this is the property at 4755 Belmont Ave, which the city lists as heritage building of primary significance.\nEffective retention policies are likely going to aim at the building stock around and somewhat above the teardown threshold of 5%. Ideally character retention policies should aim to strengthen the economic viability of buildings with character merit so that they don’t fall into the range where the teardown pressure becomes overwhelming. With this in mind, and setting the age cutoff to 1935 instead of 1940, we can investigate the existing pre-1935 building stock by teardown pressure, separating out the stock that faces high teardown pressure from the stock with moderate teardown pressure and the stock with low teardown pressure.\n And we can view all three at the same time.\nWithout taking these underlying economic factors into consideration, character (as well as heritage) retention policies are likely to be less effective at retention than they otherwise would be. And have broader adverse effects on buildings without significant heritage or character merit. The city’s approach to deal with heritage retention seems to be to target all properties built before 1940, indiscriminate of what shape the buildings are in. In the next days the consultant report the city ordered will become available, maybe there are some nuggets in there that make this whole endeavour sound reasonable."
  },
  {
    "objectID": "posts/2016-11-26-character-retention/index.html#rt",
    "href": "posts/2016-11-26-character-retention/index.html#rt",
    "title": "Character Retention",
    "section": "RT",
    "text": "RT\nThe current character retention effort is entirely focused on RS. This seems to be motivated by the fact that RT already has character retention guidelines. The way it works is that much of RT is downzoned, with extra density conditional on character retention. This seems to be the basic blueprint of the proposal for RS, so let’s take a close look how this works.\n Mount Pleasant gives a good example of what this can look like. In RT-6 zoning we have allowed stratified 4 and 5-plexes on single family lots under the character retention policies. We have looked at this before, this results in the property getting sliced up and stratified into 3 to 5 family-sized ground-oriented units. In terms of prices, the assessments pegged at July 2015 (which became surprisingly accurate again) puts RT-6 single family lots between $1m and $6m (median $1.9m), and units in those multiplexes between $300k and $1.8m (median $800k).\n This kind of development is great. We need more of that, much more. But this same model can’t be immediately taken and expanded across the city. One look across town to RT-7, where similar guidelines are in place, shows that multi-plexes only appear in the pocket at 16th and Arbutus. That’s mainly because the Mt Pleasant model of character “retention” requires large lots, and most of RT-7 is on smaller lots that make the process of “retention” into multi-plexes according to current city rules unattractive. The rules need to be amended to take lots size (and other parameters) into account to unlock this kind of development across the city.\nThis resulted in RT-6 only having 12% of the building stock currently facing high teardown pressure, compared to 34% of RT-7. In the pocket of RT-7 east of MacDonald, where lots are larger and more properties underwent the character retention process, only 17% of the building stock faces high teardown pressure. And yes, RT-6 started out with buildings in a better state than RT-7. Which is another part of the reason why the character retention program was more successful in RT-6.\nWe can try to understand better where the character retention program was utilized and where it wasn’t by looking at all the properties that have been built since 2000 in RT, keyed by whether it was a single family, duplex or multi-plex that got built. We removed RT-11 from the map, it is too new to be meaningful. It’s amazing to me how in some pockets almost no multi-plexes get built. We should try and fine-tune this process before transplanting it to RS. RT-11 offers a cautionary tale. Looking at what got built in current RT-11 after 2000\nwe see lots of single family homes. A huge missed opportunity. If we had upzoned that area a decade earlier we could have allowed for homes that suit our families much better.\nIn summary, we should look carefully at how character retention performed in RT and going forward devise policies for RS and RT. It’s time to drop that artificial divide that has been overtaken by the reality that RS already allows 3 units on each property, more than RT."
  },
  {
    "objectID": "posts/2016-11-26-character-retention/index.html#hostage-taking",
    "href": "posts/2016-11-26-character-retention/index.html#hostage-taking",
    "title": "Character Retention",
    "section": "Hostage Taking",
    "text": "Hostage Taking\nThe display boards suggests that the current plan for “character retention” in RS is essentailly to copy over the RT blueprint with some fine-tuning. Downzone RS and give density for undergoing “character retention”.\nIn essence that’s holding gentle, ground-oriented density hostage to character retention, and it’s a terrible thing to do. Whatever the question is, downzoning can’t be part of the answer any more. Vancouver is starved for gentle, sensible, ground-oriented family-friendly development.\nOriginally, when the character retention guidelines for RT were made, this kind of gentle density came in the Trojan horse of character retention. And even if much of that was faux character, I don’t mind. I take that kind of density any way I can.\nBut we are quite a bit further along now. Unaffordability skyrocketed. Families are starved for housing options.\nWe need to scale this process up. And just expanding the area where we deploy it to also cover RS won’t do the trick. We average about 10 RT character retention projects a year. Out of 11k RT properties. If we scale that up to all 68k RS properties, we will increase that number to about 70 a year. Just for perspective to see how inadequate that is, we tear down around 1000 single family homes a year. And replace them with single family homes.\nWhat we need is a program that brings this type of density to RS and RT independent of the character retention program. And let’s drop the “retention” pretence. If the character home look is what Vancouverites want in return for gentle density, then let’s prescribe the exterior look of new-builts and let the design review process handle it."
  },
  {
    "objectID": "posts/2016-11-26-character-retention/index.html#new-carrots",
    "href": "posts/2016-11-26-character-retention/index.html#new-carrots",
    "title": "Character Retention",
    "section": "New Carrots",
    "text": "New Carrots\nIf density won’t be the main carrot (or a stick) for character retention, than what can pull up the slack? Property taxes is an obvious one. The owner could be allowed to re-claim property taxes against improvements and maintenance that aim to maintain or underline the character merit of the home. And these re-claimed taxes become cumulatively payable, with interest, in the event of demolition. Of course this would raise everyone else’s property taxes, but I think that’s only fair in return for the community dictating character-homeowners the exterior look of their home.\nThe idea behind this is that this aims to directly strengthen the economic viability of the building, thus removing the economic drivers that favour tearing down the building. And over time accumulating a penalty that dissuades from tearing down the building.\nDensity can still be part of the mix, but not in a way that it precludes smart gentle density to be built without\nit. The reality is that much of our building stock will go, the economic drivers are just too strong. And we all know that we should not replace those single family homes with yet another single family home that at best the top 5% income households\ncan afford. We should allow these houses to be replaced with ground-oriented units a much larger portion of Vancouver families can afford.\nSmarter people than me have probably though about this for quite some time now. I would love to hear more ideas how character retention can be structured so that it does not get in the way of gentle density for new builts."
  },
  {
    "objectID": "posts/2016-11-18-interactive-isochrones/index.html",
    "href": "posts/2016-11-18-interactive-isochrones/index.html",
    "title": "Interactive Isochrones",
    "section": "",
    "text": "Mapzen again upped their game by publishing their Mobility API. This is super exciting for anyone interested in a whole range of mobility questions. A question I have seen is how to adapt that to specific needs. So here is a quick example how to customize walksheds.\n\nAll we do is set up a quick map that computes the 5 and 10 minute walksheds when the user clicks on the map.\n\n\nTo get a better view you can also take the map full-screen.\nFeel free to just grab the html and adjust it for your needs. Please go and register for your free Mapzen API key and replace the key in the downloaded html file with yours. Refer to the Mobility API to customize this for your needs.\nHere is the relevant code to generate the isochrones:\n    var mapzenApiKey=\"&lt;your api key&gt;\";\n    var marker,isochrone;\n\n    function httpGetAsync(theUrl, callback)\n    {\n        var xmlHttp = new XMLHttpRequest();\n        xmlHttp.onreadystatechange = function() {\n            if (xmlHttp.readyState == 4 && xmlHttp.status == 200)\n                callback(xmlHttp.responseText);\n        };\n        xmlHttp.open(\"GET\", theUrl, true); // true for asynchronous\n        xmlHttp.send(null);\n    }\n\n    map.on('click',function(e) {\n        if (marker) marker.removeFrom(map);\n        marker = L.marker(e.latlng).addTo(map);\n        var json={locations:[{lat:e.latlng.lat,lon:e.latlng.lng}],costing:\"pedestrian\",contours:[{time:5,color:\"006400\"},{time:10,color:\"006400\"},{time:15,color:\"006400\"}]};\n        var url='http://matrix.mapzen.com/isochrone?json='+JSON.stringify(json)+'&api_key='+mapzenApiKey;\n        httpGetAsync(url,function(data){\n            var geojsonFeatures=JSON.parse(data);\n            geojsonFeatures.features.forEach(function(f){\n                f.geometry.type=\"Polygon\";\n                f.geometry.coordinates=[f.geometry.coordinates];\n            });\n            if (isochrone) isochrone.removeFrom(map);\n            isochrone=L.geoJSON(geojsonFeatures, {style: function(feature){return {color:'#'+feature.properties.color, opacity:feature.properties.opacity}}}).addTo(map);\n        });\n    });\nHappy mapping.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2016,\n  author = {von Bergmann, Jens},\n  title = {Interactive {Isochrones}},\n  date = {2016-11-18},\n  url = {https://doodles.mountainmath.ca/posts/2016-11-18-interactive-isochrones},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2016. “Interactive Isochrones.”\nMountanDoodles (blog). November 18, 2016. https://doodles.mountainmath.ca/posts/2016-11-18-interactive-isochrones."
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html",
    "href": "posts/2016-08-31-incomes/index.html",
    "title": "Quick Guide to Income Data",
    "section": "",
    "text": "The news media has an extraordinarily difficult time with income data. And that’s only partially the media’s fault. In general, income data is a funny beast. So I decided to give a quick overview over income data that’s available through Statistics Canada. To make things a little more concrete I focused on Vancouver as an example, but everything applies equally well to any other area in Canada.\nMost journalists I talked to care about accurate and relevant statements. If that’s you, you will need to keep reading and dive down the slightly wonkish income rabbit hole.\nFor those that don’t really care, I made a quotable random income generator. Just hit the “generate” button copy & paste the result and link this post as source. (Use at own risk!)\nThe income numbers don’t fit the story? No problem, just play again until you get something you like!"
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html#geographic-regions",
    "href": "posts/2016-08-31-incomes/index.html#geographic-regions",
    "title": "Quick Guide to Income Data",
    "section": "Geographic Regions",
    "text": "Geographic Regions\nThe first thing to do when quoting income numbers of any sort is to decide what region to use. Seems straight forward enough, but in reality hardly ever done. In the Vancouver context, it simply comes down to deciding if one wants to report on income for the City of Vancouver or Metro Vancouver. Both are frequently referred to as just “Vancouver”. Which is just fine as long as it is clear from the context which region we are talking about. And more importantly, whatever the income is compared to (e.g. housing price data) are taken for the same region."
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html#base-demographics",
    "href": "posts/2016-08-31-incomes/index.html#base-demographics",
    "title": "Quick Guide to Income Data",
    "section": "Base Demographics",
    "text": "Base Demographics\nNext we have to choose a base demographic we are interested in, choose which income distribution is relevant for our context. In some cases this choice is obvious. If we are writing a story on lone parents in the City of Vancouver, we would want to look at the income distribution of lone parent households. If we are looking at the development of wages, we should consider individual incomes. For a story on consumer spending we may want to look at after-tax income.\nWe may also be interested in specific type of income, for example only in investment income. Or the net capital gains and losses. Or pension benefits. Or self-employment income."
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html#statistic",
    "href": "posts/2016-08-31-incomes/index.html#statistic",
    "title": "Quick Guide to Income Data",
    "section": "Statistic",
    "text": "Statistic\nOften we would like to distill everything down to a single number. Typically we would choose the average or median income from the distribution we are interested in, but we may also choose other statistics, for example the top quintile. Which one we choose really depends on the story. As a rule of thumb, choose median incomes if you are interested in the ‘typical’ person or household. Choose average income if you are interested in the questions relating to the total amount of money earned in a given area. Income quantiles are great to focus in on certain subgroups of the population."
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html#availability",
    "href": "posts/2016-08-31-incomes/index.html#availability",
    "title": "Quick Guide to Income Data",
    "section": "Availability",
    "text": "Availability\nGenerally we also have to consider the availability of information. The census has very detailed income data in their standard release that allow us to be very specific about what kind of incomes we are interested in. And if that’s not enough we can make a special request to StatsCan to narrow the incomes down to a group we are interested in, for example incomes of renter households as was done in the rental housing index.\nAnnual income data is available from CANSIM (Statistics Canada’s socioeconomic database), but the geography is coarser than in the census. Generally we can only expect to get annual data for the whole country, provinces, census metropolitan areas, but not census subdivisions (municipalities) or finer geographies."
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html#sources-and-comparability",
    "href": "posts/2016-08-31-incomes/index.html#sources-and-comparability",
    "title": "Quick Guide to Income Data",
    "section": "Sources and Comparability",
    "text": "Sources and Comparability\nAll income data eventually come down to CRA taxfiler data. But the devil is in the details. For the 2006 and 2011 censuses, there was a mixture between self-reporting and cross-referencing of individual level income data with CRA data. And even self-reported data would again undergo quality control and get somewhat adjusted according to CRA data.\nBut there are important differences in the data, both on what kind of income is included and also what people are included.\nThe details are endless, many of them won’t matter. Here are a couple to look out for:\n\nCANSIM is a comprehensive sample, census data is roughly a 1 in 5 subsample which may lead to sampling bias especially in small geographies.\nCENSUS data is sensitive to non-return bias. Not just no-return bias introduced by the NHS, but also by census undercounts which, for example, were in the 10% range for the 25 to 29 year old demographic in both the 2006 and 2011 censuses.\nCANSIM and the Census have different denominators for reporting average or median incomes. CANSIM reports on all T1 taxfilers, as well as various family types. Some data can be broken out into age groups at the 15 and 65 year cutoffs, whereas Census reports only on people (15 or older) (or families or households) in “private households” with income greater than zero.\nthe definition of “private household” changed between 2006 and 2011 censuses.\n2011 and 2006 censuses differ slightly in what is included as income, for example 2006 included RRSP withdrawls, 2011 didn’t (both include pensions and RRIFs). 2011 includes net capital gains and losses as a percentage of total income in a separate category, 2006 did not include any information on capital gains.\n\nSo even when comparing CANSIM income data to census income data for the appropriate year we should expect some differences. Or comparing 2006 to 2011 income, even if hypothetically an area had identical tax returns, we would still see differences. Generally speaking, CANSIM average and median income estimates are lower than census estimates as it includes a larger pool of people that are likely to have lower income."
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html#where-to-get-the-goods",
    "href": "posts/2016-08-31-incomes/index.html#where-to-get-the-goods",
    "title": "Quick Guide to Income Data",
    "section": "Where to get the goods?",
    "text": "Where to get the goods?\n The latest census income numbers are available via Statistics Canada. I am biased and prefer the CensusMapper interface. For 2011 data, choose any 2011 CensusMapper map, select the geographic region you are interested in and press the “more” button on the popup to bring up the “census wheel”. Then drill into the “income” section, the last NHS variable in the widget.\nCANSIM has annual data based on taxfiler information for coarser geographies. So you won’t be able to get data for the City of Vancouver or neighbourhoods within the city like with census data, but you can get data for Metro Vancouver. And you can get it for more recent years. Things get complex very fast there are 213 CANSIM tables with income data\nAt CensusMapper we are still firmly planning integrate CANSIM data into the CensusMapper interface, but their API refresh got pushed back to sometime 2018 and we can’t justify the extra resources to integrate CANSIM data temporarily using the old API just to switch it over a year later."
  },
  {
    "objectID": "posts/2016-08-31-incomes/index.html#too-abstract",
    "href": "posts/2016-08-31-incomes/index.html#too-abstract",
    "title": "Quick Guide to Income Data",
    "section": "Too Abstract?",
    "text": "Too Abstract?\nWant to see how this works on an example? Just read the next post on using income data for housing affordability."
  },
  {
    "objectID": "posts/2016-08-16-mobi-a-first-look/index.html",
    "href": "posts/2016-08-16-mobi-a-first-look/index.html",
    "title": "Mobi – a First Look",
    "section": "",
    "text": "Vancouver finally has a bikeshare system. And everyone is hoping it will succeed, despite the obstacles BC’s mandatory helmet law poses for the system. So we are eager to find out how things are going with Mobi.\nTo set the background, consider that Seattle’s Pronto is getting less than 1 ride per bike per day after half a year in operation. In comparison, bike shares that are considered ‘successful’ in North America get 3 to 5 rides per bike per day. Taipei’s system, which I am particularly fond of, gets over 11 rides per bike per day.\nSo how about Mobi? It barely started, and it’s not really fair to run the numbers right now. But we just couldn’t hold our curiosity back."
  },
  {
    "objectID": "posts/2016-08-16-mobi-a-first-look/index.html#data",
    "href": "posts/2016-08-16-mobi-a-first-look/index.html#data",
    "title": "Mobi – a First Look",
    "section": "Data",
    "text": "Data\n First off, some caveats. Mobi does not have an official API for their system. In fact, they only provide a barely usable map of station statuses at the bottom of their landing page. So we decided to jump in and scrape their data to make our own map. In absence of better alternatives, the official Mobi smartphone app is still not released, the map has gotten quite popular. And in absence of an official Mobi API there are now others, like TransitApp, that are consuming our station data shadow API.\nWhat that means in terms of data quality is that while we get fairly accurate station bike counts at about a 1-minute interval, there are some issues with using the data for rigorous analysis. If a bike gets checked in while another one gets checked out at about the same time there is a good chance that we will miss it. And we can’t distinguish rebalancing from a group of people checking out or dropping off a bunch of bikes at the same time. And we don’t have individual bike data to look at travel patterns, for example what popular trip patterns are. Moreover we currently don’t collect and store weather data, an important variable that should be included in any bike share analysis.\nThat pretty much rules out anything but high level analysis."
  },
  {
    "objectID": "posts/2016-08-16-mobi-a-first-look/index.html#station-history",
    "href": "posts/2016-08-16-mobi-a-first-look/index.html#station-history",
    "title": "Mobi – a First Look",
    "section": "Station History",
    "text": "Station History\nTo get an initial idea we started to look at station history. We only started recording Saturday 13th. For the fun of it we added one day history to our Mobi bike station and bike infrastructure map. Drop me a line if you are interested in longer timeframes.\nTaking a look at the daily overall usage patterns\n\n\n\n\n\nthings are pretty much as expected. Nice Gaussians for the weekend usage, and some commute spikes for the weekday usage. Squinting really hard one might want to make out a slight lunchtime boost, we will have to collect more data to confirm that.\nAnother thing we see is that usage seems to hit a low point around 4am, so that’s a good time to divide up the days when looking at daily usage. Adding up the rides we get around 1400 rides a day. At around 3am we detect 490 bikes in the stations, so that makes about 3 rides per bike per day. On average. That a pretty impressive number for a system that is just getting off the ground. And that’s while still in “members only” mode. The nice weather has helped, but there is no getting around the fact that Mobi is off to a great start!\nOf course not every bike is getting 3 rides. Some get more, some get less. For example, a bike parked at the Ontario & Seawall station has been very popular, here is live data on the most recent days of usage.\n\n\n\n On the other hand, bikes at Yukon & 12th have been having a hard time to find riders. Both stations are at the current boundary of the system, it is hard to say what makes the difference. The particular location of the least used station is sure to get some people talking.\nMobi is undoubtedly carefully analyzing their station usage and incorporating that into their strategic planning how to expand their network. And hopefully publish a useful API for all the data geeks out there."
  },
  {
    "objectID": "posts/2016-07-26-bike-share-map/index.html",
    "href": "posts/2016-07-26-bike-share-map/index.html",
    "title": "Bike Share Map",
    "section": "",
    "text": "It is no secret that we at MountainMath like bikes. And maps. And open data and sharing. We guess you know where this is going.\nVancouver has finally gotten a bike share system, and we are loving it. So we took the occasion to take our old bike infrastructure maps, polished them up a bit using Mapzen’s bike map style and adapted it for our purposes.\nThe result is our Vancouver Bike Share Map. (Fair warning. While this map works great on your iOS and Android phone, it may not work on old desktop computers.)\nAdd it to your phone home screen, and it will be right at your fingertips when you need it!\nAnd we threw the Mobi Bike Share station data on top. That last part turned out a little more messy than expected, the API that was linked at the Bike Share Research website is broken, and the Mobi redirects are malformed and no guesswork would yield a working API. Luckily the bike share map wizard on Twitter had the answer how to tease out the station data. And some of the stations are duplicate, so it took some time to clean up the data properly.\nWe tried to keep our map as simple as possible. This iteration highlights three types of infrastructure, separated, bike lanes and shared lanes. The map has some hill-shading for reference, and at high zoom level it shows some bike points-of-interest. The rest is kept clean and simple.\nFor the stations, we weren’t fans of the icons and presentation used on many other maps. We decided on a simple donut-style presentation to visualize available bikes to check out and empty slots to return them. This is the most crucial information for a bike share user. Where can I check out a bike, and where can I return one. There is nothing more frustrating than having to go to a meeting and being unable to check the bike back into a station because it is full. The map gives a quick visual guide for this, although the Mobi data lags quite a bit right now. Hopefully that will change in the future once they get their API up and running.\nOn touch/hover the bike stations expand to give exact counts and the station name. Station sizes vary and counts can be useful at times. We opted to keep the icon size independent of station size, trading added information for a cleaner presentation by only revealing it on user interaction.\nThis map focuses on the bike share user. If you are more interested in stats and overview of various bike share systems you want to head over to Oliver O’Brien’s excellent bike share map."
  },
  {
    "objectID": "posts/2016-07-26-bike-share-map/index.html#bike-infrastructure",
    "href": "posts/2016-07-26-bike-share-map/index.html#bike-infrastructure",
    "title": "Bike Share Map",
    "section": "Bike Infrastructure",
    "text": "Bike Infrastructure\nThe bike data we are using comes straight out of OpenStreetMap. That has some disadvantages as it can be less accurate as the city bike data. But it also has a huge advantage. Anyone can edit OpenStreetMap to fix incorrect or missing infrastructure designations. We kept some of the targeted editing capabilities intact on our map, if you view it on desktop and shift-click anywhere, it will take you right into the OpenStreetMap editor at the position you clicked so you can edit the infrastructure there.\nIf you are planning on helping out and improve OpenStreetMap bike data, you may want to read the excellent targeted editing post from Mapzen and use their map to brouse OSM tags. You may notice the similarity to our map, we borrowed heavily from that one. Updates you make will take between 1 to 4 hours before they percolate through into our map.\nData in OpenStreetMap is automatically accessible to other services too, so improving OSM will help out everyone. And not just by making the maps better, routing gets better too!\nAnd the other big advantage of OpenStreetMap is that our map is automatically global. Right now we still focus on Vancouver, and we restrict the search function to Vancouver addresses. But in principle the infrastructure part of the map works for anywhere on the planet."
  },
  {
    "objectID": "posts/2016-07-26-bike-share-map/index.html#next-steps",
    "href": "posts/2016-07-26-bike-share-map/index.html#next-steps",
    "title": "Bike Share Map",
    "section": "Next Steps",
    "text": "Next Steps\nAdding bike routing is an obvious next step. And there are lots of other refinements we can think of.\nHill shading has some issues right now, only some hill aspects are visible right now. We may have to add some colour to hill shading to visualize all hill aspects better, but that requires care to ensure the map retains it’s clean looks.\nThe next step in the map evolution is to make a multi-modal map, toghet with multi-modal routing. The map will highlight infrastructure based on what modes are preferred, and routing will do routing for these modes.\nUsing the great work of the Bike Share Research website we can also parse the feeds for all global bike share systems registered there and expand the bike share map to other systems around the world."
  },
  {
    "objectID": "posts/2016-07-26-bike-share-map/index.html#feedback",
    "href": "posts/2016-07-26-bike-share-map/index.html#feedback",
    "title": "Bike Share Map",
    "section": "Feedback",
    "text": "Feedback\nGot ideas how to make the map better? Want some pointers how to adapt this map for your own purposes? Send us a tweet or a message!"
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html",
    "title": "SDH Zoning and Land Use",
    "section": "",
    "text": "Recently the question around the amount of space taken up (exclusively) by single detached houses has show up on my Twitter feed citing that SFH take up 70%, 66%, and 57%, 56% (timestamp 3:50). I personally have thrown in 34% as a contender. And, just for the fun of it, by the end of this post I will have thrown 33% and 28% and my favorite, 81%, into the mix.\nWhat’s going on, how can there be such a large range of estimates for what seems to be a simple question? The answer lies in the details of what exactly the question is asking, all of the above numbers are correct answers to one particular version of very similar questions."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#tldr",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#tldr",
    "title": "SDH Zoning and Land Use",
    "section": "TL;DR",
    "text": "TL;DR\nThe takeaway I think is most useful for general purposes:\n\n35% of all households live on single family and duplex properties making up 81% of Vancouver’s residential land, while the remaining 65% of households live on 19% of the residential land.\n\n\nThere are two fundamental ways to attack the question. One is through zoning, that is to look at area where only SDH can be built. The other is through land use, that is looking at what area is currently occupied by SDH lots."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#single-detached-houses",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#single-detached-houses",
    "title": "SDH Zoning and Land Use",
    "section": "Single Detached Houses",
    "text": "Single Detached Houses\nFirst we need to understand what we mean by single detached house. There are two common, but quite different definitions out there. One stems from the zoning code and refers to a property that is not stratified and has a single main structure on it, possibly with one suite, and possibly a laneway house. This is the definition most commonly used. When someone says they own a single family house, this is what they are talking about. Let’s call them single detached properties. There are around 75,000 of these in the City of Vancouver.\nStats Canada employs a different definition. To them a single detached dwelling is single dwelling unit comprised of a free-standing (detached) structure. A single detached property may contain exactly one dwelling unit, in which case Stats Canada would also call it a single detached dwelling. If the house has a secondary suite, Stats Canda would classify it as a duplex. If it has a laneway house, Stats Canada would classify the laneway house as a separate single detached dwelling. Stats Canada reports 47,530 single detached dwellings in the City of Vancouver.\nBecause of this discrepancy, sometimes it is useful to look as single family houses and duplexes combined. Especially if one wants to mix in census data to for example look at the number of households that live in these dwellings. While there are about 75,000 single detached properties in Vancouver, it is fair to assume that there are significantly more than 75,000 households living on these properties once we account for secondary suites and laneway houses."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#zoning",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#zoning",
    "title": "SDH Zoning and Land Use",
    "section": "Zoning",
    "text": "Zoning\n Vancouver is divided up into zones that determine what can (and cannot) be legally built on the land. If one is primarily concerned with the area of land that where only single detached houses can be built, then zoning is the right metric to look at.\nThere are two zones that essentially only allow single family houses as residential properties, the RS zones and, to some extent, First Shaughnessy. Each of these has it’s own pitfalls. The first issue is that parks are generally zoned RS, but one cannot build single family homes in parks. Moreover, while RS does not allow residential land uses other than single detached houses, it does allow for other land uses, like parks, schools, assisted living and other institutional uses. So RS zoned land area is a poor proxy for the question of where single family houses could be built. At the very  least we should exclude parks from the equation, but probably also schools and other institutional land uses. After all, it is delusional to think that even if a school were to close down and the land be sold for development that single detached dwelling would be built on that land. Next comes the question if we should only include the lots that single family homes could be built on, or also include the surrounding roads. After all, the roads are needed to serve the lots. But they also serve the schools and parks, so maybe only a certain portion of the roads should be included? It gets messy, the second image shows only non-institutional and non-park properties (no roads) coloured by zone, interactive version here.\nThe numbers between 70% and 56% mentioned up top are (probably) derived using some version of zoned area, possibly excluding parks or schools for some, but including roads within RS zoning. It is easy to underestimate the space roads take up in the City of Vancouver, overall roads right of ways make up 28% of City of Vancouver land.\nAt the end of this though process, we are starting to gravitate away from zoning and toward land use."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#land-use",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#land-use",
    "title": "SDH Zoning and Land Use",
    "section": "Land Use",
    "text": "Land Use\nWe can simplify the discussion by asking how much space currently is taken up by single family properties. The answer to this is 33%. I have previously written about the land use breakdown for Metro Vancouver municipalities, but single detached homes were not broken out separately, single detached and duplex properties combined take up 34% of Vancouver’s land. One can play the numbers a little more, maybe only look at single detached properties within RS zoning. That puts the number at 28%. Throw in Shaugnessy and it rises to 29%. Recall that this excludes roads right of way.\n Here is an interactive (data-heavy) map that shows the land use for just for the City of Vancouver (plus Musqueam 2)."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#multi-unit-housing",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#multi-unit-housing",
    "title": "SDH Zoning and Land Use",
    "section": "Multi-Unit Housing",
    "text": "Multi-Unit Housing\nOne can ask the same question about other housing types, and we can read off the numbers from the land use breakdown. 3.2% of land is taken up with low rise apartments, 1.9% with townhouses, 1.2% with high rise, 0.3% by non-market housing, 0.9% by mixed use low rise and 0.7% by mixed use high rise. Adding all of this up we get that 8.2% of land is taken up by housing that is not single detached or duplex. And it also includes some commercial space in the mixed use developments.\nThat means that overall in Vancouver 42.2% of the land is used for residential purposes, 81% of which is occupied by single detached and duplex properties and the remaining 19% is occupied by all other building forms."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#share-of-households-in-different-land-uses",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#share-of-households-in-different-land-uses",
    "title": "SDH Zoning and Land Use",
    "section": "Share of Households in Different Land Uses",
    "text": "Share of Households in Different Land Uses\nWe may be interested in the population density these different land use patterns entail. That’s where things get a little tricky, we will turn to the census for answers. From the standard census releases we can only discern the share of dwellings that are single detached and duplex properties combined, and that number is 93,370 dwelling units. Overall there are 264,575 dwelling units, so single family and duplex units make up 35% of all dwelling units in CoV. Glossing over the issue that some of these may not be occupied we conclude that 35% of all households live on single family and duplex properties making up 81% of Vancouver’s residential land, while the remaining 65% of households live on 19% of the residential land."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#fine-print",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#fine-print",
    "title": "SDH Zoning and Land Use",
    "section": "Fine Print",
    "text": "Fine Print\nThere are a number of issues with the data that might change the numbers slightly, but not by much. The first is that the land use dataset I use lump the City of Vancouver together with Musqueam 2. That slightly changes the land use mix and areas reported, but the effect on all numbers mentioned here is very small. Next up is the number of dwelling units on single detached or duplex properties. The census number is likely to under report some secondary suites (although it captures more than BCAssessment does), and properties with more than one secondary suite (so some illegal suites) will show up as an apartment building in census data. Moreover, the census data is from 2011, the land use data we used is about a year old, and we have manually adjusted for some recent upzoning. So there is some give or take in the numbers, I would not expect more than a one percent move in the numbers I reported as a result."
  },
  {
    "objectID": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#further-questions",
    "href": "posts/2016-06-17-sdh-zoning-and-land-use/index.html#further-questions",
    "title": "SDH Zoning and Land Use",
    "section": "Further Questions",
    "text": "Further Questions\nThis does not answer the question what percentage of the population lives on single family or duplex properties. To estimate that one would have to first estimate the number of people per houshold for the different land uses, as we expect them to differ significantly. One can start exploring how the number of people per dwelling unit varies on CensusMapper."
  },
  {
    "objectID": "posts/2016-05-16-my-global-bike-map/index.html",
    "href": "posts/2016-05-16-my-global-bike-map/index.html",
    "title": "My Global Bike Map",
    "section": "",
    "text": "Last week Mapzen announced that they included bike data in their OSM vector tiles. That’s just what the doctor ordered to continue on my path towards the perfect bike map that I started on, explored routing and looked at improving OSM bike data. Now that I don’t need to extract bike data myself any more it is time to take these maps global. And maybe add some minor improvements.\n\nTo make the bike map boils down to deciding what OSM roads, paths, etc to map and how to map them. And what contextual information to display. The only choices we have to make is how to organize the bike related data that exists in open street maps. Let’s be a little radical! Our emphasis will be on safety and cycling comfort for the casual rider.\nWe will emphasise separated cycletracks by making them brightest and thickest. Shared pedestrian/bike infrastructure will get a separate colour and be a little bit thinner. Next we layer on designated bike infrastructure by prioritizing lanes on residential roads, then “shared” residential roads, then residential roads, then lanes and “shared” designations on increasingly busy roads. Lastly we add residential and larger roads. And we only label roads that we are actually interested in.\nWe allow some customization by emphasizing or de-emphasizing certain types of infrastructure. By default, roads without special bike designation are de-emphasized.\nFor good measure we added basic land use categories to highlight possible destinations, and we add building footprints for reference as available.\nThis scheme is geared toward the safety conscious and motor-traffic-adverse cyclist in North America. In Europe other designations exist that might call for a different priority. MAMILs might want a different colour scheme altogether.\n\n\nTo get a better view you can also take the map full-screen.\n\nData Quality\nThe data for the map comes right out of Open Streets Map. That means that if something is missing or not right you can just go and fix it! To make things a little easier when you are on a desktop computer, just “shift-click” where the problem is and it will bring the open street map editor up for that position. You might have to log in first or create a free account before you can help improve the bike map.\nThere is a bit of a lag between changes being made in open street map and them filtering down to the mapzen tiles so that they will show up on the map on the map, so don’t fret if things aren’t up to date immediately.\n\n\nGlobal\n And this bike map is global. The quality of the map depends on the quality of OSM data, in the case of Amsterdam it makes it abundantly clear why casual cyclists keep dreaming about the infrastructure in this city.\nThe choice of emphasis for the maps are based on North America, or more narrowly Vancouver, where I live. So this might need adjusting for local context to better fit local OSM conventions. Use the search bar and explore your region.\n\n\nThe Real Deal\n Sadly, all of this effort is really coming at things from the wrong angle. The best way to do this, as we outlined earlier, is to base a bike map on how cyclists actually use the roads and pathways, instead of how they got classified in Open Street Map or what municipalities think they should be classified. The problem is that we don’t have that data. At least not yet. But there are really cool efforts out there to get that data and make it available.\nI really like the approach that Ride Report has taken. Strava works reasonably well for recreational cyclists, but it does a terrible job at capturing where transportation cyclists go and makes no effort in understanding their comfort level. Ride Report takes a different approach by automagically figuring out what you are doing and recording your cycling trips. All without you ever having to press a “start” or “stop” button, which is key if you want to understand where transportation cyclists go. On top of that it prompts you to simply rate your bike trip once you are done, and you can do this easily from your lock screen. Then Ride Report aggregates the data to produce stress maps, and possibly other important data that can guide infrastructure improvements.\nIn this day and age of data driven decision making I would really like more municipalities to support efforts like Ride Report that generate real meaningful data on transportation cycling. And as a by-product one could take cycling maps to the next level and add actual cycling usage and comfort.\n\n\nIssues\nThere are still some issues. Some bike infrastructure, for example OSM paths, only become visible at zoom level 13 or higher. That’s just the way the vector tile service works. Secondary roads are only visible at zoom level 12 or higher, and this includes “bike boulevards”. Cyclepaths are only visible at zoom level 11 or higher. If one wants to have high-level bike maps one probably needs a dedicated vector tile service for bike infrastructure.\n\n\nApologies\nSince this is just a blog post I have not gone out of my way to make sure this map works on every computer and browser. Basically, I am leaveraging off of Mapzen’s awesome Tangram mapping engine to keep things simple on my end, but that means the maps will only show on modern computers (and modern browsers, so no IE). If your computer is having trouble displaying the map just pull out your phone, chances are it it is much better equipped to handle modern web technology.\nAlso, I made a bunch of choices how to display bike infrastructure. These are just a function of my personal preferences and what OSM data has to offer. If you notice issues with the OSM data, you can fix it in OSM and it will show on the map once the changes have filtered through to Mapzen’s tile service. If you don’t like my personal prefernces, just grab the html for the map and the scene file and change it at your heart’s content. You don’t need any special server to host these files, I dumped them on my static blog. Even a public dropbox folder can host these maps.\nSo grab the code and customize it at your heart’s content. Take full control on how to display bike infrastructure and roads, and what amenities to show and how to style them.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2016,\n  author = {von Bergmann, Jens},\n  title = {My {Global} {Bike} {Map}},\n  date = {2016-05-16},\n  url = {https://doodles.mountainmath.ca/posts/2016-05-16-my-global-bike-map},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2016. “My Global Bike Map.”\nMountanDoodles (blog). May 16, 2016. https://doodles.mountainmath.ca/posts/2016-05-16-my-global-bike-map."
  },
  {
    "objectID": "posts/2016-05-02-census-week/index.html",
    "href": "posts/2016-05-02-census-week/index.html",
    "title": "Census Week",
    "section": "",
    "text": "At CensusMapper we are super excited about Census Week 2016. Data is increasingly getting incorporated into local and regional decision making. At CensusMapper we have been working to facilitate this by making this wealth of information more accessible to everyone. To celebrate Census Week 2016 we have updated CensusMapper with 2006 census data for easy comparisons with the 2011 data. And we have given CensusMapper a face lift by adding a histogram widget that interacts with the mapping data. Watch for more exciting updates this week or read on for a sneak peak!"
  },
  {
    "objectID": "posts/2016-05-02-census-week/index.html#census",
    "href": "posts/2016-05-02-census-week/index.html#census",
    "title": "Census Week",
    "section": "2006 Census",
    "text": "2006 Census\n  We re-worked the backend to make it easy to add and map older censuses. And of course the 2016 census, once it becomes available. At the first step, this allows side-by-side comparisons between different census years through our map-based interface. As we explore this further we will add better ways to simultaneously browse two difference censuses, and we will add a way to map data from two censuses on one the same map. Apart from some technical challenges that we know how to handle, this also presents challenges when directly comparing the long form census to the 2011 NHS, which had different collection methods that introduced biases that can be of the same magnitude as the changes one may want to measure."
  },
  {
    "objectID": "posts/2016-05-02-census-week/index.html#widgets",
    "href": "posts/2016-05-02-census-week/index.html#widgets",
    "title": "Census Week",
    "section": "Widgets",
    "text": "Widgets\nAt CensusMapper we are all about interaction. We love exploring data visually, and our dynamic zoomable maps make browsing census data very easy. But the maps are just scratching the surface. Often it is also usefuly to view a simple graph. That’s where the graph widgets come in, the first of which is the histogram that dynamically shows a histogram of the data visible in the current map view. \nAnd of course the widget is fully linked with the map. Hover over a map region, and it will show where the mapped value fits into histogram. Pan and zoom and the histogram updates automatically. The other way around, hover over a histogram bin and the corresponding regions on the map get highlighted. That makes exploring the data so much easier."
  },
  {
    "objectID": "posts/2016-05-02-census-week/index.html#make-your-own-maps",
    "href": "posts/2016-05-02-census-week/index.html#make-your-own-maps",
    "title": "Census Week",
    "section": "Make your own maps",
    "text": "Make your own maps\nWe will roll out basic map making capabilities for everyone later this week. How will this work? Census data is huge, so it won’t go completely without pain. But we try to make it as easy as possible. Once the new “Make a Map” option becomes available you can browse through the list of census variables and simply select which one you want to map. You will have the option to map the value or, if it makes sense, the value as a percentage of a “parent” variable. \nA click of the mouse and you have a new map. Head over to the colour scheme picker and select the colour cutoffs to customize it. You can even log in and save the map and write your own map story for the side bar, so that you can share and interactive version of your map online. We can’t wait getting census data into everyone’s hands!"
  },
  {
    "objectID": "posts/2016-04-06-tod/index.html",
    "href": "posts/2016-04-06-tod/index.html",
    "title": "TOD",
    "section": "",
    "text": "Just saw this excellent post looking at density around transit stations today and though I should pull out some numbers to go with the pretty visuals. And with CensusMapper’s new capabilities of populating custom geometries with census data estimates it’s super-easy to do.\nPopulation density is only one measure of interest here, job density or amenities density would be others. But for now let’s focus on population, so how many people live near the rapid transit stations. To keep things simple we checked for a 400m and an 800m radius circle around the stations. At least roughly, if you need to know the details check the bottom.\nTo visualize this we simply map our concentric circles around the stations and use the population density as height. Click, touch or hover to get the exact values. And sorry again, only new computers will get a meaningful result. Most smart phones and tablets will have no problems. &lt;iframe src=“/html/skytrain_tod_map.html” width=“80%” height=“500”, style=“margin:5px 10%;”&gt; Full screen view\nWhat immediately stands out is that for some stations inner orange circle is lower than the outer yellow circle. That means the 400m radius area is less dense than the 800m radius area. It’s as if there was an inverse-TOD development. That’s where local knowlege becomes important to see how exectly things are like around the stations. Maybe the station primarily servers jobs or amenities, which we did not map. This definitely deserves more scrutiny.\nOf course we can also view this on the CensusMapper population density map comes in handy, if we zoom in it breaks it down to the census block level. And CensusMapper will also function on your old computer if that’s what you are using."
  },
  {
    "objectID": "posts/2016-04-06-tod/index.html#next-steps",
    "href": "posts/2016-04-06-tod/index.html#next-steps",
    "title": "TOD",
    "section": "Next steps",
    "text": "Next steps\nOne clear drawback of the analysis is that it uses 2011 numbers, which are quite old by now. So it’s probably worthwhile to re-visit this once 2016 population numbers become available. In the meantime I should run the same analysis on the 2006 data to see how density around transit stations changed. Some other day.\n###Details This section is only interesting for people who want to know how exactly the numbers were derived. Conveniently Dmitry had the station data handy, which I took and then threw out the duplicates data for each station platform.\nGetting exact number of people in a given circle radius around a station would require a getting custom tabulation for Stats Canada. This is clearly overkill for the application we are looking at, and if we really wanted to be more precise we should probably start by using an isochrone, that is an area that is at a 10 minute walking distance from the station, instead of using a circle.\nIn the future we will bake the ability to compute isochrones right into CensusMapper. Right now CensusMapper estimates population data for arbitrary regions, including circular ones, by checking which census blocks intersect each circle and add up the populations. Actually, we have three differnt ways to do this\n\nby just taking all dissemination blocks that intersect\nby just taking all dissemination blocks with the majority of its area in the intersection with the region\nby scaling all properties of the dissemination block according to the proportion of its area in the overlap with our region.\n\nEach has its own advantages and disadvantages, we felt that option 2 is best suited for our purposes.\nSo we end up with a slightly different areas from the circle, the area given by the census blocks we included, and we compute population densities based on that area. Since we visualize the circles instead of the union of the census blocks used the visualization is a little bit misleading, but there is some value in the simplicity of sticking with circles.\nSince we are only interested in population data we are done. In the more general case, when we are interested in estimating other census variables, for example the average rent in the area, CensusMapper would now go through the census geographies and match up appropriate geographies with the census blocks to derive the appropriate estimates. The actual algorithm gets quite complex here. Extra weights, like the proportion of households that rent (and not own), come into play, as well as data quality estimates. And yes, if we did opt to use area overlap weights in the block level computation, we would base them on households instead of population. If I am vague here it’s because the algorithm is quite complex and not really fit for a blog post."
  },
  {
    "objectID": "posts/2016-03-28-on-mixed-use/index.html",
    "href": "posts/2016-03-28-on-mixed-use/index.html",
    "title": "On Mixed Use",
    "section": "",
    "text": "Redeveloping single storey commercial properties into mixed use is taking off in Vancouver right now. It’s a little frustrating to see how pretty much every story I have seen on this get the effect this has on property taxes wrong, including one on the generally quite good Price Tags blog. People claim that converting single story commercial to mixed use pushes up the property taxes for the commercial tenants. Property taxes are an important piece of the puzzle in Vancouver, so I decided to go into a little detail on this.\nLong story short, re-developing single story commercial into mixed use lowers the commercial property taxes. Dramatically."
  },
  {
    "objectID": "posts/2016-03-28-on-mixed-use/index.html#commercial-property-taxes",
    "href": "posts/2016-03-28-on-mixed-use/index.html#commercial-property-taxes",
    "title": "On Mixed Use",
    "section": "Commercial Property Taxes",
    "text": "Commercial Property Taxes\nIn Vancouver, the commercial property tax rate is five times the residential rate. Toronto also has higher property tax rates for commercial, 3x in their case, I don’t know how this plays out more broadly. Nor do I know when and why these decisions were made. Generally cities have a hard time to keep space for residential areas in or close to the central business district, and higher property taxes may help to uphold the zoning restrictions in place and keep residential properties in close proximity to commercial.\nThe effect of the unequal tax rates is quite stark when visualizing taxes collected per area. And this keeps residential property taxes low.\nIf Vancouver were isolated from everyone else it would not make much of a difference. Stores pass the higher property taxes on to residents by charging higher prices, and offices pass them on to employees through lower wages. Things get more complex when Vancouver is seen as part of metro, or Canada or in the world. But that discussion goes into a different direction.\n On the interactive 3D tax density map we can clearly see that the some mixed use properties (in darker red) generate lower total property taxes than comparable pure commercial properties nearby. We will explore this in more detail in the next section.\nFor Vancouver it is also important to note that commercial leases are generally trip net, in particular the tenant pays the property taxes. And they are quite substantial. The important number is the gross rent, which is the sum of rent, taxes, utilities and other costs borne by tenants. That number is determined by the willingness of a tenant to lease the commercial space, which in turn determines the rent a commercial landlord can charge for the space. In theory at least, in practice some commercial landlords develop a relationship with their tenants and don’t increase rents for old tenants as much as the market would bear."
  },
  {
    "objectID": "posts/2016-03-28-on-mixed-use/index.html#taxes-for-commercial-vs-mixed-use",
    "href": "posts/2016-03-28-on-mixed-use/index.html#taxes-for-commercial-vs-mixed-use",
    "title": "On Mixed Use",
    "section": "Taxes for Commercial vs Mixed Use",
    "text": "Taxes for Commercial vs Mixed Use\n Let’s look at and example, the adjacent properties at 3071 and 3063 W Broadway. The first is single story commercial, the latter two story mixed use. They have almost identical lot sizes and assessed land and building values, with the mixed use property having slightly higher overall assessed value.\nHow about the property taxes? In 2015, the 2016 taxes aren’t out yet, the single story generated $47,797 in taxes, the mixed use only $32,031. The reason is simple. The single story property simply pays 5x the residential rate. For mixed use, the value gets divided up between two stories, so half the value pays 5x residential rate, the other half gets taxes at 1x residential rate. So the effective tax rate for the mixed use is 3x residential rate. Well, the actual effective rate is 3.35x residential, probably because the first floor carries a little higher value than the second floor.\nThe net effect is that the mixed use commercial tenant pays just over half of the property taxes as the pure commercial tenant next door. So the mixed use commercial landlord will charge exactly that much more rent, so that the gross rent for both commercial tenants is about the same.\nFeel free to play with the tax density and land use map and click into properties to see a more detailed break-down of taxes. Be warned that the land use dataset is not perfect in identifying every commercial vs mixed use place."
  },
  {
    "objectID": "posts/2016-03-28-on-mixed-use/index.html#a-re-developmet-example",
    "href": "posts/2016-03-28-on-mixed-use/index.html#a-re-developmet-example",
    "title": "On Mixed Use",
    "section": "A Re-Developmet Example",
    "text": "A Re-Developmet Example\n Let’s see how this works when a property gets re-developed. To keep things simple, let’s look at something that went from single story commercial to two story mixed use. 6621 Main Street got re-developed around 2009/2010 and now spans addresses 6615, 6621 and 6623.\nBehind the original 6621 are now three stratified residential units, the other two addresses are the front doors for the two commercial units that replace the two commercial units behind the original 6621 address.\n So what happened to property taxes from 2009 to 2010? In 2009 the combined tax levy was $22,102, in 2010 it was $16,859, of which $2,994 was payed by residential stratas and $13,865 by the two commercial.\nFor comparison, the commercial property next door saw their property tax climb lightly from $20,754 in 2009 to $20,794 in 2010. So the decrease in property taxes we see is really due to the re-development to mixed use."
  },
  {
    "objectID": "posts/2016-03-28-on-mixed-use/index.html#bottom-line",
    "href": "posts/2016-03-28-on-mixed-use/index.html#bottom-line",
    "title": "On Mixed Use",
    "section": "Bottom Line",
    "text": "Bottom Line\nSo what’s the bottom line for mixed use re-development? Overall it looks pretty good, let’s go down the list of all parties involved. ### City bottom line For conversion to two story mixed use, the city collects less property taxes than before and the overall mill rate goes up a tiny amount. Conversion to three stories or higher and total tax revenue for the property goes up and the overall mill rate goes down a tiny amount. ### Commercial tenant bottom line The property tax rate goes down substantially, but rent will go up as the gross rent is determined by the market. So not much changed assuming the form factor of the stores stay the same. In theory at least. If the commercial land lord was giving a long time tenant a break on their rent before re-development, the tenant will have a hard time to find a different commercial location with comparable gross rent. ### Residential tenant bottom line Added supply can only help. In some cases units will get stratified and sold off, in other cases the property owner keeps the residential units as rental. ### Property owner bottom line Especially with the recent change in lending practices that focus on cache flow, mixed use re-development becomes very attractive. After re-development, landlords can charge higher rents because commercial property taxes go down. And they can rent out the residential units for added cash flow. That way, when they take that property to the bank to get a loan for their next project they can show a large cache flow and get a bigger loan. And the re-development cycle keeps going. And with almost everyone re-financing their properties a couple of years ago at roughly the same time, when interest rates hit rock-bottom, everyone is now gearing up at the same time to re-develop."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html",
    "title": "Unoccupied Dwellings",
    "section": "",
    "text": "Today the City of Vancouver released their report on unoccupied dwelling units in the city. I watched part of the presentation and read through the report, and from all that I can see the methodology used is very solid.\nI have seen some confusion and even some incorrect reporting on this, so I thought it would be worthwhile to look into the report in detail."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html#the-data",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html#the-data",
    "title": "Unoccupied Dwellings",
    "section": "The Data",
    "text": "The Data\nSmart meter data can be used to determine very accurately which units are occupied and which ones are not. In theory at least, in practice this is quite difficult because of the sheer magnitude of the data. That’s why this is best left to companies who specialize in this kind of analysis, which is exactly what the city has done.\nThe only problem is that not all units have smart meters. I don’t know how far BCHydro has come with their rollout, but most condo units should have smart meters by now, as do newer single family units. So what to do about the housing that does not have smart meters? That’s where things get tricky. Some studies I have seen use cutoffs for monthly electricity consumption to determine which are unoccupied. The problem is to find the right cutoff. Typically these studies report rates computed using several cutoffs to address that uncertainty. The problem is that different cutoffs give very different results. For example, this study looked at 75kWh and 100kWh as monthly consumption cutoffs and reported 5.5% and 8.5% unoccupancy rate, respectively. I checked my condo unit when it was unoccupied for two months one summer, my two fridges pushed me well above the 100kWh threshold. So my unoccupied unit would have been counted as occupied in this study.\nSo how did the new study deal with this problem. We did not hear technical details, but during the presentation it was explained that smart meter data was used to “train” a model to use dumb meter data. That’s exactly the right approach. Going by the fact that everything was done right I am reasonably comfortable to assume that this part was also done in a way to give accurate aggregate numbers.\nI am hoping that the technical report will still come out so we can be confident that the details were taken care of properly. In particular I would like to know how the model dealt with different housing types, and how the housing type was tied to the electricity bill. What ratio of dwellings of each housing type had smart meter data available? And most importantly, what are the uncertainty estimates on the numbers presented?\nBottom line on data: Solid."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html#results",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html#results",
    "title": "Unoccupied Dwellings",
    "section": "Results",
    "text": "Results\nThe big takeaway is this one graph:\n It shows that no matter what timeframe is used to define “unoccupied”, the rate stayed flat over time. In context of the current affordability discussion this means that this is unlikely a driving factor of Vancouver’s skyrocketing real estate prices. But the absolute numbers are still quite large and there is a lot of untapped potential. While some people have argued that supply has not been the main part of the cause of the price hike in Vancouver, it is clear that it must be a big part of any solution. Finding effective ways to get this untapped supply online should be a priority, it is easier and more efficient than building new housing in NIMBY-zoned Vancouver.\nThe other takeaway is that unoccupied units are dominated by condos.\n The graph shows the rate for apartments, which also includes purpose built rentals. As the report points out, if we take out the purpose-built rentals, which have vacancy rates well below 1% at any given time and are extremely unlikely to stay vacant for two or more months, we arrive at a rate of about 12.5% of condos that are unoccupied in the City of Vancouver. As should be expected, that’s significantly higher than what the non-smart meter studies picked up."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html#are-rates-in-vancouver-higher-than-elsewhere",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html#are-rates-in-vancouver-higher-than-elsewhere",
    "title": "Unoccupied Dwellings",
    "section": "Are rates in Vancouver higher than elsewhere?",
    "text": "Are rates in Vancouver higher than elsewhere?\nThat depends exactly on what the question asks. And this is where, in my opinion, the city report fails to give a good answer.\n The City implied, and newspapers now report that Vancouver numbers are “in line” with other comparable cities. That claim is based on this graphic, which is included in the report and stems from a study by Urban Futures. The study shows that the Metro Vancouver’s rate of unoccupied apartments is not much different from other Metro regions.\nThe problem is that this study is read as giving the overall occupancy rates are similar across metro regions, which is not the case. This is a fine point and a little technical, let’s take a look at the actual rates of unoccupied dwellings in the larges metro areas.\n\n\n\nWe took these numbers from a CMHC report that looked into this question and used the same data and methodology as Urban Futures, except they based it on all dwelling units instead of just apartments. This difference is quite stunning, and it can be explained by the fact that e.g. Metro Vancouver’s housing units are to 41% apartments, vs 20% for Metro Calgary. And that various reports showed that the rate of unoccupied units is much higher in condos, so we can’t substitute comparisons for unoccupied apartments for comparisons of unoccupied dwellings overall.\nSo what do the numbers look like when comparing the City of Vancouver to other cities in the region and across Canada. In short, I do not know. Without going into too much details, the number of unoccupied dwelling units is derived by taking the number of units that are “not occupied by usual residents”, which is part of the standard release of the census data, and subtracting the “unusual residents”, that is foreign or temporary residents. So residents that have a permanent address abroad or elsewhere in Canada and on Census day occupied in a dwelling unit that is not their primary residents. Think students in a dorm room. Or someone in his vacation home. Or someone that works far from home and has a second unit close to work.\nAnd to get the number of foreign or temporary residents requires a custom tabulation, which costs some time and money. But overall not that much compared to the scope of the study the city just undertook, so it is not clear why they did not do this and settle this question properly."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html#why-do-i-see-so-many-different-unoccupancy-rates",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html#why-do-i-see-so-many-different-unoccupancy-rates",
    "title": "Unoccupied Dwellings",
    "section": "Why do I see so many different unoccupancy rates?",
    "text": "Why do I see so many different unoccupancy rates?\nAgain, that depends exactly on the question asked. The report offers a variety of different rates, using the definition of unoccupied for 2 months, 6 months or 12 months. Each one gives a different rate. Then there is also the definitions used by Stats Canada, the building being unoccupied on census day (and the questionnaire was not returned). Again resulting in a differnt number. Then we can throw in the “unusual residents”, getting yet another number. Confused? You should be! Difficult questions often don’t have an easy answer.\nThe most important thing is to be consistent when comparing these numbers. Has the problem gotten worse? Use numbers that were derived using the same methodology at different times. Is is worse here than elsewhere? Use numbers derived using the same methodology across geographies."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html#geographic-distribution",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html#geographic-distribution",
    "title": "Unoccupied Dwellings",
    "section": "Geographic distribution",
    "text": "Geographic distribution\n In absence of the data on the foreign and temporary residents, we can take a closer look at the publicly released census data on “dwellings not occupied by usual residents”. The percentage of dwellings occupied by foreign or temporary residents are quite low, in the 0.4% to 0.2% range for these metro areas we looked at above. They tend to cluster around universities, but may also accumulate in some other areas. Keeping that in mind we can head over to CensusMapper and zoom, pan around and use the search bar to explore the geographic distribution. The city data has shown that there was not much change in the number of unoccupied dwellings over time, no matter how the data was sliced. That gives us some confidence that the geographic distribution has also been somewhat stable over that time frame.\nAt CensusMapper we are working on adding previous censuses, so this will be a good test case to see how the 2006 and 2001 geographic distribution of unoccupied dwellings stacks up. For City of Vancouver the corresponding numbers are 7.5% and 5.1%, respectively. The 2006 number is nicely in line with Vancouver’s 2011 rate of 7.7%. It is not immediately clear how the significantly lower 2001 rate squares with the city report showing a flat trendline, but could well be due to shifting definitions from one census to another. The 2001 Census used slightly different definitions, it requires more work to compare these numbers. The devil is in the details."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html#next-steps",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html#next-steps",
    "title": "Unoccupied Dwellings",
    "section": "Next Steps",
    "text": "Next Steps\nThe city study settles an important questions. The rates of unoccupied homes have not gone up. Perceptions often don’t match facts. And there are lots of little things to poke at of course. What about people having a cleaning person stop by every other week? How about homes awaiting renovation or demolition permits? How about AirBnB?\nNow all these are valid questions. But before using this to discount the study do some estimates to see how large of an effect your porposed hole actually has. How does it compare to the number of units that were found unoccupied and how would it change that rate? In all these cases it is quite small. Then ask yourself how likely it is that the problem would be growing.\nThen you find that AirBnB probably needs more scrutiny. Best current estimates that I have seen peg entire units rented fulltime at AirBnB at 1200 in Vancouver. Counting those would bring the rates of units taken out of the rental market by being unoccupied for a year or rented out full time via AirBnB from 4.8% to 5.3%. Noticeable, but doesn’t make that much of a difference. Yet. That number is very likely to grow if left unchecked and should probably be taken seriously and regulated properly right now."
  },
  {
    "objectID": "posts/2016-03-08-unoccupied-dwellings/index.html#update",
    "href": "posts/2016-03-08-unoccupied-dwellings/index.html#update",
    "title": "Unoccupied Dwellings",
    "section": "Update",
    "text": "Update\nA more detailed report is now available to give more insight into the methodology. Nothing unexpected there, although the claim that the 12.5% rate of unoccupied condos is consistent with earlier claims of 5.5% to 8.5% is hard to follow.\nStill waiting for the technical report to get an idea of the error estimates (by housing type). Seeing the graphs for the 2-month and 4-month cutoffs by housing type would be nice too, as well as the respective ones using the 15 day criterion."
  },
  {
    "objectID": "posts/2016-03-02-property-taxes-and-land-use/index.html",
    "href": "posts/2016-03-02-property-taxes-and-land-use/index.html",
    "title": "Property Taxes and Land Use",
    "section": "",
    "text": "Since I started thinking about tax density, the amount of property taxes collected per area, I always felt that the data presentation in the map fell short.\nProperty taxes are somewhat insulated from the ups and downs of the real estate market as they are need-based and the mill rate changes to flatten out the crazyness of the market. But what they lack in interesting patterns over time they more than makes up for in interesting patterns in space.\nTo recognize these spacial patterns, one needs to switch layers in my assessment maps and that’s a little awkward and most people won’t do this.\nI can’t remember where I first saw some 3D data visualization around something like this, maybe in this excellent Strong Towns post that explores tax density and parking. When I was chatting with Darren Proulx about this I realized this is the perfect case to rev up Mapzen’s awesone 3D map engine for data visualization. Previously I had tried out the 3D mapping capabilities to map the physical form of Vancouver’s buildings obtained from LIDAR data, now it was time to map abstract data in 3D."
  },
  {
    "objectID": "posts/2016-03-02-property-taxes-and-land-use/index.html#d-tax-density-maps",
    "href": "posts/2016-03-02-property-taxes-and-land-use/index.html#d-tax-density-maps",
    "title": "Property Taxes and Land Use",
    "section": "3D Tax Density Maps",
    "text": "3D Tax Density Maps\n The spacial patterns of tax density are closely related to zoning, so the obvious thing to do is to map both at the same time. Tax density as the height and the zoning as the colour. But the relation between tax density and zoning is indirect, it is given through land use. So when I folded the Metro Vancouver land use dataset into the asessment data I added another option to colour the map by land use. And the visualization works pretty well to show the relationship between the variables. Check out the interactive map."
  },
  {
    "objectID": "posts/2016-03-02-property-taxes-and-land-use/index.html#some-quick-observations",
    "href": "posts/2016-03-02-property-taxes-and-land-use/index.html#some-quick-observations",
    "title": "Property Taxes and Land Use",
    "section": "Some Quick Observations",
    "text": "Some Quick Observations\nWhen comparing this to the Strong Towns post mentioned above it is interesting to note that the “tax contribution of bare pavement” is actually quite high relative to many other commercial properties. For example, the giant parking lot around Safeway on 10th near UBC can’t be discerned from the map. Similarly, Oakridge Mall does not stand out as a low tax area. That’s basically because in today’s property market the value of the property is dictated by the land value and few properties, even few commercial properties outside of downtown, have building values in the same order of magnitude as the land value.\nAnd that keeps the pressure on to redevelop the low building value commercial stock, typically replacing it with mixed use. One example is the Safeway on Granville and 70th, that went from parking lot to mixed use. The high land values make this kind of redevelopment attractive. Typically this redevelopment process will raise commercial rents, lower commercial property taxes, add housing supply and increase tax density.\n Another interesting area is the condo/apartment triangle at Arbutus and 33rd. These condos/apartments abut Quilchena Park, have a grocery store in proximity but otherwise sit in a see of single family housing. I would like to highlight two alternative development models. One is the single family housing that surrounds this area, the other is one where one takes the condo/apartment complex and throws in the adjacent Quilchena Park. The park does not house any people and does not generate tax revenue but provides great recreational value. Overall, the tax density, that is the amount of taxes collected per area, of those two models are the same. So the condo complex together with the entire park collects the same amount of taxes per area as single family housing. On the other hand, the condo complex plus park houses 3 to 4 times as many people per (combined) area as the surrounding single family housing.\nPresenting these as alternative development models one can ask how many people would prefer condo (and one dedicated rental building) plus park over single family housing without park. Condo values range between $448k to $2.6m with median of $745k. 56% of all units have 2 bedrooms, 9% with 3 bedrooms. The surrounding single family housing start at around $2.5m for a teardown. My guess is that there are more people interested in the condo plus park model than the current Vancouver market offers."
  },
  {
    "objectID": "posts/2016-03-02-property-taxes-and-land-use/index.html#more-detailed-analysis",
    "href": "posts/2016-03-02-property-taxes-and-land-use/index.html#more-detailed-analysis",
    "title": "Property Taxes and Land Use",
    "section": "More Detailed Analysis",
    "text": "More Detailed Analysis\nNow let’s take the tax density information and measure it against city services rendered. In other words, let’s see who pays their “fair share” of property taxes. Well, that’s where I ran out of juice. There is lots of work involved to do it right, too much to fit into my evening side project time budget.\nSome have done some really amazing work trying to understand the give and take of property taxes and services rendered, to the bar is pretty high.\nVancouver’s situation, with the commercial property tax 5x the residential rate, has it’s own quirks to add to the complexity.\nIf someone is interested in spending the time to sieve through the city budget and process the data I would be happy to help out with graphing and mapping. As long as the data is clean enough that it stays under my “one evening side project time budget”. :-)"
  },
  {
    "objectID": "posts/2016-03-02-property-taxes-and-land-use/index.html#looking-forward",
    "href": "posts/2016-03-02-property-taxes-and-land-use/index.html#looking-forward",
    "title": "Property Taxes and Land Use",
    "section": "Looking Forward",
    "text": "Looking Forward\nGeneral public awareness about taxes, zoning and fairness seem to be rising. Recently a class action alleging discrimination in New York’s property tax code was launched. An interesting precedent, and it will be interesting to watch the outcome. And to see how this could apply to the tax and services balance of low density vs high density. Or exclusionary zoning policies as the motor of unequal tax density. Or minimum parking requirements which disproportionally hurt poor people.\nAs this kind of data becomes more publicly accessible and better visualizations make this complex issue more accessible I expect these kind of questions to gain more traction.\n\n\nI can’t view the 3D map!\nSorry. Welcome to the world of modern web technology. (WebGL in this case.) If you try to look at the interactive map and don’t see anything like the image above, then that’s probably because you either have and old (windows?) computer and/or an old (Internet Explorer) browser. If switching to Chrome or Firefox does not fix this for you your best option is to cramp that map into your phone screen. Your phone is probably newer and supports modern web technology much better than your computer. Or you can always view the 2D maps linked above."
  },
  {
    "objectID": "posts/2016-02-10-on-condos/index.html",
    "href": "posts/2016-02-10-on-condos/index.html",
    "title": "On Condos",
    "section": "",
    "text": "Lots has been said about the upper end of owned dwellings. The movement of the “million dollar line”, the emergence of the “two million dollar line” and “multi-million dollar lines”. Most of that discussion is focused on single detached homes or on proxies for “single detached” like RS zoned properties.\nBut all of these maps have a clear bias toward the more expensive homes. Everyone knows by now where the most expensive properties are. But where are the more affordable ones?\nBefore continuing and getting disappointed at the end, we want to highlight the huge limitations when looking for affordable homes. There is no freely available data on the floor area or number of bedrooms for each dwelling unit. Next to the price and location (which we have), these are the most important features of a dwelling. And this limits the usefulness of all that follows.\nWhy continue? It gives a glimpse of what kind of analysis could be done if the information would be freely available."
  },
  {
    "objectID": "posts/2016-02-10-on-condos/index.html#the-elephant-in-the-room",
    "href": "posts/2016-02-10-on-condos/index.html#the-elephant-in-the-room",
    "title": "On Condos",
    "section": "The Elephant in the Room",
    "text": "The Elephant in the Room\nIn principle all this information is available, BCAssessment has all this data. And they even make it available on their (quite nice) eValue website. But their terms of use prevent us and others from using this information. Most of the information used in the analysis here originates from BCAssessment, but it comes via the City of Vancouver that has made the data available through their open data portal.\nAs I understand it, the main reason why this data is not freely available is that BCAssessment is charged to recover their own operating cost. So they hold onto their data and try to sell it for cost recovery. In the process of which they harm the ability of municipalities to plan properly and the public to get a clear idea where things are at and have a fact-based discussion on how to move forward. Which creates large amounts of friction and may lead to social and economic damages far exceeding any revenue collected by BCAssessment.\nPersonally, I don’t believe that holding back data is a smart way to run a government. You might want to ask BCAssessment to #giveUsData.\nFor now, let’s put the data that we do have to the best use and see what we can tease out."
  },
  {
    "objectID": "posts/2016-02-10-on-condos/index.html#locating-the-most-affordable-dwellings",
    "href": "posts/2016-02-10-on-condos/index.html#locating-the-most-affordable-dwellings",
    "title": "On Condos",
    "section": "Locating the Most Affordable Dwellings",
    "text": "Locating the Most Affordable Dwellings\n Stating the obvious, the most affordable homes aren’t “single detached”, they are condos. And condos have been largely absent from the affordability debate, although they make up the majority of owned dwelling units in the City of Vancouver.\nLet’s start off with a map of the roughly 4,500 stratified residential or mixed use properties in Vancouver housing a total of about 10,2000 strata units. The exact numbers are hard to pin down. (#giveUsData)\nThe distribution of stratified units by the total number of strata units per building gives an idea of the types of strata units that are out there.\n\n\n\n\n\n\n Number of strata units by size of strata \n\n\n\nThe horizontal axis is not linear, and the bin sizes are not equal, which makes the graph a little difficult to read. We chose bins at 2, 4, 8, 10, 16, 20 at the low end, then of width 10 up to 200, from there width 20 up to 300 and then 400, 500 and 700 at the top end. Lazy me apologizes for the poor graphic.\nWe immediately notice the 4,152 strata units in stratas with exactly 2 units usually referred to as “duplexes”. There are a lot of strata units are in stratas of size between 20 and 50, but otherwise the units are fairly well distributed over different building sizes. At the high end there are fewer buildings, but each with lots of units. So there our graph becomes a little jerky and heavily depends on the cutoffs we choose.\nTo get a basic idea on how much these units cost we plot the number of strata units by price bracket.\n\n\n\n\n\n\n Number of strata units by price \n\n\n\nThe distribution looks largely as one would expect. It peaks between the $400k and $500k mark, with a median value of $482,000. But there are a couple of things that jump out. Firstly, there are suspiciously many condos below $100k, more than half of which (445 to be precise), are less than $50k. There are places where one could by a condo for that price, but not in Vancouver. These are stratified parking spaces or other amenity spaces.\nNext let’s focus in on the 8,313 duplex and multi-plex units with 8 or fewer units (in 2,995 buildings) and plot these separately.\n\n\n\n\n\n\n Number of multiplex units by price \n\n\n\nWe see that the price distribution for multiplex units peaks at a higher price between $800k and $900k, but the overall numbers are quite small when compared to all strata units."
  },
  {
    "objectID": "posts/2016-02-10-on-condos/index.html#affordable-strata-units",
    "href": "posts/2016-02-10-on-condos/index.html#affordable-strata-units",
    "title": "On Condos",
    "section": "Affordable Strata Units",
    "text": "Affordable Strata Units\nLet’s try to understand where the “affordable” housing stock is, which we take to be units below $500k, or roughly the bottom half of the distribution. We would like to map the properties containing housing units for each of our price brackets, but this gets tricky since the dataset does not hold information which units are parking spaces and which are commercial.\nWe need to take care of the problem with needing to distinguish parking spaces from housing units. And from commercial units. (#giveUsData) So we are left with using proxies, so let’s set a cutoff price so that most units below are parking stalls and most units above are (residential) units.\nGenerally, parking spaces should not cost more than $50,000 which is roughly the cost to build underground parking. Around $40,000 if it’s only one level, getting up to $75,000 (or sometimes even more) when having to go down deeper or water becomes an issue. The fact that some spaces go for significantly less is the result of mandatory parking minimums. Some parking spaces may be worth more than $100,000 as some people are willing to pay a premium for a convenient spot. That may mean a spot next to the elevator by the exit ramp, or simply a parking spot in a specific building that is under supplied and trekking across the street to a spot somewhere else is inconvenient enough. Or some stratified parking spaces might consist of several parking spots."
  },
  {
    "objectID": "posts/2016-02-10-on-condos/index.html#location-of-affordable-condos",
    "href": "posts/2016-02-10-on-condos/index.html#location-of-affordable-condos",
    "title": "On Condos",
    "section": "Location of Affordable Condos",
    "text": "Location of Affordable Condos\n Since it is impossible to pick out the housing units out of all the strata units, all we can do is map all strata units, understanding that those below $50k are most likely parking spaces, those between $50k and $100k could be parking or housing (or commercial) and the majority of units above $100k are housing.\nAnd we can’t actually map the individual units, only the buildings that house the units. Here are maps of the buildings housing the units in each of the lower brackets.\n\n&lt; $100k (715 units in 17 buildings)\n$100k – $200k (2,415 units in 198 buildings)\n$200k – $300k (10,270 units in 644 buildings)\n$300k – $400k (19,128 units in 1,112 buildings)\n$400k – $500k (20,538 units in 1,219 buildings)\n\nClickin into a particular building and hitting the “more” button will pull up (a slightly cleaned) tax roster where you can get more information on the units. And if you are really interested in finding out more about a particular one you can always head on over to BCAssessment’s eValue website to look up more of those details that BCAssessment keeps in public view but locked off from systematic public scrutiny. (#giveUsData)"
  },
  {
    "objectID": "posts/2016-02-10-on-condos/index.html#makeup-of-condos",
    "href": "posts/2016-02-10-on-condos/index.html#makeup-of-condos",
    "title": "On Condos",
    "section": "Makeup of Condos",
    "text": "Makeup of Condos\nThe built quality of strata units is generally higher, dollar for dollar, than that of single family homes. This should not come as a surprise as land is very expensive and stata units tend to use land more efficiently. There are only 27 residential or mixed use strata buildings that classify as teardowns, with a teardown coefficient below 5%. And 25 of these are duplexes, one is a 3-plex and one an 8-plex. There are no condos buildings with more than 8 units that fit our most restrictive definition of teardown.\nNext we explore what kind of buildings the affordable units are in by graphing the number of units in several price ranges per size of the strata building it is in.\n\n\n\n\n\n\n\n\nIt looks like buildings with 20 to 60 units are quite good at producing affordable units. Partially that’s due to their abundance, but graphing the percentage of units in each price bracket confirms that these buildings tend to produce a nice mixture of low and high value condos. But larger condo buildings can also achieve this, although the actual performance of what has been built is mixed. Other factors, for example building age, are likely also at play here.\n\n\n\n\n\n\n\n\nTo see what the distribution of strata units by age is, we use the same price cutoffs and sort the strata units into age brackets. Since we are mostly interested in the more affordable units we sort out units in buildings with 4 or fewer units. These units might skew some of the age brackets.\n\n\n\n\n\n\n\n\nTo highlight the proportional makeup we again and graph this again as percentages.\n\n\n\n\n\n\n\n\nWe see how generally the percentage of more affordable units increases with the age of the building. Newer units generally tend to be more expensive at first and becoming more affordable over time as the building ages."
  },
  {
    "objectID": "posts/2016-02-10-on-condos/index.html#taking-stock",
    "href": "posts/2016-02-10-on-condos/index.html#taking-stock",
    "title": "On Condos",
    "section": "Taking Stock",
    "text": "Taking Stock\nIf you read this far you probably agree that this exercise was mostly a waste of time. Hopefully you are earger to see this analysis split up by number of bedrooms and floor area. If you are all riled up at the lack of data and BCAssessment not giving out this information with a clean open data license you might want to drop them a line and ask for at least the information they publish eValue to be made available with an #openData license!\nI am still hopeful that this might happen some day, and we can get a much better picture of the buildings in BC cities and check how they perform in fulfilling the needs of the community. And learn from that to make informed policy choices."
  },
  {
    "objectID": "posts/2016-01-31-land-use/index.html",
    "href": "posts/2016-01-31-land-use/index.html",
    "title": "Land Use Data",
    "section": "",
    "text": "Metro Vancouver has a fairly good land use dataset that I imported quite some time ago so that I could take a good look at it. But then I forgot about it until @HealthyCityMaps reminded me just at the time when the discussions about assessmened values in Vancouver were flaring up again.\nSo finally I decided to mark up the City of Vancouver assessment dataset with the land use from the Metro Vancouver land use dataset. And I used the occasion to update my color schemes from two years ago and gave it a dark background that works better with the colorbrewer palettes.\nThe result of the land use and assessment data mashup is a much better understanding what kind of properties we have right now in Vancouver.  ## Land Use for Assessment Data There is a strong relationship between land use and zoning. But that relationship is far from perfect, zoning is a funny beast. For example, zoning laws prevent low-rise apartments to be built in areas zoned for single family housing. But they permit single family housing in areas zoned for low-rise apartments. Or in areas zoned as commercial. The land use dataset allows to tag every property based on what it is actually used for.\nOf course the land use dataset is not perfect and has some issues, but overall it gives a far superior view on how land is used when compared to soley relying on zoning. But putting the two datasets together one can even further filter down to tease out actual land use."
  },
  {
    "objectID": "posts/2016-01-31-land-use/index.html#the-forgotten-single-family-houses",
    "href": "posts/2016-01-31-land-use/index.html#the-forgotten-single-family-houses",
    "title": "Land Use Data",
    "section": "The Forgotten Single Family Houses",
    "text": "The Forgotten Single Family Houses\nSingle family houses have received a lot of attention lately, not least because of the incredible gains in land value during the last year.\nBut how to narrow down the assessed properties to only single family housing? The easiest way to do this is to use zoning as a proxy, so only use RS zoned properties. But there are a number of problems with that.\n Firstly, only filtering by zone will still include lots of unwanted properties like schools and parks. I used to filter out parks using the City of Vancouver parks dataset, but it actually does not capture all parks. And then I filtered by area to get rid of large properties that typically house schools or other institutions. But that keeps smaller institutions in and throws larger single family houses out. Using the land use data we can better focus in on all the residential properties in the single family housing zone. But even then, there are 1,255 residential properties in ‘RS’ zoned areas that are not single family houses.\n Secondly, and probably more importantly, it misses the 15% (12,111 total) of the single family stock that is situated in other zones. ‘Upzoning’ a neighbourhood only means that other forms of housing are also permitted, but people can still build single family homes there if they would like. Zoom into the map and choose different ways to colour the properties if you want to learn more about where they are."
  },
  {
    "objectID": "posts/2016-01-31-land-use/index.html#all-single-family-houses",
    "href": "posts/2016-01-31-land-use/index.html#all-single-family-houses",
    "title": "Land Use Data",
    "section": "All Single Family Houses",
    "text": "All Single Family Houses\n When adding in the land use data we can filter by actual land use. That immediately takes care of parks and schools and other institutions, and at the same time it allows to find all “single family or duplex” properties. That gets us closer to what we are looking for, but it also includes duplexes. But those are easy to detect in the assessment dataset, they show up with two separate tax bills. So combining the two we get the actual map of all single family homes.\nAre these really all single family homes? Not quite. There are some issues with the land use dataset that mis-classify a couple of properties. In particular some RS lots without a house are classified as ‘Undeveloped’, and some are classfied as ‘Single Detached & Duplex’, probably depending on what was there around the time the data was collected. The correct classification really depends on what one is interested in, and will likely be outdated fast.\nMoreover, the property assessment dataset of the City of Vancouver is to correctly link a couple of hundred properties to their tax data. This won’t be an issue when analysing the tax data, but it does cause a problem when mapping the properties as they won’t have any assessment or zoning data associated with them.\nAnd there are issues on the fringes. Some properties in Southlands are used as single family homes on large lots, rather than the limited agricultural that it is zoned at and labeled as in the land use dataset.\nBut overall, adding in the land use information gives a huge improvement on narrowing down single family homes. We should remember that “single family homes” are better described as “single owner properties that can’t be stratified”, as they may include secondary suites and laneway houses."
  },
  {
    "objectID": "posts/2016-01-31-land-use/index.html#historical-data",
    "href": "posts/2016-01-31-land-use/index.html#historical-data",
    "title": "Land Use Data",
    "section": "Historical Data",
    "text": "Historical Data\nOne shortcoming of the land use dataset is that there are no historic versions available. It would be very nice to be able to see how land use changed over time, but that dataset won’t do the trick. The City of Vancouver assessment dataset comes with historic data all the way up to 2006, and can be used to do some fun analysis. But the historic dataset is incomlete in that zoning information is only available from 2014 forward and in that historic property tax data can’t always be linked to physical properties if the property has been re-developed. But overall putting the datasets together allows to refine the analysis previously made. For example re-running the thumb twiddling rates using only Single Family Homes, we get pretty much the same medians, but the averages are higher. We could also refine our teardown analysis to include only single family homes and better filter out institutional properties, although some issues with the lack of historic data remain."
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html",
    "href": "posts/2016-01-18-redevelopment/index.html",
    "title": "On Teardowns",
    "section": "",
    "text": "On the heels of the new assessment data we can start to slice the data in different ways to understand various aspects of the real estate landscape in Vancouver. The fact that Vancouver Open Data makes historic data available gives the ability to look for changes over time.\nOur maps explore this by visualizing some aspects of these changes for all properties, but it might also be useful to filter the properties we show to focus in on specific criteria.\n“Teardowns” always triggers lots of emotions in Vancouver. Without looking at the emotional side and trying to avoid any judgement we will investigate the data to understand what buildings have been torn down recently and predict which buildings will get torn down next. And map them. Long story short, we predict that 1 in 6 buildings on this map (and then some more with lower teardown probability) will get torn down and rebuilt by 2026."
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html#building-age-temporal-distribution",
    "href": "posts/2016-01-18-redevelopment/index.html#building-age-temporal-distribution",
    "title": "On Teardowns",
    "section": "Building age temporal distribution",
    "text": "Building age temporal distribution\n\nTo start understanding teardowns and rebuilds let’s look at the age of the building stock.\nTo get a better overview of the building stock through time we can graph the number of buildings by age. We look at buildings, not units. So a stratified building with 100 units would still count as one building in our graph. And it is not looking at how many buildings were built in each year, but how many buildings that were built in a given year are still standing today.\nWe still have 7 buildings in Vancouver that were built before 1900 (the earliest from 1800). Skipping these we graph the rest to get:\n\n\n\n\n\n\n Number of Buildings by Building Age \n\n\n\nStarting with 1950 the distribution of buildings by age is quite uniform, with a short peak around the early 1990s.\nThe dip at the end is due to some lag in new buildings showing up in the property dataset. Looking at the more recent history it is safe to assume that the number of buildings still standing corresponds well to the buildings of units built in that year. So the pace of new buildings right now seems to fit in quite well with the recent history and is a little lower than the peak in the early 1990s."
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html#recent-building-stock-and-recent-teardowns-spatial-distribution",
    "href": "posts/2016-01-18-redevelopment/index.html#recent-building-stock-and-recent-teardowns-spatial-distribution",
    "title": "On Teardowns",
    "section": "Recent Building stock (and recent teardowns) spatial distribution",
    "text": "Recent Building stock (and recent teardowns) spatial distribution\n The next question is to focus on the spatial distribution of recent redevelopment by filtering out older buildings. Being too lazy to add a bush for dynamic selection of time ranges I just made a static (in time) view only showing the 6883 properties built after 2006. It is quite safe to assume that most of those new buildings replaced older ones that were torn down. So this map of new buildings is also a map of locations of buildings that were torn down in the last 10 years.\nWhat’s interesting is when selecting relative building value view that there are some properties that have been recently re-developed with increadibly low building value, like the property at 5649 Dunbar St. This gives a window into some of the imperfections of the BC Assessment process where the building value after re-development is not properly reflected in their dataset. In this case it seems to be a property whose only “improvement” seems to be the pavement on it.\nIt also shows that recent building (or teardown) activity is fairly uniform across the city, with only some areas standing out as having little development like the West End, parts of Kitsilano and Strathcona."
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html#what-gets-torn-down-and-rebuilt-next",
    "href": "posts/2016-01-18-redevelopment/index.html#what-gets-torn-down-and-rebuilt-next",
    "title": "On Teardowns",
    "section": "What gets torn down and rebuilt next?",
    "text": "What gets torn down and rebuilt next?\nThe big question is of course where new buildings get built next. In a built up space like Vancouver there are few sites left where building a new building does not mean tearing down an old one. So another way to ask that question is: What gets torn down next?"
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html#teardown-probability",
    "href": "posts/2016-01-18-redevelopment/index.html#teardown-probability",
    "title": "On Teardowns",
    "section": "Teardown Probability",
    "text": "Teardown Probability\nPredicting which building will get torn down next is of course impossible. So what we try to do is assign a “teardown probability” to each building.\nLet’s first try to understand why a particular building might get torn down as opposed to the one next door. Typically buildings get torn down at the time when they change ownership. So if a building is not sold, it is far less likely to get torn down. So what makes a building more likely to get torn down when it is sold? One hypothesis would be that the value of the building relative to the land should play an important factor. Let’s test this hypothesis using the data.\nWe take the 2006 tax dataset as a baseline and check how many of the buildings have been torn down by 2016. Refer to the Methodology and Data section at the botton for the messy details. We only count buildings, so we count a strata lot with 100 units in the same building as one building. Then we use the 2016 dataset to check how many of them are still around, identifying them by their tax coordinate and again asking they be marked as being built no later than 2006.\nThese criteria capture well what we are looking for, but they are not perfect. As a predictive variable we use the\n\n\nTeardown Coefficient\n\nThe teardown coefficient is the percentage of the total assessed value that is attributed to the building. More formally it’s the ratio of the building value by the sum of the building and land values.\n\nSo we sort the properties by their teardown coefficient using the 2006 tax assessment data and we check how each group fares.\nFirst up a graph of the distribution of buildigs in 2006 by their teardown coefficient.\n\n\n\n\n\n\n Number of Buildings by teardown coefficient \n\n\n\nNext up the number of buildings in each category that got torn down and rebuilt by 2016:\n\n\n\n\n\n\n Number of Torn Down Buildings by teardown coefficient \n\n\n\nWe see that our initial hypothesis seems to hold up quite well. The number of buildings that got torn down and rebuilt decreases as the teardown coefficient increases. Remember that we defined the teardown coefficient to be the percentage of the building value out of the total value of the property.\nRefer to the methodology and data section for further information on how these numbers were extracted.\nTo explore this further let’s graph the frequency with which a building in a given teardown coefficeint range gets torn down. To keep things cleaner where we only plot up to a teardown coefficient of 50%:\n\n\n\n\n\n\n Probability of Building being torn down\n\n\n\nWe see that the teardown coefficient has high predictive value for a building to be torn down and being rebuilt in the following 10 years. Buildings with a teardown coefficient below 5% have about an 18% chance, and the probability declines exponentially down to zero at a teardown coefficient of about 50%.\nIf we were more serious about this we would fit and exponential curve to the data and compute how well it fits the data, repeat the computation for other time frames, run it on individual neighbourhoods and maybe also on data from other municipalities to properly validate our model. We could also refine the model by refining our filters, see the methodology and data section for more details.\nAnd we could add other factors that likely effect the teardown probability, like building age, proximity to arterials and others. Of course these are not independent factors, so this kind of analysis requires more time."
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html#predicting-teardowns",
    "href": "posts/2016-01-18-redevelopment/index.html#predicting-teardowns",
    "title": "On Teardowns",
    "section": "Predicting Teardowns",
    "text": "Predicting Teardowns\nNow to the main part: Predicting teardowns. How many buildings will get torn down and rebuilt in the next 10 years? Let’s use what we have just learned to extrapolate.\nFirst up the graph of the 2016 building stock by teardown coefficient:\n\n\n\n\n\n\n 2016 Building stock by teardown coefficient \n\n\n\nTo estimate how many buildings will get torn down and rebuilt in each category we simply multiply each bin with the teardown probability from the frequency graph above:\n\n\n\n\n\n\n Estimate of Buildings rebuilt by 2026 \n\n\n\nBottom line, we predict around 8,000 buildings to be torn down and rebuilt by 2026. That’s significantly more than the around 5,900 buildings that we identified as going through this process during the prior 10 years."
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html#open-question",
    "href": "posts/2016-01-18-redevelopment/index.html#open-question",
    "title": "On Teardowns",
    "section": "Open Question",
    "text": "Open Question\nThere are lots of assumptions that went into this estimate. While we are confident in our analysis that properties with low teardown coefficient are the ones most likely to be torn down, it is less clear if the number of properties being torn down grows linearly as the properties with low teardown coefficient grow. In our case the number of properties with teardown coefficient below 5% grew from 20492 (21% of the 2006 stock) to 32509 (33.5% of the 2016) stock, which may be out of the range where our simplistic extrapolation holds. One could try to understand this by carefully analyzing all available tax years, and not just the two extremes of the available spectrum.\n###Mapping Teardowns Now that we understand how to assign a teardown probability to buildings, let’s map them! To keep things as simple as possible let’s focus in on the homes with a teardown coefficient below 5%. They make up the bulk in our prediction and have the simple interpretation that a little more than 1 in 6 of these will get replaced by something else by 2026. So here is the interactive map of just these 31301 buildings, where we have filtered out some parks, marinas and rail lines. And this only accounts for the 5,700 buildings predicted to be torn down with a teardown coefficient below 5% cutoff and neglects the roughly 2,000 more that are predicted to be torn down that have a teardown coefficient above 5%."
  },
  {
    "objectID": "posts/2016-01-18-redevelopment/index.html#methodology-and-data",
    "href": "posts/2016-01-18-redevelopment/index.html#methodology-and-data",
    "title": "On Teardowns",
    "section": "Methodology and Data",
    "text": "Methodology and Data\nOnly for people who love getting their hands dirty or who want to reproduce or expand on the analysis.\nFirst thing to note is that there is no way to detect “teardowns” in the dataset, the only way is to look at what has been rebuilt and what has ‘dropped off’. To be more precise, there data fields to look at is the “land coordinate”, which links a taxable property to a physical structure, and the “year built”. And both fields have problems.\nThe “land coordinate” gets de-commissioned and re-assigned during certain re-develpments. And the city dataset provides no way to link the old one to the new one. One way to do that is through the polygons that mark the property boundaries, that would allow tracking of complex re-assemblies of land. But the city does not publish historic records of property polygons.\nThe “year built” also has lots of issues. Sometimes it is blank even though it records the value of the building as greater than zero. Sometimes the “year built” will be set to a date later than the date of the dataset, for example the 2006 tax dataset has buildings with “year built” all they way up to 2013.\nThen comes the issue of filtering. We decided to filter out parks, rail lines and marinas without structures on them. The algorithm is somewhat simplistic, it’s the same one that was used to filter properties for the maps. Additionally we filter out properties from the heritage dataset. There is definitely room for improvement here, but without a clear question of what exactly to measure (only single family homes, or also condos or apartments, treat commercial separately, …) it does not make much sense to invest energy into this. After all, this is just looking for a rough model.\nSo how do we detect rebuilds? We take the land coordinates from properties identified as park or heritage and sieve through the 2006 tax data to retrieve all records that don’t match these land coordinates and have a “year built” column set as 2006 or earlier or don’t have a “year built” set at all but change from zero to non-zero building value from 2006 to 2016.\nPretty messy. We mapped about 6,900 buildings were built after 2006, but only traced 5,869 buildings in the 2006 tax dataset as being torn down and rebuilt. That difference is largely explained by different selection criteria. The map only considers properties with a “year built” field set, but for the analysis we also added properties that don’t have that field set but go from zero building value in 2006 to non-zero building value in 2016 which gets us to 7,784 “rebuilds”. On the other hand in the analysis we don’t consder the roughly 140 heritage buildings that would pass our filter of being built after 2006, and the 2016 tax dataset has 2,422 more buildings than the 2006 dataset, some of which can be seen on this map and are due to subdivisions being split off of the original property.\nAnyway, if you want to get your hand dirty on this shoot me a message and I will hook you up with my scripts."
  },
  {
    "objectID": "posts/2015-12-27-canvas-vs-svg/index.html",
    "href": "posts/2015-12-27-canvas-vs-svg/index.html",
    "title": "Canvas vs SVG",
    "section": "",
    "text": "The idea behind CensusMapper is that it takes away all the technical barriers to dealing with census data. So how does CensusMapper work behind the scenes?"
  },
  {
    "objectID": "posts/2015-12-27-canvas-vs-svg/index.html#censusmapper-behind-the-scenes",
    "href": "posts/2015-12-27-canvas-vs-svg/index.html#censusmapper-behind-the-scenes",
    "title": "Canvas vs SVG",
    "section": "",
    "text": "The idea behind CensusMapper is that it takes away all the technical barriers to dealing with census data. So how does CensusMapper work behind the scenes?"
  },
  {
    "objectID": "posts/2015-12-27-canvas-vs-svg/index.html#censusmapper-data-workflow",
    "href": "posts/2015-12-27-canvas-vs-svg/index.html#censusmapper-data-workflow",
    "title": "Canvas vs SVG",
    "section": "CensusMapper Data Workflow",
    "text": "CensusMapper Data Workflow\n\nThe general setup is quite simple. We use the lean javascript open mapping platform leaflet as the base for mapping information. Leaflet handles the logic of dealing with zoom and pan and keeping track of the geographic boundaries that should be mapped. That information gets then passed on to the CensusMapper servers.\nCensusMapper will then send the appropriate census geographic polygons to the browser for leaflet to display. Once the geographic data is available for mapping, some custom code checks what kind of information the user wants to display and requests the census data required to make the map. The census information is then assembled on the server, sent down and attached to the polygons and drawn on the browser window within leaflet. This two-tier process allows the highly dynamic mapping in CensusMapper where the data-heavy geographic polygons are kept separately thus can be cached and re-used."
  },
  {
    "objectID": "posts/2015-12-27-canvas-vs-svg/index.html#drawing-census-data",
    "href": "posts/2015-12-27-canvas-vs-svg/index.html#drawing-census-data",
    "title": "Canvas vs SVG",
    "section": "Drawing Census Data",
    "text": "Drawing Census Data\nThere are a number of ways how we can display census data in the browser. At CensusMapper we have played with three different technologies to map data that vary in performance and browser support. They all have in common that they won’t run on Internet Explorer 8 or earlier, but we have just about reached the point in time where it is acceptable to ignore IE8- in products meant for the “general internet audience”.\n\nSVG\nSVG is what our maps have been using so far. SVGs are fairly high level, which means it’s very little work to implement and map information. One simply passes a polygon to the browser, tells it how to color it, and the browser takes care of the rest. SVG elements can easily be styled via CSS, so there is essentially no work involved to deal with highlight on hover, scaling for retina displays, patterns for census data quality flags, etc. We use d3.js to attach the geographic and census data right to the SVG elements for easy manipulation.\nWhile mapping data this way is very easy, for CensusMapper there are two problems.\n\nWe are restricted in how we can display information by the capabilities of SVG.\nSVG rendering is done by the browser, and not all browsers are equal. Most importantly, SVG rendering in Internet Explorer is excruciatingly slow. So slow, that CensusMapper becomes essentially useless within Internet Explorer. We felt compelled to add a warning messeage that displayed when people opened CensusMapper with Internet Explorer. And when you do that, that’s a sure sign that your app has a serious problem.  So how to get around those issues? Enter Canvas.\n\n\n\nCanvas\nCanvas offers a way to draw images in a browser. Unlike SVG, the drawing has to be done “by hand”. And the result is just an image, with no clear way to tell where it came from. There is no way to attach any information to individual structures drawn on a canvas. All the logic for highlight on hover, figuring out what data is associated with the mouse position, dealing with retina displays, etc. needs to be added by hand.\nOn the upside, a good canvas implementation is a lot faster than SVG. And it opens the door to changes in how the data is handled that bring additional performance improvements. In particular, we can now chop up census polygons and render the pieces separately, greatly cutting down on the size of the downloaded data, as well as the complexity of the polygons that get rendered. And the performance improvements are noticeable across all browsers and platforms.\nAt the end of the day it is actually not that much work and we flipped the switch on this just before the Christmas break. CensusMapper is now running using canvas instead SVG for the main maps. We kept the look and feel the same, so unless you dig into the code you won’t notice the difference. Some parts of CensusMapper still utilized SVGs, like the d3-based Census Wheel.\n\n\nWebGL\nWebGL also draws on a canvas element, but the work is offloaded onto the GPU (graphics processor) giving enormous speed improvements. Regular canvas rendering is fast enough for our purposes, but with WebGL we can do more complex renderings that previously we could not even dream of: Shaders and Interactive 3D data maps. We had previously toyed with 3D data visualization to explore Vancouver’s household density in 3D using Three.js, but did not pursue this further because of the complexities of writing code for navigating a Canada-wide map. Then we came across the super-customizable 3D open mapping platform built by Mapzen, and that suddenly made it extremely easy to do interactive 3D data mapping live in the browser. A quick test using Vancouver’s open LIDAR generated building height data showed how easy Mapzen’s tangram engine is to use.\nAfter digging deeper into tangram, and with help from the friendly people at Mapzen, we figured out a way to fit CensusMapper’s two-stage data workflow into tangram’s mapping engine. The result are real-time 3D maps where height and color of each geographic area can be independently (and dynamically) controlled. Here is an example where mouseover trigged the area west of Coal Harbor to ‘pop up’. \nAt the same time we gain the ability to easily pull in all kinds of other data and map it. On our canvas or svg maps we added regular image tiles, either a road and label’s overlay or a base map (which then requires opacity to be added to the census data that is mapped on top of that) as orientation aid. Short of baking our own image tiles we have very little control over the look and feel of this. With Mapzen’s tangram we can very easily pick and style individual geographic objects from Mapzen’s OSM vector tile server, resulting in crisp and clear maps. In the above example we decide dynamically what level of roads to render, how to style them, what labels to display and we also added bodies of water, where we filter by size depending on the zoom level.\nAt this stage it is still an ongoing project to get this production-ready. One obvious obstacle is that WebGL browser support is still lagging. And on top of that it also requires updated graphics card drivers, which is a big problem on windows machines that are already a couple of years old. So for now we still need to have a plain canvas or svg fallback.\nAnd then there are the details that need to get worked out. 3D maps sounds great, but it will take us some time to figure out how to best utilize this in thematic maps. But even without utilizing 3D capabilities, the dynamic shaders and increased rendering performance are already pushing the boundary of what’s possible in web maps."
  },
  {
    "objectID": "posts/2015-12-14-routing/index.html",
    "href": "posts/2015-12-14-routing/index.html",
    "title": "Bike Routing",
    "section": "",
    "text": "Routing is a hard problem. Routing for drivers is pretty good at this point, mostly because we have been very good at designing for cars and creating predicable infrastructure. Routing for bikes is a whole other story, data quality is poor and the physical infrastructure is, at least in North America, not strongly predictive of cycling comfort/safety. And cycling comfort/safety is the top priority for the vast majority of (potential) cyclists.\nAnd it’s the ones that don’t cycle frequently, often out of concern for safety, that would benefit most from effective bike routing.\nRead on or go directly to the routing demo.  Google does a decent job directing a relatively experienced cyclist from A to B, but it has a hard time to learn about places where cyclists can go but cars can’t. And it won’t be able to answer my fundamental question: Can I bring my 6 year old along?. And Apple doesn’t even try and offer bike routing."
  },
  {
    "objectID": "posts/2015-12-14-routing/index.html#routing-test",
    "href": "posts/2015-12-14-routing/index.html#routing-test",
    "title": "Bike Routing",
    "section": "Routing test",
    "text": "Routing test\nSo what’s really needed apart from better bike maps is better bike routing. So building on yesterday’s post, I decided to take a quick look at routing. Time to try out Mapzen’s routing engine which, as expected, was really easy to set up:\nFeel free to drag the endpoints to test your favorite routes.\n\n\nFull screen view\nInitial testing seems to indicate that this works reasonably well. And while the engine allows for some customization on rider needs, right now there is no way to get the “dad’s routing” that I would like to have.\nPart of the problem is of course that I still don’t have enough information in OSM to even make a “dad’s map” as I lamented earlier. But at least OSM gets me half-way there by giving me a finer control over distinguishing infrastructure that I may deem as generally more suitable so that I can fade selected bike infrastructure out by checking the appropriate boxes in the map."
  },
  {
    "objectID": "posts/2015-12-14-routing/index.html#route-costing-options",
    "href": "posts/2015-12-14-routing/index.html#route-costing-options",
    "title": "Bike Routing",
    "section": "Route costing options",
    "text": "Route costing options\nThe bike routing options in Mapzen’s routing engine allow for some level of control on wheter gravel should be avoided (great feature for the lycra cowed but useless for dads), whether hill should be avoided (helpful) and whether to avoid roads without bike infrastructure. But when I cycle with my 6 year old a bike lane between parked cars and 50 traffic is as good as no bike infrastructure at all. And there is currently no way to cost different types of bike infrastructure, so they can’t be used as a proxy for cycling comfort."
  },
  {
    "objectID": "posts/2015-12-14-routing/index.html#where-to-go-from-here",
    "href": "posts/2015-12-14-routing/index.html#where-to-go-from-here",
    "title": "Bike Routing",
    "section": "Where to go from here",
    "text": "Where to go from here\nNext steps are to look deeper into Mapzen’s routing engine and see how hard it would be to hack some of these more advanced costing options into their routing engine and open up a feature request.\nWrapping up the three-night trials in bike mapping is the post on data."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html",
    "href": "posts/2015-10-24-census-drilldown/index.html",
    "title": "Census Drilldown",
    "section": "",
    "text": "The Census Mapper Project is moving along slowly, public beta unearthed some bugs and we gathered feedback (thanks to everyone reporting back!). There are still a couple of steps that need to be taken care of before we can unleash the full map making power to all users. We feel that the complexity of census data requires more guidance than the current map making system is providing. Anyone who does not mind getting there hands dirty and having to look up census variable definitions by themselves when making maps is welcome to contact us and we will hook you up with a beta mapmaking account.\nIn the meantime we added one important feature to the CensusMapper.\n\n\nCensusMapper is a great way to explore single census variables (or a single function built out of census variables) across many geographic regions and aggregation levels. But sometimes we would like to do the opposite: Drill down into a specific census region and explore other census variables. We have now added an easy way to do this. To access it simply select the “more” button in the basic popup when you select a census region. This brings up the census wheel, which is our way to navigate through census data. \nTry it out right away on CensusMapper or read on for details on how this works.\n\nThere are almost 4,000 census variables available, right now we do not offer to split up by gender, which reduces the available variables to 1,429. To further simplify things we throw out all variables with zero values for the give geographic area, still leaving a sizeable number of variables to browse through.  Each arc in the census wheel represents a variable, or a category of variables. Selecting an arc will zoom into that arc and turn it into the “center” of the wheel, collapsing all other components. That’s the content drilldown process. Once it makes sense to display data as proportions, we switch to the proportional view which shows the data as hierarchical pie chart.\nThis gives a visual representation of the proportions of each of the variables. Hovering (or touching) an arc will display more detailed information, selecting one will drill down further. To reverse that process either select the center or us the content breadcrumbs at the top that were created during the drilldown process."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html#next-steps-in-censusmapper",
    "href": "posts/2015-10-24-census-drilldown/index.html#next-steps-in-censusmapper",
    "title": "Census Drilldown",
    "section": "",
    "text": "The Census Mapper Project is moving along slowly, public beta unearthed some bugs and we gathered feedback (thanks to everyone reporting back!). There are still a couple of steps that need to be taken care of before we can unleash the full map making power to all users. We feel that the complexity of census data requires more guidance than the current map making system is providing. Anyone who does not mind getting there hands dirty and having to look up census variable definitions by themselves when making maps is welcome to contact us and we will hook you up with a beta mapmaking account.\nIn the meantime we added one important feature to the CensusMapper.\n\n\nCensusMapper is a great way to explore single census variables (or a single function built out of census variables) across many geographic regions and aggregation levels. But sometimes we would like to do the opposite: Drill down into a specific census region and explore other census variables. We have now added an easy way to do this. To access it simply select the “more” button in the basic popup when you select a census region. This brings up the census wheel, which is our way to navigate through census data. \nTry it out right away on CensusMapper or read on for details on how this works.\n\nThere are almost 4,000 census variables available, right now we do not offer to split up by gender, which reduces the available variables to 1,429. To further simplify things we throw out all variables with zero values for the give geographic area, still leaving a sizeable number of variables to browse through.  Each arc in the census wheel represents a variable, or a category of variables. Selecting an arc will zoom into that arc and turn it into the “center” of the wheel, collapsing all other components. That’s the content drilldown process. Once it makes sense to display data as proportions, we switch to the proportional view which shows the data as hierarchical pie chart.\nThis gives a visual representation of the proportions of each of the variables. Hovering (or touching) an arc will display more detailed information, selecting one will drill down further. To reverse that process either select the center or us the content breadcrumbs at the top that were created during the drilldown process."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html#data-problems",
    "href": "posts/2015-10-24-census-drilldown/index.html#data-problems",
    "title": "Census Drilldown",
    "section": "#Data Problems",
    "text": "#Data Problems\nCensus data is messy. Now that all census data for each region is generally accessible in CensusMapper we need to explain some of the inherent data problems."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html#rounding",
    "href": "posts/2015-10-24-census-drilldown/index.html#rounding",
    "title": "Census Drilldown",
    "section": "##Rounding",
    "text": "##Rounding\nCensus Canada will round (almost) all data to preserve anonymity and don’t create false impressions of accuracy that the data does not achieve. Data is generally reported in increments of 5, rounding includes randomness to preserve anonymity. The value of the measured value is within 4 of the reported one. And remember that even the measured variable is only an estimate of the actual value of the variable. Rounding may lead to situations where, for example, the sum of all people listed by age bracket will not add up to the total number of people. Generally, this difference will be small and we ignore it in our visualization."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html#omitted-data",
    "href": "posts/2015-10-24-census-drilldown/index.html#omitted-data",
    "title": "Census Drilldown",
    "section": "##Omitted Data",
    "text": "##Omitted Data\nCensus Canada will at times not report data. This could be due to very low return rates or other problems that make data so unreliable that it is better not reported at all. Or it could be that releasing the information could compromize the anonymity of the census data for some people in that area. The latter can take the form of Census Canada not reporting any data for the region, or Census Canada zeroing out specific variables that “are too low to be reported”.  We have not been able to find clear guidelines how the “zeroing” works, but often this will leave detectable traces in the data. Looking at the example in the image, looking at “Mode of Transport” to work only “Driving” has non-zero values, the other options “Passenger”, “Transit”, “Bike”, “Walk” and “Other” are all zero. There were 160 people getting to work, 115 are listed as “Driver”, leaving 45 unaccounted for. This is outside of the range that could be explained by the rounding of variables. We alert the user by adding in a grey area for the missing 45. This also ensures that the visual representation remains accurate."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html#multiple-responses",
    "href": "posts/2015-10-24-census-drilldown/index.html#multiple-responses",
    "title": "Census Drilldown",
    "section": "##Multiple Responses",
    "text": "##Multiple Responses\nSome census questions allow for multiple responses. For exaple “Language Spoken Most Often At Home”. In this particular case the census variable breaks out single responses and multiple responses and is very transparent to the user. In other cases, for example “Ethnicity”, single and multiple responses are not reported separately but responses are all added up. This leads to the sum of lower level variables being higher than the base variable. We alert the user to this by overlaying small white dots on the base variable.  In this particular case the total for for “Ethnic Origin” was 12,140 people. But there were 1,430 more responses at the next level, so up to 1,430 people had given multiple responses to this question listing more than one of the aggregate (mostly continent level) origins, some possibly listing more than two. The same patter repeats at different ethnic origin aggregation levels, for example 2,565 people claimed at least one of the “British Island origins”, but many listed more than one resulting in the sum of the individual regions with the British Islands exceeding the British Island count by 1,445. Again, we alert the user by overlaying dots over the “British Island origins” arc. Hovering over the arg will display the exact numbers of the “overcounting” due to multiple responses.\nIn these cases where mulitple repsonses are not broken out the dots will aler the user that the proportional representation in the hierarchical pie chart does not represent proportions out of a total given by the value of the variable at the centre (or lower level), but as a proportion of all responses which exceeds the value of the lower level variable."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html#basic-census",
    "href": "posts/2015-10-24-census-drilldown/index.html#basic-census",
    "title": "Census Drilldown",
    "section": "##Basic Census",
    "text": "##Basic Census\nThe Basic Census is generally speaking quite reliable, every single person is required to fill it out and return rates are generally above 95%. Serious problems will only occur if response rates are very low. We alert the user by shading geographic regions is this has been the case."
  },
  {
    "objectID": "posts/2015-10-24-census-drilldown/index.html#nhs",
    "href": "posts/2015-10-24-census-drilldown/index.html#nhs",
    "title": "Census Drilldown",
    "section": "##NHS",
    "text": "##NHS\nThe National Household Survey is quite different in nature, it was only sent out to a smaller portion (~30%) of society and return rates were much lower (~69%). Even with 100% return rates there are likely to be geographic regions where the results severely misrepresent reality in that region due to sampling bias. For each region that bias is small, but the probability for bias grows as the number of people in the geographic region declines. So this is mostly a problem for Dissemination Areas. But even there, the probability of severe sampling bias in each region is small, but there are many regions and the probability that some of these regions suffer from sever sampling bias is quite large.\nOn top of this basic statistical sampling bias, we also have self selection bias due to some deomgraphics being more likely to return the survey than others. This bias is a product of the decision of scrapping the madatory ‘long form census’ and replacing it with the voluntary NHS. The return rates can give some indication of the likelyhood of self-selection bias, we shade regions with a non-return rate lower than 50%, the cutoff Census Canada set for reliability of the NHS. It is especially problematic when trying to detect change in variables (for example poverty) from one census to another as the differences in the variable over time are often small and similar in magnitude to possible self-selection bias.\nThe 50% cutoff we highlight in CensusMapper is just a guideline, the exact return rates are displayed on hover or when selecting regions and should always to be taken into account when interpreting results, especially at the Dissemination Area level.\nIf all this information did not turn you off, head over to CensusMapper and drill down into some geographic areas."
  },
  {
    "objectID": "posts/2015-08-25-the-hidden-mortgage/index.html",
    "href": "posts/2015-08-25-the-hidden-mortgage/index.html",
    "title": "The Hidden Mortgage",
    "section": "",
    "text": "Some months ago I did a little side project and put some Census Canada data for Vancouver on an interactive map online. After it generated quite some interest, to a large part due to Ian Young’s reporting utilizing some of the data in the South China Morning Post, I talked to my friend Alejandro and we decided to set up a Census Mapper that allowes laymen to map any census variables of their choosing Canada wide.\nSince this is only a side project, it will take some time to set this up properly. In the meantime, I get to have some fun to play with census data and map some variables of my choosing.\nFor a sneak preview of what the Census Mapper will be able to do read below the fold. No interaction, no chosing your own variables and panning around Canada yet. Just some screenshots with a story to tie them together."
  },
  {
    "objectID": "posts/2015-08-25-the-hidden-mortgage/index.html#census-mapper-sneak-preview",
    "href": "posts/2015-08-25-the-hidden-mortgage/index.html#census-mapper-sneak-preview",
    "title": "The Hidden Mortgage",
    "section": "Census Mapper Sneak Preview",
    "text": "Census Mapper Sneak Preview\nOne of my many pet pieves has been the Drive Until You Qualify phrase. The idea is that you buy the house closest to your place of work for which the bank will aprrove a loan. The obvious problem with this is that your commuting costs will go up the further away from work you live – and you disposable income goes down accordingly. But the bank does not seem to be bothered by that at all and often does not get much attention by househunting families either.\nThe fix? Easy. The cost of commuting and housing should not be separated by viewed in concert with one another. People househunting right will have to do this based on their individual parameters, but let’s take a look at what choices people have made in the past based on Census Canada’s 2011 NHS data.\n\nDwelling Value\nIn mapping terms, let’s start with the map of Median Dwelling Values around Vancouver, taken from the NHS. Although a little dated, it still serves to make the basic point.\n\nThe image is the familiar one, homes are expensive on the west side, a little less so toward the east. In Vancouver proper the downtown area is somewhat affordable with prices dipping below $300,000 for the median dwelling. We are considering the whole range, one bedroom condos all the way up to single family homes.\n\n\nCommute Time\nNext up, how much time to these people spend commuting? The NHS got you covered.\n\nThe median one-way commute time give us a picture of how much time people spend to go to work. And we will assume they spend the same time coming back.\nHow much does it cost to commute. We will think of the commute cost as being made up of two components. The money spent to get to and from work and the dollar value of the time. That leaves us with two more questions. How much money do people spent and how much is their time worth.\n\n\nTransportation Cost\nLuckily, again the NHS has something to say about this. To figure out the money spent on the commute we look at the mode of transport. NHS tell us how many people drive, are passengers, take transit, cycle or walk to work. To keep things simple we will compute the ‘median commute cost’ by setting transit cost at $120 per month (roughly the price for a 2-zone transit pass) and for driving we set the car2go rate of 41c/min (which is quite generous, most people will spent more if they own a car, and pay for gas, insurance, parking themselves and spent time to maintain it). For simplicit we set the cost for passengers, cyclists and pedestrians at zero. So we sum over the number of drivers and transit users and divide by the total number of commuters in each dissemination area to estimate that cost. Generally speaking it turns out to be much smaller than the ‘time cost’ calculated next.\n\n\nTime Cost\nTo compute the ‘time cost’ of commuting for the average person in each dissemination area we need to know what people’s time is worth. The NHS has a simple answer for that, the Median After Tax Household Income.\n\nWe estimate the median annual time cost of the commute by taking the ratio of the daily commute time to the daily work time, assumed to be eight hours, and multiplying it by the annual after tax income. That’s more or less the money the average person in that dissemination area makes in the time the person spent commuting. If there are several people in the household contributing to the income, there will be several people spending time commuting, so that effect roughly cancels out.\n\n\nAnnual Commute Cost\nPutting it together we get the annual cost of the commute map.\n\nThis is only an estimate. From a technical perspective we probably should have been working with averages instead of median values, but the result is very similar. Translating between time and money is inherintly tricky, most people can’t easily scale up or down the time they spent working to translate time into money. The easiest way to think of this is someone who tries to work less hours (and get less pay) in order to spend more time with their family. One way to do exactly that is to cut down on commute time.\n\n\nThe Hidden Mortgage\nNow it’s time to tie this back up with the price for housing. After all, the time you spent on your commute and your mode choice for that commute, are intimately tied to where you choose to live. How do we combine the commute cost with the housing cost? As a rule of thumb, one makes $400 monthly payments for each $100,000 of mortgage. Give or take, but our numbers are quite rough anyway. So taking the annual commute cost, dividing by 400 and multiplying by 100,000 we get the ‘commute cost mortgage equivalent’. If we map it it looks the same as the previous map, just with different labels. But since I like maps, here it is.\n\nIn other words, we translated the commute cost into ‘hidden mortgage’ payments. Except, unlike a real mortgage, paying it off does not generate any value for you.\n\n\nCombined Dwelling Value and Commute Cost\nFinally we have all we need for the end result. The Combined Dwelling Value and Commute Time/Mode Value Map where we simply add the estimated commute mortgage cost onto the dwelling value.\n\nIt does not look radically different from the dwelling value map. It’s almost the same around downtown where commute time tends to be low and more expensive further out, where the ‘hidden mortgage’ of commute time can add up to half a million dollars.\nUsing Census Data can only help illustrate the hidden mortgage that people are already paying. It can only highlight some general trends and ideas, it cannot make any statement about particular individual households, nor can the genral reasoning used to derive the numbers apply to everyone. Or maybe even most people. The estimates are quite rough, likely underestimating commute costs for drivers, not taking into account the well understood health benefits of active transportation nor the negative health implications of driving. The commute time to money computation is very rough and will have to be adjusted if applied to real-world examples. But this is only meant to illustrate ageneral point.\nThe takeaway should be that the ‘hidden mortgage’ is real, and it’s huge. And often overlooked."
  },
  {
    "objectID": "posts/2015-05-31-density-in-vancouver/index.html",
    "href": "posts/2015-05-31-density-in-vancouver/index.html",
    "title": "Tax Density in Vancouver",
    "section": "",
    "text": "The other day I saw that Downtown brings in 23% of CoV tax revenue but only makes up 5% of the city area. Intrigued by that I decided to add a ‘Tax Density’ layer to my Vancouver Assessment Map. The idea was to try and understand the tax revenue generated by different areas in Vancouver. The data is already available in the CoV open data property dataset so it would only take half an hour to ad it to my map.\nThere has been a lot of discussion around density in Vancouver. One aspect of density is that it will generally increase the tax revenue that the city can collect per square metre. At the same time the city spending for services of the added density increases at a much lower rate. Leaving a net gain of revenue for the city. As property taxes are need-based, this means lower property taxes for everyone when density increases. In short, density leads to efficiency increases that materialize in form of lower property taxes for everyone.\nThat also points to one possible way to break some of the resistance to density increases that Vancouver currently sees. If a neighbourhood (or smaller region) accepts density increases, maybe some of the associated benefits of lower property taxes should be applied locally instead of everything being spread out over the entire city."
  },
  {
    "objectID": "posts/2015-05-31-density-in-vancouver/index.html#so-what-exactly-does-the-tax-density-layer-show",
    "href": "posts/2015-05-31-density-in-vancouver/index.html#so-what-exactly-does-the-tax-density-layer-show",
    "title": "Tax Density in Vancouver",
    "section": "So what exactly does the Tax Density layer show?",
    "text": "So what exactly does the Tax Density layer show?\n\nThe Tax Density I mapped is simply the amount of taxes collected per m². Under that measure, Downtown comes out at $127.1/m², followed by the West End at $72.7/m² and Fairview at $42.7/m². The Vancouver average is $20/m², only 6 of the 23 neighbourhoods have an above-average Tax Density. A complete list of Tax Density by neighbourhood is at the bottom.\nFor this calculation I did not count the parks doward the area of the neighbourhoods, but school for example are included. In light of this some of the aggregated data should be viewed with caution."
  },
  {
    "objectID": "posts/2015-05-31-density-in-vancouver/index.html#where-to-go-from-here",
    "href": "posts/2015-05-31-density-in-vancouver/index.html#where-to-go-from-here",
    "title": "Tax Density in Vancouver",
    "section": "Where to go from here?",
    "text": "Where to go from here?\nI also thought about mapping the density of housing units, but sadly the data in the CoV open data catalogue is ill-suited to do this. I can easily extract the number of tax entities and map the density of these. This works great for a traditional single family home (one tax entity) and for stratas (one tax entity per unit), but it becomes a problem because e.g. laneway houses and granny suites don’t show up as separate tax entities. And it gets really bad with rental apartments, where the whole building will be a single tax entity with potentially a very large number of units. And then the dataset does not distinguish between commercial and residential units. That data could be reverse engineered from the tax data, but that’s more work than my usual half hour tolerance level for this kind of side project.\nAnother option is to use census data and merge the datasets. But that gets messy, the aggregation levels don’t match and this is well beyond a side project time frame.\nSimilarly, it would be interesting to map the changes in tax density and the changes in unit density in Vancouver, but again the CoV dataset does hold the necessary information. In particular, when properties get a new tax coordinate (for example when they get re-developed) the CoV dataset drops the old tax data associated with that physical location. This makes it impossible to map the changes in tax density that redevelopment has brought.\nLong story hort, if you are interested in browsing Vancouver by tax revenue collected per square meter, click through to the Vancouver Assessment Map and select the Tax Density layer."
  },
  {
    "objectID": "posts/2015-05-31-density-in-vancouver/index.html#tax-density-by-neighbourhood",
    "href": "posts/2015-05-31-density-in-vancouver/index.html#tax-density-by-neighbourhood",
    "title": "Tax Density in Vancouver",
    "section": "Tax density by neighbourhood",
    "text": "Tax density by neighbourhood\n\nDowntown: $127.1/m²\nWest End: $72.7/m²\nFairview: $42.7/m²\nKitsilano: $27.8/m²\nDowntown Eastside: $26.4/m²\nMount Pleasant: $25.4/m²\nArbutus Ridge: $17.5/m²\nGrandview-Woodland: $16.4/m²\nWest Point Grey: $16.3/m²\nDunbar Southlands: $15.0/m²\nRiley Park: $14.4/m²\nOakridge: $13.8/m²\nShaughnessy: $13.5/m²\nKensington-Cedar Cottage: $12.8/m²\nRenfrew-Collingwood: $12.4/m²\nStrathcona: $12.2/m²\nKerrisdale: $11.9/m²\nMarpole: $11.8/m²\nSouth Cambie: $11.7/m²\nSunset: $11.2/m²\nHastings-Sunrise: $10.6/m²\nVictoria-Fraserview: $10.0/m²\nKillarney: $7.7/m²"
  },
  {
    "objectID": "posts/2015-03-23-vancouver-assessment-data/index.html",
    "href": "posts/2015-03-23-vancouver-assessment-data/index.html",
    "title": "Vancouver Assessment Data",
    "section": "",
    "text": "The City of Vancouver has recently updated their open data catalogue with historic tax data. We figured that would be a good time to take our previous rather clumsy attempts to map tax data and polish that up a bit.\nThis time we put in a couple of evenings to build a more responsive map with multiple options of what to map. Head straight for ‘the map’ or read on for some background information, including some methodology on data cleaning and interpolation.\n\n\nOur previous ‘teardown map’ and ‘high value map’ suffered from being a static websites, and they loaded large amounts of data all at once, 30,000 properties for the teardown map, and display the all at once. Options were limited for displaying more detailed data when selecting individual properties. For the viewer this resulted more in an exercise of patience than in an ‘interactive’ experience.\nWe decided to remedy this by importing the CoV data into a database and chop the data up dynamically and serve it as vector tiles. For smaller zoom levels we aggregated data at the block level. There are lots of blocks in Vancouver, 4444 that contain properties with tax information to be precise. So again the map is a little sluggish on mobile devices and slower machines when zoomed out, but things get much faster as we zoom in.\nMoreover, we now have the ability to easily display more detailed information once the use selects an individual property or block aggregate. We can display a graph with the development of land and building values and other details. And we integrated google streetview for good measure.\nOne issue that came up is that CoV historical data has lots of gaps. For example, when a particular property was re-developed it gets a new tax number. The tax information for the old property is lost and cannot be connected to the geographic site using the information provided by CoV. So when we display data showing ‘value change’ between 2006 and 2014 we color these properties in grey. This problem persists when we aggregate information at the block level. To avoid greying out lots of blocks we extrapolate the missing data by using citywide averages on growth rates of land and building values for the properties in question. This will likely underestimate the aggregate growth in building and land value, but will not have a big impact at the block level if the individual property value in question was comparatively small. But this was not always the case. Better analysis could solve some of these issues, so does zooming in to the individual property level.\nAnother issue is that CoV does not provide historic zoning information. It would be interesting to get historic zoning and development permit information. Some of this data is already available, albeit not in an easily consumable form.\nOther than that a big thanks to the folks maintaining the CoV open data catalogue!\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2015,\n  author = {von Bergmann, Jens},\n  title = {Vancouver {Assessment} {Data}},\n  date = {2015-03-23},\n  url = {https://doodles.mountainmath.ca/posts/2015-03-23-vancouver-assessment-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2015. “Vancouver Assessment Data.”\nMountanDoodles (blog). March 23, 2015. https://doodles.mountainmath.ca/posts/2015-03-23-vancouver-assessment-data."
  },
  {
    "objectID": "posts/2014-09-07-vancouver-high-value-improvements-map/index.html",
    "href": "posts/2014-09-07-vancouver-high-value-improvements-map/index.html",
    "title": "Vancouver High Value Improvements Map",
    "section": "",
    "text": "After the teardown map that looked at the low end of Vancouver’s building stock, a natural question is to look at the high end. Just as before, we are focusing on the (assessed) value of the building relative to the total (assessed) value of land and improvements. We defined “highly improved” properties to be properties where the ratio is at least 50%, so the building is worth at least as much as the land it is built on.\nThat’s actually a pretty high bar in Vancouver, less than 1500 buildings pass that muster. And in concordance with the teardown map, the “high value improvements” seem to concentrate in downtown. See the map below the fold to explore for yourself.\n\nWhen browsing the map it becomes clear that some properties have “high value improvements” because of an abnormally low land value, rather than an abnormally high improvement value. This is probably due to intricacies of zoning in Vancouver that maybe someone else can shed some light on.\n\n\nFull screen view\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2014,\n  author = {von Bergmann, Jens},\n  title = {Vancouver {High} {Value} {Improvements} {Map}},\n  date = {2014-09-07},\n  url = {https://doodles.mountainmath.ca/posts/2014-09-07-vancouver-high-value-improvements-map},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2014. “Vancouver High Value Improvements\nMap.” MountanDoodles (blog). September 7, 2014. https://doodles.mountainmath.ca/posts/2014-09-07-vancouver-high-value-improvements-map."
  },
  {
    "objectID": "posts/2024-04-15-migrating-to-quarto/index.html",
    "href": "posts/2024-04-15-migrating-to-quarto/index.html",
    "title": "Migrating to quarto",
    "section": "",
    "text": "I started this blog 10 years ago, first using a jekyll setup. In 2017 I switched to blogdown, which linked the posts with the Rmarkdown notebook that generated the content. The setup aimed at maximum transparency, with the source living in a public Github repository that keeps all changes transparent, and deploys to the website via Netlify directly triggered from the GitHub repo when it changed. That means the website of the blog was always in sync with the repository, and the repository contained the entire history of the blog."
  },
  {
    "objectID": "posts/2024-04-15-migrating-to-quarto/index.html#footnotes",
    "href": "posts/2024-04-15-migrating-to-quarto/index.html#footnotes",
    "title": "Migrating to quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course the StatCan servers for the data used here went down on the evening this post was written ahead of the next day’s release of the March CPI data…↩︎"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Mountain Doodles Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nApr 11, 2024\n\n\nMigrating to quarto\n\n\n\n\nApr 11, 2024\n\n\nWhat if recent apartment buildings in Vancouver were 20% taller?\n\n\n\n\nJan 16, 2024\n\n\nFirst time buyer Lorenz curves revisited\n\n\n\n\nAug 17, 2023\n\n\nHousing Outcomes\n\n\n\n\nJun 27, 2023\n\n\nHousing targets\n\n\n\n\nJun 20, 2023\n\n\nMetro Vancouver Planning Regimes\n\n\n\n\nFeb 20, 2023\n\n\nInvesting in definitions and framing\n\n\n\n\nFeb 6, 2023\n\n\nA Brief History of Vancouver Planning & Development Regimes\n\n\n\n\nDec 17, 2022\n\n\nAnalyzing Ballot Composition in Vancouver\n\n\n\n\nNov 24, 2022\n\n\nNew Premier New Housing Policy\n\n\n\n\nOct 3, 2022\n\n\nStill Short: Suppressed Households in 2021\n\n\n\n\nSep 27, 2022\n\n\nWhere did all the cheap rents go?\n\n\n\n\nSep 12, 2022\n\n\nRent growth in GDP\n\n\n\n\nAug 18, 2022\n\n\n25 Years of Structural Change\n\n\n\n\nJul 29, 2022\n\n\nTumbling turnover\n\n\n\n\nJun 30, 2022\n\n\nA brief history of Canadian real estate investors\n\n\n\n\nJun 12, 2022\n\n\nIns and outs of CMHC data\n\n\n\n\nMay 27, 2022\n\n\nResidential mobility in Canada\n\n\n\n\nMay 21, 2022\n\n\nNanaimo Station\n\n\n\n\nMay 17, 2022\n\n\nOn Broadway\n\n\n\n\nMay 11, 2022\n\n\nChildren are good, actually\n\n\n\n\nMay 6, 2022\n\n\nEstimating Suppressed Household Formation\n\n\n\n\nApr 26, 2022\n\n\nPlanning for scarcity\n\n\n\n\nMar 30, 2022\n\n\nWhat’s up with Squamish?\n\n\n\n\nMar 29, 2022\n\n\nUBCM Shenanigans\n\n\n\n\nFeb 18, 2022\n\n\nVacancy rates and rent change, 2021 update\n\n\n\n\nFeb 17, 2022\n\n\nCensus quirks; using UBC area as an example\n\n\n\n\nFeb 15, 2022\n\n\nDeadbeat Neighbourhoods\n\n\n\n\nFeb 14, 2022\n\n\nUnoccupied Canada\n\n\n\n\nFeb 11, 2022\n\n\nDeadbeat zoning\n\n\n\n\nFeb 9, 2022\n\n\nCanada’s 2021 census, part 1\n\n\n\n\nJan 31, 2022\n\n\nNo shortage in Housing BS\n\n\n\n\nNov 28, 2021\n\n\nFirst Peek at Population and Household Data During COVID & Caveats\n\n\n\n\nNov 21, 2021\n\n\nThree Years of Speculation & Vacancy Tax Data\n\n\n\n\nNov 20, 2021\n\n\nActing locally on housing\n\n\n\n\nOct 17, 2021\n\n\nCensusMapper (p)review\n\n\n\n\nOct 3, 2021\n\n\nFixing parking\n\n\n\n\nSep 25, 2021\n\n\nElections fun - 2021 edition\n\n\n\n\nSep 7, 2021\n\n\nTransnational property ownership in Canada\n\n\n\n\nAug 30, 2021\n\n\nThoughts on vaccine effectivess estimates\n\n\n\n\nAug 21, 2021\n\n\nSatellites, Sprawl, and City Six-Packs\n\n\n\n\nAug 13, 2021\n\n\nCommodity and Keeping it in the Family\n\n\n\n\nJul 25, 2021\n\n\nLots of Opportunity: Estimating the Zoning Tax in Vancouver\n\n\n\n\nJun 8, 2021\n\n\nBasement Confidential: Vancouver’s Informal Housing Stock\n\n\n\n\nMar 29, 2021\n\n\nForced Out in Canada: New Data from CHS\n\n\n\n\nMar 10, 2021\n\n\nVaxx vs VOCs\n\n\n\n\nMar 4, 2021\n\n\nData variants\n\n\n\n\nFeb 21, 2021\n\n\nOn COVID and exponential growth\n\n\n\n\nFeb 10, 2021\n\n\nIndustrial Strength Zombies: Vancouver Edition\n\n\n\n\nFeb 3, 2021\n\n\nBartholomew’s dot destiny\n\n\n\n\nJan 30, 2021\n\n\nOn COVID Trend lines\n\n\n\n\nJan 25, 2021\n\n\nRethinking the “foreignness” of owners living abroad\n\n\n\n\nJan 15, 2021\n\n\nCapital Gains Income\n\n\n\n\nJan 6, 2021\n\n\nVancouver’s pandemic weather\n\n\n\n\nJan 2, 2021\n\n\nBC back to (COVID) school\n\n\n\n\nDec 21, 2020\n\n\nCovid testing data in BC\n\n\n\n\nDec 7, 2020\n\n\nWhat to Expect from an Empty Homes Tax\n\n\n\n\nNov 10, 2020\n\n\nTongFen\n\n\n\n\nOct 22, 2020\n\n\nCOVID-19 Data in BC\n\n\n\n\nOct 2, 2020\n\n\nCHS Core Housing Need\n\n\n\n\nSep 28, 2020\n\n\nFirst-time buyer Lorenz curves\n\n\n\n\nSep 21, 2020\n\n\nIncome mixing and segregation\n\n\n\n\nSep 8, 2020\n\n\nCovid school modelling\n\n\n\n\nAug 27, 2020\n\n\nKeeping the Leavers\n\n\n\n\nAug 25, 2020\n\n\nCovid Series 3 Survey\n\n\n\n\nJul 6, 2020\n\n\nCanadian 1996 Census\n\n\n\n\nMay 27, 2020\n\n\nOn mixing covid-19 and census data\n\n\n\n\nMay 25, 2020\n\n\nProjections and self-fulfilling prophecies\n\n\n\n\nMay 20, 2020\n\n\nToward universal TongFen: Change in polling district voting patterns\n\n\n\n\nMay 13, 2020\n\n\nCOVID-19 data in Canada, effective reproduction rates\n\n\n\n\nApr 23, 2020\n\n\nCensus tract level T1FF tax data\n\n\n\n\nApr 19, 2020\n\n\nCOVID deaths in context by weeks\n\n\n\n\nApr 10, 2020\n\n\nCovid-19 data in Canada\n\n\n\n\nMar 31, 2020\n\n\nContext for Covid-19 mortality so far\n\n\n\n\nMar 22, 2020\n\n\nBehaviour change in response to COVID-19\n\n\n\n\nMar 9, 2020\n\n\nKnock Knock, Anybody Home?\n\n\n\n\nMar 3, 2020\n\n\nOvernight Visitors and Crude Travel Vectors\n\n\n\n\nFeb 19, 2020\n\n\nWealth vs income\n\n\n\n\nFeb 2, 2020\n\n\nDisaster response maps\n\n\n\n\nJan 27, 2020\n\n\nMythical oversupply\n\n\n\n\nJan 26, 2020\n\n\nUnoccupied dwellings data\n\n\n\n\nJan 24, 2020\n\n\nKeep on moving\n\n\n\n\nJan 15, 2020\n\n\n2019 CMHC Rental Market Survey\n\n\n\n\nJan 6, 2020\n\n\nFlow Maps\n\n\n\n\nDec 9, 2019\n\n\nFun with parking tickets\n\n\n\n\nNov 26, 2019\n\n\nProperty tax snacks\n\n\n\n\nNov 23, 2019\n\n\nCanadian Housing Survey\n\n\n\n\nOct 29, 2019\n\n\nCommuter growth\n\n\n\n\nOct 22, 2019\n\n\nElections fun\n\n\n\n\nOct 17, 2019\n\n\nRents and incomes\n\n\n\n\nOct 16, 2019\n\n\nRents and vacancy rates\n\n\n\n\nOct 7, 2019\n\n\nSpatial autocorrelation & co\n\n\n\n\nSep 6, 2019\n\n\nJob vacancies\n\n\n\n\nSep 2, 2019\n\n\nLow income vs new dwellings\n\n\n\n\nAug 19, 2019\n\n\nRunning on Empties\n\n\n\n\nAug 12, 2019\n\n\nMIRHPP tradeoffs\n\n\n\n\nAug 1, 2019\n\n\nOn Vancouver population projections\n\n\n\n\nJul 14, 2019\n\n\nTaxing Toxic Demand: Early Results\n\n\n\n\nJul 7, 2019\n\n\nShaughnessy Townhomes\n\n\n\n\nJun 25, 2019\n\n\nHow not to analyze the roots of the affordability crisis\n\n\n\n\nJun 21, 2019\n\n\nFrequent transit zoning\n\n\n\n\nJun 18, 2019\n\n\nSome notes on investor and corporate ownership of residential properties\n\n\n\n\nJun 15, 2019\n\n\nCensus custom timelines\n\n\n\n\nJun 12, 2019\n\n\nSimple Metrics for Deciding if You Have Enough Housing\n\n\n\n\nJun 9, 2019\n\n\nVancouver population density over time\n\n\n\n\nJun 4, 2019\n\n\nMulti-Census Tongfen\n\n\n\n\nJun 3, 2019\n\n\n2001 Census Data (and TongFen)\n\n\n\n\nApr 24, 2019\n\n\nPopulation weighted densities\n\n\n\n\nApr 20, 2019\n\n\nA bedroom is a bedroom\n\n\n\n\nApr 15, 2019\n\n\nVSB X-Boundary\n\n\n\n\nMar 27, 2019\n\n\nDensity timelines\n\n\n\n\nMar 17, 2019\n\n\nCity density patterns\n\n\n\n\nMar 2, 2019\n\n\nAirbnb updates\n\n\n\n\nFeb 27, 2019\n\n\nTax Speculations\n\n\n\n\nFeb 21, 2019\n\n\nPlanned displacement\n\n\n\n\nFeb 15, 2019\n\n\nVancouver renters\n\n\n\n\nFeb 4, 2019\n\n\nVancouver election individual ballots\n\n\n\n\nFeb 3, 2019\n\n\nThere is no Brain Drain, but there might be Zombies\n\n\n\n\nJan 31, 2019\n\n\nThe Fleecing of Canadian Millenials\n\n\n\n\nJan 9, 2019\n\n\nHigh-value homes\n\n\n\n\nDec 17, 2018\n\n\nHow are condos used?\n\n\n\n\nNov 28, 2018\n\n\nMoving Penalty\n\n\n\n\nNov 28, 2018\n\n\nVacancy rate and rent change\n\n\n\n\nOct 28, 2018\n\n\nUnderstanding income distributions across geographies and time\n\n\n\n\nOct 22, 2018\n\n\nToronto wards\n\n\n\n\nOct 17, 2018\n\n\nThe rise and fall of Vancouver eligible voters\n\n\n\n\nOct 16, 2018\n\n\nNaked Buildings Map\n\n\n\n\nOct 13, 2018\n\n\nCouncil candidate neighbourhoods\n\n\n\n\nAug 21, 2018\n\n\nAirbnb and STR licences\n\n\n\n\nAug 16, 2018\n\n\nGross migration\n\n\n\n\nAug 10, 2018\n\n\nTaxing property instead of income in B.C.\n\n\n\n\nJul 28, 2018\n\n\nActive Fire\n\n\n\n\nJul 22, 2018\n\n\nFact-checking Vancouver’s Swamp Drainers\n\n\n\n\nJul 17, 2018\n\n\nMaking Room\n\n\n\n\nJul 13, 2018\n\n\nAboriginal overrepresentation in correctional services and police checks\n\n\n\n\nJun 21, 2018\n\n\nSkytrain rents\n\n\n\n\nJun 14, 2018\n\n\nInterprovincial Migration\n\n\n\n\nJun 4, 2018\n\n\nVancouver Streets and Lanes\n\n\n\n\nMay 28, 2018\n\n\nExtra School Tax Calculator\n\n\n\n\nMay 25, 2018\n\n\nShort Term Rental Licenses\n\n\n\n\nMay 23, 2018\n\n\nTeardowns and Emissions\n\n\n\n\nMay 11, 2018\n\n\nBuilding Height Profiles\n\n\n\n\nMay 1, 2018\n\n\nA modest school-tax proposal\n\n\n\n\nMar 28, 2018\n\n\nFrontage\n\n\n\n\nMar 12, 2018\n\n\nTransit Data\n\n\n\n\nFeb 28, 2018\n\n\nExtra School Tax\n\n\n\n\nFeb 24, 2018\n\n\nTaxable Dwelling Density\n\n\n\n\nFeb 22, 2018\n\n\nBuilding Permits\n\n\n\n\nFeb 8, 2018\n\n\nNeighbourhood Level Census Data\n\n\n\n\nFeb 1, 2018\n\n\nReal Estate Industry\n\n\n\n\nJan 25, 2018\n\n\nEmpty Suites\n\n\n\n\nJan 12, 2018\n\n\nPedestrian counts, or when the kindergardener needs to use the bathroom\n\n\n\n\nJan 11, 2018\n\n\nLocal vs Overseas Investors\n\n\n\n\nJan 9, 2018\n\n\nSchool Traffic\n\n\n\n\nDec 11, 2017\n\n\nSome Thoughts on the “Supply Myth”\n\n\n\n\nDec 1, 2017\n\n\nWhat’s a Household?\n\n\n\n\nNov 29, 2017\n\n\nJourney To Work\n\n\n\n\nNov 28, 2017\n\n\nUnder Construction\n\n\n\n\nNov 21, 2017\n\n\nByelection Data\n\n\n\n\nNov 15, 2017\n\n\nStatCan Web Maps\n\n\n\n\nNov 7, 2017\n\n\nRenting in Toronto\n\n\n\n\nNov 2, 2017\n\n\nUpdated Foreign Buyers Data\n\n\n\n\nNov 1, 2017\n\n\nPlaying with Medians\n\n\n\n\nOct 26, 2017\n\n\nA First Look At Vancouver Housing Data\n\n\n\n\nOct 23, 2017\n\n\nTrick-or-Treat 2017\n\n\n\n\nOct 21, 2017\n\n\nBoomer Exodus?\n\n\n\n\nOct 15, 2017\n\n\nVSB Vulnerable Students\n\n\n\n\nSep 29, 2017\n\n\nA Retrospective Look at NHS Income Data\n\n\n\n\nSep 26, 2017\n\n\nEvolution of the Income Distribution\n\n\n\n\nSep 18, 2017\n\n\nZoned for Who?\n\n\n\n\nSep 14, 2017\n\n\nIncome - A First Look\n\n\n\n\nSep 9, 2017\n\n\nAmazon - The Canadian Data\n\n\n\n\nAug 24, 2017\n\n\ndot-density\n\n\n\n\nAug 23, 2017\n\n\nDensity\n\n\n\n\nAug 6, 2017\n\n\nMillennials Redux\n\n\n\n\nMay 16, 2017\n\n\nLifeblood\n\n\n\n\nApr 10, 2017\n\n\nSurprise Maps\n\n\n\n\nApr 3, 2017\n\n\nMarine Gateway and Joyce-Collingwood\n\n\n\n\nMar 22, 2017\n\n\nComparing Censuses\n\n\n\n\nMar 6, 2017\n\n\nRS Population Change\n\n\n\n\nMar 1, 2017\n\n\nTransit Explorer\n\n\n\n\nFeb 21, 2017\n\n\nMore on Teardowns\n\n\n\n\nFeb 10, 2017\n\n\n2016 Census Data - Part 1\n\n\n\n\nJan 25, 2017\n\n\nJane Jacobs’ Vancouver\n\n\n\n\nJan 18, 2017\n\n\nBumper Year for Thumb Twiddlers\n\n\n\n\nJan 17, 2017\n\n\nThe Coveted $1.2m - $1.6m Vote\n\n\n\n\nJan 16, 2017\n\n\n2017 Vancouver Assessment Data\n\n\n\n\nDec 13, 2016\n\n\nUpdated Property Tax Data\n\n\n\n\nNov 26, 2016\n\n\nCharacter Retention\n\n\n\n\nNov 18, 2016\n\n\nInteractive Isochrones\n\n\n\n\nOct 21, 2016\n\n\nTrick-or-Treat 2016\n\n\n\n\nOct 4, 2016\n\n\nSecondary Suites and Taxes\n\n\n\n\nSep 14, 2016\n\n\nMeasuring Housing Affordability\n\n\n\n\nAug 31, 2016\n\n\nQuick Guide to Income Data\n\n\n\n\nAug 24, 2016\n\n\nMobi Running Stats\n\n\n\n\nAug 16, 2016\n\n\nMobi – a First Look\n\n\n\n\nAug 4, 2016\n\n\nWhat’s up with RT?\n\n\n\n\nJul 26, 2016\n\n\nBike Share Map\n\n\n\n\nJul 6, 2016\n\n\nMixing Data\n\n\n\n\nJun 17, 2016\n\n\nSDH Zoning and Land Use\n\n\n\n\nMay 20, 2016\n\n\nFSR\n\n\n\n\nMay 16, 2016\n\n\nMy Global Bike Map\n\n\n\n\nMay 4, 2016\n\n\nCensus Mapping for Everyone\n\n\n\n\nMay 2, 2016\n\n\nCensus Week\n\n\n\n\nApr 20, 2016\n\n\nTOD Mode Share\n\n\n\n\nApr 6, 2016\n\n\nTOD\n\n\n\n\nApr 1, 2016\n\n\nOn Houses and Dirt\n\n\n\n\nMar 28, 2016\n\n\nOn Mixed Use\n\n\n\n\nMar 27, 2016\n\n\nSurrey Traffic Loop Counts\n\n\n\n\nMar 8, 2016\n\n\nUnoccupied Dwellings\n\n\n\n\nMar 5, 2016\n\n\nPhysical SFH Form Over Time\n\n\n\n\nMar 2, 2016\n\n\nProperty Taxes and Land Use\n\n\n\n\nFeb 29, 2016\n\n\nLand Use, Roads (and Parking)\n\n\n\n\nFeb 10, 2016\n\n\nOn Condos\n\n\n\n\nFeb 1, 2016\n\n\nLoss of Character\n\n\n\n\nJan 31, 2016\n\n\nLand Use Data\n\n\n\n\nJan 24, 2016\n\n\nWork vs Twiddling Thumbs\n\n\n\n\nJan 18, 2016\n\n\nOn Teardowns\n\n\n\n\nJan 17, 2016\n\n\nUpdated Vancouver Assessment Data\n\n\n\n\nDec 27, 2015\n\n\nCanvas vs SVG\n\n\n\n\nDec 15, 2015\n\n\nBike Data\n\n\n\n\nDec 14, 2015\n\n\nBike Routing\n\n\n\n\nDec 13, 2015\n\n\nHow to make a bike map\n\n\n\n\nOct 24, 2015\n\n\nCensus Drilldown\n\n\n\n\nSep 28, 2015\n\n\nCensus Mapper\n\n\n\n\nAug 25, 2015\n\n\nThe Hidden Mortgage\n\n\n\n\nJun 23, 2015\n\n\nVancouver 2011 Census Data on Housing\n\n\n\n\nMay 31, 2015\n\n\nTax Density in Vancouver\n\n\n\n\nApr 1, 2015\n\n\nVancouver Bike Paths\n\n\n\n\nMar 23, 2015\n\n\nVancouver Assessment Data\n\n\n\n\nNov 5, 2014\n\n\nMapping CACs\n\n\n\n\nSep 7, 2014\n\n\nVancouver High Value Improvements Map\n\n\n\n\nSep 5, 2014\n\n\nVancouver Teardown Map\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Mountain Doodles is an insomnia driven side project, born out of random questions, trying to give partial answers through data. During daytime at MountainMath."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mountain Doodles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMigrating to quarto\n\n\n\n\n\n\ngeeky\n\n\n\nSome overdue housekeeping and a new look for the blog.\n\n\n\n\n\nApr 11, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if recent apartment buildings in Vancouver were 20% taller?\n\n\n\n\n\n\naffordability\n\n\nVancouver\n\n\ncmhc\n\n\n\nWe estimate that planning decisions preventing apartment buildings built in the past 5 years in Metro Vancouver from being on average 20% taller are resulting in an annual redistribution of income from renters to existing landlords on the order of half a billion dollars across the region via higher rents.\n\n\n\n\n\nApr 11, 2024\n\n\nJens von Bergmann, Nathan Lauster\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nFirst time buyer Lorenz curves revisited\n\n\n\n\n\n\naffordability\n\n\ncansim\n\n\nVancouver\n\n\nToronto\n\n\nPUMF\n\n\n\nTaking another look at first time buyer affordability: updating with 2021 data, accounting for property taxes, and introducing a discretized version of the measure.\n\n\n\n\n\nJan 16, 2024\n\n\nKeith Stewart, Jens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Outcomes\n\n\n\n\n\n\naffordability\n\n\ncansim\n\n\ncmhc\n\n\ncancensus\n\n\nVancouver\n\n\nToronto\n\n\n\nExisting households are partially outcomes of our housing pressures, and basing analysis soley on households introduces collider bias. Which is substantial in tight housing markets and this misspecification can lead to misguided analysis and faulty policy recommendations.\n\n\n\n\n\nAug 17, 2023\n\n\nJens von Bergmann, Nathan Lauster\n\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\n\nHousing targets\n\n\n\n\n\n\naffordability\n\n\nVancouver\n\n\nzoning\n\n\n\nTaking a systematic look at how to set housing targets aimed at counteracting restrictive municipal housing policies, and what that means for Vancouver.\n\n\n\n\n\nJun 27, 2023\n\n\nJens von Bergmann, Nathan Lauster\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nMetro Vancouver Planning Regimes\n\n\n\n\n\n\ncansim\n\n\ncancensus\n\n\naffordability\n\n\n\nTaking a look at Metro Vancouver planning around housing and population growth.\n\n\n\n\n\nJun 20, 2023\n\n\nJens von Bergmann, Nathan Lauster\n\n\n27 min\n\n\n\n\n\n\n\n\n\n\n\n\nInvesting in definitions and framing\n\n\n\n\n\n\ncansim\n\n\nnewsfail\n\n\n\nTaking a closer look at the CHSP data release and clearing up some misunderstandings.\n\n\n\n\n\nFeb 20, 2023\n\n\nJens von Bergmann, Nathan Lauster\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Brief History of Vancouver Planning & Development Regimes\n\n\n\n\n\n\naffordability\n\n\ncmhc\n\n\nVancouver\n\n\nzoning\n\n\nrental\n\n\n\nTracking how Vancouver regulations for development have changed over time, illustrated with two examples.\n\n\n\n\n\nFeb 6, 2023\n\n\nJens von Bergmann, Nathan Lauster\n\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Ballot Composition in Vancouver\n\n\n\n\n\n\ngeeky\n\n\nVancouver\n\n\n\nBreaking down the individual ballot data for the 2022 Vancouver municipal election.\n\n\n\n\n\nDec 17, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nNew Premier New Housing Policy\n\n\n\n\n\n\naffordability\n\n\nEmpty Homes\n\n\ntaxes\n\n\nzoning\n\n\nrental\n\n\n\nBreaking down the housing policy announcements.\n\n\n\n\n\nNov 24, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nStill Short: Suppressed Households in 2021\n\n\n\n\n\n\naffordability\n\n\ncansim\n\n\ncmhc\n\n\nVancouver\n\n\nToronto\n\n\nrental\n\n\nPUMF\n\n\n\nChecking in on household suppression in Canada using 2021 data.\n\n\n\n\n\nOct 3, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhere did all the cheap rents go?\n\n\n\n\n\n\naffordability\n\n\ncansim\n\n\ncmhc\n\n\nrental\n\n\n\nAn investigation into the 320k “lost” units renting below $750.\n\n\n\n\n\nSep 27, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n26 min\n\n\n\n\n\n\n\n\n\n\n\n\nRent growth in GDP\n\n\n\n\n\n\naffordability\n\n\ncansim\n\n\nrental\n\n\n\nPeople in BC spend a lot of money on rent (and imputed rent), and that’s a problem. The way to decrease this “Real Estate Industry” share of GDP is to build more housing.\n\n\n\n\n\nSep 12, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n25 Years of Structural Change\n\n\n\n\n\n\ncancensus\n\n\nVancouver\n\n\nToronto\n\n\nland use\n\n\ndensity\n\n\ndotdensity\n\n\n\nTaking the long view on changes in our dwelling stock by structural type.\n\n\n\n\n\nAug 18, 2022\n\n\nNathan Lauster, Jens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nTumbling turnover\n\n\n\n\n\n\nrental\n\n\ncmhc\n\n\ncancensus\n\n\ncansim\n\n\naffordability\n\n\nmobility\n\n\nPUMF\n\n\n\nDigging deeper into Canadian residential mobility, tracking changes in mobility over time, and comparing data sources.\n\n\n\n\n\nJul 29, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nA brief history of Canadian real estate investors\n\n\n\n\n\n\nPUMF\n\n\ncanbank\n\n\nwealth\n\n\n\nRecently has been lots of talk about real estate investors, a good time to look at investors over time.\n\n\n\n\n\nJun 30, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nIns and outs of CMHC data\n\n\n\n\n\n\ncmhc\n\n\n\nCMHC produces and curates important data on housing in Canada. An overview over some of this data, it’s quirks, and how to access it.\n\n\n\n\n\nJun 12, 2022\n\n\nJens von Bergmann\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nResidential mobility in Canada\n\n\n\n\n\n\ncancensus\n\n\nrental\n\n\nPUMF\n\n\nVancouver\n\n\nToronto\n\n\n\nResidential mobility is essential for family formation, accommodating life changes, and the economy. A look at residential mobility in Canada over time.\n\n\n\n\n\nMay 27, 2022\n\n\nJens von Bergmann\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nNanaimo Station\n\n\n\n\n\n\nzoning\n\n\nVancouver\n\n\nTransportation\n\n\ncancensus\n\n\ndensity\n\n\ntongfen\n\n\n\nVancouver has been squandering opportunities around existing Skytrain stations. We take a closer look at the Nanaimo Station area.\n\n\n\n\n\nMay 21, 2022\n\n\nJens von Bergmann\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn Broadway\n\n\n\n\n\n\nzoning\n\n\nVancouver\n\n\nTransportation\n\n\naffordability\n\n\ndensity\n\n\n\nThe Broadway Plan is coming before council, time for a review of what’s being proposed, which parts are good and which might need work, and how that fits into the historical context.\n\n\n\n\n\nMay 17, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nChildren are good, actually\n\n\n\n\n\n\naffordability\n\n\nVancouver\n\n\ntongfen\n\n\nland use\n\n\nzoning\n\n\nAssessment Data\n\n\nCensusMapper\n\n\ncancensus\n\n\ncansim\n\n\n\nCities are changing, how do we know if we are headed in the right direction? Looking at the change in children gives us a simple uncontroversial metric to assess that, most people can agree that children are good for cities.\n\n\n\n\n\nMay 11, 2022\n\n\nJens von Bergmann\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Suppressed Household Formation\n\n\n\n\n\n\naffordability\n\n\ncancensus\n\n\ncmhc\n\n\nPUMF\n\n\nToronto\n\n\nVancouver\n\n\n\nHousehold formation is a complex process that is impacted by many factors. We explore the variation in household maintainer rates across Canada to estimate the CMA-level effects on household maintainer rates and suppressed household formation using Montréal as a counterfactual, paying attention to differences in age structure and cultural aspects.\n\n\n\n\n\nMay 6, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\n\nPlanning for scarcity\n\n\n\n\n\n\naffordability\n\n\nVancouver\n\n\nzoning\n\n\n\nVancouver’s planning regime is set up to reinforce housing scarcity A closer look how Vancouver plans for growth, what’s wrong with it, and some ideas how to fix it.\n\n\n\n\n\nApr 26, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s up with Squamish?\n\n\n\n\n\n\naffordability\n\n\ncancensus\n\n\nCensusMapper\n\n\ntongfen\n\n\n\nSquamish’s dwelling stock grew faster than their population, what does that mean?\n\n\n\n\n\nMar 30, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nUBCM Shenanigans\n\n\n\n\n\n\naffordability\n\n\ncansim\n\n\nVancouver\n\n\n\nTaking a look at UBCM’s misguided report.\n\n\n\n\n\nMar 29, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nVacancy rates and rent change, 2021 update\n\n\n\n\n\n\naffordability\n\n\ncmhc\n\n\nVancouver\n\n\nrental\n\n\n\nThe 2021 CMHC Rental Market Survey data finally arrived. Time to do a refresh of our vacancy rate vs rent change analysis. And take a look at where Vancouver is at.\n\n\n\n\n\nFeb 18, 2022\n\n\nJens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensus quirks; using UBC area as an example\n\n\n\n\n\n\ncansim\n\n\nCensusMapper\n\n\nVancouver\n\n\n\nCensus data is great. But census data also has lots of little quirks. We take the Point Grey Peninsula as an example to show how census data can go sideways.\n\n\n\n\n\nFeb 17, 2022\n\n\nJens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nDeadbeat Neighbourhoods\n\n\n\n\n\n\nVancouver\n\n\ntongfen\n\n\ncancensus\n\n\n\nPopulation change in Vancouver’s neighbourhoods 2016-2021. A first look.\n\n\n\n\n\nFeb 15, 2022\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnoccupied Canada\n\n\n\n\n\n\ncancensus\n\n\nEmpty Homes\n\n\nVancouver\n\n\n\nThe census ‘unoccupied by usual residents’ metric is often in the news and one of the most frequentliy misrepresnted parts of census data. A quick primer on what does does and does not say.\n\n\n\n\n\nFeb 14, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nDeadbeat zoning\n\n\n\n\n\n\naffordability\n\n\nCensusMapper\n\n\ntongfen\n\n\nVancouver\n\n\ncancensus\n\n\ndensity\n\n\n\nWith the new 2021 census data out it’s time for some analysis on how Vancouver has grown. For this time we will examine the role of low-density zoning.\n\n\n\n\n\nFeb 11, 2022\n\n\nJens von Bergmann\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nCanada’s 2021 census, part 1\n\n\n\n\n\n\nCensusMapper\n\n\ncancensus\n\n\n\nThe first tranche of the 2021 census data has arrived. Here is a quick rundown.\n\n\n\n\n\nFeb 9, 2022\n\n\nJens von Bergmann\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nNo shortage in Housing BS\n\n\n\n\n\n\naffordability\n\n\ncancensus\n\n\ncansim\n\n\nCensusMapper\n\n\nEmpty Homes\n\n\nnewsfail\n\n\nVancouver\n\n\n\nThere is a special brain worm making the rounds that Vancouver supply has been outstripping demand. Which is obvious nonsense, but maybe still deserves a detailed takedown.\n\n\n\n\n\nJan 31, 2022\n\n\nJens von Bergmann, Nathan Lauster\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Peek at Population and Household Data During COVID & Caveats\n\n\n\n\n\n\ncancensus\n\n\ncansim\n\n\ncmhc\n\n\ncovid-19\n\n\ngeeky\n\n\nVancouver\n\n\n\nA deeper look at population and hosehold estimates, how thes estimates are made and what early data can tell us about shifts during COVID times.\n\n\n\n\n\nNov 28, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nThree Years of Speculation & Vacancy Tax Data\n\n\n\n\n\n\ncansim\n\n\ntaxes\n\n\nEmpty Homes\n\n\n\nWe now have three years of SVT data, time to take a look at where things stand, how this fits in with related datasets, and what we can learn.\n\n\n\n\n\nNov 21, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nActing locally on housing\n\n\n\n\n\n\ncansim\n\n\naffordability\n\n\n\nHousing faces lots of challenges, some of the global. But our policy levers are mostly local, increasing housing supply is the most effective (and ethical) local action we can take.\n\n\n\n\n\nNov 20, 2021\n\n\nNathan Lauster, Jens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensusMapper (p)review\n\n\n\n\n\n\nCensusMapper\n\n\n\nCensusMapper is six years old now. A review, and a preview where things could be heading.\n\n\n\n\n\nOct 17, 2021\n\n\nJens von Bergmann\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nFixing parking\n\n\n\n\n\n\nVancouver\n\n\ntaxes\n\n\nTransportation\n\n\nzoning\n\n\ncancensus\n\n\nCensusMapper\n\n\naffordability\n\n\n\nPaving the way for removing minimum parking requirements.\n\n\n\n\n\nOct 3, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nElections fun - 2021 edition\n\n\n\n\n\n\ngeeky\n\n\n\nPlaying with Canadian 2021 federal elections data.\n\n\n\n\n\nSep 25, 2021\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nTransnational property ownership in Canada\n\n\n\n\n\n\ncansim\n\n\nPUMF\n\n\nToronto\n\n\nVancouver\n\n\n\nWe talk a lot about people living abroad owning property in Canada, let’s take a look at Canadians owning property abroad.\n\n\n\n\n\nSep 7, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on vaccine effectivess estimates\n\n\n\n\n\n\ncovid-19\n\n\n\nWe now have some data on vaccination status of COVID cases and hospitalizations in BC. It’s not really enough for robust vaccine effectivness estimates, but given the public interest let’s see how far we can get.\n\n\n\n\n\nAug 30, 2021\n\n\nJens von Bergmann\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nSatellites, Sprawl, and City Six-Packs\n\n\n\n\n\n\nland use\n\n\n\nFun with global satellite-derived land use time series.\n\n\n\n\n\nAug 21, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nCommodity and Keeping it in the Family\n\n\n\n\n\n\ncansim\n\n\nCensusMapper\n\n\n\nA look at non-market transfers of market properties.\n\n\n\n\n\nAug 13, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nLots of Opportunity: Estimating the Zoning Tax in Vancouver\n\n\n\n\n\n\naffordability\n\n\nAssessment Data\n\n\ndensity\n\n\nland use\n\n\nVancouver\n\n\nzoning\n\n\n\nZoning bylaws restrict the size and frontage of lots, preventing lots from getting subdivided. The opportunity cost of freezing City of Vancouver land use in RS zoned areas in amber is enormous, it amounts to around $40 billion from preventing 2:1 lots splits, and an additional $100 billion from preventing further subdivision beyond that.\n\n\n\n\n\nJul 25, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nBasement Confidential: Vancouver’s Informal Housing Stock\n\n\n\n\n\n\naffordability\n\n\ncancensus\n\n\nEmpty Homes\n\n\nland use\n\n\nrental\n\n\nVancouver\n\n\nzoning\n\n\n\nBasement suites are the Schrödinger’s cat of dwelling units, they span the space betweem formal and informal housing, viewed by some as the problem of, and by others as the solution to Vancouver’s housing woes.\n\n\n\n\n\nJun 8, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nForced Out in Canada: New Data from CHS\n\n\n\n\n\n\naffordability\n\n\nrental\n\n\nVancouver\n\n\nPUMF\n\n\n\nTaking a deeper look at forced moves using CHS microdata. And how it changed over time.\n\n\n\n\n\nMar 29, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nVaxx vs VOCs\n\n\n\n\n\n\ncovid-19\n\n\n\nVaccines are here to rescue us, but variants of concern threaten to spoil it. What to expect in BC over the next two months as vaccines and variants of concern battle for dominance.\n\n\n\n\n\nMar 10, 2021\n\n\nJens von Bergmann\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nData variants\n\n\n\n\n\n\ncovid-19\n\n\n\nWe don’t have data on variants of concern in BC, so let’s take a look at different variants of building proxies.\n\n\n\n\n\nMar 4, 2021\n\n\nJens von Bergmann\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn COVID and exponential growth\n\n\n\n\n\n\ncovid-19\n\n\n\nWith BC public health officials doubting mathematical modelling it might be time again for an explainer of exponential growth.\n\n\n\n\n\nFeb 21, 2021\n\n\nJens von Bergmann\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nIndustrial Strength Zombies: Vancouver Edition\n\n\n\n\n\n\nCensusMapper\n\n\ncancensus\n\n\nVancouver\n\n\nSurrey\n\n\n\nWhat industries are dominant in Vancouver? People throw around all kinds of crazy stories, time take a look at the data and put some zombies to rest. At least for a day or two before someone else digs them up again.\n\n\n\n\n\nFeb 10, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nBartholomew’s dot destiny\n\n\n\n\n\n\ndensity\n\n\ndotdensity\n\n\nVancouver\n\n\nzoning\n\n\n\nBartholomew made projections of what a Central Vancouver penninsula (UBC, Musqueam 2, Vancouver, Burnaby, New Westminster) with 1 million people would look like. We no just about hit that number time to compare how his projections stock up.\n\n\n\n\n\nFeb 3, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn COVID Trend lines\n\n\n\n\n\n\ncovid-19\n\n\ngeeky\n\n\n\nTrend lines help us distinguish noise from the signal in data. COVID-19 case trend lines is an important tool for understanding where we are at, how we got there and which direction we are trending. This deserves more attention than it has been getting.\n\n\n\n\n\nJan 30, 2021\n\n\nJens von Bergmann\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nRethinking the “foreignness” of owners living abroad\n\n\n\n\n\n\ncansim\n\n\nEmpty Homes\n\n\ntaxes\n\n\nVancouver\n\n\n\nComparing CHSP and SVT data we try to tease out how foreign our forein owners really are.\n\n\n\n\n\nJan 25, 2021\n\n\nJens von Bergmann, Nathan Lauster\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nCapital Gains Income\n\n\n\n\n\n\ncansim\n\n\nVancouver\n\n\ncancensus\n\n\ntaxes\n\n\n\nIncome concepts in Canada generally only include regular income and in particular miss (taxable) capital gains. But capital gains makes up an important income source and we should pay more attention to it.\n\n\n\n\n\nJan 15, 2021\n\n\nJens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver’s pandemic weather\n\n\n\n\n\n\ncovid-19\n\n\nVancouver\n\n\n\nThe weather during the 2020 pandemic, especially the fall and winter, has felt worse than usual. How much of that is just perception and how much is real?\n\n\n\n\n\nJan 6, 2021\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBC back to (COVID) school\n\n\n\n\n\n\ncovid-19\n\n\n\nSchool is about to re-start after Christmas break, lots of questions remain.\n\n\n\n\n\nJan 2, 2021\n\n\nJens von Bergmann, HsingChi von Bergmann\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nCovid testing data in BC\n\n\n\n\n\n\ncovid-19\n\n\n\nTesting data in BC shoud be straighforward to interpret, but it’s surprisingly tricky.\n\n\n\n\n\nDec 21, 2020\n\n\nJens von Bergmann\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to Expect from an Empty Homes Tax\n\n\n\n\n\n\ncancensus\n\n\nEmpty Homes\n\n\n\nWith more Canadian cities announcing their interestest in an Empty Homes Tax, what should they expect?\n\n\n\n\n\nDec 7, 2020\n\n\nNathan Lauster, Jens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nTongFen\n\n\n\n\n\n\ntongfen\n\n\nCensusMapper\n\n\ncancensus\n\n\n\nTongfen is now on CRAN, time for a short overview of what tongfen is and how it aids research on longitudinal spatial data on different yet congruent geographies.\n\n\n\n\n\nNov 10, 2020\n\n\nJens von Bergmann\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 Data in BC\n\n\n\n\n\n\ncovid-19\n\n\n\nLooking into what’s going wrong with BC Covid data\n\n\n\n\n\nOct 22, 2020\n\n\nJens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nCHS Core Housing Need\n\n\n\n\n\n\naffordability\n\n\ncansim\n\n\n\nA quick overview over the freshly released Canadian Housing Survey data\n\n\n\n\n\nOct 2, 2020\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFirst-time buyer Lorenz curves\n\n\n\n\n\n\ngeeky\n\n\naffordability\n\n\n\nTaking a look at affordability for first-time buyers, and what useful metrics can be constructed to measure this.\n\n\n\n\n\nSep 28, 2020\n\n\nKeith Stewart, Jens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nIncome mixing and segregation\n\n\n\n\n\n\ncansim\n\n\ncancensus\n\n\ngeeky\n\n\n\nTaking a look at the new StatCan D-index and related income mixing metrics\n\n\n\n\n\nSep 21, 2020\n\n\nJens von Bergmann\n\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\n\nCovid school modelling\n\n\n\n\n\n\ncovid-19\n\n\n\nLooking at the impact of different test, trace and isolation protocols.\n\n\n\n\n\nSep 8, 2020\n\n\nJens von Bergmann\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nKeeping the Leavers\n\n\n\n\n\n\nVancouver\n\n\nEmpty Homes\n\n\n\nSome people have a hard time making room for newcomers, but how about making room for people to stay?\n\n\n\n\n\nAug 27, 2020\n\n\nJens von Bergmann, Nathan Lauster\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nCovid Series 3 Survey\n\n\n\n\n\n\ncovid-19\n\n\ncansim\n\n\n\nA quick tour through Covid series 3 survey data.\n\n\n\n\n\nAug 25, 2020\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nCanadian 1996 Census\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\n\nExpanding timelines, the Canadian 1996 census is now available on CensusMapper and via {cancensus}.\n\n\n\n\n\nJul 6, 2020\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn mixing covid-19 and census data\n\n\n\n\n\n\ncancensus\n\n\ncovid-19\n\n\n\nDoing this with Canadian data is a tall order. But since people are doing it, we might as well explain some of the ins and outs.\n\n\n\n\n\nMay 27, 2020\n\n\nJens von Bergmann\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nProjections and self-fulfilling prophecies\n\n\n\n\n\n\ncancensus\n\n\ncmhc\n\n\nVancouver\n\n\n\nHousing and population growth are endogenous in high-demand areas. Which gives cities the tools to exclude people, but should they? Deciding how to grow is a values question, not a technocratic one.\n\n\n\n\n\nMay 25, 2020\n\n\nJens von Bergmann, Nathan Lauster\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nToward universal TongFen: Change in polling district voting patterns\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\ntongfen\n\n\n\nExpanding tongfen to arbitrary geometries, with an example application to Canadian federal election polling districts.\n\n\n\n\n\nMay 20, 2020\n\n\nJens von Bergmann\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 data in Canada, effective reproduction rates\n\n\n\n\n\n\ncovid-19\n\n\n\nA follow-up on what data we need.\n\n\n\n\n\nMay 13, 2020\n\n\nJens von Bergmann\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensus tract level T1FF tax data\n\n\n\n\n\n\ncancensus\n\n\ncansim\n\n\nCensusMapper\n\n\n\nCanadian T1 taxfiler data 2000 to 2017 on census tracts as open data.\n\n\n\n\n\nApr 23, 2020\n\n\nJens von Bergmann\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID deaths in context by weeks\n\n\n\n\n\n\ncovid-19\n\n\n\nAn updated view at covid-19 mortality and excess mortality.\n\n\n\n\n\nApr 19, 2020\n\n\nJens von Bergmann, Nathan Lauster\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCovid-19 data in Canada\n\n\n\n\n\n\ncovid-19\n\n\n\nCovid-19 data for Canada is hard to come by. Where to get the data, what it tells us and what’s missing.\n\n\n\n\n\nApr 10, 2020\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nContext for Covid-19 mortality so far\n\n\n\n\n\n\ncansim\n\n\ncovid-19\n\n\n\nCovid-19 related deaths are mounting, \n\n\n\n\n\nMar 31, 2020\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nBehaviour change in response to COVID-19\n\n\n\n\n\n\nSurrey\n\n\ncovid-19\n\n\ngeeky\n\n\nTransportation\n\n\n\nLooking into real-time metrics to measure behaviour change.\n\n\n\n\n\nMar 22, 2020\n\n\nJens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nKnock Knock, Anybody Home?\n\n\n\n\n\n\ncancensus\n\n\nEmpty Homes\n\n\n\nEmpty Homes Taxes can be useful, but let’s keep the numbers and expectations straight.\n\n\n\n\n\nMar 9, 2020\n\n\nNathan Lauster, Jens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nOvernight Visitors and Crude Travel Vectors\n\n\n\n\n\n\nVancouver\n\n\nTransportation\n\n\ncovid-19\n\n\ngeeky\n\n\n\nChecking in on Vancouver travel data. And the novel corona virus.\n\n\n\n\n\nMar 3, 2020\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nWealth vs income\n\n\n\n\n\n\ngeeky\n\n\n\nWealth and income are not the same thing. And it matters. Especially in BC.\n\n\n\n\n\nFeb 19, 2020\n\n\nNathan Lauster, Jens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nDisaster response maps\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\n\nSome thoughts on the recent StatCan effort to leverage their data for disaster response efforts.\n\n\n\n\n\nFeb 2, 2020\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nMythical oversupply\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nEmpty Homes\n\n\nnewsfail\n\n\nVancouver\n\n\n\nGoing back to the ‘supply myth’ well.\n\n\n\n\n\nJan 27, 2020\n\n\nJens von Bergmann\n\n\n26 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnoccupied dwellings data\n\n\n\n\n\n\nCensusMapper\n\n\nVancouver\n\n\nToronto\n\n\ncancensus\n\n\nEmpty Homes\n\n\n\nA publicly available xtab for Structural type by Document type.\n\n\n\n\n\nJan 26, 2020\n\n\nJens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nKeep on moving\n\n\n\n\n\n\ncansim\n\n\nrental\n\n\nVancouver\n\n\n\nMore on reasons to move.\n\n\n\n\n\nJan 24, 2020\n\n\nNathan Lauster, Jens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n2019 CMHC Rental Market Survey\n\n\n\n\n\n\ncancensus\n\n\ncansim\n\n\ncmhc\n\n\nrental\n\n\nVancouver\n\n\n\nChecking in with the new Rms data.\n\n\n\n\n\nJan 15, 2020\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFlow Maps\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nTransportation\n\n\n\nFun with flow maps.\n\n\n\n\n\nJan 6, 2020\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nFun with parking tickets\n\n\n\n\n\n\nVancouver\n\n\nTransportation\n\n\n\nLooking for excuses to showcase my {VancouvR} R package to access Vancouver Open Data.\n\n\n\n\n\nDec 9, 2019\n\n\nJens von Bergmann\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nProperty tax snacks\n\n\n\n\n\n\ngeeky\n\n\nVancouver\n\n\ntaxes\n\n\n\nA short post munching through some property tax musings.\n\n\n\n\n\nNov 26, 2019\n\n\nJens von Bergmann, Nathan Lauster\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCanadian Housing Survey\n\n\n\n\n\n\ncansim\n\n\nrental\n\n\nVancouver\n\n\nToronto\n\n\n\nTaking a first look at the new Canadian Housing Survey data\n\n\n\n\n\nNov 23, 2019\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCommuter growth\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nland use\n\n\nVancouver\n\n\nSurrey\n\n\nTransportation\n\n\nzoning\n\n\n\nAs our population and jobs grow, so do commuters. Taking a look how commuters grow.\n\n\n\n\n\nOct 29, 2019\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nElections fun\n\n\n\n\n\n\ngeeky\n\n\n\nPlaying with Canadian 2019 federal elections data.\n\n\n\n\n\nOct 22, 2019\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nRents and incomes\n\n\n\n\n\n\ncansim\n\n\ncmhc\n\n\ncancensus\n\n\nVancouver\n\n\nrental\n\n\n\nIn Vancouver (both City or Metro), median rents have tracked median incomes quite well. (Although some misleading information that is making the rounds suggests otherwise.)\n\n\n\n\n\nOct 17, 2019\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nRents and vacancy rates\n\n\n\n\n\n\ncmhc\n\n\nrental\n\n\nVancouver\n\n\n\nAdding some context to vacancy rates by rent segment.\n\n\n\n\n\nOct 16, 2019\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial autocorrelation & co\n\n\n\n\n\n\ngeeky\n\n\n\nCommon (and commonly ignored) problems in spatial analysis.\n\n\n\n\n\nOct 7, 2019\n\n\nJens von Bergmann\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nJob vacancies\n\n\n\n\n\n\ncansim\n\n\nVancouver\n\n\n\nThe missing part of the Labour Force Survey\n\n\n\n\n\nSep 6, 2019\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nLow income vs new dwellings\n\n\n\n\n\n\ncancensus\n\n\ngeeky\n\n\nVancouver\n\n\nToronto\n\n\ndensity\n\n\n\nDoes adding homes decrease the low income population? A look at the Canadian data.\n\n\n\n\n\nSep 2, 2019\n\n\nJens von Bergmann\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nRunning on Empties\n\n\n\n\n\n\ncancensus\n\n\nVancouver\n\n\nnewsfail\n\n\nEmpty Homes\n\n\n\nPutting Canadian empty homes data into context.\n\n\n\n\n\nAug 19, 2019\n\n\nJens von Bergmann, Nathan Lauster\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nMIRHPP tradeoffs\n\n\n\n\n\n\ncancensus\n\n\nrental\n\n\nVancouver\n\n\n\nHow does MIRHPP work, and what are the tradeoffs?\n\n\n\n\n\nAug 12, 2019\n\n\nJens von Bergmann\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn Vancouver population projections\n\n\n\n\n\n\ncancensus\n\n\ncmhc\n\n\nVancouver\n\n\ncansim\n\n\n\nA closer look at the the Regional Growth Strategy and population projections\n\n\n\n\n\nAug 1, 2019\n\n\nJens von Bergmann\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nTaxing Toxic Demand: Early Results\n\n\n\n\n\n\ncansim\n\n\nEmpty Homes\n\n\ntaxes\n\n\nVancouver\n\n\n\nChecking in on the Speculation and Vacancy Tax preliminary data.\n\n\n\n\n\nJul 14, 2019\n\n\nNathan Lauster, Jens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nShaughnessy Townhomes\n\n\n\n\n\n\ncancensus\n\n\nrental\n\n\nVancouver\n\n\nzoning\n\n\n\nSome thoughts on council’s vote to kill the Shaughnessy townhomes.\n\n\n\n\n\nJul 7, 2019\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow not to analyze the roots of the affordability crisis\n\n\n\n\n\n\ncancensus\n\n\ncansim\n\n\nnewsfail\n\n\nVancouver\n\n\n\nTaking a closer look at Josh Gordon’s “Solving Wozny’s Puzzle” working paper.\n\n\n\n\n\nJun 25, 2019\n\n\nJens von Bergmann\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrequent transit zoning\n\n\n\n\n\n\nOSM\n\n\nTransportation\n\n\nVancouver\n\n\nzoning\n\n\n\nQuantifying zoning in Vancouver’s frequent transit network.\n\n\n\n\n\nJun 21, 2019\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSome notes on investor and corporate ownership of residential properties\n\n\n\n\n\n\ncansim\n\n\n\nA quick post on the recent CHSP data release.\n\n\n\n\n\nJun 18, 2019\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensus custom timelines\n\n\n\n\n\n\nCensusMapper\n\n\ndensity\n\n\nland use\n\n\nVancouver\n\n\n\nPlaying with fine geography custom tabulation back to 1971.\n\n\n\n\n\nJun 15, 2019\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Metrics for Deciding if You Have Enough Housing\n\n\n\n\n\n\n\n\nHow do we know if we have too much, the right amount, or too little housing?\n\n\n\n\n\nJun 12, 2019\n\n\nNathan Lauster, Jens von Bergmann\n\n\n33 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver population density over time\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\ndensity\n\n\nVancouver\n\n\nzoning\n\n\n\nA detailed look at growth in the City of Vancouver\n\n\n\n\n\nJun 9, 2019\n\n\nStuart Smith, Jens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Census Tongfen\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\n\nComparing multiple censuses across fine geographies.\n\n\n\n\n\nJun 4, 2019\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n2001 Census Data (and TongFen)\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nEmpty Homes\n\n\nVancouver\n\n\n\nA quick overview over the 2001 Census data newly imported into CensusMapper, as well as how to compare census data across censuses.\n\n\n\n\n\nJun 3, 2019\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation weighted densities\n\n\n\n\n\n\ndensity\n\n\n\nSimplifying density into a single number. And animating it.\n\n\n\n\n\nApr 24, 2019\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nA bedroom is a bedroom\n\n\n\n\n\n\nVancouver\n\n\nzoning\n\n\n\nHow can we compare density for differnt types of dwellings? Just look at the bedrooms!\n\n\n\n\n\nApr 20, 2019\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nVSB X-Boundary\n\n\n\n\n\n\nVancouver\n\n\ncancensus\n\n\n\nExamining VSB cross-boundary data.\n\n\n\n\n\nApr 15, 2019\n\n\nJens von Bergmann\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nDensity timelines\n\n\n\n\n\n\ndensity\n\n\n\nGlobal city density patterns across time.\n\n\n\n\n\nMar 27, 2019\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCity density patterns\n\n\n\n\n\n\ndensity\n\n\n\nHow do city densities compare around the globe?\n\n\n\n\n\nMar 17, 2019\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nAirbnb updates\n\n\n\n\n\n\nVancouver\n\n\nrental\n\n\n\nChecking in on the state of Airbnb in Vancouver.\n\n\n\n\n\nMar 2, 2019\n\n\nJens von Bergmann\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nTax Speculations\n\n\n\n\n\n\ncancensus\n\n\ncansim\n\n\nCensusMapper\n\n\nEmpty Homes\n\n\ntaxes\n\n\nVancouver\n\n\n\nEstimating what to expect from the Speculation and Vacancy Tax.\n\n\n\n\n\nFeb 27, 2019\n\n\nJens von Bergmann, Nathan Lauster\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nPlanned displacement\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nland use\n\n\nOSM\n\n\nTransportation\n\n\nVancouver\n\n\nzoning\n\n\ndensity\n\n\nrental\n\n\n\nHow can we ensure that densifying Vancouver won’t be taken out on the backs of the most vulnerable?\n\n\n\n\n\nFeb 21, 2019\n\n\nJens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver renters\n\n\n\n\n\n\ncancensus\n\n\ncansim\n\n\nCensusMapper\n\n\nrental\n\n\nVancouver\n\n\n\nUnderstanding renter households in Vancouver\n\n\n\n\n\nFeb 15, 2019\n\n\nJens von Bergmann\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver election individual ballots\n\n\n\n\n\n\nVancouver\n\n\n\nIndividual ballot data is great, let’s put it to work!\n\n\n\n\n\nFeb 4, 2019\n\n\nJens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no Brain Drain, but there might be Zombies\n\n\n\n\n\n\ncancensus\n\n\nnewsfail\n\n\nVancouver\n\n\n\nDigging into yet another round of claims of some group of people leaving Vancouver.\n\n\n\n\n\nFeb 3, 2019\n\n\nJens von Bergmann, Nathan Lauster\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fleecing of Canadian Millenials\n\n\n\n\n\n\ncansim\n\n\n\nIncome and net worth by age group in Canada.\n\n\n\n\n\nJan 31, 2019\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nHigh-value homes\n\n\n\n\n\n\ncancensus\n\n\nVancouver\n\n\ncansim\n\n\n\nWho lives in Metro Vancouver’s high-value homes?\n\n\n\n\n\nJan 9, 2019\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow are condos used?\n\n\n\n\n\n\ncancensus\n\n\ncmhc\n\n\nrental\n\n\nVancouver\n\n\nToronto\n\n\n\nComparing how condos are used across Canada.\n\n\n\n\n\nDec 17, 2018\n\n\nJens von Bergmann, Nathanael Lauster, Douglas Harris\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nMoving Penalty\n\n\n\n\n\n\ncmhc\n\n\nToronto\n\n\nVancouver\n\n\nrental\n\n\n\nWhat’s the penalty to move f you are renting in a purpose-built rental building in Canada.\n\n\n\n\n\nNov 28, 2018\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nVacancy rate and rent change\n\n\n\n\n\n\ncmhc\n\n\ncansim\n\n\nVancouver\n\n\nToronto\n\n\nrental\n\n\n\nThe relationship between rent change and vacancy rates in Canada.\n\n\n\n\n\nNov 28, 2018\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding income distributions across geographies and time\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\ncansim\n\n\nVancouver\n\n\nToronto\n\n\n\nSome thoughts on income distributions and labelling neighbrourhoods as low, middle, or high income: Most of our neighbourhoods are neither, they are truely mixed-income.\n\n\n\n\n\nOct 28, 2018\n\n\nJens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nToronto wards\n\n\n\n\n\n\nToronto\n\n\ndotdensity\n\n\nCensusMapper\n\n\ncancensus\n\n\ngeeky\n\n\n\nFun with Toronto wards and census data.\n\n\n\n\n\nOct 22, 2018\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe rise and fall of Vancouver eligible voters\n\n\n\n\n\n\nVancouver\n\n\nnewsfail\n\n\ncancensus\n\n\n\nRunning the numbers on eligible voters by neighbourhood.\n\n\n\n\n\nOct 17, 2018\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nNaked Buildings Map\n\n\n\n\n\n\nVancouver\n\n\n\nJust buildings.\n\n\n\n\n\nOct 16, 2018\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nCouncil candidate neighbourhoods\n\n\n\n\n\n\ncancensus\n\n\nVancouver\n\n\n\nComparing what neighbourhoods council candidates live in.\n\n\n\n\n\nOct 13, 2018\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAirbnb and STR licences\n\n\n\n\n\n\nVancouver\n\n\nrental\n\n\n\nChecking in on Aribnb operator complicance\n\n\n\n\n\nAug 21, 2018\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nGross migration\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nVancouver\n\n\n\nCounting those coming and leaving.\n\n\n\n\n\nAug 16, 2018\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nTaxing property instead of income in B.C.\n\n\n\n\n\n\ncancensus\n\n\ntaxes\n\n\nAssessment Data\n\n\n\nWhat if we were more like Washington and replaced the provinial income tax by a property tax?\n\n\n\n\n\nAug 10, 2018\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nActive Fire\n\n\n\n\n\n\nOSM\n\n\ngeeky\n\n\n\nPlaying with fire data.\n\n\n\n\n\nJul 28, 2018\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nFact-checking Vancouver’s Swamp Drainers\n\n\n\n\n\n\ncancensus\n\n\ncansim\n\n\nnewsfail\n\n\nVancouver\n\n\n\nSwampy facts: the dark, broken, and ugly side of housing talk in Vancouver.\n\n\n\n\n\nJul 22, 2018\n\n\nNathan Lauster, Jens von Bergmann\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Room\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\ndensity\n\n\nVancouver\n\n\n\nHow is low-density housing currently used?\n\n\n\n\n\nJul 17, 2018\n\n\nJens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nAboriginal overrepresentation in correctional services and police checks\n\n\n\n\n\n\ncansim\n\n\ncancensus\n\n\nVancouver\n\n\n\nThe long legacy of Canadian aboriginal relations\n\n\n\n\n\nJul 13, 2018\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSkytrain rents\n\n\n\n\n\n\ncancensus\n\n\nrental\n\n\nTransportation\n\n\nVancouver\n\n\n\nHow much is it to rent a 1 or 2 bedroom near a skytrain station?\n\n\n\n\n\nJun 21, 2018\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nInterprovincial Migration\n\n\n\n\n\n\ncansim\n\n\n\nPlaying with the cansim package to access and process NDM data.\n\n\n\n\n\nJun 14, 2018\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver Streets and Lanes\n\n\n\n\n\n\nVancouver\n\n\nTransportation\n\n\nland use\n\n\n\nHow much space is taken up by roads right-of-ways?\n\n\n\n\n\nJun 4, 2018\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nExtra School Tax Calculator\n\n\n\n\n\n\nAssessment Data\n\n\nVancouver\n\n\n\nApparently 8th grade math ability is inversly proportional to property values, here is a simple calculator for those still suffering from consequences of sub-par math education.\n\n\n\n\n\nMay 28, 2018\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nShort Term Rental Licenses\n\n\n\n\n\n\nVancouver\n\n\n\nA first look at STR license data\n\n\n\n\n\nMay 25, 2018\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nTeardowns and Emissions\n\n\n\n\n\n\nAssessment Data\n\n\ncancensus\n\n\ncmhc\n\n\ncansim\n\n\nland use\n\n\nVancouver\n\n\nzoning\n\n\n\nAnalysing the impact of Vancouver’s teardown cycle on carbon emissions.\n\n\n\n\n\nMay 23, 2018\n\n\nJens von Bergmann\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Height Profiles\n\n\n\n\n\n\ndensity\n\n\nland use\n\n\nVancouver\n\n\nToronto\n\n\n\nBuilding heights by distance from city centre.\n\n\n\n\n\nMay 11, 2018\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nA modest school-tax proposal\n\n\n\n\n\n\nAssessment Data\n\n\ntaxes\n\n\nVancouver\n\n\n\nLooking into the school tax outrage.\n\n\n\n\n\nMay 1, 2018\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrontage\n\n\n\n\n\n\nAssessment Data\n\n\nVancouver\n\n\nzoning\n\n\ndensity\n\n\n\nFrontages for commercial zoned properties in Vancouver.\n\n\n\n\n\nMar 28, 2018\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nTransit Data\n\n\n\n\n\n\nVancouver\n\n\ngeeky\n\n\nTransportation\n\n\n\nPlaying with our new R API wrapper for transit data from Transitland\n\n\n\n\n\nMar 12, 2018\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nExtra School Tax\n\n\n\n\n\n\nAssessment Data\n\n\ntaxes\n\n\nVancouver\n\n\n\nImpacts of the Extra School Tax in City of Vancouver\n\n\n\n\n\nFeb 28, 2018\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nTaxable Dwelling Density\n\n\n\n\n\n\nVancouver\n\n\nEmpty Homes\n\n\nAssessment Data\n\n\n\nWhen density maps mostly just show population density.\n\n\n\n\n\nFeb 24, 2018\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Permits\n\n\n\n\n\n\nVancouver\n\n\n\nTrawling through permit data.\n\n\n\n\n\nFeb 22, 2018\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nNeighbourhood Level Census Data\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nVancouver\n\n\n\nThe value of stable geographies.\n\n\n\n\n\nFeb 8, 2018\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReal Estate Industry\n\n\n\n\n\n\ncansim\n\n\n\nDigging through NAICS definitions.\n\n\n\n\n\nFeb 1, 2018\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nEmpty Suites\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nEmpty Homes\n\n\nVancouver\n\n\n\nWhat we have learned from the “Supply Myth” debate.\n\n\n\n\n\nJan 25, 2018\n\n\nJens von Bergmann\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nPedestrian counts, or when the kindergardener needs to use the bathroom\n\n\n\n\n\n\nSurrey\n\n\nTransportation\n\n\n\nBeg buttons.\n\n\n\n\n\nJan 12, 2018\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nLocal vs Overseas Investors\n\n\n\n\n\n\nVancouver\n\n\ncancensus\n\n\n\nCreative uses of non-resident owner data.\n\n\n\n\n\nJan 11, 2018\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nSchool Traffic\n\n\n\n\n\n\nSurrey\n\n\nTransportation\n\n\n\nMeasuring dropoff and pickup traffic.\n\n\n\n\n\nJan 9, 2018\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSome Thoughts on the “Supply Myth”\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\ncmhc\n\n\nVancouver\n\n\nEmpty Homes\n\n\n\nThe never ending discussion.\n\n\n\n\n\nDec 11, 2017\n\n\nJens von Bergmann\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s a Household?\n\n\n\n\n\n\nCensusMapper\n\n\ncancensus\n\n\n\nWhat kind of households does Vancouver have?\n\n\n\n\n\nDec 1, 2017\n\n\nJens von Bergmann\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nJourney To Work\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\n\nCity of Vancouver commuting and reverse commuting.\n\n\n\n\n\nNov 29, 2017\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnder Construction\n\n\n\n\n\n\ncmhc\n\n\n\nWe have record under construction, but we also have record length of construction.\n\n\n\n\n\nNov 28, 2017\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nByelection Data\n\n\n\n\n\n\nVancouver\n\n\n\nResults by voting place.\n\n\n\n\n\nNov 21, 2017\n\n\nJens von Bergmann\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nStatCan Web Maps\n\n\n\n\n\n\nCensusMapper\n\n\ncancensus\n\n\n\nWelcome to the world of web mapping!\n\n\n\n\n\nNov 15, 2017\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nRenting in Toronto\n\n\n\n\n\n\nrental\n\n\nToronto\n\n\n\nHow much space to you get for $1500 in Toronto.\n\n\n\n\n\nNov 7, 2017\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated Foreign Buyers Data\n\n\n\n\n\n\nVancouver\n\n\n\nForeign buyers data is back!\n\n\n\n\n\nNov 2, 2017\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Medians\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\n\nWhy overly focussing on medians is dangerous.\n\n\n\n\n\nNov 1, 2017\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nA First Look At Vancouver Housing Data\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nVancouver\n\n\n\nMaking sense of the new census data.\n\n\n\n\n\nOct 26, 2017\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nTrick-or-Treat 2017\n\n\n\n\n\n\nCensusMapper\n\n\n\nHalloween Mapping Fun.\n\n\n\n\n\nOct 23, 2017\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nBoomer Exodus?\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\nVancouver\n\n\n\nChange in size of age groups vs net migration\n\n\n\n\n\nOct 21, 2017\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nVSB Vulnerable Students\n\n\n\n\n\n\ncancensus\n\n\nVancouver\n\n\n\nCounting children in poverty by school catchment area.\n\n\n\n\n\nOct 15, 2017\n\n\nJens von Bergmann\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Retrospective Look at NHS Income Data\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\ngeeky\n\n\n\nHow bad were the NHS income numbers?\n\n\n\n\n\nSep 29, 2017\n\n\nJens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of the Income Distribution\n\n\n\n\n\n\nCensusMapper\n\n\ncancensus\n\n\ngeeky\n\n\n\nDigging deeper into the evolution of incomes\n\n\n\n\n\nSep 26, 2017\n\n\nJens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nZoned for Who?\n\n\n\n\n\n\nAssessment Data\n\n\nVancouver\n\n\nzoning\n\n\n\nWhat income do you need to buy a single family home in Vancouver?\n\n\n\n\n\nSep 18, 2017\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nIncome - A First Look\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\n\nIncomes are complex.\n\n\n\n\n\nSep 14, 2017\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon - The Canadian Data\n\n\n\n\n\n\ncancensus\n\n\nCensusMapper\n\n\n\nPlaying the Amazon Game\n\n\n\n\n\nSep 9, 2017\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\ndot-density\n\n\n\n\n\n\nCensusMapper\n\n\ndotdensity\n\n\ncancensus\n\n\ngeeky\n\n\n\nMulti-category dot-density maps are hard.\n\n\n\n\n\nAug 24, 2017\n\n\nJens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nDensity\n\n\n\n\n\n\nnewsfail\n\n\nCensusMapper\n\n\ncancensus\n\n\ndensity\n\n\n\nThinking about density.\n\n\n\n\n\nAug 23, 2017\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nMillennials Redux\n\n\n\n\n\n\nnewsfail\n\n\nCensusMapper\n\n\n\nMillennials are still not fleeing.\n\n\n\n\n\nAug 6, 2017\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nLifeblood\n\n\n\n\n\n\nCensusMapper\n\n\n\nExploring net migration patterns.\n\n\n\n\n\nMay 16, 2017\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSurprise Maps\n\n\n\n\n\n\nCensusMapper\n\n\ngeeky\n\n\n\nShowing only what matters.\n\n\n\n\n\nApr 10, 2017\n\n\nJens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarine Gateway and Joyce-Collingwood\n\n\n\n\n\n\nnewsfail\n\n\nVancouver\n\n\nEmpty Homes\n\n\nCensusMapper\n\n\n\nPitfalls in empty condos.\n\n\n\n\n\nApr 3, 2017\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Censuses\n\n\n\n\n\n\nCensusMapper\n\n\ngeeky\n\n\n\nNot so unique GeoUIDs and other pitfalls.\n\n\n\n\n\nMar 22, 2017\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nRS Population Change\n\n\n\n\n\n\nVancouver\n\n\nCensusMapper\n\n\nzoning\n\n\n\nDid RS zoned land loose population?\n\n\n\n\n\nMar 6, 2017\n\n\nJens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nTransit Explorer\n\n\n\n\n\n\nVancouver\n\n\nTransportation\n\n\nMapzen\n\n\n\nDynamic Isochrones!\n\n\n\n\n\nMar 1, 2017\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nMore on Teardowns\n\n\n\n\n\n\nVancouver\n\n\nteardowns\n\n\nAssessment Data\n\n\nzoning\n\n\n\nSome more details behind our data story on teardowns.\n\n\n\n\n\nFeb 21, 2017\n\n\nJens von Bergmann, Joe Dahmen\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n2016 Census Data - Part 1\n\n\n\n\n\n\nCensusMapper\n\n\n\nA first look at the 2016 census.\n\n\n\n\n\nFeb 10, 2017\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nJane Jacobs’ Vancouver\n\n\n\n\n\n\nVancouver\n\n\nOSM\n\n\n\nNeigbourhood typologies within Metro Vancouver.\n\n\n\n\n\nJan 25, 2017\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nBumper Year for Thumb Twiddlers\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\n\nVancouver land working harder than ever.\n\n\n\n\n\nJan 18, 2017\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Coveted $1.2m - $1.6m Vote\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\n\nMapping the poor recipients of a tax break\n\n\n\n\n\nJan 17, 2017\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n2017 Vancouver Assessment Data\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\n\nVisualizing the latest land value gains.\n\n\n\n\n\nJan 16, 2017\n\n\nJens von Bergmann\n\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated Property Tax Data\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\ntaxes\n\n\ndensity\n\n\n\nUpdaing our tax density visualizations.\n\n\n\n\n\nDec 13, 2016\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nCharacter Retention\n\n\n\n\n\n\nVancouver\n\n\n\nAbuse of Character.\n\n\n\n\n\nNov 26, 2016\n\n\nJens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Isochrones\n\n\n\n\n\n\nMapzen\n\n\nIsochrones\n\n\nOSM\n\n\n\nExploring Isochrones.\n\n\n\n\n\nNov 18, 2016\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nTrick-or-Treat 2016\n\n\n\n\n\n\nCensusMapper\n\n\n\nHalloween Mapping Fun.\n\n\n\n\n\nOct 21, 2016\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary Suites and Taxes\n\n\n\n\n\n\nVancouver\n\n\ntaxes\n\n\n\nThe tax implications of RS zoning.\n\n\n\n\n\nOct 4, 2016\n\n\nJens von Bergmann\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Housing Affordability\n\n\n\n\n\n\nVancouver\n\n\n\nAffordable for whom?\n\n\n\n\n\nSep 14, 2016\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Guide to Income Data\n\n\n\n\n\n\nVancouver\n\n\n\nProgressing past medians.\n\n\n\n\n\nAug 31, 2016\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nMobi Running Stats\n\n\n\n\n\n\nOSM\n\n\nMapzen\n\n\nbikes\n\n\n\nBike share stats for all.\n\n\n\n\n\nAug 24, 2016\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMobi – a First Look\n\n\n\n\n\n\nOSM\n\n\nMapzen\n\n\nbikes\n\n\n\nScraping Mobi data.\n\n\n\n\n\nAug 16, 2016\n\n\nJens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s up with RT?\n\n\n\n\n\n\nVancouver\n\n\nzoning\n\n\n\nScraping Mobi data.\n\n\n\n\n\nAug 4, 2016\n\n\nJens von Bergmann\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nBike Share Map\n\n\n\n\n\n\nOSM\n\n\nMapzen\n\n\nbikes\n\n\n\nWe weren’t fond of Mobi’s map, so we made our own.\n\n\n\n\n\nJul 26, 2016\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nMixing Data\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\nCensusMapper\n\n\n\nMixing datasets allows a deeper view.\n\n\n\n\n\nJul 6, 2016\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSDH Zoning and Land Use\n\n\n\n\n\n\nVancouver\n\n\nland use\n\n\nzoning\n\n\ndensity\n\n\n\nHow much land do single detached and duplex houses consume?\n\n\n\n\n\nJun 17, 2016\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nFSR\n\n\n\n\n\n\nVancouver\n\n\ndensity\n\n\n\nUsing LIDAR data to estimate building floor space?\n\n\n\n\n\nMay 20, 2016\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nMy Global Bike Map\n\n\n\n\n\n\nOSM\n\n\nbikes\n\n\nMapzen\n\n\n\nThe beauty of building things with global datasets.\n\n\n\n\n\nMay 16, 2016\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensus Mapping for Everyone\n\n\n\n\n\n\nCensusMapper\n\n\n\nOpening up StatCan open data, one step at a time.\n\n\n\n\n\nMay 4, 2016\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensus Week\n\n\n\n\n\n\nCensusMapper\n\n\n\nNew data is on the way.\n\n\n\n\n\nMay 2, 2016\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nTOD Mode Share\n\n\n\n\n\n\nVancouver\n\n\nTransportation\n\n\nCensusMapper\n\n\n\nAdding mode share to our TOD map.\n\n\n\n\n\nApr 20, 2016\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nTOD\n\n\n\n\n\n\nVancouver\n\n\nTransportation\n\n\nCensusMapper\n\n\n\nPopulation densities around rapid transit stations.\n\n\n\n\n\nApr 6, 2016\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn Houses and Dirt\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\n\nVisualizing land vs building values.\n\n\n\n\n\nApr 1, 2016\n\n\nJens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn Mixed Use\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\ntaxes\n\n\n\nHow mixed use redevelopment can lower overall property taxes.\n\n\n\n\n\nMar 28, 2016\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSurrey Traffic Loop Counts\n\n\n\n\n\n\nSurrey\n\n\nTransportation\n\n\n\nExploring induction loop counter data.\n\n\n\n\n\nMar 27, 2016\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnoccupied Dwellings\n\n\n\n\n\n\nVancouver\n\n\nCensusMapper\n\n\nEmpty Homes\n\n\n\nMaking sense of the new city data.\n\n\n\n\n\nMar 8, 2016\n\n\nJens von Bergmann\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nPhysical SFH Form Over Time\n\n\n\n\n\n\nVancouver\n\n\nzoning\n\n\n\nDigging into the data of SFH.\n\n\n\n\n\nMar 5, 2016\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nProperty Taxes and Land Use\n\n\n\n\n\n\nVancouver\n\n\nMapzen\n\n\ntaxes\n\n\nzoning\n\n\nland use\n\n\n\nVisualizing tax density and land use.\n\n\n\n\n\nMar 2, 2016\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nLand Use, Roads (and Parking)\n\n\n\n\n\n\nVancouver\n\n\nland use\n\n\nTransportation\n\n\n\nHow do we use land in Metro Vancouver?\n\n\n\n\n\nFeb 29, 2016\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn Condos\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\ngeeky\n\n\n\nWhat can open data say about condos?\n\n\n\n\n\nFeb 10, 2016\n\n\nJens von Bergmann\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nLoss of Character\n\n\n\n\n\n\nVancouver\n\n\n\nWhat can open data say about condos?\n\n\n\n\n\nFeb 1, 2016\n\n\nJens von Bergmann\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nLand Use Data\n\n\n\n\n\n\nVancouver\n\n\nland use\n\n\n\nMetro Vancouver Land Use Data\n\n\n\n\n\nJan 31, 2016\n\n\nJens von Bergmann\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nWork vs Twiddling Thumbs\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\nCensusMapper\n\n\n\nVancouver land works harder than it’s people\n\n\n\n\n\nJan 24, 2016\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn Teardowns\n\n\n\n\n\n\nVancouver\n\n\nzoning\n\n\nteardowns\n\n\ngeeky\n\n\n\nDigging into teardowns\n\n\n\n\n\nJan 18, 2016\n\n\nJens von Bergmann\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated Vancouver Assessment Data\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\n\nVanRE Madness Continues\n\n\n\n\n\nJan 17, 2016\n\n\nJens von Bergmann\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nCanvas vs SVG\n\n\n\n\n\n\nCensusMapper\n\n\ngeeky\n\n\n\nWeb performance tuning\n\n\n\n\n\nDec 27, 2015\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBike Data\n\n\n\n\n\n\nOSM\n\n\nbikes\n\n\n\nThe bike data I want\n\n\n\n\n\nDec 15, 2015\n\n\nJens von Bergmann\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBike Routing\n\n\n\n\n\n\nOSM\n\n\nMapzen\n\n\nbikes\n\n\n\nTweaking bike routing\n\n\n\n\n\nDec 14, 2015\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a bike map\n\n\n\n\n\n\nOSM\n\n\nbikes\n\n\nMapzen\n\n\n\nModern Web Mapping Technology Meets Bikes\n\n\n\n\n\nDec 13, 2015\n\n\nJens von Bergmann\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensus Drilldown\n\n\n\n\n\n\nCensusMapper\n\n\n\nDrilling into census regions\n\n\n\n\n\nOct 24, 2015\n\n\nJens von Bergmann, Alejandro Cervantes\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nCensus Mapper\n\n\n\n\n\n\nCensusMapper\n\n\n\nMaking StatCan open data more accessible\n\n\n\n\n\nSep 28, 2015\n\n\nJens von Bergmann, Alejandro Cervantes\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hidden Mortgage\n\n\n\n\n\n\nCensusMapper\n\n\nTransportation\n\n\n\nHousing vs Transportation Costs\n\n\n\n\n\nAug 25, 2015\n\n\nJens von Bergmann\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver 2011 Census Data on Housing\n\n\n\n\n\n\nVancouver\n\n\n\nPlaying with Census Data\n\n\n\n\n\nJun 23, 2015\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nTax Density in Vancouver\n\n\n\n\n\n\nVancouver\n\n\ntaxes\n\n\ndensity\n\n\n\nInvestigating taxes and land use\n\n\n\n\n\nMay 31, 2015\n\n\nJens von Bergmann\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver Bike Paths\n\n\n\n\n\n\nVancouver\n\n\nbikes\n\n\n\nNaked bike maps\n\n\n\n\n\nApr 1, 2015\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver Assessment Data\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\n\nMapping Vancouver Assessment Data\n\n\n\n\n\nMar 23, 2015\n\n\nJens von Bergmann\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nMapping CACs\n\n\n\n\n\n\nVancouver\n\n\n\nCACs\n\n\n\n\n\nNov 5, 2014\n\n\nJens von Bergmann\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver High Value Improvements Map\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\n\nPlaying with Assessment Data\n\n\n\n\n\nSep 7, 2014\n\n\nJens von Bergmann\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nVancouver Teardown Map\n\n\n\n\n\n\nVancouver\n\n\nAssessment Data\n\n\nteardowns\n\n\n\nPlaying with Assessment Data\n\n\n\n\n\nSep 5, 2014\n\n\nJens von Bergmann\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2014-09-05-vancouver-teardown-map/index.html",
    "href": "posts/2014-09-05-vancouver-teardown-map/index.html",
    "title": "Vancouver Teardown Map",
    "section": "",
    "text": "There has been lots of talk about old homes being torn down and replaced in Vancouver. The likeliest targets are houses of relatively low value sitting on expensive land. So how many teardown candidates are there in Vancouver, and where in Vancouver are they located?\nTo answer this we use data from Vancouver’s open data catalogue to build an interactive map to explore the low end of the building stock. More specifically, we set a somewhat arbitrary ratio of (assessed) house value to total (assessed) property value (house + land) of 5% and label everything below that a “teardown candidate”.\nThe short answer is that almost one in three properties in Vancouver fall into this category. And they are distributed quite evenly across all of Vancouver with some areas like downtown looking a little better.\n\nOn the level of the individual property the “teardown candidate” designation might be unfair and somewhat inflammatory. By our above definition, a million dollar house sitting on a 20 million dollar property is a “teardown candidate”. In some cases BC Assessment may have failed to properly account for recent renovations resulting in the building being undervalued and more likely to show up as a “teardown candidate”. And of course not every “teardown candidate” will be torn down, other options may be preferable. Depending on circumstance extensive renovations may be the better option, and the pervasiveness of “teardown candidates” suggest that many people in Vancouver are happy living in a house that is worth less than 5% of the overall assessed value of the property.\nTo aid a more individualized look at properties we included the address, the assessed land and improvement values, the tax levy (CoV just added this information a couple of days ago), zoning information, the land area and the relative land value. To display this information just hover your mouse over a property (or touch it on your mobile device). Opening a second browser with a map with satellite imagery (or Apple maps) can provide additional context.\nTake a look at the map and explore for yourself. You will need a reasonably fast computer and modern browser to enjoy the map, it contains lots of data and may feel quite slow on mobile devices.\n\n\nFull screen view\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2014,\n  author = {von Bergmann, Jens},\n  title = {Vancouver {Teardown} {Map}},\n  date = {2014-09-05},\n  url = {https://doodles.mountainmath.ca/posts/2014-09-05-vancouver-teardown-map},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2014. “Vancouver Teardown Map.”\nMountanDoodles (blog). September 5, 2014. https://doodles.mountainmath.ca/posts/2014-09-05-vancouver-teardown-map."
  },
  {
    "objectID": "posts/2014-11-05-mapping-cacs/index.html",
    "href": "posts/2014-11-05-mapping-cacs/index.html",
    "title": "Mapping CACs",
    "section": "",
    "text": "After reading the post on Price Tags, originally from Karen Sawatzky’s blog, I got curious about getting a geographic overview over the CACs in Vancouver in 2010, 2011 and 2012.\nAs opposed to the teardown map the dataset is quite small, so there should be no problems viewing this on a mobile device. As always, map is below the fold.\n\nSome of the property addresses did not match any address in the CoV open data property database, probably because the developments are quite new and new addresses got assigned. In these cases I mapped the CAC property to a nearby property and added the ‘Mapped Address’ field in the info pane. Enjoy.\n\n\nFull screen view\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2014,\n  author = {von Bergmann, Jens},\n  title = {Mapping {CACs}},\n  date = {2014-11-05},\n  url = {https://doodles.mountainmath.ca/posts/2014-11-05-mapping-cacs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2014. “Mapping CACs.”\nMountanDoodles (blog). November 5, 2014. https://doodles.mountainmath.ca/posts/2014-11-05-mapping-cacs."
  },
  {
    "objectID": "posts/2015-04-01-bike-paths/index.html",
    "href": "posts/2015-04-01-bike-paths/index.html",
    "title": "Vancouver Bike Paths",
    "section": "",
    "text": "Motivated by the excellent Washington Post Wonkblog I mapped Vancouver’s bike infrastructure. Looks good at first, but when you take out the ‘fake’ “Local Street” bikeways more in line with Wonkblog’s methodology it’s looking pretty bleak in large portions of the city.\nOf course the devil is in the details and infrastructure cannot just be judged by how it’s labeled. If one were to look for 8-80 infrastructure, then the separated lanes would make the cut. Some of the bike lanes would be marginal, but most would probably make the cut. Although I can’t say that I would be particularly keen to let an 8yo cycle on some of them.\nVancouver’s “Shared Lanes” are not for the faint of heart. They have 50 speed limits on multi-lane roadways and feature sharrows to make it a little easier for cyclists to ‘take the lane’. In some cases cyclists share a lane with the buses, which is a little better.\nThe local streets are mostly car tunnels like the ‘Off-Broadway’ and won’t qualify as 8-80, but there are some exceptions like the local street portion of Pt. Grey Rd, where cycling is rather pleasant.\nFull screen view\nMethodology is simple, it’s just the city’s bike path data from their open data catalogue. The data comes with two fields, ‘name’ and ‘type’. The ‘type’ was used for the checkboxes at the bottom to selectively turn different bike lane types on or off and the ‘name’ will be displayed on hover."
  },
  {
    "objectID": "posts/2015-04-01-bike-paths/index.html#update",
    "href": "posts/2015-04-01-bike-paths/index.html#update",
    "title": "Vancouver Bike Paths",
    "section": "Update",
    "text": "Update\n\nSome more Canadian cities:\nCalgary on street, Calgary pathways, Calgary trails, Victoria, Montreal, Toronto Portland NYC Amsterdam Taipei\nA word of caution. I have done zero data cleaning or verification. Some of these maps are missing some type of infrastructure. I am familiar with cycling conditions in Calgary, and their the extensive network along the river and other areas are missing in their main file. I added two more separate maps for their trails and pathways, but was too lazy to merge them. For Taipei, I noticed that the off-street paths along streets are missing, for example the one on Dunhua Bei Lu that I was using a lot a year ago. Not sure if or where these are available. So some more ground truth is needed for proper interpretation. But fun anyway.\nWant to map another city’s data? No problem, just read on. \n\nLocate the city’s bikeway data and download it.\nConvert the data to geojson, with coordinates in latitude and longitude.\nPut the geojson file online somewhere, e.g. your public dropbox folder.\nOptionally look at the geojson file for a bikeway type descriptor and bikeway name, if available.\nBuild a url for your map by using [http://doodles.mountainmath.ca/html/bike_paths.html] as a base url and add query strings\n\ndataUrl=&lt;url to your geojson&gt;\ncity=&lt;city name&gt;\noptionally type=&lt;bikeway type property&gt;\noptionally name=&lt;bikeway name property&gt;\noptionally zoom=true if you want to be able to zoom and pan on the map\n\n\nFor example, to map Calgary’s bike network you need to got to their open data website and download the Tranportation Bikeways shapefiles (SHP). To convert them go geojson using ogr2ogr\nogr2ogr -f GeoJSON -simplify 1 -s_srs CALGIS_TRAN_BIKEWAY.prj -t_srs \"EPSG:4326\" CALGIS_TRAN_BIKEWAY.geojson CALGIS_TRAN_BIKEWAY.shp\nfrom the GDAL package.\nI uploaded the geojson to https://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson, so that will be the value of the dataUrl query string.\nLooking at the resulting geojson file you will see that the data does not include bikeway names, but it does include types and the property is called BICYCLE_CL.\nThe rather lengthy link to the map would then be https://doodles.mountainmath.ca/html/bike_paths.html?city=Calgary&type=BICYCLE_CL&dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson\n\n\nOr if you want to zoom into Calgary’s sprawling suburbs, you could also enable zooming (using double-click) and panning https://doodles.mountainmath.ca/html/bike_paths.html?city=Calgary&type=BICYCLE_CL&zoom=true&dataUrl=http://doodles.mountainmath.ca/data/CALGIS_TRAN_BIKEWAY.geojson\nHappy Mapping!"
  },
  {
    "objectID": "posts/2015-06-23-vancouver-2011-census-data-on-housing/index.html",
    "href": "posts/2015-06-23-vancouver-2011-census-data-on-housing/index.html",
    "title": "Vancouver 2011 Census Data on Housing",
    "section": "",
    "text": "Last week my friend Alejandro emailed me some census data. After sitting on it for a couple of days I decided one evening to take a look and map the dissemination areas. The dataset contained 125 fields around housing and income. So I started plucking some of the fields off the table and mapping them.\nIt did not take long and colorful maps started showing up. Another evening and lots of map layers later I put the Vancouver Census Map online for people interested to look through the census graphs. The number of layers became a little unwieldly, just pick and choose what you are interested in.\n\nSo what exactly does the Census Data show?\n\nThe data is aggregated at the Dissemination Area level and rounded to protect privacy. That leaves some artefacts, but generally gives a very detailed picture of what is going on in different parts of the city. The biggest drawback is the age, the data is now 4 to 5 years old. Lots of things happened in the meantime, we will have to wait until 2016 for the next Census Canada dataset.\n\n\nWhere to go from here?\nWhen I get around to it I might map all of Metro Van Census data. It won’t take any time to re-run the import script for the larger dataset, but I will need to show higher aggregation level data at lower zoom levels to keep it repsonsive for slower machines, just like I did for the Vancouver Assessment Map\nI should probably also take a look at what other fields are available, the ‘affordability’ map linked above uses individual income for full-time employees instead of combined household income, which differes from how affordability is usually calculated. The reason is that the data I had did not have the household income. Not sure if it is available at the dissemination area aggregation level, but if it is that one should be used. But I am lazy, so for now that’s it.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2015,\n  author = {von Bergmann, Jens},\n  title = {Vancouver 2011 {Census} {Data} on {Housing}},\n  date = {2015-06-23},\n  url = {https://doodles.mountainmath.ca/posts/2015-06-23-vancouver-2011-census-data-on-housing},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2015. “Vancouver 2011 Census Data on\nHousing.” MountanDoodles (blog). June 23, 2015. https://doodles.mountainmath.ca/posts/2015-06-23-vancouver-2011-census-data-on-housing."
  },
  {
    "objectID": "posts/2015-09-28-census-mapper/index.html",
    "href": "posts/2015-09-28-census-mapper/index.html",
    "title": "Census Mapper",
    "section": "",
    "text": "The project on mapping census data for Canada is entering the next stage. We are now mapping 3875 different variables from the 2011 census, or any combination of them, across 67215 geographic regions covering all of Canada.\nWe are opening up the Census Mapper to a limited public beta. Limited means that anyone can view maps created in CensusMapper, but we are only opening up the map creation tools to selected beta users.\nBe aware that the web app makes use of modern web technology and renders large amount of data. It will only work on modern browsers, best viewed in Chrome or Safari. Firefox works ok, Internet Explorer might grind to a halt and should be avoided.\n\nWe are a little restrictive on creating maps right now for the simple reason that census data is somewhat tricky to understand and at this point we don’t have a comprehensive guide explaining all the variables and warning against many of the pitfalls. We are planning to slowly integrate this and open up the map creation tools to the general public."
  },
  {
    "objectID": "posts/2015-09-28-census-mapper/index.html#why-censusmapper",
    "href": "posts/2015-09-28-census-mapper/index.html#why-censusmapper",
    "title": "Census Mapper",
    "section": "Why CensusMapper?",
    "text": "Why CensusMapper?\n\nCensus Canada data is extremely rich and useful in many cirumstances, but it is not being widely used. There are many reasons for this, the somewhat unmanageable amount of data being one of them, the difficulty of accessing and standardizing the in principle freely available datasets is another.\nCensus data is inherently geographic in nature, working with the data without proper visualization tools can be challenging too. And even for people that have good access to the data and that are well-versed in mapping geographic data, it can still take quite a bit of time to generate maps visualizing the data. CensusMapper greatly speeds up this process by allowing straightforward mapping of any function derived from census variables through all geographic aggregation levels Canada wide."
  },
  {
    "objectID": "posts/2015-09-28-census-mapper/index.html#storytelling",
    "href": "posts/2015-09-28-census-mapper/index.html#storytelling",
    "title": "Census Mapper",
    "section": "Storytelling",
    "text": "Storytelling\nCensus mapper does more than just mapping census data. It is designed as a storytelling tool. Few maps are so crisp and clear that they are self-explanatory. A map of population density might fall into that category. But most census variables are sufficiently complex that maps derived from them warrant narration. We think of CensusMapper as a storytelling tool that allows ‘readers’ of the map to interact with it, zoom in, zoom out, pan around, and jump to other maps linked in the story provided by the mapmaker."
  },
  {
    "objectID": "posts/2015-09-28-census-mapper/index.html#directions",
    "href": "posts/2015-09-28-census-mapper/index.html#directions",
    "title": "Census Mapper",
    "section": "Directions",
    "text": "Directions\nThere are many ways to expand on this. On the map creation side we can offer more diverse coloring tools, allowing user input and user defined map locations to be used in the mapping function, add data from previous census. We could allow limited upload of user data to be integrated with census data, statistical and spacial analysis tools, custom mapping projects."
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html",
    "title": "How to make a bike map",
    "section": "",
    "text": "Here come some general thoughts on bike maps. Not throught through yet, just jotting down some ideas so that I don’t forget and maybe to start a discussion.\nBike enthusiasts, OSM folks and mapping technology wonks read on!"
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html#why-make-a-new-bike-map",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html#why-make-a-new-bike-map",
    "title": "How to make a bike map",
    "section": "Why make a new bike map?",
    "text": "Why make a new bike map?\nPlainly put, I don’t like a lot of the bike maps out there. Time for my little bike map rant:\n\nLots of maps don’t accomplish their core mission: accurately map bike infrastructure. For example, when in real life a bike lane vanishes 100m in front of an intersection and reappears 100m after the intersection, leaving cyclists exposed right where protection is needed the most, most bike maps will mark the entire section as having a bike lane.\nMost bike maps focus on the wrong issue. They focus on physical infrastructure, as a cyclist I am interested in cycling comfort level. Can I take my 6 year old along that route? Can I cycle leisurely or will I feel hastened by a car breathing down my neck? Will I arrive at my destination relaxed or will I be riled up by 3 near misses and 5 drivers swearing at me? Some of these questions can be answered by mapping infrastructure. A physically separated bike path will give me the comfort I need to bing my 6 year old and enjoy my ride. But when it comes to “bike lane” or “shared road” designations, I have no idea what it will feel like until I get there.\nMost bike maps have not yet made the transition from paper/PDF based maps to clickable and zoomable maps that can be easily consumed on the go.\nMost maps are static, they don’t adapt to the needs of the user. Why do I have to deal with 5 different colors and 3 types of shading when reading a bike map, when all I want to know is if I can bring my 6 year old along for the ride. I want an “AAA” button that fades out all routes that are not suitable for all ages and abilities. For that purpose I don’t care if a road has infrastructure like a bike lane wedged between parked cars and 50km/h traffic. If we absolutely have to ride that way it will be on the sidewalk."
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html#how-to-make-a-new-bike-map",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html#how-to-make-a-new-bike-map",
    "title": "How to make a bike map",
    "section": "How to make a new bike map?",
    "text": "How to make a new bike map?\nFirst let’s start with examples of how other have tried to solve these problems. Probably one of the best maps out there is the Capital Regional District Bike Map. I chatted with Corey who worked for CRD on these maps on how it was done. They collected all the bike infrastructure in the region by contacting all the municipalities.  Then they meticulously checked that what the municipalities reported actually existed on the ground. Consider Fort road on their bike map where they accurately show the bike lane cutting out. And they added in shortcuts and connections that the municipalities did not report on, like a short section of footway that may technically require walking the bike but shortcuts out of a suburban cul-de-sac maze.\nNext the CRD map does not only map infrastructure, but color-codes it to show cycling comfort.  This gives me a good idea how comfortable the rider will feel. Unfortunately all that complexity starts to make it difficult to read the bike map.\nThat’s where technology could come to the rescure. The CRD map is a PDF map and suffers the usual limitations that come with it. The map cannot adapt to the needs of the reader. So how about moving the map online and letting the user decide how to display it?"
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html#bike-maps-that-adapt-to-the-user",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html#bike-maps-that-adapt-to-the-user",
    "title": "How to make a bike map",
    "section": "Bike maps that adapt to the user",
    "text": "Bike maps that adapt to the user\nThe Washington Post Wonkblog had an excellent feature mapping only the bike infrastructure, nothing else. And for good measure, they removed the shared streets bike infrastructure. The rational was that in most cases, comfort level on shared street bike routes is really no different from cycling on a parallel road. Inspired by that I decided map Vancouver and a couple of other cities and I added in some button to allow selectively turning on or off typed of infrastructure.  That’s getting one step closer to adaptable bike maps, but it is still missing the main point that as a cyclist I am most interested in comfort level, and infrastructure type is only a proxy for that. And a poor one in many cases."
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html#inspiration",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html#inspiration",
    "title": "How to make a bike map",
    "section": "Inspiration",
    "text": "Inspiration\nNot convinced we need a new bike map? Let’s draw on some local (for me) maps that inspired this effort.\nUBC Campus Planning keeps tweeting out a bike map that lists several dangerous roads as “designated on street cycling routes”, including an unlit divided 4 lane highway with a posted speed limit of 60km/h and typical traffic speeds significantly exceeding that. And all of that apparently aimed at people that don’t cycle to campus yet but are considering doing so.\nTransLink just updated their bike map and marks said highway as “recommended by cyclists”. The do acknowlege that the road “does not have special treatment for cyclists”. So why include it on the map? Filling in the the gaps in the cycling network should happen in real life, not just on the map.\nEven HUB keeps marking Wesbrook Mall north of Agronomy as “existing cycling route” when in real life it is everything but that."
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html#how-to-make-a-bike-map",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html#how-to-make-a-bike-map",
    "title": "How to make a bike map",
    "section": "How to make a bike map",
    "text": "How to make a bike map\nSo how should we make a bike map? I was always thinking about setting up my own database and somehow adding and rating infrastructure. Then Alex decided to start making a bike map for the UBC area. And he chose the most straight-forward path: Editing Open Street Map data. That way the edits are immediately (better: after their map tile refresh cycle) available online, for example on OpenCycleMaps. The map is easily accessible on the go, it zooms and scrolls. It is missing a ‘locate me’ button (which is easy to fix). The look and feel is a little dated. But most importantly, it does not adapt to the user’s needs. I can’t ask it to display the “dad’s version” of the cycle network, showing me only pieces that I will be comfortable cycling with my 6 year old.\nTechnology has moved along since that map was built. A very easy way to solve these issues is to utilized the awesome tools built by the folks at Mapzen. Their tangram mapping engine taps into their OSM vector tile service to make flexible mapping of bike data a breeze. And to top things off, they offer very easy to use and extremely powerful ways to style the map. Only problem: Mapzen’s OSM extracts don’t have cycling information. Not a big problem though, we can just pull them out separately and add them on by hand. Here is an example where only the bike routes near UBC are added.\n\n\nFull screen view\nOne small drawback is that WebGL, the technology Tangram is based on, is not available for much of the windows world. WebGL requires modern browsers (IE9 does not count) and also modern hardware/graphics card drivers. A couple of years old windows machine will likely not be able to render WebGL no matter what browser you use. But the main target is mobile, and iOS and most android won’t have a problem with WebGL. If really needed, could add a fallback or use older and web technology to make the map, but Mapzen’s Tangram makes it so ridiculously easy to make and style nice maps…"
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html#the-main-problem-left-to-solve",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html#the-main-problem-left-to-solve",
    "title": "How to make a bike map",
    "section": "The main problem left to solve",
    "text": "The main problem left to solve\nOne major problem left. OSM does not have and “cycling comfort” tags right now. There are tags for physical infrastructure, and in some cases the comfort level can be correctly inferred from those. But in many cases it can’t.\nThere are two ways around that. One could keep those tags in a separate file, but that becomes difficult to maintain when OSM features change. Or one can add the tag to the OSM data. That way better bike maps can scale easily, and the information can also be used in OSM-based routing services where their real value lies.\nNot sure which is the way to go here, a similar tag has been proposed before but apparently did not go anywhere. Looking at the tag guidelines the fact they these new tags are very useful for routing seems to speak for adding them to OSM, but the fact that they are measuring something explicitly non-geographic by going beyond the already existing physical infrastructure markers might mean they are better kept in an external database. Will have to think this over, feedback appreciated."
  },
  {
    "objectID": "posts/2015-12-13-how-to-make-a-bike-map/index.html#where-to-go-from-here.",
    "href": "posts/2015-12-13-how-to-make-a-bike-map/index.html#where-to-go-from-here.",
    "title": "How to make a bike map",
    "section": "Where to go from here.",
    "text": "Where to go from here.\nThe logical next step is to add routing. Again, Mapzen’s flexible routing service seems to be a natural match. Also, adding editing capabilities right onto the map would be quite useful. The bike map is where data issues are best seen, and saving the step to head over to OSM to fix it (after creating an account) and then waiting for the data to update on the map seems like a workflow that will discourage wide participation.\nWe follow up by exploring routing and data quality."
  },
  {
    "objectID": "posts/2015-12-15-bike-data/index.html",
    "href": "posts/2015-12-15-bike-data/index.html",
    "title": "Bike Data",
    "section": "",
    "text": "Maps live and die with the quality of the underlying data. So I decided to dive a little deeper down the OSM bike data rabbit hole. Task number one was to expand display data for a wider region. My primitive workflow to pull data out of OSM only allows for extracting a quarter of a degree at a time. For playing around with all of Vancouver’s data I again turned to Mapzen for their metro extracts as a convenient shortcut for OSM data. For the case of Vancouver the word “metro” is a bit of an exaggeration, it just covers the City of Vancouver. But good enough for some more testing."
  },
  {
    "objectID": "posts/2015-12-15-bike-data/index.html#more-data",
    "href": "posts/2015-12-15-bike-data/index.html#more-data",
    "title": "Bike Data",
    "section": "More data",
    "text": "More data\n\nMore data means a a larger variety of bike infrastructure types. At first glance tagging seems to be reasonably accurate, but on closer inspection one quickly spots lots of issues. First off, the edits Alex made earlier are not showing up, that’s because the metro extracts are only done weekly. Just a matter of time. showing up now after updating with new OSM data.\nAnd then there are lots of little issues. Off street paths don’t connect to roads (or anything for that matter), making the useless for routing. Some changes in the bike network are tagged differently, take Point Grey Road as an example. Some paths are labeled to be ok for bikes, but bikes aren’t allowed there. Some tags seem off.\nAnd I don’t seem to have captured all relevant bike infrastructure, will have to spend some time one of these days to check through all the different taggings in OSM to make sure I pull out all the correct ones.\nSo to make more sense out of the network I made a new map, added some more categories for optionally fading them out when displayed. And on hover I display the tags, just for interest."
  },
  {
    "objectID": "posts/2015-12-15-bike-data/index.html#improving-data",
    "href": "posts/2015-12-15-bike-data/index.html#improving-data",
    "title": "Bike Data",
    "section": "Improving data",
    "text": "Improving data\nThis gets us to task number two. The folks at Mapzen gave me some friendly pointers to allow easy editing of features. So I pasted a couple of lines of code in so if you hold down the shift key and click on a feature, it will take you automatically to OSM to edit the feature. You may have to sign in the first time you do this. If you hold the shift key while clicking anywhere else it also takes you to the OSM editor for that spot, in case you want to add something there. It takes a lot of the pain out of editing bike infrastructure, I just fixed a whole bunch of things in short time.\n\n\nGo for it and fix some problems that you see, either by shift-clicking on the embedded map or by taking the map full-screen first. Remember that some of the issues may be fixed already, the bike map will not reflect updates until a week later or so."
  },
  {
    "objectID": "posts/2015-12-15-bike-data/index.html#where-to-go-from-here",
    "href": "posts/2015-12-15-bike-data/index.html#where-to-go-from-here",
    "title": "Bike Data",
    "section": "Where to go from here?",
    "text": "Where to go from here?\nIt looks like making a decent bike map with OSM data is feasible. The hard work will be to collaboratively do all the OSM edits required to get the data into good shape.\nOne problem is that edits won’t show up on the bike map for up to a week, that’s the frequency at which the metro extracts are updated. And then I have to update the file for displaying the bike data. A minor inconvenience, in theory there are ways to seed this up if one wants to be serious about this. But then again, once the OSM bike data is fixed up in a given region, it won’t need updating very frequently.\nI guess I will have to mull this over and decide how deep down this rabbit hole I would like to go. Looking back at my rant on what’s wrong with most bike maps I am asking myself how much the current map has accomplished.\n\nThe accuracy of the infrastructure mapped is in the hands of the OSM community. With easy access to the OSM editing functionality from the map (desktop only) it’s in the hands of the people that know best: cyclists.\nStill coming up blank when it comes to showing comfort level. Will have to think more how to best do this, it might have to go into a separate database. Which is doable, but requires work to get it right and is probably a little too involved to do it off the side.\nClick and zoom is not an issue, but now I am in the opposite corner where it might be nice sometimes to actually have a glossy big paper map. And it’s really easy to save the map as an image. Some tinkering should be able to produce a nice high-resolution one covering a large area.\nThe map already adapts to user preferences quite well, different types of infrastructure will fade out if deselected.\nNot on the original list, but routing should be part of this. Routing works reasonably well to get this off the ground, but work is still required to make it work properly with user preferences. This may well require running a custom router, again a little more than I bargained for.\n\nSo is this really worth the trouble to make yet another bike map? I am not sure. It’s always easy to through up a quick proof of concept, but to do it really well requires quite a bit more work. For now I could just build one focused on Vancouver and see how it goes. And it would be great if the comfort level rating could somehow be automated.\nWhich brings to another one of pet projects that I never took beyond the testing realm. My CycleTrack App that records all bike trips in the background (as long as you carry your phone with you). No pressing of “start” or “stop” buttons, the app notices when you are moving and will take gyroscope and accelerometer readings to figure out if you are cycling, running, walking, driving or taking a train. Then it stores your cycling trips and computes aggregate data. It avoids using GPS so not to drain your battery too much, on a typical day it will consume about 3% of battery power. The downside is that the accuracy and frequency of the location updates is not as high, so things get a little messy. But not too bad.\nHow does this fit into the bike map project? Simple. If one can collect regular cycling data from normal people cycling (not just the lycra crowd that presses start cycles in circles and presses stop again and recharges their phone while taking a shower), once can infer a lot about comfort levels just by looking at the data. And to collect data from regular “citizen cyclist” one cannot expect them to press “start” and “stop” to delineate their bike trips, and one cannot have an app that will require them to recharge their phone after every trip.\nBut then the project gets even bigger…"
  },
  {
    "objectID": "posts/2016-01-17-updated-vancouver-assessment-data/index.html",
    "href": "posts/2016-01-17-updated-vancouver-assessment-data/index.html",
    "title": "Updated Vancouver Assessment Data",
    "section": "",
    "text": "The friendly folks at Vancouver Open Data just updated their property assessment data with the fresh 2016 property tax assessments. Time to run the script to update the Vancouver Assessment Map with the new data. And for good measure I pasted over some of the thematic map engine from CensusMapper to improve the mapping performance."
  },
  {
    "objectID": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#maps",
    "href": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#maps",
    "title": "Updated Vancouver Assessment Data",
    "section": "Maps",
    "text": "Maps\n\nThe interactive assessment map offers several views. In the panel on the top right we can select how to view the data. It offers standard thematic maps for value change, total value, building value and building age and zoning. And there are some options that warrant more explanation:\n\nRelative Land Value: The colours on the map show each property by the land value per m². We can immediately spot the east-west land gradient, as well as how zoning affects land value. When zooming in we also see the effect of lot size on land value.\nTax Density: This map looks at the tax dollars collected by the city by area. It tells us the relative rate at what each property is contributing to city services. We can again observe the impact of exclusionary zoning.\nRelative Value of Building:  This map simply divides the building value by the total property value. There are many ways to interpret this map, my favourite is to use this as a “Teardown Predictor”. Essentially, as the percentage of the building value approaches zero the probability that it will get torn down in the near future increases. Imagine someone spending $1.5m to buy a property with a house valued at $37k. Many people don’t mind living in a house worth $37k, but someone who is spending $1.5m would probably prefer to buy a different property with a higher quality house. Or spend more money to upgrade the house. How will it be upgraded? Renovating is in most cases economically unsound, most people will choose to tear down and rebuild. In fact, the teardown threshold is likely higher than the 2.5% in the example given. The percentage of properties in Vancouver where the building value is less than 2.5% of the total value has slightly decreaed in the last year from 17.9% to 17.8%, but the percentage of properties with building value less than 5% of the total value has increased from 32% to 33.5% during the last year."
  },
  {
    "objectID": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#the-data",
    "href": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#the-data",
    "title": "Updated Vancouver Assessment Data",
    "section": "The Data",
    "text": "The Data\nThe data originates with BC Assessment, which estimates land and building values of each property based on recent sales of comparable properties. The assessment was done in summer 2015 and is based on sales before that, so at this point in time the data lags the market by about one year. Values for individual properties may well be off, depending how well renovations and improvements were reported and how well the BC Assessment estimates work for the given property. On average they should reflect the market about half a year to one year ago.\nSadly, BC Assessment does not give out their data with a license that would allow mapping it the way I do, so we have to rely on municipalities to release it through their open data portals. The format of the data from each municipality is different, so lazy me is only importing data from City of Vancouver, although some other nearby municipalities are also releasing there data.\nThe motivation behind the map was to understand the building stock. Some effort was made to filter out parks, but the algorithm is far from perfect and will often includes parks that host building structures, as well as marinas with structures on them.\nThe new city dataset does not include the 2016 tax levy, so we still only show the 2015 tax levies until CoV updated their dataset."
  },
  {
    "objectID": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#history",
    "href": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#history",
    "title": "Updated Vancouver Assessment Data",
    "section": "History",
    "text": "History\nHere is a quick history of the overall land and building values aggregated for Vancouver between 2006 and 2016.\n\n\n\n\n\n\n Land Value \n\n\n Building Value \n\n\n\nWhen looking at all properties in the city, the increase in land value year over year was 21.4% ($45.2bn), while overall building values increased by 7.3% ($5bn). Hover, click or touch the points in the graph to get the values for the corresponding year."
  },
  {
    "objectID": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#neighbourhoods",
    "href": "posts/2016-01-17-updated-vancouver-assessment-data/index.html#neighbourhoods",
    "title": "Updated Vancouver Assessment Data",
    "section": "Neighbourhoods",
    "text": "Neighbourhoods\nLastly a quick overview over the neighbourhoods. Land and building values have not increased evenly throughout in the year. I aggregated all tax data by neighbourhood and split it into land value and building value increases. These numbers should be used as guidance only, they mix lots of different types of properties and include parks.\nHere is the breakdown by neighbourhood:\n\nRenfrew-Collingwood: Land: 30.6% ($1.2bn), Building: 5.8% ($46.0m)\nSunset: Land: 26.6% ($1.6bn), Building: 5.9% ($74.2m)\nOakridge: Land: 17.6% ($1.2bn), Building: 12.6% ($207.4m)\nDowntown: Land: 16.9% ($0.8bn), Building: 3.2% ($91.2m)\nKerrisdale: Land: 18.7% ($1.3bn), Building: 3.3% ($54.1m)\nVictoria-Fraserview: Land: 28.0% ($1.5bn), Building: 5.6% ($65.2m)\nGrandview-Woodland: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)\nWest End: Land: 22.6% ($2.5bn), Building: 3.5% ($188.4m)\nHastings-Sunrise: Land: 25.1% ($1.1bn), Building: 4.9% ($63.8m)\nKillarney: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)\nMarpole: Land: 17.8% ($0.3bn), Building: 14.1% ($241.6m)\nKitsilano: Land: 21.6% ($3.3bn), Building: 4.3% ($134.5m)\nShaughnessy: Land: 18.6% ($1.7bn), Building: 7.4% ($117.7m)\nWest Point Grey: Land: 20.2% ($2.3bn), Building: 5.2% ($88.3m)\nFairview: Land: 19.4% ($2.0bn), Building: -1.0% (-$52.4m)\nDowntown Eastside: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)\nKensington-Cedar Cottage: Land: 24.9% ($1.7bn), Building: 0.7% ($17.4m)\nRiley Park: Land: 26.4% ($1.0bn), Building: 4.0% ($40.2m)\nMount Pleasant: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)\nSouth Cambie: Land: 20.9% ($1.5bn), Building: 21.4% ($268.1m)\nStrathcona: Land: 21.9% ($2.6bn), Building: 4.9% ($194.9m)\nDunbar Southlands: Land: 21.8% ($1.0bn), Building: 11.2% ($126.6m)\nArbutus Ridge: Land: 21.4% ($1.6bn), Building: 1.1% ($17.3m)\n\nIt becomes immediately clear that the increase in property values is mostly driven by land, note that total value increases for land and buildings are reported in billions and millions, respectively. The building stock does not have time to catch up, with the exception of South Cambie. Fairview stands out with declining overall building values."
  },
  {
    "objectID": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html",
    "href": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html",
    "title": "Work vs Twiddling Thumbs",
    "section": "",
    "text": "In the ongoing debate on affordability of housing in the City of Vancouver we regularly hear voices expressing concern for “elderly folks of modest means who had been counting on using their home as a pension” to argue against doing anything to ease the housing affordability problems many (younger) people are facing. In these arguments the concern for “elderly folks of modest means” seems curiously focused to those who own property as opposed to, for example, the 20% of the elderly people in Vancouver that live in poverty (15k total).\nWhich got me a little thinking about “earning money” in this day and age."
  },
  {
    "objectID": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#work",
    "href": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#work",
    "title": "Work vs Twiddling Thumbs",
    "section": "Work",
    "text": "Work\nMost people earn money by working. We are interested in after tax household income, for simplicity we will look at 2010 income data reported in the 2011 census. The median after-tax household income in Vancouver was $50k (the average was $65k), although there is large regional variability. The pre-tax households incomes were $56k for the median and $80k for the average income.\nIncomes have grown a bit since then, median income levels in Vancouver grew by 7% from 2010 to 2013 (the latest year Stats Canada reports on) and will have grown a little more since then, but these are relatively small changes compared to other numbers I want to talk about.\nMost of this income stems from employment income which also includes self-employment, but 22% of pre-tax income in Vancouver came from outher sources, mostly investment income but also government transfer payments like child benefits.\nCumulative, the after tax income put $17.8bn into Vancouverite’s pockets."
  },
  {
    "objectID": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#twiddling-thumbs",
    "href": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#twiddling-thumbs",
    "title": "Work vs Twiddling Thumbs",
    "section": "Twiddling Thumbs",
    "text": "Twiddling Thumbs\nLet’s compare this to how much Vancouver home owner households earned last year by twiddling thumbs while sitting on their property. For simplicity we focus on SFH (“Single Family” or “Single Detached”) homes. We will consider the change in land value only, changing the building value because of renovation or rebuilding certainly does not happen by twiddling thumbs.\nThe typical SFH household made $262,000 last year by twiddling thumbs (so that’s the median land value rise for SFH homes between 2015 and 2016 property tax assessment years, the average was $318,877), for a tidy total of $24,651,312,001. Not too shabby. And considering that the typical owner household lives in their home, these are tax-free earnings once the owners sell. That’s substantially more than what the typical household in Vancouver takes home. The total land value increase for just the SFH properties amounts to $24bn out of the $45.2bn by which land values in Vancouver increased overall. For context, recall that the overall income earned by all Vancouverites in 2010 was $17.8bn after tax (21.3bn before tax).\nIn other words, just the SFH property owners alone earned more by twiddling their thumbs than the entire population of the City of Vancouver did by – actually working. And in most cases homeowners don’t even have to pay taxes on their thumb-twiddle earnings. Yet another indication of how out of touch the property market is with the local enconomy.\nThe numbers are just meant to provide a general frame of reference, they are not adjusted for inflation, and costs like property taxes and property transfer tax are not taken into account."
  },
  {
    "objectID": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#mapping-the-result-of-thumb-twiddling",
    "href": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#mapping-the-result-of-thumb-twiddling",
    "title": "Work vs Twiddling Thumbs",
    "section": "Mapping the Result of Thumb Twiddling",
    "text": "Mapping the Result of Thumb Twiddling\nThere is a map for this. (I probably would not take the time to write about this otherwise.) Last year’s assessment increase was quite large, not every year is like this (although it looks like the coming one will also be substantial). To even things out a little I mapped the average yearly gross land value increase for each property. For properties that I could not trace back all the way to 2006 I averaged over as many years as I could find.\n Looking at the map we see that the average yearly increase depends to a large extend on the square footage of the property. Large properties naturally stand out, unless they are stratified where the land value increase gets distributed across all the owners.\nAveraged over 10 years, the yearly land value increase for the typical SFH property in Vancouver was $79,800. Using Main as a divider the numbers are $66,700 for the typical east of Main and $141,700 for the typical west of Main property (using median increases).\nA good portion of the east-west difference is due to larger lot size, but even the land values increase per m² for properties on the west side was 35% higher than that of properties on the east side. These are yearly increases, to be accumulated and collected tax free in most cases when the property is sold."
  },
  {
    "objectID": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#twiddling-in-3d",
    "href": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#twiddling-in-3d",
    "title": "Work vs Twiddling Thumbs",
    "section": "Twiddling in 3D",
    "text": "Twiddling in 3D\n We can get even more fancy and show the effect the thumb-twiddling has on land values by animating the land and building value rise over time. You will need a modern computer to view this.\nIf you are interested there is more background on the visualization. I put a little more work into this, the numbers in the animation are inflation adjusted and it also includes duplexes and stratified condo units. There is a slider to filter by number of strata units in each building to explore the effects of stratifying a standard lot into several units has on affordability."
  },
  {
    "objectID": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#hourly-rate-for-twiddling-thumbs",
    "href": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#hourly-rate-for-twiddling-thumbs",
    "title": "Work vs Twiddling Thumbs",
    "section": "Hourly Rate for Twiddling Thumbs",
    "text": "Hourly Rate for Twiddling Thumbs\nJust for fun we can work out the hourly rate for twiddling thumbs. All we need to do is divide by the number of work hours by year. We can look up the average work weeks for a Vancouver worker in the 2011 census and, for style points also consider the number of household maintainers. After all, when we compare household income with land value increases we should consider how many people in the household work. 63% of Vancouver households have only one household maintainer, but 33% have two and 4% three or more household maintainers contributing to household income. On average a Vancouver household worked a total of 62 person-weeks per year to earn that income. Assuming a 40 hour work week, we compute the average after-tax earning to be $26 per hour from regular work and average after tax earnings of $126 per hour from twiddling thumbs. That’s assuming all household maintainers are twiddling the same amount of time that they dedicate to regular work. (In this calculation we use average not median income and land value increases, medians don’t lend themselves well to these kind of reasoning.)\nUsing the 10 year averaged numbers instead of focusing on just the last year we still get a healthy average after-tax twiddling thumbs rate of $44 per hour.\nI am not entirely sure why anyone would want twiddling thumbs to stay so lucrative. Unless maybe the super self-centered thumb-twiddlers. Or why earnings from thumb-twiddling should remain tax-free. While employment income is taxed (as it should be).\nConsidering to change your line work to thumb twiddling? To bad thumb twiddling is so unaffordable!"
  },
  {
    "objectID": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#data-dump",
    "href": "posts/2016-01-24-work-vs-twiddling-thumbs/index.html#data-dump",
    "title": "Work vs Twiddling Thumbs",
    "section": "Data Dump",
    "text": "Data Dump\nHere is the raw output of the stats run on the single family properties for ayone interested.\n\nSFH Land Value Rise (2015 - 2016)\n\nCity Wide: Average $313,072 Median $262,000, Count: 78740, Total: $24,651,312,001, Hourly: $126\nEast of Main: Average $230,106 Median $230,000, Count: 46014, Total: $10,588,122,001, Hourly: $92\nWest of Main: Average $429,725 Median $375,000, Count: 32726, Total: $14,063,190,000, Hourly: $173\n\n\n\nSFH Land Value Rise (2006 - 2016)\n\nCity Wide: Average $109,969 Median $79,800, Count: 77981, Total: $8,575,547,711, Hourly: $44\nEast of Main: Average $68,656 Median $66,700, Count: 45647, Total: $3,133,947,703, Hourly: $27\nWest of Main: Average $168,293 Median $141,700, Count: 32334, Total: $5,441,600,008, Hourly: $67\n\nThe total number of units for the 2006 to 2016 analysis are a little lower since not all properties can be traced over that time period without resorting to more tedious analysis."
  },
  {
    "objectID": "posts/2016-02-01-loss-of-character/index.html",
    "href": "posts/2016-02-01-loss-of-character/index.html",
    "title": "Loss of Character",
    "section": "",
    "text": "At the heart of the Vancouver affordability crisis is a phrase that comes up again and again. Loss of Character. To effectively move forward I think it is important to look closely what it means and how it is used.\nAnd what the data can tell us about loss of character."
  },
  {
    "objectID": "posts/2016-02-01-loss-of-character/index.html#loss-of-character",
    "href": "posts/2016-02-01-loss-of-character/index.html#loss-of-character",
    "title": "Loss of Character",
    "section": "Loss of Character",
    "text": "Loss of Character\nVery broadly interpreted, change of character is how people describe the process when the environment around them is changing in profound ways. And the neighbourhoods in Vancouver are indeed changing fast.\nLoss of character, as opposed to change of character, adds a value judgement. It implies that the former “character” was preferable to the new one.\nI want to highlight two ways in which this change manifests itself in Vancouver: Change in buildings and change in demographics."
  },
  {
    "objectID": "posts/2016-02-01-loss-of-character/index.html#change-in-buildings",
    "href": "posts/2016-02-01-loss-of-character/index.html#change-in-buildings",
    "title": "Loss of Character",
    "section": "Change in buildings",
    "text": "Change in buildings\nThere has been a lot of focus on buildings getting torn down and rebuilt. When a city runs out of land to build new housing, part of the existing stock will get increasingly cannibalized to make space for new buildings. It pays off to try and understand how this renewal process works.\nLet’s take a look at the age distribution of Vancouver’s single family homes.\n\n\n\n\n\n\n\n\nWe divided the building stock for each age into two groups, one with building value less than 5% of the total property value, and one with building value greater than 5%. The significance of that number is that historically over the last 10 years, buildings with this percentage below 5% had a 1 in 6 change of being torn down and rebuilt over a 10 year time frame, with sharply dropping frequency of tear downs of buildings above that threshold. Check the link for a more detailed analysis.\nWe interpret this graph as giving us a decent prediction of what part of our building stock will get torn down and rebuilt in the next 10 years, 1 in 6 of the red is predicted to go.\nWhat’s remarkable is that essentially the entire SFH stock built between 1935 and 1965 is at high risk of being torn down, while the period before that has a much better chance of surviving. This comes out of purely looking at the data, it would be helpful if people out there that understand the characteristics of the homes built at different times could shed some light onto why this maybe be the case.\nAppreciating some of the economic drivers of this process, we can try to understand how people feel about the process. There seem to be several different reactions to and thoughts on buildings being torn down that can be summed up with the words ‘heritage’, ‘waste’ and ‘McMansion’.\n\nHeritage\n Some feel strongly about ‘heritage homes’ which typically goes beyond the City of Vancouver list of heritage sites or more specifically the single family houses on that list. The term is used differently by different people and may be confined to offical heritage sites, or ones that have been ‘overlooked’ and should be part of that list, to houses of a certain age or style (“cottage homes”), to including pretty much anything old or decent looking like this 1996 building that made the news recently. Many of those heritage sites are well kept up, with a teardown coefficient well above 5%. But there are also some in quite desolate state.\nMost people see value in heritage, for this term to be a more usefuly driver in the discussion it would greatly help to narrow it down.\n\n\nWaste\nAnother narrative is that of ‘waste’, when a house gets sent to the landfill. And waste is an issue everyone is concerned about, land fill space is limited and the impact of the building industry is huge, as described in this 2011 article. The City of Vancouver has enacted some rules on how much of a building needs to be recycled, but they are very weak. In particular, there are no binding recycling requirements at all for post 1940 houses beyond recycling drywall. Generally the sentiment seems to be more concerned with the fact that a building is getting torn down, rather than focused on how to improve the process of salvaging and recycling the old home.\nThe good news is that most of the teardown activity in Vancouver has been focusing on low relative building value properties, so cases like the demolition of that 1996 building assessed at $1.9m are quite rare. That building has a teardown coefficient of 25%, and historically buildings in that range had a 1 in 200 chance of being torn down in a 10 year period.\nWhat’s concerning though is that as land values increase, so do the value (dollar amount) of the buildings that move into the high-risk category of teardown coefficient below 5%.\n\n\nMcMansion\nThe last important word is ‘McMansion’, which tackles the question of what the building gets replaced with. Typically the term ‘McMansion’ is used for a building whose primary concern is to maximize the allowable footprint. Economically speaking, this is a rational reaction to the exorbitant land values. Esthetically it can be quite jarring.\nThis is where the value judgement in “loss of character” becomes most apparent. While some people feel visually offended by them, people that buy these properties clearly prefer them over what was there before. Either way, they serve as constant reminders of the change the neighbourhoods are experiencing."
  },
  {
    "objectID": "posts/2016-02-01-loss-of-character/index.html#change-in-demographics",
    "href": "posts/2016-02-01-loss-of-character/index.html#change-in-demographics",
    "title": "Loss of Character",
    "section": "Change in Demographics",
    "text": "Change in Demographics\nProbably more important than the change in buildings is the change in demographics. Vancouver seems to be experiencing a gentrification process where the traditional financial elite of ‘high income’ households is priced out of the market by the new financial elite of ‘high net worth’ households. That change is dramatic and it has been going on for long enough now to have left very visible signs in the 2011 Census data.\n Looking at the ‘local affordability’ in Vancouver in 2011 we see that in many (not all) neighbourhoods the people that live there could not afford to buy again (based on their income). What that means is simply that the people moving in will be demographically very different from the ones living there right now. And yes, this problem is much worse in Vancouver than in other places in Canada, which can easily be seen by searching and panning in the map.\nThis problem has likely gotten much worse since, as housing prices continued to climb much more steeply than income since 2011.\n Another indicator of just how disconnected the property market, especially that of single family homes, is from the local economy is that the combined rise of land value of all single family homes combined was $24.6bn, more than the entire population of Vancouver combined income through work, investment (other than housing) or registered retirement income and government transfer payments during that year. Even considering pre-tax income (although the housing price increase is tax free for all those living in their house the year before they sell). Read here more details on this or browse through the map to see just how much each Vancouver homeowner made last year twiddling thumbs and waiting for land values to rise.\nSo how can people still buy in this market? Except for the rare cases of people with very large salaries to be able to save large sums for down payments and handle the mortgage payments, those purchases are founded in wealth, not income. Local wealth, maybe inherited or handed down from parents when they cached out on their home, or money brought in from abroad.\nWhat’s clear is that this wealth-based gentrification process is real and rapidly changing the social character of the neighbourhoods."
  },
  {
    "objectID": "posts/2016-02-01-loss-of-character/index.html#hard-truths",
    "href": "posts/2016-02-01-loss-of-character/index.html#hard-truths",
    "title": "Loss of Character",
    "section": "Hard truths",
    "text": "Hard truths\n\nWe can’t stop teardowns. The economic drivers are too strong. We can do better at designating and preserving heritage sites and nudging the process in other ways. Naively projecting past data forward we predict around 8,000 properties will be torn down and rebuilt in the next 10 years.\nAffordable single family homes are a thing of the past. Demand is growing and supply is shrinking, so prices will remain high and nothing short of Soviet-style interventions can change that.\nHousing in many neighbourhoods is now out of reach for working professionals that can’t rely on outside financial help like inheritance or mortgages backed by the assets of their parents.\nForeign money may have played some role in getting us here, but any solution that solely revolves around dealing with foreign money won’t solve the problem. (Someone should run some custom tabulations through Stats Canada to get a better handle on the signs of foreign money that can be picked up from the basic 2011 Census data.)"
  },
  {
    "objectID": "posts/2016-02-01-loss-of-character/index.html#preserving-and-restoring-character-my-2c",
    "href": "posts/2016-02-01-loss-of-character/index.html#preserving-and-restoring-character-my-2c",
    "title": "Loss of Character",
    "section": "Preserving and Restoring Character (my 2c)",
    "text": "Preserving and Restoring Character (my 2c)\nLooking forward, how can the character of the neighbourhoods be preserved or restored? Most of the public discussion focuses on the how to preserve the build form; the question how to preserve the social structure of the neighbourhoods receives much less attention.\nI don’t have a good overview about what ideas are out there, so I won’t attempt to summarize them. Usually I try to focus on data and keep my opinions out of this, but I could not help myself on this one. So here comes my 2c on what should be done.\n\nCharacter of Buildings\nFor buildings, we need to focus on what’s important to preserve. We need better rules on building waste. And we need to create policies that guide what teardowns will replace.\n\n\nSocial Character\nTo preserve and restore the social character of the neighbourhoods we need to create housing for the types of people that used to be able to buy in the neighbourhood and are now being pushed out. That is working professionals. Given that single family houses will remain out of reach, the only way to do that is to allow (low rise) condos in neighbourhoods. I don’t know if that means just upzoning arterials, or creating pockets of low-rise throughout neighbourhoods like it is common in my  native Germany. One example of this in Vancouver is the low-rise pocket at Arbutus and 33rd. That complex, when taken together with all of Quilchena park, still houses roughly 3 times more people per area than the typical single family housing neighbourhood with the same total area. And it collects the same amount of property taxes per square metre as the typical single family neighbourhood (without a park) while it is cheaper to service.\nAlternatives, like encouraging laneway houses and secondary suites can also increase supply and help ease the housing crunch. They are great at increasing rental stock. But they don’t increase supply fast enough, don’t serve the demand for home ownership and scatter up the green space into lots of tiny yards instead of freeing up space for larger parks that can be effectively used for kids to play and recreation. Focusing only on towers instead just intensifies the gentrification process, but there probably are areas where they make sense. There is no doubt that increasing supply has to be part of the solution, any ideas like ‘downzoning’ that fail to increase supply are counterproductive in restoring the social character of the neighbourhoods.\n If we are concerned about creating space for families the question becomes where to house them. With land limited, let’s map the density of households with children, so the number of households with children per km². We immediately see that the existing pockets of mid and high-rise are very effective at housing children households. Using Census data to model presence of children in households on physical housing parameters we see that having 3 or more bedrooms is the single most determening factor. If accommodating families is a priority, then we need to ensure that all neighbourhoods have 3 bedroom options, which is currently not the case (2011 data).\n\n\nDealing with a Wealth-based market\nMoving from a housing market supported by income to one supported by wealth, either of domestic or foreign origin, requires new rules to ensure fairness and preserve the basic social contract that our society is founded on: That everyone starts out with more or less the same opportunities, and work and grit can lead to social and economic upward mobility.\nWealth, especially foreign wealth, tends to be much more mobile and can decide on short notice to leave Vancouver. This introduces enormous risks to the local economy, and smart policies are needed to guard against these risks.\nIn Canada, people don’t pay capital gains on their primary residence. That works fine as long as increases in land value are relatively low, but with current situation where people earn more money sitting on their house than actually working the idea that income from work should be taxes and income from twiddling thumbs should not is just ludicrous. Let’s tax all capital gains on housing. We can deduct investments into improvements and can argue about details like indexing taxable gains to inflation. And let’s tax it at a rate that is higher than the one on income or investments in stocks to re-focus housing as being treated primarily to house people and not as investment.\nThis train of thought is similar in nature to the Vacant Housing Tax in that it would have minimal direct impact on the market (so it in itself won’t bring prices down in a meaningful way), but it would generate substantial amounts of revenue that could be used to provide relief. And the revenue is large enough that, if spent strategically (for example on building affordable rental), could have a meaningful impact on the overall property market. And restore fairness in a market where initial capital barriers are so high only the rich can participate.\nNext, with the property market driven by wealth (especially for single family houses), while the way we finance public services is based on income, we need to ask the question if our current taxation system is still adequate. A capital gains tax on housing is just one part of this, the proposal to tax vacant properties is another. We could strengthen legislation and enforcement on foreign income, assuming that this is a big enough issue (Give us better data!). Other options that should get discussed are a higher property transfer tax, progressive property taxes (possibly discounted against income tax) and other measures that pay more attention to the role of wealth vs income in the Vancouver property market. This discussion is important, but requires better data to be meaningful and effective.\n\n\nData\nThe profound lack of data is a major problem to find effective ways to fix the affordability crisis. Lack of clear data can provide a convenient excuse for inaction.\nBut data does not need to be perfect.\nMuch can be learned from the data at hand. A good list of data sources and reproducible findings would be a great way to establish a baseline, build better arguments and keep the discussion focused on the “big fish”.\nMaybe a dedicated GitHub repo could provide a suitable format for this."
  },
  {
    "objectID": "posts/2016-02-29-land-use/index.html",
    "href": "posts/2016-02-29-land-use/index.html",
    "title": "Land Use, Roads (and Parking)",
    "section": "",
    "text": "The other day at the SFU’s City Conversations someone asked a question about space dedicated to roads, and how that could be unlocked to aid housing. He mentioned what percentage of space is currently dedicated to roads. I forgot the number, but I thought to myself that I should look that up for all Metro Vancouver communities. So here we go.\nActually, it would be interesting to compare how land is allocated to all kinds of land uses across Metro Vancouver, not just roads. And with their excellent land use dataset it’s easy enough to do, justifying spending two hours on this. Looking at the land use map one can see that there are some issues with using the dataset for that purpose, for example the roads within Stanly Park are missing, as parking lots in Vancouver parks. So this slightly overestimates the green space. But when added up this won’t do much to change the overall area of each land use."
  },
  {
    "objectID": "posts/2016-02-29-land-use/index.html#land-use-breakdown",
    "href": "posts/2016-02-29-land-use/index.html#land-use-breakdown",
    "title": "Land Use, Roads (and Parking)",
    "section": "Land Use Breakdown",
    "text": "Land Use Breakdown\n Let’s start with a simple chart summing up the land use in each of Metro Vancouver’s Municipalities. Just select the one you are interested in from the dropdown.\n\n\n\nCity of Vancouver stands out as the municipality with the largest proportion of area dedicated to roads right of way. The right of way includes nature strips, sidewalks, and of course on-street parking."
  },
  {
    "objectID": "posts/2016-02-29-land-use/index.html#built-area-land-use",
    "href": "posts/2016-02-29-land-use/index.html#built-area-land-use",
    "title": "Land Use, Roads (and Parking)",
    "section": "Built Area Land Use",
    "text": "Built Area Land Use\nFor some of the municipalities large “Natural Areas” and “Undeveloped Area” make it difficult to discern the makeup of the built up areas. So here comes the same graph with some of the large space-cosuming categories taken out.\n\n\n\nWhat’s remarkable is that City of Vancouver still is the clear winner in terms of space taken up by roads right of way."
  },
  {
    "objectID": "posts/2016-02-29-land-use/index.html#roads-right-of-way",
    "href": "posts/2016-02-29-land-use/index.html#roads-right-of-way",
    "title": "Land Use, Roads (and Parking)",
    "section": "Roads right of way",
    "text": "Roads right of way\nIt might be interesting to compute more precisely how the roads right of way is used. But that requires a lot more work. The City of Vancouver has a dataset with road widths, which would help separate area taken by roads from area taken by sidewalks and nature strips. One could use google maps to estimate to estimate the amount of space taking up by on-street parking by sampling a couple of roads and scaling this according to road area. Too much work for me right now, maybe one day I will have a good enough reason to dedicate some time to this."
  },
  {
    "objectID": "posts/2016-02-29-land-use/index.html#parking",
    "href": "posts/2016-02-29-land-use/index.html#parking",
    "title": "Land Use, Roads (and Parking)",
    "section": "Parking",
    "text": "Parking\nParking in particular is a land use that is artificially inflated. We pay lots of money to buy private property to live on, and then continue to pay property taxes for that privilege. But we pay nothing to store our vehicles on public roads.\nAt current land values in Vancouver it just does not make any sense to socialize the cost of parking. A 12m² on-street parking space at a low-balled $3000/m² value is worth $36,000. If we were to price parking at value, rather than socializing the cost we could have a discussion about using that space more effectively. Killing the parking subsidy would have a substantial impact on demand (and car ownership), freeing up space to be re-allocated for other uses. For example for housing by increasing lot sizes and making multi-unit structures more feasible, especally on corner lots."
  },
  {
    "objectID": "posts/2016-02-29-land-use/index.html#update",
    "href": "posts/2016-02-29-land-use/index.html#update",
    "title": "Land Use, Roads (and Parking)",
    "section": "Update",
    "text": "Update\nSaw a great little data visualization fly by today that is just the missing link in the above visualizations. A simple way to visualize where all the municipalities stand in relation to one another. Thought I would throw that in real quick."
  },
  {
    "objectID": "posts/2016-03-05-physical-sfh-form-over-time/index.html",
    "href": "posts/2016-03-05-physical-sfh-form-over-time/index.html",
    "title": "Physical SFH Form Over Time",
    "section": "",
    "text": "I was curious how the physical parameters of Single Family Houses changed over time.\nUsing the assessment dataset merged with the land use dataset allows to fairly accurately pick out single family houses, and also holds the age of most properies. Together with the City of Vancouver LIDAR-generated building dataset that I have played with before we can look at physical building parameters.\nThe city dataset is a little coarse, it only contains the main building and does not map things like garages. It’s a little rough and should probably be interpreted cautiously on an individual building level. One could spend the time and derive the building data directly from the raw LIDAR dataset optimized for this purpose, but for some overview statistics the building dataset that was derived by the city is just fine. The LIDAR was taken in 2009, so we only consider houses built before that time."
  },
  {
    "objectID": "posts/2016-03-05-physical-sfh-form-over-time/index.html#site-coverage",
    "href": "posts/2016-03-05-physical-sfh-form-over-time/index.html#site-coverage",
    "title": "Physical SFH Form Over Time",
    "section": "Site coverage",
    "text": "Site coverage\n One question is how the site coverage of the buildings have changed over time. We simply line up the buildings by the year they were built, compute the site coverage of the main building relative to the parcel size and graph the quintiles for each year. For good measure, we throw in the 10 and 90 percentile whiskers.\n\n\n\n\n\n\n\n\nWhat jumps out immediately is that around 1988 there apparently was a change in zoning law that reduced site coverage of the main building. I don’t know anything about the history of building codes, maybe someone that does could shed some light onto this. Twitter was fast to provide the answer it appears that changes in building code were implemented to counteract the growing footprint of houses at the time, details can be found in Barbara Pettit’s Ph.D. thesis."
  },
  {
    "objectID": "posts/2016-03-05-physical-sfh-form-over-time/index.html#building-height",
    "href": "posts/2016-03-05-physical-sfh-form-over-time/index.html#building-height",
    "title": "Physical SFH Form Over Time",
    "section": "Building Height",
    "text": "Building Height\nThe next obvious point of analysis is how building height, in meters, changed over time.\n\n\n\n\n\n\n\n\nWe observe a similar jump in the data around 1988, with building height jumping up as site coverage decreases."
  },
  {
    "objectID": "posts/2016-03-05-physical-sfh-form-over-time/index.html#building-volumes",
    "href": "posts/2016-03-05-physical-sfh-form-over-time/index.html#building-volumes",
    "title": "Physical SFH Form Over Time",
    "section": "Building Volumes",
    "text": "Building Volumes\nNow let’s look at how “massive” the buildings are, measured by volume in cubic metres.\n\n\n\n\n\n\n\n\nThe graph does show that in general houses have gotten more massive starting in the late 80s, but after that have more or less maintained the same volume. It shows that the combined effect of the regulation change in the late 80s that resulted in smaller building footprints and taller buildings was that buildings overall got bulkier."
  },
  {
    "objectID": "posts/2016-03-05-physical-sfh-form-over-time/index.html#roof-type",
    "href": "posts/2016-03-05-physical-sfh-form-over-time/index.html#roof-type",
    "title": "Physical SFH Form Over Time",
    "section": "Roof Type",
    "text": "Roof Type\nLastly we see how the roof type changes over time, plotting the number of buildings with given roof type for each year.\n\n\n\n\n\n\n\n\nFlat roofs seem to have been quite popular between the mid 60s to mid 80s."
  },
  {
    "objectID": "posts/2016-03-05-physical-sfh-form-over-time/index.html#length-to-width-ratio-update",
    "href": "posts/2016-03-05-physical-sfh-form-over-time/index.html#length-to-width-ratio-update",
    "title": "Physical SFH Form Over Time",
    "section": "Length to Width Ratio (Update)",
    "text": "Length to Width Ratio (Update)\nA question about a regulation change in 1938 came up on Twitter, so I thought I should check into the length-to-width ratio to see if anything can be seen there. A ratio of 1 would mean a square footprint, a ratio of 2 means the house is twice as long as wide.\n\n\n\n\n\n\n\n\nNo obvious changes around 1938, but our houses became a lot more square after the late 80s zoning changes."
  },
  {
    "objectID": "posts/2016-03-05-physical-sfh-form-over-time/index.html#next-steps",
    "href": "posts/2016-03-05-physical-sfh-form-over-time/index.html#next-steps",
    "title": "Physical SFH Form Over Time",
    "section": "Next Steps",
    "text": "Next Steps\nAfter starting to see what kind of data can be derived from LIDAR data we can start to explore different questions. The graphs open a small window into the physical parameters of single family homes over time and are only of limited general interest.\nFor more serious analysis we would most likely have to start from the raw LIDAR data, which takes a little bit of effort."
  },
  {
    "objectID": "posts/2016-03-27-surrey-traffic-loop-counts/index.html",
    "href": "posts/2016-03-27-surrey-traffic-loop-counts/index.html",
    "title": "Surrey Traffic Loop Counts",
    "section": "",
    "text": "Surrey published a beta version of their traffic loop counts, which is pretty awesome. Real life traffic data is very exciting, and there are lots of fun things one could do with that. So last night I decided to take a look and make a quick map. Nothing exciting yet, just to feel may way around what’s there.  To keep things simple I again took advantage of the awesome Tangram mapping enginge and turned it onto the traffic loop data. All I did was plot circles for each traffic loop with the area proportional to the count. Just to get an idea what the traffic data looks like.\nFull screen view"
  },
  {
    "objectID": "posts/2016-03-27-surrey-traffic-loop-counts/index.html#what-do-we-see",
    "href": "posts/2016-03-27-surrey-traffic-loop-counts/index.html#what-do-we-see",
    "title": "Surrey Traffic Loop Counts",
    "section": "What do we see?",
    "text": "What do we see?\nReally not much at this point, it’s just a snapshot of one hour of traffic on March 24 between 9am and 10am. But it’s pretty obvious where to go from here. One should add some dynamic way to select the time frame. And add some animations to better represent car traffic and the direction in which it is moving. The cool tron demo simulates traffic movement, maybe the loop count data could be massaged in a way to give a more accurate representation of actual traffic.\nAnd of course one could start to run some analysis. The fifteen minute aggregates which the server sends down are a little rough for doing some traffic flow analysis, but I am sure that together with the lane direction information attached to the traffic loop geographic data something interesting can be done."
  },
  {
    "objectID": "posts/2016-03-27-surrey-traffic-loop-counts/index.html#hickups-and-feedback",
    "href": "posts/2016-03-27-surrey-traffic-loop-counts/index.html#hickups-and-feedback",
    "title": "Surrey Traffic Loop Counts",
    "section": "Hickups and feedback",
    "text": "Hickups and feedback\nThere were a couple of hickups along the way. The Surrey Open Data API sends down GeoJSON data in NAD 83 UTM Zone 10 instead of the default WGS84. Which is just fine, but when using the Mapzen Tangram engine we need to transform it into WGS84. The fancy way would be to do that dynamically after directly consuming the Surrey API, but we were lazy and just downloaded and transformed the file. So now we have the loop counter locations and can map them.\nNext up, we want to add the traffic count data to the loop counter locations. Ideally we want to consume the Surrey open data API and link it with the geographic loop data, but the API does not set the Access-Control-Allow-Origin: * http header to allow cross-origin requests to consume the API directly from their web app. Again, no big deal when building a small testing app, we simply downloaded the traffic loop counts for noon on March 23 and threw them up on our server.\nPretty smooth overall for a first run. Timestamps are a little funny, they are in local time. While easy to interpret, this will cause problems when analysing data around daylight savings time changes.\nLastly we needed to add a quick hack to the Tangram mapping engine. For some reason the transform function does not get called when adding static (non-tiled) data sources. But that was easy to fix, although it took some sloothing to track down.\nAt this point it seems that not all traffic loops are hooked up to the API yet. Going through the data, on the March 23 there are only a handful of traffic loops active, on the 24th the number of active loops jumps up significantly. I also plotted the traffic loop locations without data (in yellow), just to give and idea what will become available. Curious to see where this will go once out of beta mode."
  },
  {
    "objectID": "posts/2016-03-27-surrey-traffic-loop-counts/index.html#want-to-make-your-own-map",
    "href": "posts/2016-03-27-surrey-traffic-loop-counts/index.html#want-to-make-your-own-map",
    "title": "Surrey Traffic Loop Counts",
    "section": "Want to make your own map?",
    "text": "Want to make your own map?\nIt’s pretty easy. Just grab the html file from the “full screen” link above and download the Tangram scene file and add your own twist to the map. Also grab a copy of the tangram engine with the quick-fix for the transform function."
  },
  {
    "objectID": "posts/2016-04-01-on-dirt-and-houses/index.html",
    "href": "posts/2016-04-01-on-dirt-and-houses/index.html",
    "title": "On Houses and Dirt",
    "section": "",
    "text": "The story of Vancouver real estate is mostly a story of dirt. After spending a bit of time to collect relevant data I am now wondering how to make better visualizations to make that data more accessible.\nLooking at my old maps based around assessment data, there are a couple of things that bother me. They are not very good at showing changes over time. And they are not very good at highlighting the different role of dirt and the houses on top. Yes, they do have separate views for building value, land value and various changes over time. But the visuals aren’t catchy and intuitive. When mapping building values I show the properties the buildings sit on, instead of the building footprint. When mapping value increases I just map a some kind of ratio of values between specific years. I use colours to visualize values, highlighting large differences but losing the smaller differences within the colour brackets.\nWhat would a better visual look like? I want to better explain the different roles played by dirt and houses. But retain interactivity and the ability to zoom and pan around."
  },
  {
    "objectID": "posts/2016-04-01-on-dirt-and-houses/index.html#better-visualizations",
    "href": "posts/2016-04-01-on-dirt-and-houses/index.html#better-visualizations",
    "title": "On Houses and Dirt",
    "section": "Better Visualizations",
    "text": "Better Visualizations\nThe idea is to map buildings and properties on the same map. Using 3D mapping we can uses height as an intuitive representation for value, adding colour brackets for additional reference as values push through emotionally relevant brackets, like the $1m barrier. To show change over time we just add a time slider and animate the whole thing. To keep things simple and comparable we will focus on single family homes for now. \nWe build three maps, one showing land and buildings on the same map, one just showing land values and one just showing building values. All three can be animated over time.\nWe normalized all dollar amounts to 2015 by adjusting for inflation. There are good reasons for going either way, adjusting for inflation or keeping it in original dollar amounts. For this animation we felt the it was better to show real (inflation adjusted) value growth. Keep in mind that this means is that if a property cost $1,000,000 in 2005 dollars we will show the value for 2005 to be $1,175,686 in 2015 dollars."
  },
  {
    "objectID": "posts/2016-04-01-on-dirt-and-houses/index.html#the-rub",
    "href": "posts/2016-04-01-on-dirt-and-houses/index.html#the-rub",
    "title": "On Houses and Dirt",
    "section": "The Rub",
    "text": "The Rub\nI find these maps much more intuitive and instructive than earlier maps I did. But it comes at a price – these maps will give your GPU a run for it’s money. And they are only visible on modern computers and browsers. They can be viewed on mobile too, but don’t try to zoom out. They eat up quite a bit of memory and may crash your mobile browser.\nGo test how “modern” your computer is. It’s kind of fun to view the houses in different areas evolve over time. Try out the interactive animations with just the houses or with both, houses and land."
  },
  {
    "objectID": "posts/2016-04-01-on-dirt-and-houses/index.html#local-stories",
    "href": "posts/2016-04-01-on-dirt-and-houses/index.html#local-stories",
    "title": "On Houses and Dirt",
    "section": "Local Stories",
    "text": "Local Stories\n Zooming into these maps we can start to see the local stories of the Vancouver single family homes during the last 11 years. Over time one sees where the land value is doing almost all of the lifting, but occasionally one sees re-development or extensive re-modeling activity that pushes up the building value as buildings try to catch up with the sharp rise in land value.\nAnd this is really the essence of the Vancouver real estate story when viewing dirt vs buildings. The land values are rising so fast that the building values don’t have time to catch up. In fact, during the last year alone the aggregate land values of all single family homes exceeded the combined pre-tax income of all Vancouverites.\nThe speed of the rise in land values is also driving the rapid re-development of buildings as explained in an earlier post."
  },
  {
    "objectID": "posts/2016-04-01-on-dirt-and-houses/index.html#unaffordable-dirt",
    "href": "posts/2016-04-01-on-dirt-and-houses/index.html#unaffordable-dirt",
    "title": "On Houses and Dirt",
    "section": "Unaffordable Dirt",
    "text": "Unaffordable Dirt\n The visualizations show that Vancouver’s “unaffordable housing” is really all about “unaffordable dirt” and not “unaffordable houses”. Looking at only the houses, we see that while there are some multi-million dollar houses, most houses are very affordable. It’s just the dirt they sit on that is really expensive.\nThis distinction does not matter much if one is buying a house. But it does matter when making policy decisions. One way to make housing more affordable is to reduce the impact of the land value. Converting a single detached house into a duplex immediately cuts the land value in half. If we replace it with a 4-plex, land values go down to a quarter for each housing unit."
  },
  {
    "objectID": "posts/2016-04-01-on-dirt-and-houses/index.html#dividing-dirt-update",
    "href": "posts/2016-04-01-on-dirt-and-houses/index.html#dividing-dirt-update",
    "title": "On Houses and Dirt",
    "section": "Dividing Dirt (Update)",
    "text": "Dividing Dirt (Update)\nWe added stratified residential units into the visualization so we can take a closer look what happens when we divide dirt, that is when we allow more housing units on a single property. In the RT zones the city allows duplexes or coach houses, so two stratified units on a single property. In other zones, like RM, some CD and some commercial (mixed use) the city allows for more strata units per property. We added a unit slider that allows to filter residential properties by how many strata units they house. If upper and lower cutoffs are set to 1, we just get single detached properties. If both are set to 2, we get duplexes. If the slider range is 4-20 then we see all buildings with somewhere between 4 and 20 units. Note that we did not map any purpose built rental buildings, that is residential buildings that have more than 2 units but are not stratified.\n We then use the average value of the land and the building per unit to map colour and height. Starting out mapping only buildings with at least 40 units we see that the average price of a unit is solidly in the bottom color bracket for most properties. As we slowly add in buildings with fewer units we see that only once we get down to 3 and 4-plexes do we see a clear increase in more expensive properties on our colour scale. Adding in duplexes then leaves a clear mark with, in genreal, twice the value of 4-plexes on a comparably sized property, and then another jump by a factor of two as we add in the single detached properties.\nLooking at the general condo picture, we generally see savings on a per-unit basis as we increase the number of units per property. Those savings are most pronounced for lower unit counts and seem to flatten out once we hit around 20 units as building values per unit stabilize and land values also rise proportionally to the unit number.\nSome luxury developments stand out, but even there land values dominate. There are very few strata buildings where building values exceed land values. I would be curious to hear what economists think about this, to me it looks like a sign that land values of condo buildings are inflated because of the artificial sparsity of available land for such developments through restrictive zoning code."
  },
  {
    "objectID": "posts/2016-04-01-on-dirt-and-houses/index.html#data-only-if-you-need-to-know",
    "href": "posts/2016-04-01-on-dirt-and-houses/index.html#data-only-if-you-need-to-know",
    "title": "On Houses and Dirt",
    "section": "Data (only if you need to know)",
    "text": "Data (only if you need to know)\nThe data used for this consist of merging four datasets. The Vancouver property boundary data set, the Vancouver tax assessment dataset (for tax years 2006 and 2016, estimating values on July 1st of each previous year), the Vancouver building footprint dataset (based on 2009 LIDAR data) and the Metro Vancouver land use dataset to help identify single family homes, especially the 15% outside of RS zoned areas. The tax assessment dataset gives an estimate of the values on July 1st of the year indicated in the visualization.\nThe datasets all have their issues and are a little out of sync. The property dataset is for a fixed point in time and may not properly reflect the legal state of properties for all years (when for example properties were joined or split). The tax dataset is not in all cases properly linked to the properties, in particular if the tax code changed which happens in some but not all cases when the building gets redeveloped. There are 854 such single family home properties in Vancouver that we can’t trace for the whole time period. On the map they will be visible as greyed out properties for earlier years that in later years spring to life as we can trace them.\nEspecially in the earlier years the dataset will not properly identify all single detached homes if they have been converted to duplexes or other multi-family units in the meantime.\nThe building dataset represents a snapshot in time around 2009. We don’t show properties that did not have a building in 2009 (767 properties, 562 of which are single detached), and if properties have been re-built since 2009 we show the old building. The Land Use dataset is lagging behind a bit and has for example not caught up with recent re-developments along the Cambie corridor.\nWe also dropped 296 residential properties that did not have a land value for 2016, most lightly because they are being re-developed and linking the assessment to the property failed.\nThe land and building value data is estimated by BC Assessment and is based on sales of comparable properties in the time before and after July 1 and takes into account information from building permits and other evaluations. On an individual building level, especially for older buildings that have not been sold for a long time, the data can be somewhat inaccurate at times. But in aggregate it provides a fairly accurate snapshot of the property and building values.\nNeither the zoning information, not the land use have been updated to catch some of these freshly developed land assemblies. I have manually removed a good dozen obvious ones from the dataset, there are probably some more hidden in there.\nMost of the problems stem from the fact that BCAssessment data, which has much more comprehensive and cleaner data, is not directly accessible."
  },
  {
    "objectID": "posts/2016-04-20-tod-mode-share/index.html",
    "href": "posts/2016-04-20-tod-mode-share/index.html",
    "title": "TOD Mode Share",
    "section": "",
    "text": "Just saw a comment on the Pricetags blog pointing to a nice master’s thesis investigating various TOD metrics around skytrain stations. I got curious how the 2011 transit mode share compares to the earlier census years listed in the thesis. And how the mode share varies spatially.\nWith CensusMapper at my finger tips and building on the visuals from the previous post this is an easy exercise.\nWe again simply map our concentric circles around the stations, but this time we turn them into pie charts to show the commute to work mode share. And we show the values for the inner 400m circle and the 400m to 800m annulus separately. I also added some basic land use data, green for parks, pink for instututional and light blue for commercial or industrial to provide more context.\nWhat we would expect to see is that the transit mode share is larger in the 400m buffer, but curiously this is not always the case.\nClick, touch or hover to get the exact values. And sorry again, only new computers will get a meaningful result. Most smart phones and tablets will have no problems. &lt;iframe src=“/html/skytrain_mode_map.html” width=“80%” height=“500”, style=“margin:5px 10%;”&gt; Full screen view\nThe City of Vancouver average transit mode share is 30%, for Metro it’s 20%. Head over to CensusMapper to explore transit mode share in the region (or anywhere else in Canada) in more detail."
  },
  {
    "objectID": "posts/2016-04-20-tod-mode-share/index.html#what-i-missed",
    "href": "posts/2016-04-20-tod-mode-share/index.html#what-i-missed",
    "title": "TOD Mode Share",
    "section": "What I missed",
    "text": "What I missed\nThe master’s thesis was careful enough to exclude areas across the river from the analysis. That’s good thinking, I only took circular buffers around the stations to do this. As indicated in the previous post, in the future we will add the capability to automatically generate isochrones, so areas within 5 or 10 minutes walking distance, and use these as regions, instead of using circular 400m and 800m buffers around the stations."
  },
  {
    "objectID": "posts/2016-05-04-census-mapping-for-everyone/index.html",
    "href": "posts/2016-05-04-census-mapping-for-everyone/index.html",
    "title": "Census Mapping for Everyone",
    "section": "",
    "text": "At CensusMapper, we have come a long way from first dabbling in census data to building out a platform to map data Canada wide, adding the ability to easily drill down into individual census regions, improving mapping efficiency, adding the ability to automatically populate custom geographic data with census variables and adding older census data.\nWe believe all of these are comparatively small steps compared to the opening up of map making capabilities for everyone that we are rolling out today. Free to use, free of charge. Statistic Canada opened up census data for anyone to use, but the sheer volume of available data and complexities of mapping geographic data has kept this inaccessible to many.\nBy opening up basic map making capabilities to everyone we are putting the liberating that data from depth of spreadsheets and database servers and opening it up to everyone. One map at a time. Read on for tips and tricks or head right on over to Censusmapper to make your first CensusMapper map!"
  },
  {
    "objectID": "posts/2016-05-04-census-mapping-for-everyone/index.html#simple-maps",
    "href": "posts/2016-05-04-census-mapping-for-everyone/index.html#simple-maps",
    "title": "Census Mapping for Everyone",
    "section": "Simple Maps",
    "text": "Simple Maps\n To make your own map, start by choosing “Start New 2011 Map” from the “Make a Map” menu. If you are planning on saving and sharing your map you should log in or create an account first, but you can play with mapping census data without an account.\n We tried to make logging in super simple by providing oAuth signup with popular providers, but you can also go the old fashioned way and create an account manually by providing username (email) and custom password.\n To start your new map, navigate down to the census variable picker at the bottom of the sidebar. Click the bars to expand or collapse them. Once you found a variable you want to map, click the ‘globe’ icon and select how you want to map the variable.\n Depending on what variable you choose you have the option to map simply map the variable, in this case the total number of people that are married or living with a common law partner, or map the variable as a percentage of a “parent” variable, so in this case map the percentage of people that are married or living with a common law partner out of all people 15 years or over. If in doubt what the variables mean, head over to the Census Dictionary to look up the definitions.\n Once you press the button CensusMapper will do it’s magic and map the data for you. Zoom pan and use the search bar to expore the data across different regions and geographic aggregation levels in Canada. You may want to edit the automatically generated map label to make it more concise.\n To style your map you can select the “Colors” option in the orange map editor menu in the sidebar.\n There you can style your map by selecting a custom color scheme and fine tune your domain cutoffs.\n If you were signed in you could now select the “Save Map” option from the map editor menu and add a map title and a map description. Map descriptions can be styled in markdown.\nMaps can tell many different stories, use the map description to give some background and rational for your map. A good map story might link to different views of the map to support the story telling. You can use the “Paste link to current view into description” button to link to different map views. You can also link to different CensusMapper maps to create deeply integrated map stories. Rich map stories can provide powerful insights into our communities.\nIf you want to share your map with others, make sure you set the map visibility from “Private” to “Published”. Otherwise others will get an “access denied” message when they attempt to view the map. Published maps will automatically appear on the extended map list, so please take some time and write appropriate descriptions that others can enjoy them.\nHead on over to Censusmapper to get started."
  },
  {
    "objectID": "posts/2016-05-04-census-mapping-for-everyone/index.html#advanced-maps",
    "href": "posts/2016-05-04-census-mapping-for-everyone/index.html#advanced-maps",
    "title": "Census Mapping for Everyone",
    "section": "Advanced Maps",
    "text": "Advanced Maps\nThe ability to quickly map any census variable is quite liberating. At the same time it is also restrictive. Imagine all the things that could be mapped if you could combine census variables in other ways. And CensusMapper has that capability to dynamically weave a number of census variables into a function to be mapped. But with that comes another level of complexity and potential pitfalls that, at the moment, prevents us from opening this up to the general public.\nAt the same time, we are still trying to figure out the overall funding model at CensusMapper. We believe that we have plenty of value in our advanced mapping capabilities, as well as in our ability to automatically populate custom geographic data with census variables and our backend data analysis capabilities to focus on paid products that allows us to open up the Simple Map model for everyone for free. At CensusMapper, we love open data. And with time as we refine our funding model we may open up more advanced mapping capabilities to put more data into the palm of everyone’s hands."
  },
  {
    "objectID": "posts/2016-05-04-census-mapping-for-everyone/index.html#our-worries",
    "href": "posts/2016-05-04-census-mapping-for-everyone/index.html#our-worries",
    "title": "Census Mapping for Everyone",
    "section": "Our Worries",
    "text": "Our Worries\nWe are also a bit worried about how CensusMapper will be used. The internet has the ability to turn the most beautiful thing into something ugly, and we are hoping that mappers will use the tool responsibly and make meaningful maps that tell a story, instead of vanity maps or maps meant to isolate or smear specific groups of people. If we feel that the tool is being abused we might have to enforce a real name policy or partially close down CensusMapper. Please map responsibly."
  },
  {
    "objectID": "posts/2016-05-04-census-mapping-for-everyone/index.html#bugs",
    "href": "posts/2016-05-04-census-mapping-for-everyone/index.html#bugs",
    "title": "Census Mapping for Everyone",
    "section": "Bugs",
    "text": "Bugs\nIf you find any, let us know. The tool should work well on Mac using Chrome, Safari or Firefox and should work reasonably well on Windows with Chrome and Firefox. iPad, iPhone and modern Android should also work well. Please contact us if you find anything not working properly."
  },
  {
    "objectID": "posts/2016-05-20-density/index.html",
    "href": "posts/2016-05-20-density/index.html",
    "title": "FSR",
    "section": "",
    "text": "Ever since I played with the LIDAR-generated building height data I thought that I should use that to map gross floor area (GFA) and floor space ratio (FSR). Gross floor area is the total floor area inside the building envelope. So for a three storey building, it is the area of the footprint times three. Floor space ratio is the GFA divided by the area of the parcel it sits on. There are several reasons why this may be interesting.\nGFA is the currency of the developer. Especially once we enter the world of apartment or commercial buildings, gross floor area is directly proportional to the amount of money a developer can charge for a property, either when stratified and sold or when rented out.\nFSR on the other hand is a measure of density, and it is the currency of the zoning code. Height and site coverage are also regulated in the zoning code and there is an obvious relationship to FSR. But because GFA is so important to the developer, FSR usually receives more attention in public discussions.\nRead on or head straight for the interactive map to browse Vancouver by FSR. Read the disclaimer at the bottom before using any data for purposes other than general reference."
  },
  {
    "objectID": "posts/2016-05-20-density/index.html#maximum-allowable-fsr-and-spot-zoning.",
    "href": "posts/2016-05-20-density/index.html#maximum-allowable-fsr-and-spot-zoning.",
    "title": "FSR",
    "section": "Maximum Allowable FSR and Spot Zoning.",
    "text": "Maximum Allowable FSR and Spot Zoning.\nThe maximum allowable FSR is set by the zoning code. Because land is so valuable in Vancouver, most people will try to maximize GFA when building a new building, so they typically go right up to the maximum allowable FSR. Previously we have examined the effects of various related metrics like height and site coverage for single family homes.\n At least this is how it works for RS/RT zones. If you are looking to develop a different property, say a mixed use or an apartment building or commercial building you will probably be looking to up-zone. That is, you will try to convince the city (and the local population) to allow you to build more GFA than zoning would allow. If successful the city will create a new zone just for your property and ask for some compensation in form of CACs (aiming to recoup about 80% of the uplift in value the up-zoning creates) in return. That process is called spot zoning and it can be seen in form of the little “Comprehensive Development” zones that are visible as yellow sprinkles in the otherwise quite uniform zones on the map.\nIn reality the process of up-zoning a property is a lot more complex, arduous and time consuming, which favours larger developers with deep pockets that can take the risk and sit out the long process of eventually obtaining higher GFA. One of the many reasons why the “missing middle” is missing."
  },
  {
    "objectID": "posts/2016-05-20-density/index.html#mapping-density",
    "href": "posts/2016-05-20-density/index.html#mapping-density",
    "title": "FSR",
    "section": "Mapping Density",
    "text": "Mapping Density\n From Census data we know the nighttime density, that is where people live and sleep. But we don’t know where they are during the daytime.\n FSR is also often seen as such a measure of density, although it only focuses on density on property parcels and neglects all the space between parcels, like roads right of ways and parks.\nIn mostly residential neighbourhoods the two densities correspond quite closely, but in areas with large commercial buildings (nighttime) population density can be quite low while FSR is very high as can easily be seen by comparing the two maps.\n Tax density, that is property taxes collected per area, give another view of “density” in the city that highlights relative property values and emphasises commercial land use that pays five times the residential tax rate.\nThere are lots of other measures of density that are important for different purposes, for example household density (in full 3D, requires modern computer), the density of private dwelling units or toddler density just to name a few."
  },
  {
    "objectID": "posts/2016-05-20-density/index.html#next-steps",
    "href": "posts/2016-05-20-density/index.html#next-steps",
    "title": "FSR",
    "section": "Next Steps",
    "text": "Next Steps\nThere are several directions I want to take this. Now that FSR is available for all properties, I can compare purpose built rental buildings to strata buildings. For example, in my Buildings and Dirt map I can’t integrate purpose built rentals because I don’t know how many rental units they have. The assessment data from the Vancouver Open Data portal, does not have that information, and BC Assessment won’t share it (in a hassle-free way). But I can make a similar map based on GFA instead of number of units and examine how GFA effect land and building values across different land uses. And in many ways GFA may be a better unit for comparisons than number of suites.\n Or I can just make a straight-up map of total assessed value per GFA. And maybe add a filter to only show this for residential and mixed use units like in the image if that’s what we are interested in.\nAnother interesting application is to use this data to do something along the lines of this excellent analysis the New York Times published for Vancouver. FSR, next to building heights and lot coverage that I have already analysed from a different angle, is an important metric to detect buildings that could not be rebuilt under regular zoning.\nLastly, having data on building GFA, together with assessment open data, allows for reverse-engineering of the GFA of individual strata units. That makes the analysis of condo assessment data more meaningful as we can analyse assessed condo values by unit size. One of the reasons why we are only talking about single family homes (or in some cases RS zoned homes) and not condos is that it is very easy to compare single family homes. But comparing condos doesn’t make much sense unless one also has data on the size of each condo. And this has probably focused the public debate too much on single family homes when we should be paying a lot more attention to condos. And purpose built rental units."
  },
  {
    "objectID": "posts/2016-05-20-density/index.html#disclaimer",
    "href": "posts/2016-05-20-density/index.html#disclaimer",
    "title": "FSR",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe FSR was estimated using coarse building data extracted from LIDAR data. The motivation was to use the FSR for aggregate analysis and not for getting accurate estimates of individual buildings.\nSecondary buildings like garages or laneway houses are only inconsistently detected and mapped. The data is from 2009, buildings built after that are greyed out.\nThe algorithm to compute FSR from this building data is rather crude and only makes rudimentary attempts to adjust for varying floor heights of buildings different for land uses, overhangs and eyebrows and dealing with pitched roofs or mechanical units.\nBuilding data was clipped to property polygons to deal with cases of buildings spanning several legal lots or not properly separated.\nThe FSR computed only roughly resembles the FSR defined in building codes. In particular our model with grossly overestimate the FSR for buildings with overhanging features like the Lookout on the Harbour Centre.\nBlock level aggregates will underestimate FSR in the presence of properties excluded from the FSR calculation.\nDo not be use the FSR estimates as an authoritative source for any individual building FSR."
  },
  {
    "objectID": "posts/2016-07-06-mixing-data/index.html",
    "href": "posts/2016-07-06-mixing-data/index.html",
    "title": "Mixing Data",
    "section": "",
    "text": "Census data is very rich and with CensusMapper we now have about 1 billion data points right at our fingertips. And we have managed to open up some of our interfaces for everyone to explore data and make their own maps and freely share them.\nThings really get interesting when one mixes custom data with census data. While, at this point, these are not part of the freely available CensusMapper functionality we still wanted to share what can be done.\nAt CensusMapper we have developed three basic ways to rapidly mix custom data with census data. So this is really three blog posts in one."
  },
  {
    "objectID": "posts/2016-07-06-mixing-data/index.html#overlay-mapping",
    "href": "posts/2016-07-06-mixing-data/index.html#overlay-mapping",
    "title": "Mixing Data",
    "section": "1. Overlay Mapping",
    "text": "1. Overlay Mapping\nFirst up, advanced users can upload custom datasets and map them on top of census data.  For example, we took a business license dataset from the City of Vancouver open data catalogue, filtered it by the BusinessType field to only include businesses starting with “Restaurant” or “Liquor Establishment” and uploaded them to CensusMapper to map it on top of census data. We have used a map for median age that was recently created using CensusMapper’s free public interface (have you made your own CensusMapper map yet?), we have faded out areas outside of Vancouver and coloured Liquor Establishments in red and Restaurants in blue.\nGiving a visual impression of your own dataset in relation to census data is the first step to location analysis."
  },
  {
    "objectID": "posts/2016-07-06-mixing-data/index.html#populate-custom-data-with-census-data",
    "href": "posts/2016-07-06-mixing-data/index.html#populate-custom-data-with-census-data",
    "title": "Mixing Data",
    "section": "2. Populate Custom Data with Census Data",
    "text": "2. Populate Custom Data with Census Data\nNext up is to populate your own dataset with census data for further analysis. For the restaurants, we may be interested in attaching population estimates in 5 minute walking distance from each location. We might also be interested in specific age brackets, or numbers of recent immigrants from specific countries that we may want to target with a new restaurant, or maybe even income data.\nThis can be a time consuming and painful process, but we have automated this at CensusMapper.\nTo showcase how this works we will show an example using elementary school catchment areas in the City of Vancouver. The areas we have (from the Vancouver Open Data Catalogue) are quite out of date, but for the purpose of this example they work as we will compare them to 2011 census data. We will look at the school aged population in each catchment.\n\n Using the visual overlay we notice that the catchment areas do fit boundaries of Dissemination Areas shown on the map reasonably well, with some exceptions. The same cannot be said for Census Tracts, we can be reasonably confident that Dissemination Area data is fairly accurate.\nNext we use the built-in CensusMapper functionality to automatically populate the catchment areas with the census data we are interested in. When Dissemination Areas don’t line up with the catchment boundaries we go down to Dissemination Blocks to estimate how many children live on what side of the catchment boundary. We previously explained this process in more detail, the result is a spreadsheet with the population data by age for each catchment area.\n\n\n\nHere we show the results by school, the select element can be used to select any school of interest. The whole process of populating the school data with census data just required uploading the catchment boundaries and selecting which variables to attach."
  },
  {
    "objectID": "posts/2016-07-06-mixing-data/index.html#custom-census-data-mapping",
    "href": "posts/2016-07-06-mixing-data/index.html#custom-census-data-mapping",
    "title": "Mixing Data",
    "section": "3. Custom Census Data Mapping",
    "text": "3. Custom Census Data Mapping\nSometimes it is not practical to map custom data on CensusMapper. Maybe the custom data is too sensitive to be uploaded to CensusMapper servers. Or it is quite complex and is better mapped separately. So we created an API to pull in census data from CensusMapper to easily show census data on custom maps. And dynamically mix in your own data. As an example we mix census data with data from BCAssessment, again obtained through the Vancouver Open Data Catalogue (and enriched with open data from Metro Vancouver). For demonstration purposes take Dissemination Area geographies and Dwelling Characteristic data from CensusMapper and mash it up with our processed assessment data to explore the differences in how single family properties are classified.\n For example we can compare BC Assessment single family lot count to Stats Canada unit count in the dissemination areas that are exclusively made up of single family lots. This gives an indication of how many suites and laneway houses there are in those areas. The census is prone to undercount units, but still does a better job at estimating them than other data sources, like the city or BC Assessment.\nOne of the reasons why census unit counts come up higher than other methods is that the census also counts illegal units, which naturally are not part of other official government counts. There are different reasons why a suite may be illegal:  It could simply be that the owner has not made the effort to register it. Or the suite may not be up to code. And in some cases, a property may have more than one suite in the main building, which is illegal in the City of Vancouver. The latter ones we can pick out in census data, since a house with two secondary suites – so three dwelling units in one building in total – is classified as being an “Apartment, building that has fewer than five storeys”. So, in census dissemination areas that only have duplex or single family lots as residential land uses based on assessment and land use data, we can look for how many dwelling units the census places in an “Apartment, building that has fewer than five storeys”. And map them."
  },
  {
    "objectID": "posts/2016-08-04-rt/index.html",
    "href": "posts/2016-08-04-rt/index.html",
    "title": "What’s up with RT?",
    "section": "",
    "text": "RT is Vancouver’s zoning for duplexes. Over time, various areas have been zoned to allow duplexes. Examples are Kits Point, much of Point Grey Road reaching up to Broadway, much of Granview-Woodlands, parts of Mount Pleasant and many other areas.\nRecently I have had some interesting conversations on Twitter regarding RM-6 and over BBQ dinner about RT-7. Then the Granview-Woodland plan passed by council, and it contains a curious provision of reducing the outright FSR for the RT-zoned properties from 0.6 to 0.5.\nAll of which got me thinking. What is RT supposed to accomplish, how does the diverse RT-zoning rules influence development and how is RT overall performing?\nTo make things complicated many RT zoning rules contain provisions to allow for higher FSR, and more than two units, under the “heritage preservation” program. Most people like the multi-plex developments that come out of this process, but the road there is quite adurous. Permitting takes a long time and is marred by uncertainty. Those projects are done by small developers, that are ill-equipped to deal with risky and drawn-out rezoning processes.\nThese would be great projects carried out by small developers that lead to gentle and affordable density, as this kind of re-development does not require land-assembly and does not lead to crazy land-value lifts that require CACs to claw back some of that value rise as we explained before.\nWe have some ideas what a refreshed RT (and RS) should look like, and there are some efforts to push for changes. We leave the details of this to the experts and take a little field expedition to see how the different flavours of RT (and RM) perform in data."
  },
  {
    "objectID": "posts/2016-08-04-rt/index.html#why-rt",
    "href": "posts/2016-08-04-rt/index.html#why-rt",
    "title": "What’s up with RT?",
    "section": "Why RT?",
    "text": "Why RT?\nOne central question to ask is: What is the purpose of RT (or any) zoning? The initial idea was to allow higher density than RS, with two stratified properties on one lot. But since the creation of RT the RS zoning has undergone significant changes and allows, in many cases, for higher FSR and higher unit count, although not stratified. In much of RS three units are allowed on each lot, the main house, a secondary suite and a laneway house.\n So how does RT perform in the wild? There seems to be little effect RT vs RS zoning has on land values. The boundaries between RS and RT can’t be discerned from a land value map of all residential properties in RS, RT (and First Shaugnessey) zones.\nHow good are RT lots at realizing “zone capacity”, that is how many of the historically SFH get turned over into duplexes, or in some cases multi-plexes (through the heritage presercation program)? That depends as the following graph shows.\n\n\n\n\n\n We can also study this by mapping out all residential lots in RT zones, and color them depending if they are single family lots, duplexes or multi-plexes.\nSome RT areas are essentially indistinguishable from RS areas, they are almost entirely comprised of single family lots. A good example is the RT-4, most of which is in Grandview-Woodlands. And being in RT rathern than RS they often don’t even get the benefits of a laneway house. Coach houses, which are legal in many RT areas and could be stratified, come with such onerous side setbacks that they are very hard to build on regular lots.\nThere is a lot of untapped “zoned capacity”. Similarly, some of the newer RM zones show little appetite for realizing “zoned capacity”, possibly due to the added difficulty of having to assemble lots to do so.\nOne can argue about whether this is good or bad. Or one can look at the variation between the different types of RT and RM zones to see what causes the difference."
  },
  {
    "objectID": "posts/2016-08-04-rt/index.html#downzoning",
    "href": "posts/2016-08-04-rt/index.html#downzoning",
    "title": "What’s up with RT?",
    "section": "Downzoning",
    "text": "Downzoning\nTo understand how downzoning works we want to consider two different types on analysis. One is to look at what happened after RS zone was changed to reduce site coverage in the 1980 in response to concerns about ‘monster homes’. We ran an analysis using LIDAR data to understand the effect on the physical form of SFH and found that while site coverage indeed decreased sharply, the bulk of the building remained unchanged. and the rule change seems to have done little in easing concerns about ‘monster homes’, it simply changed some of the parameters determining what ‘monster homes’ look like.\nAnother way to look at the matter is to compare how re-development happened in difference zones. This is very hard, since there are many factors other than zoning that determine if a building gets re-developed. To get some idea we compiled two numbers for each of the RS, RT and RM zones (ignoring some of the delicacies in the zoning code and lumping together all sub-zones with the same leading number. So for example we lumped RT-4, RT-4A, RT-4N and RT-4AN into one category we simply label “RT-4”.\nFirst we check how many residential properties got re-developed since 2000.\n\n\n\n\n\n\nNext we take a look what percentage of the existing residential stock is in immediate danger of being torn down, using the methodology developed earlier.\n\n\n\n\n\nWhile there is some correspondence between re-development activity and ratio of teardowns, there are a number of notable exceptions. RT-1 and RM-9 stand out, but these are oddball cases with just a few properties.\n We can also map just residential properties in RT that were re-developed since 2000 and colour them by SFH, Duplex and Multi-plex status. We can clearly see how some RT areas, like RT-7, don’t see much re-development, others, like RT-6 see mostly multi-plexes being developed. Some zones, like RT-10, 11, 12 are too new to be judged on development happening since 2000. For example, zooming in on developments in RT-10 since 2009 paints a very different picture.\nThere is lots of stuff to explore, but for today I want to look at RT-7 and RT-6 in more detail.\n\nRT-7\n A much more interesting case is RT-7, with 35% of properties teardowns but only 4.1% of properties re-developed since 2000. Looking at the map we see that RT-7 is comprised of two pockets on the west side, and the teardown candidates stand out in red and orange. Filtering further to see what has been re-developed since 2000 we see that, considering the size of the two pockets, re-development favours the eastern pocket near 16th and Arbutus. We can clearly see that that the eastern pocket has an overall more valuable building stock, although it is not clear if that is due to higher rates of re-development or for other reasons, for example the presence of some slightly larger lots. We also note that re-development does produce duplex units and even some multi-plex.\nSo what is going on here? Looking at the RT-7 zoning we see that the area has been downzoned to 0.4 FSR as was rencently pointed out to me, which can conditionally be upzoned to 0.6 FSR. Additionally, there are caps on the number of units per hectar to further restrict density.\n\n\nRT-6\n Another interesting example is RT-6. RT-6 has seen modest levels of re-develpment and has a relatively healthy building stock. What’s even more interesting is that it contains many multi-plexes. And more importantly, developments since 2000 have mostly produced multi-plexes.\nPrime examples of the gentle density that we keep hearing about. I would say that something is working very well here.\nSo what’s the difference? In contrast to RT-7 RT-6 zoning allows for 0.6 FSR outright, conditionally increased to 0.75. But there are other important differences too. The RT-7 lots are typically smaller with many at about 340m² compared to the 580m² typical RT-6 lots. (Although RT-7 also contains some 580m² lots)."
  },
  {
    "objectID": "posts/2016-08-04-rt/index.html#conclusion",
    "href": "posts/2016-08-04-rt/index.html#conclusion",
    "title": "What’s up with RT?",
    "section": "Conclusion",
    "text": "Conclusion\nDoes downzoning work? It depends what the goal is. Looking at the RT-7 example, downzoning has slowed re-development compared to other areas in the city, but it also lead to a deterioration in building stock in RT-7. This is a stop-gap measure, eventually that lower value building stock will get re-developed.\nThe combination of larger lots and the heritage retention program in the RT-6 zoning seems to work in producing gentle density, except that the permitting process takes too long and it is not immediately clear what purpose the heritage preservation has that allows stripping the building down to the studs and possibly moving it to the front and then building an infill in the back. There must be a better way to deal with concerns behind heritage preservation (not much is “preserved” in this process) and at the same time cut down on the time it takes to jump through all the permit hoops involved.\nIt is clear that RS and RT (and RM) zoning would hugely profit from a clearer vision what these zonings should accomplish and by using data to benchmark how these targets are met."
  },
  {
    "objectID": "posts/2016-08-24-mobi-running-stats/index.html",
    "href": "posts/2016-08-24-mobi-running-stats/index.html",
    "title": "Mobi Running Stats",
    "section": "",
    "text": "I keep getting questions about Mobi stats these days. Rather than ansering them one by one I decided to just offer a live view into data generated by our shadow API. I made two simple views, the most recent month of daily bike checkout counts and the most recent week of hourly bike checkout counts. The data issues mentioned in our previous post still apply. For data geeks, here is a link to a very useful paper that compared estimates like I make to real usage data.\nApart from the usual caveats when dealing with scraped data, Mobi data comes with additional issues like duplicate stations that need to be filtered out to get proper counts. The view into the database below filters out these and other current known issues with Mobi data. But as this is a live view, it might not correctly deal with future unforseen issues with Mobi data. New stations, once Mobi adds them, will automatically show up on our Mobi map and will also be counted in the live usage graphs."
  },
  {
    "objectID": "posts/2016-08-24-mobi-running-stats/index.html#daily-usage",
    "href": "posts/2016-08-24-mobi-running-stats/index.html#daily-usage",
    "title": "Mobi Running Stats",
    "section": "Daily Usage",
    "text": "Daily Usage\nThis graphs shows the daily total counts of bikes checked out (with the usual data quality caveats) for the past month (starting from August 12, 2016)."
  },
  {
    "objectID": "posts/2016-08-24-mobi-running-stats/index.html#hourly-usage",
    "href": "posts/2016-08-24-mobi-running-stats/index.html#hourly-usage",
    "title": "Mobi Running Stats",
    "section": "Hourly Usage",
    "text": "Hourly Usage\nThis graphs shows the hourly total counts of bikes checked out (with the usual data quality caveats) for the past week.\n\n\n\n\n\n\nAuto Updating\nThe above graphs are dynamic, they display the latest available numbers. Feel free do come back to this page to check how usage develops over time. If you come back in one hour, it will have the latest hourly counts. If you come back tomorrow, it will have an additional day of data.\n\n\nMap Update\n We also thought that adding a little more context to the bike share map would be useful, so we added some gentle colouring to highlight parks, retail and institutional land uses.\nThe bike infrastructure (and land use) data comes straight from OpenStreetMap. Everyone is welcome to help update OpenStreetMap data. If you are on a desktop and shift-click into our map it will bring up the OpenStreetMap editor. There you can add the newest separated bike lane or make other changes (after logging in or signing up for an account). It will take between 1 to 5 hours for changes to go live on our map, and the changes you make will also be available to any other app relying on OpenStreetMap data."
  },
  {
    "objectID": "posts/2016-09-14-measuring-housing-affordability/index.html",
    "href": "posts/2016-09-14-measuring-housing-affordability/index.html",
    "title": "Measuring Housing Affordability",
    "section": "",
    "text": "Housing affordability is a serious issue that deserves attention. Affordability is generally defined comparing incomes to housing costs. And ideally also factoring in transportation cost, although that’s seldom done and we will not attempt today.\nRecently we took a detailed look at income data. Using what we learned, together with our detailed data on the development of prices of single family properties in Vancouver, we turn to the never dying question of affordability of single family homes.\nLet’s go through the motions of how different choices in income data can lead to different stories. Some general choices of what income to use is often pretty obvious. Others are more subtle and a matter of judgement. For simplicity we focus entirely on the situation in 2010, because that’s the last year we have detailed income data for the City of Vancouver and we also have detailed publicly available single family housing price data.\nWhen talking about housing, the starting point is to look at income of (private) households (as opposed to individual income). We may also want to consider the owner-renter split in the region we are interested in. For simplicity we will skip this."
  },
  {
    "objectID": "posts/2016-09-14-measuring-housing-affordability/index.html#household-income",
    "href": "posts/2016-09-14-measuring-housing-affordability/index.html#household-income",
    "title": "Measuring Housing Affordability",
    "section": "Household Income",
    "text": "Household Income\nHouseholds are as diverse as dwelling types. If we write a story about single family housing, household income of all households is too broad to be relevant. For example, in the city of Vancouver there are about 75,500 single family lots, but 264,573 households. So at most 28% of households can live in the main house on a single family lot. Taking the median household income makes no sense in this situation. We need to narrow down the target group. In Calgary on the other hand, 59% of the dwelling stock is single detached (and there are more properties if we count the ones with suites like we did for Vancouver), so in that case the assumption that the median household is looking to buy a single family home is more reasonable. Although the median household probably won’t be looking to buy the median single family home but a lower-priced one."
  },
  {
    "objectID": "posts/2016-09-14-measuring-housing-affordability/index.html#family-income",
    "href": "posts/2016-09-14-measuring-housing-affordability/index.html#family-income",
    "title": "Measuring Housing Affordability",
    "section": "Family Income",
    "text": "Family Income\nSo how should we measure the affordability of single family properties? That’s not an easy question. A natural choice may be to focus in on the 151,330 census families in Vancouver. Again, there are more than twice as many candidates as single family properties. So maybe narrow it down even further an only look at the 88,515 census families with children at home. Or the 63,790 couple families with children at home. Realistically speaking, even going with couple families with children at home there just aren’t enough single family properties to go around. Unless we are willing to force out all of the roughly 45% of the occupants of the main house on single family lots that currently only have 1 or 2 people living in there. But overall, couple families with children is probably a better metric for affordability for single family homes. Others will of course also buy single family homes, but maybe we will be less worried if they can’t afford one."
  },
  {
    "objectID": "posts/2016-09-14-measuring-housing-affordability/index.html#family-with-children-income",
    "href": "posts/2016-09-14-measuring-housing-affordability/index.html#family-with-children-income",
    "title": "Measuring Housing Affordability",
    "section": "Family with Children Income",
    "text": "Family with Children Income\nTo complete the example, in 2010 the median single family property cost $848,000 (average $1,133,000) and the median couple family with children income in private households was $92,068 (average $123,252). That gives a dwelling value to income ratio of around 9.2, showing that the median single family property is not affordable for the median couple family with children. By affordable we mean that with a 20% down payment the shelter costs stay below 30% of pre-tax income, which requires a ratio of at most 6.6 assuming an interest rate of 3% and 25 years amortization.\nIf we were to more realistically assume that the median income couple family with children were to look to buy a single family property priced at the 10 percentile level of $606,100 we get a dwelling value to income ratio of 6.6, barely squeezing in below our affordability cutoff."
  },
  {
    "objectID": "posts/2016-09-14-measuring-housing-affordability/index.html#matching-income-quantiles-with-housing-quantiles",
    "href": "posts/2016-09-14-measuring-housing-affordability/index.html#matching-income-quantiles-with-housing-quantiles",
    "title": "Measuring Housing Affordability",
    "section": "Matching Income Quantiles with Housing Quantiles",
    "text": "Matching Income Quantiles with Housing Quantiles\nAnother way to slice the data is to look at the household income distribution. By definition there are 37,750 single family homes more expensive than the median single family home. From the income distribution there are 44,215 households with income over $125,000. Taking $125,000 as our income we get a median dwelling value to income ratio of 6.8. Only when throwing in some secondary suite rental income can we get to the ‘barely affordable’ 6.6 ratio. Adding rental income is double-dipping though, rental income should already be included in our total income numbers. Assuming secondary suite rental income is correctly reported."
  },
  {
    "objectID": "posts/2016-09-14-measuring-housing-affordability/index.html#using-median-household-income",
    "href": "posts/2016-09-14-measuring-housing-affordability/index.html#using-median-household-income",
    "title": "Measuring Housing Affordability",
    "section": "Using Median Household Income",
    "text": "Using Median Household Income\nUsing the household income instead we would have gotten a ratio of 13.4, which is ridiculously unaffordable. But then again, the notion that the median household should be able to purchase the median single family property (or even a low-value single family property) is also ridiculous.\n Somewhat more reasonable is to compare household income to all housing, not just single family. This still ignores that half of the population in Vancouver is renting, but still can yield some interesting insights if one is interested in more detailed spatial distribution of affordability and compare it to other cities.\nAdding a scatter plot of local affordability index by proportion of owner households shows how in Vancouver, unlike in comparable cities, the percentage of owner households is a weak determinant of local affordability."
  },
  {
    "objectID": "posts/2016-09-14-measuring-housing-affordability/index.html#what-about-affordability-today",
    "href": "posts/2016-09-14-measuring-housing-affordability/index.html#what-about-affordability-today",
    "title": "Measuring Housing Affordability",
    "section": "What about Affordability Today?",
    "text": "What about Affordability Today?\nThis analysis assumes that all these households are comfortable to max out when buying a home and that there are no other competing significant financial obligations like heavy student loans. And, just as a reminder, we were looking at 2010. As of 2015 the price of the median single family home has increased 67%. We don’t have 2015 income numbers for the City of Vancouver (yet), but Couple Family median income for Metro Vancouver has increased 13% between 2010 and 2014, the last year with available data. Extrapolating out that trend to 2015, and assuming that whatever other income metrics we were looking at, we get an estimated income increase by 16%. So any affordability ratio is about 1.4 times worse in 2015 compared to the 2010 ratio. What may have been ‘barely affordable’ in 2010 is clearly unaffordable in 2015.\nSo we used the detailed income data available for 2010 to get very good understanding of the affordability situation in 2010. Then we used regional income trends to extrapolate that data to 2015 and matched it with 2015 housing data to get more up-to-date estimates. This is generally an effective method that yields good results."
  },
  {
    "objectID": "posts/2016-10-04-secondary-suites-and-taxes/index.html",
    "href": "posts/2016-10-04-secondary-suites-and-taxes/index.html",
    "title": "Secondary Suites and Taxes",
    "section": "",
    "text": "A couple of weeks ago I started thinking about secondary suites, laneway houses and taxes in the City of Vancouver. The number of secondary suites and laneway houses has been continuously growing. Rental income is probably one of the main reasons people choose to activate a secondary suite. What are the tax implications?"
  },
  {
    "objectID": "posts/2016-10-04-secondary-suites-and-taxes/index.html#secondary-suites",
    "href": "posts/2016-10-04-secondary-suites-and-taxes/index.html#secondary-suites",
    "title": "Secondary Suites and Taxes",
    "section": "Secondary Suites",
    "text": "Secondary Suites\nHow many secondary suites are out there? Nobody really knows, but we have some estimates. Comparing single family properties from the assessment roll to census counts provides one estimate. This is likely a lower bound as the census is prone to undercount suites. Using this method, we arrive at a lower bound of 30,000 single family properties with a suite in 2011, up from about 27000 in 2006. The above map gives a rough idea of their location, although it also includes the roughly 2000 stratified duplex properties.\nOf these we estimate that 7,000 had more than one suite in 2011, with a similar number in 2006. The number of laneway houses was quite small in 2011, likely less than 400. But by now the number has grown to over 2,000.\nIn total that adds up to an estimated around 37,000 suites in the city of Vancouver in 2011. Probably more in the recent 5 years, maybe fewer in the years between 2006 and 2011. Most of these will be operated as suites, with the main unit serving as a principal residence of the property owner. But this also includes investment properties where each unit, including the main unit in the house, are rented out.\nFor this post we will focus entirely on the suites. We might expand on this analysis at some later point and separate out owner-occupied homes from investment properties. Moreover we will use the more conservative estimate of 30,000 suites. Feel free to adjust that estimate in the interactive at the bottom see how this effects the estimates we make."
  },
  {
    "objectID": "posts/2016-10-04-secondary-suites-and-taxes/index.html#income-tax",
    "href": "posts/2016-10-04-secondary-suites-and-taxes/index.html#income-tax",
    "title": "Secondary Suites and Taxes",
    "section": "Income Tax",
    "text": "Income Tax\nSecondary suite rental income needs to be recorded on CRA form T776 and, after claiming deductions, taxed.\nEstimating the average suite rental income at $1,000, claiming $200 in deductions leaves us with $800 or $9,600 in monthly taxable income. With single family homeowners generally above the median income we use a marginal tax rate of 35% to estimate $3,360 of taxes per secondary suite, so cumulative $100,800,000 in income taxes payed on secondary suite rental income in the City of Vancouver."
  },
  {
    "objectID": "posts/2016-10-04-secondary-suites-and-taxes/index.html#capital-gains-tax",
    "href": "posts/2016-10-04-secondary-suites-and-taxes/index.html#capital-gains-tax",
    "title": "Secondary Suites and Taxes",
    "section": "Capital Gains Tax",
    "text": "Capital Gains Tax\nAssessing the capital gains implications is more complex. The capital gains exemption applies to a housing unit. If a property with several housing units is sold, only the capital gains attributed to the unit used as principal residence is exempt from taxation. Typically the portion of taxable gains is determined by the relative areas of the rented and principal residence parts of the home.\nSo the question whether the portion of a house taken up by a secondary suites is exempt from capital gains essentially boils down to the question whether or not the secondary suite counts as a “self–contained domestic establishment for earning rental income” or as “one or more rooms in the home”. I have asked the CRA to clarify how this would apply to someone converting a portion to a house to secondary suite or building a laneway house, to which I got the following response.\n\nThe conversion of a portion of a house that otherwise is the taxpayer’s principal residence for the purpose of earning income or the creation of a separate building or structure on land that forms part of the taxpayer’s principal residence for the purpose of earning income are examples of structural changes that would result in a deemed disposition and reacquisition of a portion of the property.\n\nSimilarly, when a homeowner deactivate a suite and absorbes it into their main residence that would trigger another “deemed disposition (and reacquisition) thereof at fair market value”. Which then would trigger a “taxable capital gain attributable to the period of use of such portion of the property for income–producing purposes”, just like it would at a sale of the property.\nIn the case when the homeowner rents out their entire home and later moves back in again, the homeowner can, under certain circumstances, make an election that he can maintain the primary residence status for most recent 4 years during which the home was rented out. I have asked the CRA to clarify if a similar election could be made for a secondary suite, when for example after four years of renting the suite is deactivated and again absorbed in the main house and the response was that “such an election cannot be made where there is only a partial change in use of the property.”\nBased on this I am fairly convinced that the portion of a home used as a secondary suite or laneway house is not exempt from capital gains tax. Despite my best efforts I might still be reading this wrong, in which case I would love to have an expert weight in on this.\n\nEstimating Capital Gains Generated by Suites\nTo estimate the amount of capital gains tax based on secondary suites expected to accrue annually in the City of Vancouver we use the inflation-adjusted land value gain of a home as a proxy for the portion of the appreciation of the property that is not due to improvements the owner made to the home, i.e. the portion that would be subject to capital gains tax if this wasn’t a primary residence.\nTo compute the taxable gains for an individual property we take the difference between the current land value and the (inflation-adjusted) present land value at the time the property was bought. To simplify things a little we will consider the current average (over SFH with suite) land value, the average holding period (years between buy and sell), and the average inflation-adjusted land value gains. This can be refined, but should suffice for our purposes.\nGoing back to our estimate, the current (July 2015) average land value of single family homes was $1,640,000. Suites skew toward the east, where average land value was $1,080,000. We roughly average these and take $1,300,000 as our base value. As average holing period we take 10 years, and we compute the average inflation-adjusted land value gain between 2005 and 2015 at 10.4% (9.7% on east side). So let’s use 10%.\nThat gives an estimate of the annual land value gain of $850,000 over 10 years, or $85,000 on average per year. As a sanity check, this is in line with estimates of average annual land value gain for single family properties we have done before, although these weren’t inflation-adjusted.\nRoughly estimating that on average a secondary suite takes up about a quarter of the area of a house, we attribute $21,250 of the $85,000 annual land value gains to the secondary suite for tax purposes.\nOf course capital gains tax are only payable when the property sells, but the yearly average gains accumulate and eventually properties will sell. In which case we expect capital gains taxes to be payed on $21,250 per year the suite was rented. So if a person sells a home with a suite that was rented for the last 10 years, the person will have to pay capital gains taxes on $212,500. On average, we expect $21,250 of taxable capital gains income per house with suite per year.\nThe way capital gains tax works is that half of the gains are taxed at the marginal tax rate. As these tend to be large sums of money at a time, it is safe to assume that the the effective tax rate on the gains is relatively high, say 35% or more. Some of the taxes could be deferred by e.g. transferring the gains into RRSPs and later taxed at a lower rate. But we will ignore this, it has the effect of lowering the effective tax rate on the gains and move the tax payments into the future, but will not effect the overall amount of capital gains declared.\nAssuming an effective tax rate of 35% on the gains we expect on average $3,700 of taxes payed per home with suite per year through this mechanism. With conservatively estimated 30,000 secondary suites in Vancouver that adds up to a tidy sum of $111,000,000 for capital gains taxes paid due to secondary suites in the City of Vancouver. Per year.\n##To Rent or Not to Rent Out a Suite In the case of rental suites with owner-occupied main portion the suite incurs both income and capital gains tax obligations. This begs the question what minimum rent is required to offset the capital gains obligations in a particular year. Typically homeowners don’t know the annual value value gains ahead of time, so this is more of an academic exercise than a practical questions. The important takeaway is that the capital gains exemption has the effect of dramatically lowering the cap rate for converting part of a principal residence into a rental unit.\nThe math is quite simple, we need to compare the net rental income (rent minus deductions) to the capital gains taxes payable due to the rental. A more comprehensive way to look at this is to look at the CAP rate based on land value investment, in this case the net rental income (after-tax rental income minus deductions) per land value attributed to the suite. With our standard assumptions this sits at 1.9%.\nThis we can compare to the effective tax rate on the capital gains, which is given by multiplying half of the tax rate (as only half the gains are taxed at that rate) with the rate of land value rise, which comes in at 1.75% with our standard assumptions.\nTo conclude this calculation we define the net cap rate as the difference between the cap rate and the effective tax rate on the capital gains. If this is positive, it is the advantageous to rent. If it is negative, leaving the property vacant will yield higher returns.\nMore specifically if we want to understand if it was advantageous to rent out a suite in a particular year for a particular property we can adjust the assumptions in the interactive at the bottom accordingly. For example, in the year 2015 we saw an average land value gain of about 20%. Adjusting the rate in the interactive, and setting the holding period to 1 year, we get a net CAP rate of -1.6%. The break-even rent, above which it is advantageous to rent out the suite, was $1,820."
  },
  {
    "objectID": "posts/2016-10-04-secondary-suites-and-taxes/index.html#taxes-collected",
    "href": "posts/2016-10-04-secondary-suites-and-taxes/index.html#taxes-collected",
    "title": "Secondary Suites and Taxes",
    "section": "Taxes Collected",
    "text": "Taxes Collected\nIt would be interesting to compare these estimates with actual taxes collected. But this won’t be easy without internal CRA data. The census for example does not break out rental income. But it does break out capital gains income. Not all capital gains income stems from selling homes with secondary suites of course and again, the census does not break out the sources of the capital gains income. But capital gains income is rare enough, especially on the suite-heavy east side, to yield some information.\n In the City of Vancouver, a total of $830 million of net capital gains and losses was reported to the CRA in 2010, most of which was claimed by people living on the west side. When comparing to 2010 taxes we should adjust our calculation to the average east side single family land value of $620,000 in 2010 and might want to assume a very conservative inflation-adjusted land value gain of 5% per year (I have no data before 2005, but one could use sales data to estimate this better). This would yield an expected taxable annual capital gains of $10,000 per suite in each area (with expected annual taxes generated of $1,750 per suite), adding up to over one third of all capital gains reported in the City of Vancouver that year. And that’s not counting taxable capital gains from selling investment properties that were not used as principal residence at all and where all the gains are taxable. Or any other source of capital gains income.\nEyeballing the total capital gains reported suggests that tax compliance could be improved. It seems that the changes in requiring the reporting of a sale of a principal residence that were announced yesterday will do just that."
  },
  {
    "objectID": "posts/2016-10-04-secondary-suites-and-taxes/index.html#interactive",
    "href": "posts/2016-10-04-secondary-suites-and-taxes/index.html#interactive",
    "title": "Secondary Suites and Taxes",
    "section": "Interactive",
    "text": "Interactive\nFor people interested in adjusting the assumptions made for these estimates I made a quick interactive to facilitate this. Adjust to fit your own assumptions.\n\nAssumptions\n\n\n\nSecondary Suites\n\n\n\n\n\n\n\n\n\nAverage Portion of House Taken by Suite\n\n\n\n\n\n\n\n\n\nAverage Rent\n\n\n\n\n\n\n\n\n\nAverage Rent Deductions\n\n\n\n\n\n\n\n\n\nAverage Marginal Tax Rate\n\n\n\n\n\n\n\n\n\nAverage SFH Land Value\n\n\n\n\n\n\n\n\n\nAverage SFH Holding Period\n\n\n\n\n\n\n\n\n\nAverage Inflation-Adjusted Land Value Gain\n\n\n\n\n\n\n\n\n\n\nComputed Quantities of Interest\n\n\n\nCAP Rate\n\n\n\n\n\n\nEffective Cap Gains Tax Rate\n\n\n\n\n\n\nNet CAP Rate\n\n\n\n\n\n\nBreak-Even Rent\n\n\n\n\n\n\n\nAnnual Taxes Generated\n\n\n\nAnnual Income Tax Generated\n\n\n\n\n\n\n\nAnnual Cap Gains Tax Generated\n\n\n\n\n\n\nAnnual Total Tax Generated"
  },
  {
    "objectID": "posts/2016-10-21-trick-or-treat-2016/index.html",
    "href": "posts/2016-10-21-trick-or-treat-2016/index.html",
    "title": "Trick-or-Treat 2016",
    "section": "",
    "text": "A year ago, as were were just getting CensusMapper up and running, we put out three Halloween-themed census maps. Those maps almost broke our servers when they went viral. At least as viral as census data goes. They were viewed by over 150,000 Canadians over the course of three days. And many of those came back to view the maps more than once.\nLots of things have happened at CensusMapper since last year, and we heeded the call and put some of CensusMapper’s prowess to use to make some important improvements for this Halloween.\nWe are stuck in a weired place this year. We just completed the most successful census in the Canadian history this May, but the results are still undergoing quality control and will only get released starting this coming spring. The 2011 census, that last year’s maps are based on, is now 5 years old. Some children that were of prime trick-or-treating age (which we take as 5 to 14 years old) in 2011 are now in College. And some that weren’t even borne in 2011 will be out knocking on doors this Halloween.\nAs people move around, the composition of neighbourhoods does not change all that much over time. Young kids grow up to take the place of the older kids in the neighbourhood. That was our take last year, as we were still mapping 2011 data to give people an idea how many kids would show up on their door or where to expect the most foot traffic."
  },
  {
    "objectID": "posts/2016-10-21-trick-or-treat-2016/index.html#the-maps",
    "href": "posts/2016-10-21-trick-or-treat-2016/index.html#the-maps",
    "title": "Trick-or-Treat 2016",
    "section": "The maps",
    "text": "The maps\nLast year we made three maps, the Trick-or-Treat Density map that visualized the expected trick-or-treat foot traffic, the Trick-or-Treat Onslaught Map that visualized how many kids to expect at your door and the Haunted House Map that’s mapping homes occupied by ghosts and other “unusual residents”.\nThis year we improved on those maps by building a dynamic estimator that estimates conditions on the ground this Halloween. We have:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrick-or-Treat Density Estimator that estimates the number of children of prime trick-or-treat age per area, so the expected trick-or-treat foot traffic in each area. Use this map if you plan on taking your kids out trick-or-treating and want to make sure you find the area in your neighbourhood with the most kids out on the street.\nTrick-or-Treat Onslaught Estimator that estimates the number of children of prime trick-or-treat age per doorbell. Use this map if you are guarding the door and want to know how much candy you should have ready.\nHaunted Houses Map that shows the houses that were occupied only by ghosts or unusual residents in May 2011. As ghosts and unusual residents tend to be restless we were not comfortable making estimates where they may be now, even with all of CensusMapper’s power behind it. So we only have the 2011 map to give some inspiration where the best chances might be to run into ghosts or cross paths with unusual residents."
  },
  {
    "objectID": "posts/2016-10-21-trick-or-treat-2016/index.html#how-did-we-estimate-things-for-2016-halloween",
    "href": "posts/2016-10-21-trick-or-treat-2016/index.html#how-did-we-estimate-things-for-2016-halloween",
    "title": "Trick-or-Treat 2016",
    "section": "How did we estimate things for 2016 Halloween?",
    "text": "How did we estimate things for 2016 Halloween?\nAn skillful estimator strikes a balance between simplicity and effectiveness. We decided to make use of the following data:\n\nThe number of children in the age brackets 0-4, 5-9 and 10-14 in each area in 2011.\nThe number of children in the age brackets 0-4, 5-9 and 10-14 in each area in 2006.\nThe percentage of the population in each area in 2011 that did not move between 2006 and 2011.\n\nBased on this, we split the estimate of the number of children in each area in 2016 into two groups. Children that already lived in the are in 2011 and still live there in 2016, and children that moved into the area between 2011 and 2016. We estimate the proportion by using the proportion of people that did not move between 2006 and 2011 as a proxy.\nThen estimating the number of children that did not move is easy, we simply age the children we saw in 2011 forward by five years and weight them by the proportion of non-movers.\nTo estimate the number of children moving into the area take into account at the trends from 2006 to 2011 in each area and extrapolate from there to 2016. And weight the number by the proportion of the population that did move and also allow for some population growth according to the 2006 to 2011 trend.\nI will spare you the exact formula. It is possible that refining the model and adding in more variables could make some improvements to the estimates, but overcomplicating the model also leads to risks of exaggeratig biases. We feel that our simplistic model strikes a good balance between simplicity and effectiveness. It will be interesting to see how we did when 2016 data comes in."
  },
  {
    "objectID": "posts/2016-10-21-trick-or-treat-2016/index.html#the-super-nerdy-details",
    "href": "posts/2016-10-21-trick-or-treat-2016/index.html#the-super-nerdy-details",
    "title": "Trick-or-Treat 2016",
    "section": "The Super Nerdy Details",
    "text": "The Super Nerdy Details\nFor the data nerds still reading, the CensusMapper servers actually don’t do any of that work. They just server straight-up census data. Plus the instructions of how to use the data to make the map. All the computations and drawings are then done locally on each user’s browser. That’s what keeps our servers lean, responsive, and most importantly maintains maximum flexibility. With the same process we can easily map any function built from census variables. Canada-wide and across all the entire geographical census hierarchy, from all of Canada down to Dissemination Areas and, for some data, even Dissemination Blocks. Bwetween the 2006 and 2011 census variables and all the different geographic regions we have about 1 billion fields in our database that we can map dynamically. Still a far cry from “big data”, but enough to require some careful architecture choices."
  },
  {
    "objectID": "posts/2016-10-21-trick-or-treat-2016/index.html#census-mapping-for-everyone",
    "href": "posts/2016-10-21-trick-or-treat-2016/index.html#census-mapping-for-everyone",
    "title": "Trick-or-Treat 2016",
    "section": "Census Mapping for Everyone",
    "text": "Census Mapping for Everyone\nCensus data can be very complex and mixing various variables is powerful yet prone to produce meaningless maps due to misinterpretation of the census variables. Since last Halloween and now we are proud to have been able to open up some of CensusMapper’s capabilities to the general public. Everyone can now make Canada-wide interactive maps based on single 2011 census variables via a couple of mouse clicks and share the freely. You can read more about this in a previous blog post or jump right in and make your own map."
  },
  {
    "objectID": "posts/2016-10-21-trick-or-treat-2016/index.html#faq",
    "href": "posts/2016-10-21-trick-or-treat-2016/index.html#faq",
    "title": "Trick-or-Treat 2016",
    "section": "FAQ",
    "text": "FAQ\nHere are some of the questions we got most last year:\n\nAre the maps only for Vancouver?\nNo. All CensusMapper maps are fully interactive and by default Canada-wide. Use the search function or geo-location button to switch to any place in Canada that tickles your interest. Pan and zoom around to explore. You can always share the current view of the map with your friends by grabbing the URL in your browser address bar or using the build in share buttons.\n\n\nWhat’s the difference between the Trick-or-Treat Density and the Trick-or-Treat Onslaught map?\nThe maps are related, both map the number of children of prime trick-or-treat age, but they are normalized differently. The Density map considers the number of children per area. This estimates how many you will see out on the street in the area. The Onslaught map considers the number of children per door, so you can estimate how many will come knocking. The difference can be seen in high density areas like Yaletown in Vancouver. There are a lot of kids living in Yaletown in a fairly small area, so the expected foot traffic is high. But these kids distribute over the units in the highrises there and on a per household (or per door) basis, the number of kids per household does not stand out.\n\n\nHow accurate are the predictions?\nAccuracy of predictions vary. Some parents will drive their kids halfway across town to find the best area for trick-or-treating. Some areas change too fast for our estimator to catch on. And some neighbourhoods have traditions and local areas where kids congregate that we don’t have data for. So these maps are best use in conjunction with local data.\n\n\nWon’t your maps lead to more people trick-or-treating outside of their neighbourhood?\nWhile our maps can be used to facilitate driving kids across town to trick-or-treat, we encourage everyone to also use the map to find areas near where they live, and where the kids can walk to. We believe an important part of the Halloween experience is for everyone to get to know their own neighbourhood better, see things in a new light and experience the joys of running into friends and acquaintances, and making new friends, along the way.\n\n\nI am running out of candy / I got way too much candy!\nThe Onslaught map just estimates the average number of children that knock on doors in each area. Some doors will see a lot more kids than others. Take the Yaletown example from above. Most of the action happens on the street. So while on average people can only expect a moderate number of trick-or-treaters at their door, the ones at ground level will be very busy, while the ones higher up will likely just see one or two neighbour kids from the same floor."
  },
  {
    "objectID": "posts/2016-12-13-updated-property-tax-data/index.html",
    "href": "posts/2016-12-13-updated-property-tax-data/index.html",
    "title": "Updated Property Tax Data",
    "section": "",
    "text": "The property tax data for the City of Vancouver has been available for a while now, and with new assessment data becoming available soon everyone’s worried about what their property taxes will look like. The City just passed a 3.9% increase in their budget, so on average everyone will pay 3.9% more taxes than they did last year.\nThe exact change in property taxes varies from property to property. There is a nice overview on how this works in general, for the City of Vancouver there is an added complication of land value averaging meant to soften sudden land value increases, that effectively serves to lower taxes for single family homeowners in a rising market.\nIf that’s all to abstract for you, keep reading.\n\n To make the change in property taxes a little more transparent I have added a time slider to my Tax Density by Land Use Map that I have described previously. So now people can go back in time and see how property taxes changed and compare it to their neighbours. At the same time I have updated the data on my regular assessment data maps to be based on the 2016 tax data, more background on the tax data is in this post.\nCheck out the interactive map.\nThis map also serves as a good reality check on the tax productivity of the land.\nSome caveats: I am missing data for some years or some properties, and this map aggregates property taxes for all strata lots in a stratified property, you will have to dive into the data yourself if you want to see how it changed on individual strata lots. Zoning and land use data stay at 2016 and don’t animate back in time because of availability.\nSpecial thanks to Mapzen for making it so ridiculously easy to make these maps and for Vancouver Open Data and Metro Vancouver Open Data for making that data available.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2016,\n  author = {von Bergmann, Jens},\n  title = {Updated {Property} {Tax} {Data}},\n  date = {2016-12-13},\n  url = {https://doodles.mountainmath.ca/posts/2016-12-13-updated-property-tax-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2016. “Updated Property Tax Data.”\nMountanDoodles (blog). December 13, 2016. https://doodles.mountainmath.ca/posts/2016-12-13-updated-property-tax-data."
  },
  {
    "objectID": "posts/2017-01-17-the-coveted-2m-6m-vote/index.html",
    "href": "posts/2017-01-17-the-coveted-2m-6m-vote/index.html",
    "title": "The Coveted $1.2m - $1.6m Vote",
    "section": "",
    "text": "Earlier this month the province increased the threshold for the homeowner grant from $1.2 million to $1.6 million dollars. It’s an election year, and with the BC Assessment data for the City of Vancouver now being available via their open data catalogue we can ask who exactly this move was targeting.\nRestricted to the City of Vancouver, the answer is quite simple. There are about 24,000 single family homes, 1,200 duplex units and 4,000 condo units in that bracket.\nLet’s take a closer look.\nIf we focus in on just the single family homes, which make up the vast majority of the units in that bracket, we see that they are (almost) entirely located in East Vancouver. In fact, we can see how Main Street delineates these properties quite neatly. In fact, there are fewer than 500 single family homes west of Main in that bracket.\nOne could be lead to think that Main Street is the “$1.6m line”. But of the single family homes east of Main, these only make up just over half of the properties there, there are almost as many that are assessed over $1.6m and almost 2,000 assessed under $1.2m."
  },
  {
    "objectID": "posts/2017-01-17-the-coveted-2m-6m-vote/index.html#changing-homeowner-grant-status",
    "href": "posts/2017-01-17-the-coveted-2m-6m-vote/index.html#changing-homeowner-grant-status",
    "title": "The Coveted $1.2m - $1.6m Vote",
    "section": "Changing Homeowner Grant Status",
    "text": "Changing Homeowner Grant Status\nThere are a number of single family homes where the homeowner grant status changed between 2016 and 2017. There are about 3,100 single family homes who did not qualify for the homeowner grant in 2016, but do now. And about 1000 that did qualify in 2016 but won’t this year.\nYes, that’s right, there are 1000 single family homes with 2016 assessment below $1.2m and 2017 assessment over $1.6m. I pity them, having to pay an extra $50/month in property taxes just because the province did not care about them enough to set the new threshold higher than $1.6m.\nTo round things up, there were a little under 500 duplex and condo units that did not qualify for the homeowner grant in 2016 and do in 2017, and under 200 that did qualify in 2016 and don’t now."
  },
  {
    "objectID": "posts/2017-01-17-the-coveted-2m-6m-vote/index.html#how-to-get-the-grant-if-you-are-renting",
    "href": "posts/2017-01-17-the-coveted-2m-6m-vote/index.html#how-to-get-the-grant-if-you-are-renting",
    "title": "The Coveted $1.2m - $1.6m Vote",
    "section": "How to get the grant if you are renting?",
    "text": "How to get the grant if you are renting?\nYou can’t. And your landlord can’t either. No matter how much your unit is worth, the province won’t be cutting any checks to lower your rent by $50/month. The homeowner grant is just one of the many perks exclusively available home owners."
  },
  {
    "objectID": "posts/2017-01-25-jane-jacobs-vancouver/index.html",
    "href": "posts/2017-01-25-jane-jacobs-vancouver/index.html",
    "title": "Jane Jacobs’ Vancouver",
    "section": "",
    "text": "Some time ago I saw Geoff Boeing’s excellent package to generate Jane Jacobs style street grid images. It’s lots of fun to compare different cities that way.\nIt can be hard to represent one city by one square mile, so I thought it would be neat to use this to compare different parts of Vancouver. Some common themes emerge for the central parts, the more outlying areas display very differnet patterns.\n\n So I dropped a couple of points on a map, downloaded the geojson and ran the script below. These are the results:\n\n\n\nDowntown\n\n\n\n\n\nWest End\n\n\n\n\n\nGrandview Woodlands\n\n\n\n\n\nKitsilano\n\n\n\n\n\nNorth Vancouver\n\n\n\n\n\nNew West\n\n\n\n\n\nSurrey\n\n\n\n\n\nMetrotown\n\n\n\n\n\nRichmond\n\n\n\n\n\nWest Vancouver\n\n\n\n\n\nLangley\n\n\n\n\n\nPort Moody\n\n\nIf you want to make your own, just grab the lightly adapted code below. Yes, it is that easy.\n\nCode to generate the images\n# jane_jacobs.py\nimport geojson\nimport osmnx as ox\nfrom IPython.display import Image\nox.config(log_file=True, log_console=True, use_cache=True)\n\nfile=\"data/van_cities.geojson\"\nimg_folder = 'images'\nextension = 'png'\nsize = 350\ndpi = 90\n\ncities=geojson.loads(open(file,\"r\").read())\nfor city in cities.features:\n    place = city.properties['name']\n    point = (city.geometry.coordinates[1],city.geometry.coordinates[0])\n    fig, ax = ox.plot_figure_ground(point=point, filename=place)\n    Image('{}/{}.{}'.format(img_folder, place, extension), height=size, width=size)\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Jane {Jacobs’} {Vancouver}},\n  date = {2017-01-25},\n  url = {https://doodles.mountainmath.ca/posts/2017-01-25-jane-jacobs-vancouver},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Jane Jacobs’ Vancouver.”\nMountanDoodles (blog). January 25, 2017. https://doodles.mountainmath.ca/posts/2017-01-25-jane-jacobs-vancouver."
  },
  {
    "objectID": "posts/2017-02-21-more-on-teardowns/index.html",
    "href": "posts/2017-02-21-more-on-teardowns/index.html",
    "title": "More on Teardowns",
    "section": "",
    "text": "A little over a year ago we ran some analysis on teardowns of single family homes in the City of Vancouver. We used the City of Vancouver open data to understand why some single family homes got torn down and other’s don’t.\nRelying entirely on open data, there were some important questions that could not be answered. So together with Joe Dahmen at UBC’s School Of Architecture And Landscape Architecture we came back to the question and folded in transaction data from BC Assessment to add some more details and rigor.\nThe result turned out quite similar to what our initial cruder methods came up with, but it lead to some important refinements.\nWe won’t go into the details of the findings here, you can read the online data story if you are interested. Instead we will go into a little more details how the analysis was done and what is still missing.\nThe most critical piece that we added was transaction data, that is which properties got sold in what year. Almost all properties that got torn down were associated with a property transaction in the 4 years around it getting torn down rebuilt.\nThis allowed us to refine the question from “why did building A get torn own and building B did not” to ask the same question only considering transacted buildings.\nConditioning on the most important determinant of a building getting torn down, the transaction, we could focus in much better on what building-specific parameters are driving teardowns."
  },
  {
    "objectID": "posts/2017-02-21-more-on-teardowns/index.html#variables",
    "href": "posts/2017-02-21-more-on-teardowns/index.html#variables",
    "title": "More on Teardowns",
    "section": "Variables",
    "text": "Variables\nWe had annual assessment data pegged at July 2005 through July 2016, although we excluded the July 2016 data for some parts of the analysis as the value gains that year where quite extraordinary and prices have come down a bit since then. We felt that this most recent assessment may not be a good launching point to project the future from.\nUnfortunately, the number of variables for teardowns that we have is quite limited. We only have good data on assessed land values, assessed building values and lot area. For a very small subset of about 500 buildings we also have the building age of the building that got torn down. We have GFA estimates for buildings that got torn down after 2009 through the analysis of LIDAR data that we did, but those estimates are quite crude and again only cover a portion of our time frame.\nA crucial variable that we are still missing is the actual time of the building demolition. We inferred this from the time a new building got completed on that property, but this inevitably introduces noise to the data. It makes it difficult to pick the right time to calculate the relative building value. Moreover, there may be the occasional property that got built on vacant land, so nothing got torn down. This was less an issue for the analysis part, where we had ways to filter out such properties, but it did cause some problems with the visualization part of the project. We did filter out some properties manually that we could identify as being built on vacant land within the timeframe of the visualization, namely some properties on Deering Island.\nOn top of that, the decision to demolish the building was often made long before the building got torn down. Waiting times on demolition permits can be quite long, depending on the property. Having access to building permit data would help sharpen this variable. The word from the friendly open data folks is that the City of Vancouver is working on making these public, maybe an FOI request can help them speed up the process."
  },
  {
    "objectID": "posts/2017-02-21-more-on-teardowns/index.html#noise",
    "href": "posts/2017-02-21-more-on-teardowns/index.html#noise",
    "title": "More on Teardowns",
    "section": "Noise",
    "text": "Noise\nThe most important source of noise in our data is that fact that assessment data is only accurate on average. For particular buildings it can be substantially off. We suspect that this is one of the reasons why for buildings that are assessed to be essentially worthless, the teardown probability tops out at a little above 60%. So someone paying $2.5 million for a house that is worth only $10,000 to move in and live in that house makes absolutely no sense. If the building like this did not get torn down, we hypothesize one of three scenarios:\n\nThe building was purchased as a pure investment vehicle and rented out until an opportune time to re-develop or sell the property.\nThe assessment grossly undervalued the building.\nThe building was extensively renovated.\n\nWe have looked through the data and have found little evidence that scenario 3 is playing out in significant numbers. Extensive renovations show up in assessment data via building value gains and the “year improved”. We don’t have data to assess the other two hypotheses."
  },
  {
    "objectID": "posts/2017-02-21-more-on-teardowns/index.html#model",
    "href": "posts/2017-02-21-more-on-teardowns/index.html#model",
    "title": "More on Teardowns",
    "section": "Model",
    "text": "Model\nGiven that limited variables we trained a handful of models on our data to see how to best predict future teardowns. In all models we used, the relative building value was the single most predictive variable, accounting for well over 80% of explanatory power no matter what methods we used. Moreover, the performance of more complex machine learning models was not markedly better that using a simple logistic regression. Similarly, dropping all other variables except the relative building value only slightly decreased the skill of our model.\nOne way to improve on our model is to use a proper survival analysis that can better account for data that is only available for certain time frames. For example, teardown early in our time frame suffer from the shortcoming that we don’t have transaction data reaching back far enough to link the teardown to a transaction. Or more to the point, be able to compare it to other transacted properties that didn’t get torn down. Similar problems occur at the end of our time frame, and with variables that are only available in certain sub time frames."
  },
  {
    "objectID": "posts/2017-03-06-rs-population-change/index.html",
    "href": "posts/2017-03-06-rs-population-change/index.html",
    "title": "RS Population Change",
    "section": "",
    "text": "With reporting on the new census numbers gaining traction, and now Mayor Robertson picking up on single family neighbourhoods losing population we thought it is time to crunch some numbers.\nWhy does it need number crunching? All the reporting so far is based on looking at CT (Census Tract) aggregates, like e.g. in the map shown and linked to the right. But there is actually no single CT in the City of Vancouver that only contains RS zoning. Deducing results by just looking on CT aggregates can lead to misleading reporting, like we have seen with unoccupied dwellings in the “Marine Gateway Neighbourhood”. Given how prominent this topic has become it is high time to dig into the details."
  },
  {
    "objectID": "posts/2017-03-06-rs-population-change/index.html#tldr",
    "href": "posts/2017-03-06-rs-population-change/index.html#tldr",
    "title": "RS Population Change",
    "section": "TL;DR",
    "text": "TL;DR\nIn summary, we can confirm that RS (single family), RT (duplex) and FSD neighbourhoods have been dropping population. Slightly. Looking separately at the east and the west side, we notice that population in these neighbourhoods dropped by about 1% on the west side and increased slightly on the east side.\nIn all groupings that we looked at the household size dropped and the rate of unoccupied dwellings increased. This was counter-acted by a growth in dwelling units, mostly confined to RS zones where laneway houses and suites were added (or newly discovered in the 2016 census).\nWe split the analysis into core regions, blocks that lie completely within RS, RT and FSD zoning, and fringe regions, blocks that have RS, RT or FSD zoning as well as other zoning. Fringe regions grew in population and had overall lower rates of unoccupied units when compared to core regions."
  },
  {
    "objectID": "posts/2017-03-06-rs-population-change/index.html#comparing-censuses",
    "href": "posts/2017-03-06-rs-population-change/index.html#comparing-censuses",
    "title": "RS Population Change",
    "section": "Comparing Censuses",
    "text": "Comparing Censuses\nComparing data across censuses is hard. For one, definitions change from one census to the next and thus variables aren’t always comparable. Four our immediate goal of comparing population, private dwellings, and private households between the 2011 and 2016 censuses that is not a concern.\nComparing data is relatively easy when census geographies are large (i.e. CT, CSD level or higher) and the census geography matches exactly the area that we are interested in. For CSDs (Municipalities) this is often the case, but at the sub-municipal level, the CTs (Census Tracts) or other sub-municipal aggregation levels rarely line up with the regions one is interested in.\nFor example, if one is interested in changes in population in RS (single family zoned) neighbourhoods in Vancouver, looking at selected CTs will only give some initial indication. The reason is that there is actually no CT in the City of Vancouver that is entirely RS zoned. There are several that come close (the closest one is CT 9330015.01 around 41st and Thyne, which actually increased population from 5,364 in 2011 to 5,485 in 2016) but it shows how tricky it is to answer the really simple question how the population changed in RS neighbourhoods.\nSo how to deal with this issue? The cleanest way is a custom tabulation from StatCan, but that takes time, costs money, and may still result in problems when data was not geocoded correctly, which is next to impossible to detect in custom tabulations.\nAn alternative way is to compare censuses at finer aggregation levels, that is at DAs (Dissemination Areas) or DBs (Dissemination Blocks)."
  },
  {
    "objectID": "posts/2017-03-06-rs-population-change/index.html#comparing-censuses-at-da-or-db-levels",
    "href": "posts/2017-03-06-rs-population-change/index.html#comparing-censuses-at-da-or-db-levels",
    "title": "RS Population Change",
    "section": "Comparing Censuses at DA or DB levels",
    "text": "Comparing Censuses at DA or DB levels\nFor our concrete example, that means we look for DBs within RS zoning and work with these.\nThere are several difficulties with this approach. The most important is that the finer data we look at, the more likely we pick up problems with Census data (yes, there are problems) and mistake them for real world data. For our case through, we can avoid some of that by aggregating over all DBs within RS zoning to even out some of these issues. Morever, we can visually inspect the data to look for any particular DB that seems to be problematic and do some ground-truthing to see if the issues are only in the data or actually on the ground.\nThe next difficulty is technical in nature. Census geographies, including DBs and DAs, can change from one census to another and thus may not be comparable. In order to get reliable results we need to make sure that we work with a common set of geographies for both the 2011 and 2016 censuses. Luckily this is only a technical problem that can be overcome as census geographies don’t just change randomly but still retain some basic comparability.\nAnd it’s perfect timing, since we just created a “least common denominator” tiling derived from 2011 and 2016 DBs and DAs. At CensusMapper we work with “cartographic” DBs (so we clip our major bodies of water), which leads to a minor issue where between 2011 and 2016 things were clipped slightly differently which yielded two DBs in 2011 (with 16 and 17 people in it) being clipped out (and having no people in it) in 2017, likely a combination of adjusting some tolerances in the StatCan algorithms as well as some minor changes in geocoding that moved the population into an adjacent DB (like e.g. happened with 59150971009 and 59150971008 south of King Edward split along Carnarvon). Apart from that, the result is a tiling of Canada by DB and DA-based geographies that allow for consistent comparisons across the two censuses.\nIn numbers there were 493,192 cartographic DBs in 2016, 489,676 in 2016, and these resulted in a least common denominator tiling by 445,953 DB-based geographies."
  },
  {
    "objectID": "posts/2017-03-06-rs-population-change/index.html#rs-rt-and-fsd-zones-and-population-change",
    "href": "posts/2017-03-06-rs-population-change/index.html#rs-rt-and-fsd-zones-and-population-change",
    "title": "RS Population Change",
    "section": "RS, RT and FSD Zones and Population Change",
    "text": "RS, RT and FSD Zones and Population Change\n Back to our original question, how did population change in RS zoning. Before we go there, we think it makes more sense to expand the question to ask for RS, RT (duplex) and FSD (First Shaughnessy) combined as these are about equally restrictive in what we allow there.\nGrabbing the latest available zoning data and uploading it to CensusMapper makes it easy to download the 2011 and 2016 dissemination blocks that intersect RS, RT and FSD zoning. We removed the RS part that snakes along the downtown beaches and covers Stanley Park, as well as the sliver creeping up False Creek and covering the marinas there.\nWhen we intersect the census data with the zones, we also compute the overlap each DB has with the zoning and disregard any region with less than 10% overlap. Moreover we divide the dissemination blocks into core blocks where the overlap is greater than 99% and fringe blocks, where the overlap is less than 99%.\nOne should remember that a significant portion (a majority actually) of RS, RT and FSD dwellings are contained in “fringe” areas. So it is best to focus on the rates of change, we would expect the total number of population decline of all RS, RT and FSD zoned properties to be higher.\nHere are the results:\nRS, RT, FSD\ncore: 236 DB, fringe: 475 DB\nPopulation 2011 - 2016\ncore pop change: -69, fringe pop change: 6673, core total pop: 124916, fringe total pop: 284231\nDwellings 2011 - 2016\ncore dw change: 2812, fringe dw change: 7142, core total dw: 47008, fringe total dw: 121488\nHouseholds 2011 - 2016\ncore hh change: 1194, fringe hh change: 5127, core total hh: 42298, fringe total hh: 111801\n What we see is that the population in the “core” DBs did drop. Slightly. At the same time the number of dwellings increased quite noticeably by 6.3% in the core, with essentially all of the dwelling growth located within RS zones (as opposed to RT and FSD). So most of that dwelling growth is due to suites and laneway houses. Note that we only capture 47008 dwellings in the “core” RS, RT, FSD areas, which is less than half of the dwelling units in RS, RT and FSD with the remaining dwellings are located within the “fringe” regions.\n Interestingly, the number of households grew much slower than the number of dwellings in the “core” regions, increasing the rate of unoccupied units from 7% to 10%. Following our decomposition of population growth mapped here and explained in more detail in a previous post, we see that the population growth of -69 in the “core” regions can be decomposed into:\n\n-3,699 due to declining household size,\n-4,321 due to increase in unoccupied dwellings, and\n7,952 due to increase in dwelling units.\n\n We can do the same analysis for the “fringe” areas, where RS, RT and FSD zoning mixes with other zones. Here we get a population increase by 2.4%, driven by an increase in dwelling units by 6.2%, and dampened by shrinking household size and a more modest increase in the rate of unoccupied units from 6.7% in 2011 to 8% in 2016. We note that the rate of unoccupied units increased significantly less on the fringe when compared to the core.\nBreaking up the population growth of 6,673 people as before we have:\n\n-6,667 due to declining household size,\n-3,996 due to increase in unoccupied dwellings, and\n17,336 due to increase in dwelling units.\n\n\nThe West Side\nLastly, we probably can’t talk about this without running a separate analysis for the west side. So here we go.\nRS, RT, FSD\ncore: 88 DB, fringe: 184 DB\nPopulation 2011 - 2016\ncore pop change: -510, fringe pop change: 1631, core total pop: 42878, fringe total pop: 105309\nDwellings 2011 - 2016\ncore dw change: 572, fringe dw change: 2426, core total dw: 16735, fringe total dw: 49210\nHouseholds 2011 - 2016\ncore hh change: -18, fringe hh change: 1226, core total hh: 15066, fringe total hh: 44907\nWe see that this confirms conventional wisdom that the population decline in the core areas of RS, RT, FSD is stronger on the west side (and in fact population did increase overall in the core areas on the east side). The rate of unoccupied (by usual residents) units was quite similar to the overall RS, RT, FSD, climbing from 6.7% in 2011 to 10% in 2016.\nAgain splitting up the population change of -510 people in the “core” area into components we get:\n\n-458 due to declining household size,\n-1,587 due to increase in unoccupied dwellings, and\n1,535 due to increase in dwelling units.\n\nFor the “fringe” west side areas we again observe that population increased at 1.5%, dwellings by 5.1% and the rate of unoccupied units grew slower from 6.6% to 8.7%. Splitting up the population change of 1,631 people:\n\n-1,278 due to declining household size,\n-2,466 due to increase in unoccupied dwellings, and\n5,376 due to increase in dwelling units."
  },
  {
    "objectID": "posts/2017-03-06-rs-population-change/index.html#glossary",
    "href": "posts/2017-03-06-rs-population-change/index.html#glossary",
    "title": "RS Population Change",
    "section": "Glossary",
    "text": "Glossary\nWe are a little loose with our use of language. In this post “unoccupied” is always short for “not occupied by usual residents”, so in simpler terms “not used as primary residence”. “Occupied” refers to “used as primary residence”.\nWest side and east side were divided along the longitude for Ontario road.\nCore regions are Dissemination Blocks that have at least 99% overlap with RS, RT or FSD zoning.\nFringe regions are Dissemination Blocks that have between 10% and 99% overlap with RS, RT or FSD zoning."
  },
  {
    "objectID": "posts/2017-04-03-joyce-collingwood/index.html",
    "href": "posts/2017-04-03-joyce-collingwood/index.html",
    "title": "Marine Gateway and Joyce-Collingwood",
    "section": "",
    "text": "There has been some recent confusion that got confounded further about transit-oriented development in Vancouver harbouring a large number of non-primary residence homes. Good data is important in moving forward in Vancouver’s crazy housing market. Without proper context, a couple of data points can serve to paint a very misleading picture of what is happening. So I decided to fill in some gaps on the very narrow question of understanding the CT level numbers that get tossed around. No deep analysis, just looking into the CTs in question to see where the numbers that the census picked up came from."
  },
  {
    "objectID": "posts/2017-04-03-joyce-collingwood/index.html#tldr",
    "href": "posts/2017-04-03-joyce-collingwood/index.html#tldr",
    "title": "Marine Gateway and Joyce-Collingwood",
    "section": "TL;DR",
    "text": "TL;DR\nTo understand the overall rate of 24.4% non-primary residence dwelling units at the Joyce census tract, one should split the area into the Wall Centre Central Park development (99.2% non primary residence units) and the rest of the CT (3.4% non-primary residence units).\nTo understand the Marine Gateway CT (24% non-primary residence dwellings), it should be split inte the block with Marine Gateway development (13.7%), the block containing the MC2 development (67.4%), and the rest (10.1%).\nComparing any of these very recent developments to the much older Coal Harbour makes no sense. Coal Harbor is still “filling in” although at a stubbornly slow rate. It will be interesting to see if the new vacancy tax can help speed that up."
  },
  {
    "objectID": "posts/2017-04-03-joyce-collingwood/index.html#marine-gateway",
    "href": "posts/2017-04-03-joyce-collingwood/index.html#marine-gateway",
    "title": "Marine Gateway and Joyce-Collingwood",
    "section": "Marine Gateway",
    "text": "Marine Gateway\n\nTo understand the 24% non-primary residence units in the census tract containing the Marine Gateway development, we split the area into three parts. The block with the Marine Gateway development has in 460 dwelling units, 63 (13.7%) of which were found not to be primary residences. The block north of that containing MC2 had 570 dwelling units 394 (67.4%) of which were not used as primary dwellings. The remaining part of the CT as 1,507 dwelling units, 152 (10.1%) of which were not used as primary dwellings.\nWhat we see here quite nicely is how the rates of non-primary residence units are changing as the buildings they are in get older. Both Marine Gateway and MC2 are fairly recent projects, but Marine Gatway itself got completed about a year before MC2, and have significantly lower rates of non-primary residence homes.\nWhat needs further sleuthing is the rate of 10.1% for the rest of the CT, which is significantly higher than the baseline of 5.4% in 2011. There are no other larger pockets of increase of non-primary residence units in that area. This is might be best analysed in conjunction with similar increases in non-primary residence dwellings in some other single family neighbourhoods and goes beyond the scope of this short note.\n\nData:\n\nNon-primary residences\nChange in non-primary residence dwellings\nTotal change in non-primary residence dwellings"
  },
  {
    "objectID": "posts/2017-04-03-joyce-collingwood/index.html#joyce-collingwood",
    "href": "posts/2017-04-03-joyce-collingwood/index.html#joyce-collingwood",
    "title": "Marine Gateway and Joyce-Collingwood",
    "section": "Joyce-Collingwood",
    "text": "Joyce-Collingwood\n The story for Joyce-Collingwood is quite a bit cleaner (but holds some data complexities that manifest themselves in geocoding changes between censuses that can mess with fine-grained data).\nThe articles focus on the increase by 609 CT-level non-primary residence dwelling units at Joyce. The articles hypothesise that “Speculation is one of our prime suspects” and “We don’t know why, but we know it’s concentrated in a few neighbourhoods.” without making any effort to look beyond CT-level data to find out.\nDiving in we see immediately that the Dissemination Block at the north-west corner of Kingsway and Boundary had 707 non-primary residence dwelling units in 2016, vs 3 in 2011. That’s a net change of 704 non-primary residence dwelling units in just that block. It block contains the Wall Centre Central Park development, that completed shortly before the census. The census counted 713 units, only 6 of which it found occupied.\nAt the same time, population dropped by 161 people in this block, as the previous single family homes made way to the new development, easily explaining the drop in population by 57 in the CT level data that has people scratching their heads.\nSo what’s the rate of non-primary residence buildings once we take out the Wall Centre Central Park that got completed just before the census?\nIt’s 3.4%. Which explains why Jennifer Gray-Grant, executive director at Collingwood Neighbourhood House, was quoted finding the stat of 24% non-primary residence home “perplexing”.\nIt is also interesting to look back through time to see the rates in 2006, when the area around Aberdeen Park had 18.1% non-primary residence units. I am guessing that some major building projects got completed in the year before the census, by 2011 the number of non-primary residence units dropped dramatically.\n\nData:\n\nNon-primary residences\nChange in non-primary residence dwellings\nTotal change in non-primary residence dwellings"
  },
  {
    "objectID": "posts/2017-04-03-joyce-collingwood/index.html#olympic-village",
    "href": "posts/2017-04-03-joyce-collingwood/index.html#olympic-village",
    "title": "Marine Gateway and Joyce-Collingwood",
    "section": "Olympic Village",
    "text": "Olympic Village\nThe high rate of non-primary resident units in 2011 in the Olympic Village should be seen in the context of the City’s decision not to sell the units below cost but hold onto the units and sell them at a later point when the market recovers, which eventually paid off.\nTo understand the current overall rate of 9.4% non-primary residence units, it is instructional to simply zoom into the block level data and observe how the rates change depending on the completion time of the building.\n\nData:\n\nNon-primary residences\nChange in non-primary residence dwellings\nTotal change in non-primary residence dwellings"
  },
  {
    "objectID": "posts/2017-05-16-lifeblood/index.html",
    "href": "posts/2017-05-16-lifeblood/index.html",
    "title": "Lifeblood",
    "section": "",
    "text": "Ever since that Bloomberg article whose claims nobody could reproduce, where the author refused to disclose what data was used, but that got recycled all across the local press there has been a hightened interest in migration patterns in Vancouver. Nathan Lauster took it upon himself to dig deeper and look if Vancouver’s lifeblood was really leaving, which he kept elaborating on as better data became available until the most recent iteration that compares Metro Vancouver to other Candian metropolitan areas as well as the City of Vancouver to other cities within Metro Vancouver using 2016 census data.\nThis is seriously good work and we thought it would be helpful to reproduce Lauster’s methods in CensusMapper. The result is a series of maps, one for each five-year age cohort, that visualized net migration of the cohort geographically, while hovering over a region reproduces Lauster’s net migration bar graph for that region.\nWith CensusMapper, we instantly get the ability to dig into areas to observe finer net migration patterns, and to pan across the country to compare different regions.\nFor Metro Vancouver, we can see how we have strong net in-migration in all younger cohorts, including children. And as Lauster observed, we see a net out-migration of 50 to 60 year olds.\nFor the City of Vancouver we see a net in-migration for 5 to 30 year olds, with basically all other age groups exhibiting net out-migration. This is echoed by Burnaby and New Westminster, and to a lesser extend Richmond. We can zoom in further to the Census Tract level to observe spacial patterns of net migration at the sub-municipal level. There we see that net migration of the under 5 cohort and the 20 to 24 cohort are almost exact mirror images. The strongest signal comes from the town centres of the region, that get flooded as the 20 to 24 year old cohort ages into the 25 to 29 year old cohort. As they form families and have kids, the flood reverses and as many children that under 5 that lived in these areas don’t live there any more when they advance to the 5 to 9 cohort.\nSadly we don’t have common tiles for other censuses that would allow for hassle-free comparisons to detect changes over time. (It’s a surprising amount of work to get those common tilings right.) But we can compare this pattern to other population centres in Canada, and this trend seems to hold true universally.\nMetro Vancouver as a region certainly looks quite healthy in terms of the net migration of it’s age groups. The migration patterns within the region also seem to be consistent with other population centres in Canada. These alone, at least at the qualitative level that we are mapping, does not seem to explain the prevalent feeling of millennials “fleeing” Vancouver. Maybe a more quantitative approach could dig up more. Or it could be that families moving away from the central regions is generally perceived as a “move up” in most cities, but in Vancouver it is viewed as “moving down”."
  },
  {
    "objectID": "posts/2017-05-16-lifeblood/index.html#technical-details",
    "href": "posts/2017-05-16-lifeblood/index.html#technical-details",
    "title": "Lifeblood",
    "section": "Technical Details",
    "text": "Technical Details\n\nCommon Geographies\nThe first (and biggest) problem is that the 2011 and 2016 census geographies don’t necessarily match up. But we solved that problem for the 2011 and 2016 censuses.\n\n\nEasier said than done\nThe devil is of course always in the details, and as usual it is not until one has reproduced something until one can fully appreciate the work that went into the original piece. There were two main challenges for us to reproduce this within CensusMapper.\n\n\nThe basics\nWhat is net migration? To compute the net migration for an age group, say the “under 5” (in 2011) age group, we want to compare them to an age group in 2016. To do that we need to “age them forward”. Children that were under 5 in 2011 were 5 to 9 years old in 2016. But some, in the case of the under 5 age group only a very small fraction, won’t have lived to 2016. So we need to subtract those out. More formally, we need to apply the apporpriate mortality rates as we age them forward. Then we compare the aged-forward group to the 5 to 9 year old cohort that the 2016 census counted, and we divide by the size of the original under 5 year old cohort (from 2011) to get the percentage net migration.\n\n\nMortality\nThis basic fact of life makes net migration really tricky to get right. Once we get into the older age groups, and thus higher mortality rates, the process is extremely sensitive to changes in the mortality rates. In CensusMapper we are using effective mortality rates of 5 year cohorts from CANSIM for Canada as a whole and the provinces and territories. A fair amount of massaging is needed to make sure this works properly, in particular it is important to interpolate the effective mortality rates as we age a cohort forward. And one should recognise that the mortality rates are based on the age at death, but that an age cohort in the census is, on average, already half way through a given year in their life.\nAdditionally, there is little reason to believe that there is no geographic variation in mortality rates within the provinces. To deal with that we decided to add an uncertainty band to the graph that allows for a 5% variation in mortality rates.\n\n\nUncertainty\nThe next issue is that census data underwent random rounding to an adjacent number that’s divisible by 5, which leads to an expected error of 1.6. (Actual rounding gives an expected error of 1.2.) When taking differences between censuses, that yields an expected error of 2.2. Additionally, we don’t know the actual number of people in each age group, just the one the census found. The “census undercount” can be quite sizable. Overall it is around 3%, but it varies by age group and geography.\nTo deal with this we add an error term of a mis-count of 5 people that also shows up in our error bars in the graph.\n\n\nFine geographies\nCensusMapper is great for exploring all geographic levels. But that can lead to problems when our age cohorts become small in size. We divide by the size of the original 2011 age group cohort, and to avoid running into small denominator issues we cut our estimates off when there are fewer than 50 people in a cohort. Our error bars partially take care of this issue, but at 50 people our error bar already spans 10% points of net migration on either side of the computed value, showing values with even larger error bars is likely to do more harm than good."
  },
  {
    "objectID": "posts/2017-08-23-density/index.html",
    "href": "posts/2017-08-23-density/index.html",
    "title": "Density",
    "section": "",
    "text": "Density in Vancouver has been one of the recurring themese on this blog, and there are many different ways to come at it. We have looked at density in terms of land use to understand how much land is devoted to what purpose in Metro Vancouver and it’s municipalities. We have looked at density in terms of tax density to understand how property tax revenue depends on land use and zoning. We have looked at density in terms of built floor space ratio.\nAnd of course we have looked at population density through CensusMapper, and this time we want to do a quick variation on that theme."
  },
  {
    "objectID": "posts/2017-08-23-density/index.html#gross-density",
    "href": "posts/2017-08-23-density/index.html#gross-density",
    "title": "Density",
    "section": "Gross Density",
    "text": "Gross Density\nGross (population) density simply looks at the total population divided by (land) area. Ignoring census undercounts, we can simply look at the 2016 census numbers to compute these."
  },
  {
    "objectID": "posts/2017-08-23-density/index.html#net-density",
    "href": "posts/2017-08-23-density/index.html#net-density",
    "title": "Density",
    "section": "Net Density",
    "text": "Net Density\nNet density takes residential land (instead of all land) as it’s base. One measure of net density is the floor space ratio (FSR), which we have approximated and mapped in the past and that also includes commercial space next to residential living space. More details on this are in this older blog post.\nNet density is typically what people refer to in the countext of building developments. Sadly it’s hard to get a hold of good data sources that would allow for meaningful comparisons across the country. The provincial assessment authorities have that data, but in Canada this data only shared after significant financial commitments."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html",
    "title": "Amazon - The Canadian Data",
    "section": "",
    "text": "I wasn’t really getting into the Amazon HQ2 thing, but then the Upshot did some analysis that excluded Canadian metros. That’s not right. So I decided to fill in the gap. Our cancensus package is perfect for the job. This post is generated from an R markdown document, which is available on GitHub for anyone interested in refining this."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#canadian-metropolitan-areas",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#canadian-metropolitan-areas",
    "title": "Amazon - The Canadian Data",
    "section": "Canadian Metropolitan Areas",
    "text": "Canadian Metropolitan Areas\nIt probably goes without saying that omitting Canadian cities from the list is more than just a small inconvenience. The attractiveness of a location in Canada for a company that will undoubtedly want to attract talent from beyond national boundaries can’t be understated in today’s political environment. And recent changes to Canada’s work visa process for tech workers, where work visas are now issued within two weeks of application, stands in stark contrast with developments south of the border.\nSo let’s start with the Canadian Census Metropolitan Areas.\n#devtools::install_github(\"mountainmath/cancensus\")\nlibrary(cancensus)\n#options(cancensus.api_key='your api key')\nprovinces &lt;- get_census(dataset='CA16',regions=list(C='01'), geo_format='sf', level='PR')\ncmas &lt;- get_census(dataset='CA16',regions=list(C='01'), geo_format='sf', level='CMA')\n\nOur candidate regions are\n\nToronto, Montréal, Vancouver, Calgary, Ottawa - Gatineau, Edmonton, Québec, Winnipeg, Hamilton, Kitchener - Cambridge - Waterloo, London, St. Catharines - Niagara, Halifax, Oshawa, Victoria, Windsor, Saskatoon, Regina, Sherbrooke, St. John’s, Barrie, Kelowna, Abbotsford - Mission, Greater Sudbury, Kingston, Saguenay, Trois-Rivières, Guelph, Moncton, Brantford, Saint John, Peterborough, Thunder Bay, Lethbridge, Nanaimo, Kamloops, Belleville, Fredericton, Chilliwack, Red Deer, Sarnia, Drummondville, Prince George, Granby, Sault Ste. Marie, Medicine Hat, Wood Buffalo, North Bay, Grande Prairie\n\nSince none of the regions are in the Territories we will drop them from the map."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#areas-with-at-least-one-million-people-where-job-growth-is-strong",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#areas-with-at-least-one-million-people-where-job-growth-is-strong",
    "title": "Amazon - The Canadian Data",
    "section": "Areas with at least one million people where job growth is strong …",
    "text": "Areas with at least one million people where job growth is strong …\nThat’s an easy one, let’s get to work and filter down to the ones with at least 1 million people.\ncmas &lt;- cmas %&gt;% filter(Population &gt;= 1000000)\nbasemap + geom_sf(data=cmas, color=cma_color,fill=cma_color) + lambert\n\nThis leaves us with 6 regions, namely\n\nCalgary, Toronto, Vancouver, Edmonton, Ottawa - Gatineau, Montréal\n\nTo keep things simple for use we use employment growth as a proxy for job growth. And let’s use 2006 and 2011 numbers since 2016 numbers aren’t out yet and we are too lazy to look up the CANSIM statistics. The CensusMapper API tool makes it straight forward to locate the relevant variables and copy-paste the cancensus data import call, or we can use the cancensus search function.\nHiding some details (they are available in the R notebook) of pulling in the data, let’s impose a cutoff of at least 5% growth over the 5 year period between the censuses.\ncmas &lt;- cmas %&gt;% filter(employment_change &gt;= 0.05)\nbasemap + geom_sf(data=cmas, color=cma_color,fill=cma_color) + lambert\n\nWhich drops Montreal from our list and, displaying the growth rates for additional information, leaves us with\n\nCalgary: 10.2%, Toronto: 8.1%, Vancouver: 8.9%, Edmonton: 12%, Ottawa - Gatineau: 9.8%"
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-the-right-labor-pool-is-large-and-growing",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-the-right-labor-pool-is-large-and-growing",
    "title": "Amazon - The Canadian Data",
    "section": "… and the right labor pool is large and growing …",
    "text": "… and the right labor pool is large and growing …\nHere we are looking for areas where “more than one in eight workers is in an industry related to tech, science or professional services”, so let’s grab the data for the industry categories “Information and cultural industries” and “Professional, scientific and technical services”.\nApplying the filter we are left with\ncmas &lt;- cmas %&gt;% filter(tech_ratio_11 &gt;= 1/8)\nbasemap + geom_sf(data=cmas, color=cma_color,fill=cma_color) + lambert\n\n\nCalgary: 12.9% -&gt; 13.9%, Toronto: 12.8% -&gt; 13.9%, Vancouver: 12.2% -&gt; 13.3%,\n\nand we can see that the ratios were growing in all regions from 2006 to 2011."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-the-quality-of-life-is-high",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-the-quality-of-life-is-high",
    "title": "Amazon - The Canadian Data",
    "section": "… and the quality of life is high …",
    "text": "… and the quality of life is high …\nThat’s a tricky measure, let’s follow the Upshot’s lead and compare rents. Since the new census data is not in yet, and the rent data is prone to change even faster than the already fairly old employment data we used, let’s turn to CMHC to pull the newest rent data using our still fairly rough cmhc package to get the October 2016 median rents for a 2 bedroom unit in a purpose built rental. These aren’t turnover rents but taken from the entire purpose built rental stock for the metro area, but they should give us a consistent way to compare.\n#devtools::install_github(\"mountainmath/cmhc\")\nlibrary(cmhc)\nparse_integer &lt;- function(x){return(as.integer(sub(\",\", \"\", x, fixed = TRUE)))}\ncmas$rent &lt;- as.numeric(lapply(cmas %&gt;% pull(\"GeoUID\"), function(geo_uid){\n  return(get_cmhc(cmhc_primary_rental_params(as.character(census_to_cmhc_translation[geo_uid]),\"2.2.21\",\"rent_median_amt\", \"2\")) %&gt;% filter(X1 == \"2016 October\") %&gt;% pull(\"2 Bedroom\")) %&gt;% parse_integer\n}))\nThe results are\n\nCalgary: $1,204, Toronto: $1,250, Vancouver: $1,305.\n\nThat’s really not all that much information, let’s look at the broader historic trends in rental availability and in rents.\n\nWe can clearly see the boom-bust cycles in Calgary that is a major driver of the vacancy rate, and the rents reacting in response to the changing vacancy rate. In Toronto we notice a brief time around 2004 when the vacancy rate was flirting with breaking the 5% barrier where (nominal) rent increases halted, but poor data quality hinders us from finding out exactly what happened. In Vancouver we see consistent rent increases, ranging from moderate to severe at times when the Vacancy rate drops below 1%, only held at bay by rent control. And we notice a nasty looking uptick in same-sample rent growth at the end after several years of very low and dropping vacancy rate.\nGoing off the CMHC rent numbers, as well as the history of rent changes, we eliminate Vancouver. Its proximity to Seattle it made it a poor choice to start with."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-workers-can-easily-get-around-and-out-of-town",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-workers-can-easily-get-around-and-out-of-town",
    "title": "Amazon - The Canadian Data",
    "section": "… and workers can easily get around — and out of town …",
    "text": "… and workers can easily get around — and out of town …\nLet’s pull in the median commute duration and active transportation mode share.\n\nCalgary: 21.9% (25.2 minutes), Toronto: 29% (30.3 minutes).\n\nThere is no clear winner here, Toronto has a higher active transportation mode share but also higher commute times. The mountains with destinations like Banff and Lake Louise not far and fairly easily accessible give Calgary a slight up in the “getting out of town” race."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-there-is-space-and-a-willingness-to-pay-to-play",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#and-there-is-space-and-a-willingness-to-pay-to-play",
    "title": "Amazon - The Canadian Data",
    "section": "… and there is space and a willingness to pay to play",
    "text": "… and there is space and a willingness to pay to play\nLet’s assume Amazon wants an urban campus, the question of space becomes one of willingness to respond to and accommodate the necessary growth in office space and space for people to live, something that Calgary has been doing quite successfully during their energy industry boom-bust cycles. One would probably be less confident saying the same about Toronto, especially in light of the recent discussion about a single mid-rise condo building in the Annex."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#the-winner-calgary",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#the-winner-calgary",
    "title": "Amazon - The Canadian Data",
    "section": "The Winner: Calgary",
    "text": "The Winner: Calgary\nThat leaves us with a sole candidate, Calgary, a city frequently ranked near the top of global livability indices, situated in proximity to beautiful national parks in the Rocky Mountains, with the recent expansion of their light rail system and downtown bike infrastructure not yet be reflected in our numbers, with an existing and growing pool of tech-related professionals and currently a good amount of office space already built and available."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#calgary-vs-denver",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#calgary-vs-denver",
    "title": "Amazon - The Canadian Data",
    "section": "Calgary vs Denver",
    "text": "Calgary vs Denver\nInternational comparisons are hard. I have lived in both cities, in Denver for a year as a student, in Calgary for a couple of years doing research and teaching. Personally I like both cities, but would prefer Calgary as a place to live. But that’s just my personal preference."
  },
  {
    "objectID": "posts/2017-09-09-amazon-the-canadian-data/index.html#think-i-didnt-use-the-right-metrics",
    "href": "posts/2017-09-09-amazon-the-canadian-data/index.html#think-i-didnt-use-the-right-metrics",
    "title": "Amazon - The Canadian Data",
    "section": "Think I didn’t use the right metrics?",
    "text": "Think I didn’t use the right metrics?\nDo you feel I unjustly threw out some candidates? Don’t trust my numbers? Spotted a mistake? Maybe I used the wrong metrics? No worries. This post is written as an R markdown document with all the code embedded to generate the numbers. Just grab it from GitHub and modify it to fit your preferences!"
  },
  {
    "objectID": "posts/2017-09-14-income-a-first-look/index.html",
    "href": "posts/2017-09-14-income-a-first-look/index.html",
    "title": "Income - A First Look",
    "section": "",
    "text": "Income numbers for the 2016 census are out, and I am taking a first shot to dig a little into the numbers. The numbers correspond to the 2015 income tax data, and this census was the first time that all data was directly linked to CRA tax data. For all people. So the income data is part of the “100% data” this time.\nIn the standard release we got median income data (no average income numbers this year, for better or worse), individual and household income distributions, income deciles for families, and two low income measures by rough age groups. That’s a lot to digest. I have made some graphs to make sense of it, and got nudged to share my code. That’s a good idea, so I packed it into a blog post. For readability I hide some of the code, but the full R notebook lives on GitHub."
  },
  {
    "objectID": "posts/2017-09-14-income-a-first-look/index.html#median-incomes",
    "href": "posts/2017-09-14-income-a-first-look/index.html#median-incomes",
    "title": "Income - A First Look",
    "section": "Median Incomes",
    "text": "Median Incomes\nThe first look is usually at median incomes, it breaks things down into one simple, easily digestible number. The only trouble is, what the median for which income statistic should we use? Let’s get an overview of what there is.\nLet’s load in some census data to see. As an example, let’s look at income statistics for the 10 most populous cities.\n#devtools::install_github(\"mountainmath/cancensus\")\nlibrary(cancensus)\ndataset='CA16'\nlevel=\"CSD\"\nmedian_income_vectors &lt;- list_census_vectors(dataset, quiet=TRUE) %&gt;% \n  filter(type==\"Total\",grepl(\"Median\",label),grepl(\"income\",label)) %&gt;% pull(\"vector\") \nregions &lt;- list_census_regions(dataset) %&gt;% filter(level==!!level) %&gt;% top_n(10,pop) %&gt;% as_census_region_list\nLoading in the data\ndata &lt;- get_census(dataset = 'CA16',\n                   level=\"Regions\",\n                   vectors=median_income_vectors , \n                   regions=regions, \n                   geo_format = NA,\n                   labels='short')\nand graphing it gives\nplot_data &lt;- data %&gt;% select(c(\"Region Name\",median_income_vectors)) %&gt;% \n  reshape2::melt(id=\"Region Name\") %&gt;%\n  mutate(`Region Name` = factor(`Region Name`, \n                                levels = data %&gt;% arrange(desc(Population)) %&gt;% pull(\"Region Name\"),\n                                ordered=TRUE)) \nplot_data$var &lt;- factor(as.character(labels[plot_data$variable]),levels=as.character(labels),ordered=TRUE)\n\nggplot(plot_data , aes(x = `Region Name`, y = value, fill=`Region Name`)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~var) +\n  scale_y_continuous(labels=currency_format_short) +\n  labs(fill = paste0(\"Top 10 \",level,\"s by Population\"), \n       y = \"Median Income\",\n       x=\"\",\n       title=\"Median Income 2015 Various Statistics\",\n       caption=\"Canada Census 2016 via cancensus & CensusMapper.ca\") +\n  theme_bw() + \n  theme(axis.ticks.y=element_blank(),axis.text.y=element_blank()) +\n  coord_flip()\n\n Overall, the statistics look quite similar, but details matter. Comparing incomes in Toronto and Vancouver we see that while Toronto’s median income at $65,829 is higher than Vancouver’s $65,327, that’s entirely due to the different household composition of these two cities, Vancouver has proportionally more one-person households. Splitting things up we can look at median one-person household income, where Vancouver’s $38,449 edges out Toronto’s $38,018, and at median two-or-more-person household income, where Vancouver’s $89,207 easily beats Toronto’s $82,908.\nBottom line, it matters how we boil down the numbers. On CensusMapper we built and easy way to explore the various median income statistics further."
  },
  {
    "objectID": "posts/2017-09-14-income-a-first-look/index.html#income-distributions",
    "href": "posts/2017-09-14-income-a-first-look/index.html#income-distributions",
    "title": "Income - A First Look",
    "section": "Income Distributions",
    "text": "Income Distributions\nLet’s go beyond the medians and look at distributions. Let’s compare how the top 15 municipalities in Metro Vancouver stack up. We will hide code (it is on GitHub] if you are interested in playing with it), but simply grab the relevant data and graph it.\ngraph_distributions_for_year(2016)"
  },
  {
    "objectID": "posts/2017-09-14-income-a-first-look/index.html#income-through-time",
    "href": "posts/2017-09-14-income-a-first-look/index.html#income-through-time",
    "title": "Income - A First Look",
    "section": "Income Through Time",
    "text": "Income Through Time\nThe same code that visualized the income distribution for 2015 data works for other census years too (with some mild case checking), we we can easily visualize the 2005 data (from the 2016 census).\ngraph_distributions_for_year(2006)\n But doing actual comparisons through time is much harder. Firstly, the income brackets don’t line up. That’s not that big a deal though, one could re-group the data. But depending what we want to use the data for we probably want to adjust for inflation. And that messes up the bracket cutoffs anyway, and we need to become creative.\nThere are plenty of ways to mess this up, some media outlets have adjusted twice for inflation, others have compared inflation adjusted income gains against the (by definition) unadjusted HPI. We will save diving into comparing incomes across time for another post. And maybe also take a look how 2010 NHS income data fits in between 2005 and 2015, there has been lots of hand-wringing about that data and I am quite curious how the extensive StatCan post-processing of NHS income data stacks up."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html",
    "title": "Evolution of the Income Distribution",
    "section": "",
    "text": "Vancouver’s median household income has grown. But there are many ways how this could have happened. We want to take a deeper look to understand how the income distribution changed. To that end, we will investigate the change in the number of people in each income bracket between the census years. And put that into context to what happened in the region and Canada wide.\nThis is a mixture of what we have done when comparing the size of age groups between censuses. But it also has a bit of the flavour net migration that looks at the inflow and outflow of people by age groups. It really is a mixture of the two. People do get pay rises and households move up income brackets. Or one person loses their job and the household income drops. But the bigger movements are from new households coming into a region, or forming in the region, and existing households leaving the region, or splitting up. Unfortunately we won’t be able to separate these two processes without a custom tabulation.\nTo track income brackets across censuses we will resolve to using nominal incomes, not inflation adjusted ones. Adjusting this kind of analysis for inflation requires more time than our evening blog post budget allows for.\nOne caveat is that household incomes can be hard to compare across regions. Household composition plays a big role and can trick us into misleading comparisons as we have pointed out previously where we explained that the only reason the City of Vancouver has lower median household income than the City of Toronto is the different household composition. This means we should also track the the number of 1-person households across regions and time as that’s the biggest factor that can skew household incomes.\nAs usual, the code for the analysis is embedded in this post and can be accessed on Github for anyone interested to replicate, re-run for a different region, correct or expand on this analysis. We do suppress most of the code for readability, don’t hesitate to grab the full R notebook from GitHub to see the details.\nUsing the cancensus package we pick the 4 most populous municipalities within Metro Vancouver to compare the net migration by income brackets.\nAs a first step we graph how the composition of households by common income brackets changed over the years.\nWhat stands out is the significant growth in the proportion of households with income over $100,000 throughout the region, much more than would be eaten up by inflation, which amounts to about 9% over each of these 5 year inter-census time frames."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#household-composition",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#household-composition",
    "title": "Evolution of the Income Distribution",
    "section": "Household Composition",
    "text": "Household Composition\nBefore we dive into more detail we take a look at household composition. The following graph shows the shares of 1 person households in these regions, as 1 person households generally have significantly lower incomes than 2+ person households. This effect can be quite sizable, in the City of Vancouver the median 1-person household income was $38,449 while the median 2+ person household income was $89,207. \nWe see that the City of Vancouver has a significantly higher share of 1-person households, which will skew the income distribution downward when compared to other regions. Richmond has seen a marked rise in the share of 1-person households from 2011 to 2016, which puts downward pressure on the income growth for that time frame."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#absolute-change-in-households-by-income-bracket",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#absolute-change-in-households-by-income-bracket",
    "title": "Evolution of the Income Distribution",
    "section": "Absolute Change in Households by Income Bracket",
    "text": "Absolute Change in Households by Income Bracket\nWe will first take a look at the absolute change in households by income bracket. Looking at the absolute net change offers some information on the profile of marginal household between censuses in each region.\nTaking the income brackets that are common in the 2005 and 2010 income data we get. \nIn the 2005-2010 numbers we see Vancouver take the lead in dropping households in each of the income brackets below $60,000, while the other regions show more diversity there. What stands out is that almost all of the growth happened in the top income bracket of over $100,000.\nHere we should keep in mind that while the 2010 NHS income data is overall quite robust for larger geographic regions, it may have some issues at some of the individual income brackets. Looking at the 2010 to 2015 time frame that has somewhat finer brackets available we see that this trend is consistent. \nWe have finer data for the 2010-2015 time frame. The numbers have the other regions follow Vancouver’s lead in shedding households with income below $50,000. The common income brackets offer a little finer resolution at the top end, where again to top bracket dominating and households with income over $150,000 account for most of the net growth.\nFor completeness, also consider the change over the full 2005 - 2015 time frame that skips over the 2010 NHS numbers. \nHere we can clearly how CoV lead the region in dropping households in all income brackets below $50,000, with majority of added households in the $100,000+ bracket.\nThe median income rises if we add more people above the old median income that we did below. The median income rises we have seen were substantial, and here we see that the majority of the growth didn’t just happen a little above the median income line, but right at the top income bracket that we are able to track."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#normalized-change-in-households-by-income-bracket",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#normalized-change-in-households-by-income-bracket",
    "title": "Evolution of the Income Distribution",
    "section": "Normalized Change in Households by Income Bracket",
    "text": "Normalized Change in Households by Income Bracket\nTo better understand how the makeup of households by income bracket is changing we normalize the change in each of these income brackets so that the sum of the net change, i.e. the total change in households, equals 1. All the regions we are considering have gained households, so we won’t run into trouble by doing this. This gives a better way to compare the changes across regions with different total change in households.\n\nViewed this way, it really brings out just how much the income brackets in City of Vancouver has been changing. \nIn the 2010 to 2015 time frame that change is a little more muted. \nBut when looking over the whole time frame from 2005 to 2015, jumping over the 2010 NHS numbers, we see how Vancouver stands out in the region in shedding households below $50,000 and gaining households above $100,000. In fact, the normalized change in households in the above $100,000 bracket is larger than 1, meaning that that bracket gained more households than the total change growth in households between these years."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#other-regions",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#other-regions",
    "title": "Evolution of the Income Distribution",
    "section": "Other Regions",
    "text": "Other Regions\nHow does this compare to the rest of BC? To put this into context, let’s look at BC and it’s top four CMA, as well as all of Canada. As there is a large variation in the total gain in households will focus on the normalized change only.\ncanada &lt;- list_census_regions('CA16') %&gt;% filter(level=='C')\npr &lt;- list_census_regions('CA16') %&gt;% filter(level=='PR', name=='British Columbia')\ncma2 &lt;- list_census_regions('CA16') %&gt;% filter(level=='CMA',  PR_UID==pr$region) %&gt;% top_n(4,pop)\nregions=as_census_region_list(do.call(rbind,list(cma2,pr,canada)))"
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#normalized-change",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#normalized-change",
    "title": "Evolution of the Income Distribution",
    "section": "Normalized Change",
    "text": "Normalized Change\n\n\n\nLooking at the normalized change for these time periods, we see that Metro Vancouver overall fits in quite nicely with broader trends. It is interesting to see that Abbotsford - Mission shows stronger growth in the top bracket than Metro Vancouver."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#how-about-inflation",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#how-about-inflation",
    "title": "Evolution of the Income Distribution",
    "section": "How about Inflation?",
    "text": "How about Inflation?\nAdjusting for inflation and comparing the result is tricky business, and we will leave that for another blog post. There are a variety of ways one could go about doing this, but all require some sort of assumption on the income distribution within each of these brackets. Probably the cleanest way to do this is to fit continuous distribution functions to the income bracket data. These can then easily be adjusted for inflation and subtracted to see how these distributions changed."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#housing-affordability",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#housing-affordability",
    "title": "Evolution of the Income Distribution",
    "section": "Housing Affordability",
    "text": "Housing Affordability\nHow does this relate to housing affordability? The unadjusted (for inflation) numbers we have been working with lend themselves well to that comparison if we use unadjusted house prices or rents as a comparison. Or the HPI, which by definition is not adjusted for inflation.\nChanges in prices and rents are negotiated by the marginal buyers/renters. And those are overwhelmingly from the top income bracket. We don’t know how the incomes are distributed within the open-ended top bracket. Looking at the 2015 income distribution we can infer that it skews high, the top $200k+ income bracket for 2015 contained 23,310 people, or 53% of the $150k+ bracket. This highlights that using median incomes to explain changes in the real estate market is too simplistic, and that incomes do play some role in the Vancouver real estate market. Both, in supporting prices as well as in segregating households. It lead some credence to the notion that lower income households feel the pressure to move out of the central region, and are being replaced by higher income households. We don’t see the same dynamic at the metropolitan level, it still fits in well with movements in the rest of BC and Canada.\nHowever, while these income gains are substantial, it is also clear that they simply cannot explain the rapid rise of Vancouver’s real estate on their own. A quick comparison with incomes needed to afford the luxury subset of our housing market consisting of single family homes shows that the incredible rise in home prices in Vancouver can’t be explained by incomes alone, even when splitting things up by movement in individual income brackets. It is inescapable that wealth plays a significant role in this market, at the very least at the upper end of it.\nWe will have to wait for the rest of the 2016 data to trickle in to dig down further."
  },
  {
    "objectID": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#update-for-mark",
    "href": "posts/2017-09-26-evolution-of-the-income-distribution/index.html#update-for-mark",
    "title": "Evolution of the Income Distribution",
    "section": "Update for Mark",
    "text": "Update for Mark\nWe re-run the analysis for the 2010 - 2015 adjusted after tax family income deciles. The advantage of this data is that it looks at adjusted income so the income is adjusted by family size. For details refer to the census dictionary, essentially this takes into consideration that larger families need larger incomes to get by. Looking at deciles is a relative measure of how the composition by incomes changes compared to how things changed overall in Canada. The main disadvantage is that this data is not part of the standard release for 2006 so some degree of caution is advisable. The regions we chose are large enough that the NHS income numbers should be reasonably robust.\nWith a bit of copy-paste we can see how this pans out in Vancouver. \nLooking at the change in the total number of people per adjusted income decile we see that again Vancouver stands out with high gains in the top bracket and a significant drop in the second decile.\n\nNormalizing the data so that the total net change adds to 1 for each region emphasizes the trends we observed in the data. Especially Richmond is curious with a large increase in the bottom decile and a drop in the top decile (and also the two below that), a pattern that is, to a lesser extent, mimicked by Burnaby."
  },
  {
    "objectID": "posts/2017-10-15-vsb-vulnerable-students/index.html",
    "href": "posts/2017-10-15-vsb-vulnerable-students/index.html",
    "title": "VSB Vulnerable Students",
    "section": "",
    "text": "Recently there was some discussion at my son’s school about the hot lunch program, and who should pay for those who need a subsidy. Which made me curious how that works. Here is what I learned."
  },
  {
    "objectID": "posts/2017-10-15-vsb-vulnerable-students/index.html#where-are-the-poor-children",
    "href": "posts/2017-10-15-vsb-vulnerable-students/index.html#where-are-the-poor-children",
    "title": "VSB Vulnerable Students",
    "section": "Where are the poor children?",
    "text": "Where are the poor children?\nggplot(data) + geom_sf(aes(fill = poverty_rate),size=0.1) + \n  scale_fill_viridis_c(na.value=\"grey\", option=\"magma\", labels=percent) +\n  labs(caption=caption1) +\n  theme_void()"
  },
  {
    "objectID": "posts/2017-10-23-trick-or-treat-2017/index.html",
    "href": "posts/2017-10-23-trick-or-treat-2017/index.html",
    "title": "Trick-or-Treat 2017",
    "section": "",
    "text": "In 2015 CensusMapper got launched onto the national stage with our Trick-or-Treat maps that got viewed by 150,000 Canadians across the country within just three days. In 2016 we updated our map with some predictive modeling as the 2011 data it was based on was already 5 years old. This year we got the fresh 2016 data that we are serving straight-up.\nWe again offer our standard Halloween trifecta:\nThese maps are of course interactive and Canada-wide, you can explore the country by zooming, panning, geo-locating, using the search bar or the overview map in the top right to navigate across Canada. To share the window you are looking at just copy-paste the current URL of the map window."
  },
  {
    "objectID": "posts/2017-10-23-trick-or-treat-2017/index.html#toronto",
    "href": "posts/2017-10-23-trick-or-treat-2017/index.html#toronto",
    "title": "Trick-or-Treat 2017",
    "section": "Toronto",
    "text": "Toronto\nThe prize for the most views of our Trick-or-Treat maps last year goes to Toronto, so here are quick-links for the Toronto area.\n\n\n\nTrick-or-Treat Onslaught\n\n\n\n\n\nTrick-or-Treat Density\n\n\n\n\n\nHaunted Houses"
  },
  {
    "objectID": "posts/2017-10-23-trick-or-treat-2017/index.html#calgary",
    "href": "posts/2017-10-23-trick-or-treat-2017/index.html#calgary",
    "title": "Trick-or-Treat 2017",
    "section": "Calgary",
    "text": "Calgary\nCalgary earns the prize of the most views per population last year, earning the title of “most scientific Halloween preparation”.\n\n\n\nTrick-or-Treat Onslaught\n\n\n\n\n\nTrick-or-Treat Density\n\n\n\n\n\nHaunted Houses"
  },
  {
    "objectID": "posts/2017-11-01-medians/index.html",
    "href": "posts/2017-11-01-medians/index.html",
    "title": "Playing with Medians",
    "section": "",
    "text": "I want do a short post to gently remind people of pitfalls when overly relying on medians for understanding complex issues. Medians are useful because they take a complex distribution and break it down into a single, simple to understand number. This works well as long as this does not mask other aspects of the distribution that are important in the context it is used.\nA good example for the dangers of overly relying on medians is the “median multiple” metric that gets used a lot, the median dwelling value divided by the median household income in an area. I have talked about this before but want to give some more general context to illustrate some pitfalls more directly.\n\nMedian Household Income\nWhen comparing median household incomes across regions it is very important to pay attention to the significant confounding variables involved. The variance in median income when restricted to various household types is huge as can easily be observed when interactively filtering by various household types. And those differences are persistent across geographic regions. What that means is that a significant determinant of the median overall household income in a region is the composition of households in that region.\n\n\nA Tale of Two Cities\nA good example of this is median household incomes in the City of Vancouver vs the City of Toronto. As has been reported repeatedly, the City of Toronto has higher median household income than the City of Vancouver. And while this was a reasonable way to summarize the income situation in 2005, it is misleading in the context of 2015. People that have been paying close attention to these issues will be nodding their heads and don’t need to read on. For the others this needs a more detailed and clear explanation.\n\n\nSimpson’s Paradox\nTo start off, let me explain what the problem is. Median household incomes are higher in Toronto than they are in Vancouver, so how can saying this be misleading?\nOne-person households and two-or-more person households have very different income profiles. We can compare median incomes separately for these groups. If we do that we find that City of Vancouver has higher median household income for one-person households as well as for two-or-more person households when compared to the City of Toronto. How can that be? Vancouver has a much higher proportion of one-person households, which tend to have significantly lower incomes. This is the classical Simpson’s Paradox.\nTaking a more comprehensive look, let’s use cancensus to pull a number of relevant median income metrics for 2005-2006, 2010-2011 and 2015-2016 and compare how they changed over time.\n\nSome metrics aren’t available for 2005-2006, but the overall picture becomes very clear. We see that in 2005-2006 Toronto had higher median incomes in all of these metrics, but by 2015-2016 only the median household income is higher in Toronto, and Vancouver scores higher in all finer income groups.\nDifferent household composition seems to be the missing confounding factor, let’s pull these numbers to confirm this.\n\nThe difference is not huge, but enough to flip the overall median income ranking and cause the paradox.\n\n\nLinking Incomes and Housing Cost\nOne reason why this is especially important is the recent shelter cost to income numbers. In Vancouver the proportion of households that spend 30% or more of their income on shelter costs has dropped. The same still holds when looking at tenant and owner households separately. But the “median multiple” metric has increased from 13.3 in 2005-2006 to 13.4 in 2010-2011 and 16.9 in 2015-2016. The median multiple is a poor metric for many reasons, not least because it computes affordability using aggregate data instead of individual level data. Just like incomes, housing prices vary on a distribution and depend strongly on external variables like size, number of bedrooms and housing type. The individual level income-to-shelter data shows that the median multiple misses some important confounding factors, just like in the above income example. What are these? I am not sure. But this is an important question that deserves further attention.\n\n\nTakeaway\nThe takeaway is that while overall median household incomes in the City of Toronto are higher than in the City of Vancouver, this masks that the opposite is true when comparing similar households. And is masks that over the course of the past ten years Vancouver has overtaken Toronto when comparing similar households. Which is quite remarkable in it’s own right.\nWe can dig deeper into this by looking at the whole income distribution, and how it changed over time. We have done this for Vancouver where we showed how the drop in low income households and gain of households in the top income bracket was more pronounced in the City of Vancouver than in other regions in Metro Vancouver or the country-wide movement. It would be interesting to reproduce this for the City of Toronto, and also to dig deeper to understand to what extent low income earners got pushed out of the City of Vancouver, to what extent salaries increased across the board and to what extent this was aided by higher labour force participation of households members. The census data release later this month can speak to that.\nAs always, the R Notebook underlying this post lives on GitHub. Feel free to download it to reproduce the analysis and adapt it for your purposes.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Playing with {Medians}},\n  date = {2017-11-01},\n  url = {https://doodles.mountainmath.ca/posts/2017-11-01-medians},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Playing with Medians.”\nMountanDoodles (blog). November 1, 2017. https://doodles.mountainmath.ca/posts/2017-11-01-medians."
  },
  {
    "objectID": "posts/2017-11-07-renting-in-toronto/index.html",
    "href": "posts/2017-11-07-renting-in-toronto/index.html",
    "title": "Renting in Toronto",
    "section": "",
    "text": "&lt;img src=“https://pbs.twimg.com/media/DN9z1-jWkAcqLAw.jpg”, style=“width:50%;float:right;margin-left=10px;”/&gt; Earlier today I came across Gil Meslin’s tweet suggesting to reproduce this rent graph for neighbourhoods in Toronto.\nI agree that this would be fun to do. All it requires is mixing the Toronto neighbourhoods with renal listings data, which I happen to have handy. So time to get working.\n\nNeighbourhoods\nTo do this we need to grab the Toronto neighbourhoods which can be found on Toronto’s open data website.\n\n\n\nRental Listings Data\nWith that in hand, we need to turn to rental data. The tweet is asking what property we can “get”, for that we should use turnover rents, so how much people would likely have to pay if they wanted to rent today. We turn to scrapes of a popular rental listings platform to answer that, while broadening our time frame to the past 3 months to make sure we get a decent sample. The per square foot price skews lower for higher bedroom listings, so we restrict ourselves to studios, 1 or 2 bedroom listings.\nlibrary(rental)\nlistings &lt;- get_listings(\"2017-08-06\",\"2017-11-06\",st_union(nbhds$geometry),beds=c(\"0\",\"1\",\"2\"),filter = 'unfurnished')\nlistings %&gt;% as.data.frame %&gt;% group_by(beds) %&gt;% summarize(count=n()) %&gt;% kable\n\n\n\nbeds\ncount\n\n\n\n\n0\n1936\n\n\n1\n8345\n\n\n2\n7308\n\n\n\nNext we sort the listings into their neighbourhoods and compute some quantities of interest, including rent per square foot and the average size of the unit we can expect to rent for CA$1,500 per month.\nnbhd_rpsf &lt;- st_join(listings,nbhds) %&gt;% as.data.frame %&gt;% \n  group_by(AREA_NAME) %&gt;% \n  summarize(count=n(), rpsf=mean(price/size, na.rm=TRUE)) %&gt;%\n  mutate(size_for_1500=round(1500/rpsf))\n\n\nRental Tree Map\nPerfect, all that’s left to do is to make a tree map graph for the neighbourhoods, restricting ourselves to the ones with at least 100 listings.\n\n\n\nGeographic Distribution\nTo round things up we quickly map the data to see the geographic distribution, where we map all neighbourhoods with at least 10 data points.\n\nAs always, the R Notebook that generated this post is available on GitHub. Unfortunately in this case it requires access to non-public listings data, so the reproducibility of the post is limited to people with access to rental data of some sort that will have to substitute their own get_listings logic.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Renting in {Toronto}},\n  date = {2017-11-07},\n  url = {https://doodles.mountainmath.ca/posts/2017-11-07-renting-in-toronto},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Renting in Toronto.”\nMountanDoodles (blog). November 7, 2017. https://doodles.mountainmath.ca/posts/2017-11-07-renting-in-toronto."
  },
  {
    "objectID": "posts/2017-11-21-byelection-data/index.html",
    "href": "posts/2017-11-21-byelection-data/index.html",
    "title": "Byelection Data",
    "section": "",
    "text": "The election data got posted on the Vancouver Open Data website so we decided to take a very quick peek at how the candidates fared by polling station. Citizens can vote at any station they want, so there is are no voting districts. But proximity to home is probably a large factor in determining where people vote, although some may choose locations close to work or somewhere else convenient. For anyone that wants to refine the analysis, the R Notebook that generated this post lives on GitHub.\n\nVote split by Voting Place\nThis graph gives an overview how the votes were split at each individual polling station.\n\n\n\nMost Votes by Voting Place\nFor each voting place we looked at who got the most votes and plotted the result on a map to get an overview of the geographic distribution.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Byelection {Data}},\n  date = {2017-11-21},\n  url = {https://doodles.mountainmath.ca/posts/2017-11-21-byelection-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Byelection Data.”\nMountanDoodles (blog). November 21, 2017. https://doodles.mountainmath.ca/posts/2017-11-21-byelection-data."
  },
  {
    "objectID": "posts/2017-11-29-journey-to-work/index.html",
    "href": "posts/2017-11-29-journey-to-work/index.html",
    "title": "Journey To Work",
    "section": "",
    "text": "The last larger dump of census data arrived today with lots of interesting variables. We wanted to have a quick look at commuting data.\n\nJourney to Work\nJourney to work data tells us about where people work relative to where they live.\nWe first look at what proportion of the population of Metro Vancouver municipalities lives in the same community that they work in. \nOn CensusMapper we have an interactive Canada-wide map for that. As the central municipality, the City of Vancouver has the highest proportion of residents that work in the same community that they live in. Let’s take a deeper look at the commute patters for workers that work in the City of Vancouver, as well as those that live there.\nOverall there are 344,440 people working in the City of Vancouver, 187,355 of which (54.4%) are living in the City of Vancouver and the rest commuting in from outside the city. Most of these come from within Metro Vancouver, but NA, or NA%, are coming from outside of Metro Vancouver.\nThe top 15 trip to work destinations outside of the City of Vancouver for workers living inside the city are \nWe can also map Metro Vancouver communities by what proportion of City of Vancouver workers live there.\n\n\n\nReverse Commuting\nNext we look at reverse commuting, that is what municipalities the 88,135 among the 275,490 workers (32%) living in the City of Vancouver that work outside the city work it.\nThe top 15 trip to work origins outside of the City of Vancouver are \nWe can also map Metro Vancouver communities by what proportion of City of Vancouver workers live there.\n\n\n\nThe Flow\nAnother way to look at the commute data within Metro Vancouver is to make a chord diagram to show the flow of commuters within and between municipalities. To keep things manageable we contain ourselves with the 13 most commuter-heavy communities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginally I wanted this as interactive diagram, but could not figure out how to include html widgets into my blog setup. So a static images it is. Ok, I hacked my blog to include the interactive widget.\nI think it would be nice to have some interactive and animated maps that allow one to explore commuting patterns. But this will have to wait for another day. As always, the underlying R Notebook that generated this post lives on GitHub for anyone interested in reproducing or adapting this post for a different region or elaborating on it in other ways.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2017,\n  author = {von Bergmann, Jens},\n  title = {Journey {To} {Work}},\n  date = {2017-11-29},\n  url = {https://doodles.mountainmath.ca/posts/2017-11-29-journey-to-work},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2017. “Journey To Work.”\nMountanDoodles (blog). November 29, 2017. https://doodles.mountainmath.ca/posts/2017-11-29-journey-to-work."
  },
  {
    "objectID": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html",
    "href": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html",
    "title": "Some Thoughts on the “Supply Myth”",
    "section": "",
    "text": "Smarter people than I have already responded to the recent furor about John Rose’s working paper, which is (part of?) the result of one year of research on affordability by the author, arguing that Vancouver has no supply problem. Most notably Nathan Lauster in a series of blog posts taking a look at the theoretical framework and methods used and running some numbers himself. I don’t have much to add to the first post, which took the time to highlight some of the useful demand measures that are either already implemented or currently under discussion and makes the point that the idea that we have enough housing in Vancouver, especially enough rental housing, requires an extraordinarily strong argument in the face of sub 1% rental vacancy rates. And the arguments made were weak at best.\nThe second post digs into some of the census data issues that the working paper had, and introduces construction and demolition data as another measure to contrast and contextualize changes in the census methods, which is interesting. It seems that in the evening blog post writing some of the numbers may have gotten mixed up, which should not distract from the larger critique presented.\nIn this post I would like to,"
  },
  {
    "objectID": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html#supply-in-the-working-paper",
    "href": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html#supply-in-the-working-paper",
    "title": "Some Thoughts on the “Supply Myth”",
    "section": "Supply in the Working Paper",
    "text": "Supply in the Working Paper\nThe working paper indirectly defines “supply” as a housing unit that is not used as a primary residence. So a housing unit that is for sale and empty, for rent and empty, occupied temporarily by someone (e.g. a student who thinks of their parent’s place as their permanent residence), a vacation property or a unit left empty for shorter periods of time (in between moving) or for longer periods of time, or a newly constructed housing unit that has not been moved in yet. It excludes units for sale if someone still lives in them.\nThis somewhat unorthodox definition of supply is then put in relation to “affordability” for aspiring owner households, that is the change in home prices. I am not sure that’s a particularly useful way to to approach this. The working paper then looks at the change in “supply” between 2001 and 2016 and the change in “affordability” between these years. The author notes that Vancouver registers as both, strong in increase of “supply” and strong decrease in “affordability”, and concludes that this provides proof against calls for adding “supply” (which in context of this conclusion seems to be re-defined more generally as “housing units”) to aid “affordability”."
  },
  {
    "objectID": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html#dwellings-not-occupied-by-usual-residents",
    "href": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html#dwellings-not-occupied-by-usual-residents",
    "title": "Some Thoughts on the “Supply Myth”",
    "section": "Dwellings, not occupied by usual residents",
    "text": "Dwellings, not occupied by usual residents\nThis is again all about the evergreen topic of private dwellings, not occupied by usual residents that many people, including myself, have talked about over the years. Vancouver does have a somewhat elevated rate of these dwelling units, and there are several good measures underway to incentivize a better use of the existing stock. The City of Vancouver has enacted the Empty Homes Tax, and the province is (hopefully?) still mulling over including the BC Housing Affordability Fund into it’s upcoming provincial budget.\nThat said, the rate of private dwelling units that are not occupied by usual residents (or non-primary residences for short) can’t be pushed down to zero. There will always be a base vacancy rate just for moving friction, similar to the unemployment rate or the job vacancy rate never hitting zero. Then there will always be “temporary” residents, with temporary foreign workers and students listing their parent’s residence (or not sticking around for the census after the end of the semester) being examples. Getting a third of these units into use as primary residences would be a worthwhile, yet ambitious goal."
  },
  {
    "objectID": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html#why-do-we-add-dwellings",
    "href": "posts/2017-12-11-some-thoughts-on-the-supply-myth/index.html#why-do-we-add-dwellings",
    "title": "Some Thoughts on the “Supply Myth”",
    "section": "Why do we add dwellings?",
    "text": "Why do we add dwellings?\nThe author also seems to suggest that the only reason to add dwelling units is to lower prices. While adding dwelling units will under (almost) all realistic conditions lower prices (at least in the sub market in which they are added), that certainly is not the only reason to add dwelling units. Adding dwellings is first and foremost about accommodating people.\nThere are many people that want to live in our region, some that are currently living here and that have a hard time to find a place to rent, some that want to move here but can’t find adequate housing options. With the rental vacancy rate now for many years below 1% in the region, and a record job vacancy rate that climbed to 4% in the past quarter, the pressure on the existing stock is building. And it’s the lower income people that tend to get pushed out when the higher income people compete for the same stock. As the centre of the region, the City of Vancouver is feeling the strongest pressure, and we have seen how low income people get pushed out of the city and people pile on in the top income bracket between the past three censuses. In that time the City of Vancouver has overtaken the City of Toronto in terms of incomes if measured apples-to-apples. The rest of Metro Vancouver has not yet seen that strong of a reaction in the data, but as housing prices and rent increases ripple outward through the region the pressure is building everywhere."
  },
  {
    "objectID": "posts/2018-01-11-local-vs-overseas-investors/index.html",
    "href": "posts/2018-01-11-local-vs-overseas-investors/index.html",
    "title": "Local vs Overseas Investors",
    "section": "",
    "text": "Nathan Lauster just opened up an interesting way to look at CHSP data – by folding in the SFS. I have played with SFS data in the past but it clearly is time to revisit this and reproduce Lauster’s numbers. Let’s also fold in census estimates for that to see how these numbers match up. I have nothing to add to the excellent commentary from Lauster’s original post, so please head over there for good context of these estimates.\n\nEstimates based on total property value\n\nFor Metro Vancouver the data matches up quite nicely, giving us some confidence that these estimates could be useful to combine with the CHSP data. For Toronto the census estimates are outside of the 95% confidence interval of the SFS data. This will be hard to resolve without taking a deep dive into the SFS methods.\nNow to the exciting part: comparing this to CHSP estimates.\n\nFor Vancouver, where we have a little more confidence in the data with the SFS and Census tracking closely, we can combine the estimates to arrive at Lauster’s graph. To even out some of the intercomparability issues we average the census and SFS data, hopefully increasing the usefulness of the Toronto estimate. People with more insight into the discrepancy are welcome to grab the code and make the appropriate adjustments.\n\nAgain, we have higher confidence in the Vancouver estimates compared to the Toronto estimates.\n\n\nEstimates based on number of properties\nWe can repeat the same analysis based on number of properties instead of total value, this may remove some of the problems we have been facing.\n\nWe see that the numbers match better, with both estimates within the SFS confidence interval. So let’s add in the CHSP data.\n\nArmed with that we again average over SFS and census numbers and use that to separate out properties that are not owner-occupied but with owners living in Canada, aka “local speculators”.\n\nAgain, these estimates probably aren’t terribly precise, but probably good enough to push out some graphs. Matching different data sources is hard and I should probably put in a little more work to cross-check things.\nAs always the R notbook that built this post lives on GitHub, feel free to download and adapt it for your purposes.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Local Vs {Overseas} {Investors}},\n  date = {2018-01-11},\n  url = {https://doodles.mountainmath.ca/posts/2018-01-11-local-vs-overseas-investors},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Local Vs Overseas Investors.”\nMountanDoodles (blog). January 11, 2018. https://doodles.mountainmath.ca/posts/2018-01-11-local-vs-overseas-investors."
  },
  {
    "objectID": "posts/2018-01-25-empty-suites/index.html",
    "href": "posts/2018-01-25-empty-suites/index.html",
    "title": "Empty Suites",
    "section": "",
    "text": "One of the constructive outcome of the “Supply Myth” discussion was the spotlight it shone on secondary suites. While we have already touched on this topic, it is wroth to fully flesh this out. With the perpetual empty home discussion in Vancouver one would have thought that the numbers by structural type of dwelling would have percolated through to the empty home hive mind.\nTurns out that even after years (decades?) of hand-wringing, we still know very little about empty homes."
  },
  {
    "objectID": "posts/2018-01-25-empty-suites/index.html#dwelling-types",
    "href": "posts/2018-01-25-empty-suites/index.html#dwelling-types",
    "title": "Empty Suites",
    "section": "Dwelling Types",
    "text": "Dwelling Types\nFor dwelling types, we will stick with census terminology. We will also make reference to “single family homes” as the housing form regulated through “single family zoning” bylaws.\nThe relation between single family homes and census classifications is complex and subject to much confusion in the public discussion. A single family home without a secondary suite is classified by the census as a “single detached house”, a single family home with one secondary suite is classified as a “duplex” and results in two “apartment or flat in a duplex” units. A single family home with more than one secondary suite is classified as an “apartment, fewer than 5 storeys”. A laneway home on a single family lot is classified as a separate single detached dwelling in the census.\nThroughout we will use the term “duplex” as defined in the census. What is commonly referred to as “duplex”, a detached structure divided into two units with a vertical division (as opposed to a division that is horizontal or a mixture of horizontal and vertical) is classified as two “semi-detached” dwelling units."
  },
  {
    "objectID": "posts/2018-01-25-empty-suites/index.html#empty-dwellings",
    "href": "posts/2018-01-25-empty-suites/index.html#empty-dwellings",
    "title": "Empty Suites",
    "section": "Empty Dwellings",
    "text": "Empty Dwellings\n\n\n\n\nDwelling Universe\n\n\nWe will stick with census definitions of the occupancy status of private dwellings. The standard census releases report on the total number of dwelling units, as well as those “occupied by usual residents”. The latter is, by definition, equal to the number of households. The guiding principle behind this definition is that each person should only be counted once, as part of the household that is their primary residence. Dwelling units that are not “occupied by usual residents” can be either “unoccupied”, or “occupied by temporary or foreign residents”, which means by people that have their primary residence elsewhere in Canada or abroad. Examples of this are students that list their parent’s home as their “primary” residence while they “temporarily” reside elsewhere near their place of study. Or temporary workers that maintain a residence for a fixed time near their work.\nMuch of the public discussion has focused on dwelling units not “occupied by usual residents”, even through looking at “unoccupied” would have been better suited for the arguments made. This is probably due to data on unoccupied dwellings not being as readily available and general laziness of obtaining appropriate data."
  },
  {
    "objectID": "posts/2018-01-25-empty-suites/index.html#data",
    "href": "posts/2018-01-25-empty-suites/index.html#data",
    "title": "Empty Suites",
    "section": "2011 Data",
    "text": "2011 Data\nThe 2011 data is best suited to answer our question about empty suites.\nLooking at the Metro Vancouver overall dwelling stock we see that some of the categories are very under-represented.\n\nWhen looking at this data we should be aware that some of the categories don’t contain many dwelling units, a problem that will only get worse when looking at sub-regions. When looking at unoccupied dwellings we are often interested in percentages at or below the 1% range. There are only 1,110 dwelling units in the “Other single-attached house” category at the Metro Vancouver level, so random rounding can lead to errors of 0.5% to 1% when breaking out the occupancy status. To strike a balance between accuracy and breadth of information we will suppress ratios with a denominator below 500 dwelling units.\nAnother caveat is that there are two important confounders when looking at vacancy rates of apartment buildings, the age of the building and the ratio of purpose-built rental units. New buildings take time to occupy and completions shortly before the census can skew the ratio of unoccupied units, especially in areas with a smaller base of apartments. A high ratio of purpose-built rental buildings skew the numbers in the other direction, purpose-built rentals have had consistently low vacancy rates. One could fold in CMHC data to back out the purpose-built rental units that have near zero vacancy rate to arrive at a rate for condos. We will leave this for another day or another person to do.\n\nThis graph shows that census duplex units (mostly single family homes with suites) have the highest frequency of being unoccupied in all regions, with the exception of Surrey and Richmond where high-rise apartment units are more likely to be unoccupied. In the Cities of Vancouver and North Vancouver mid/high rise apartments overtake duplex units if we add in units occupied by temporary or foreign residents.\nWe filtered out Electoral A and Bowen Island as they are somewhat different in nature and show them separately for completeness. \nThe high number of unoccupied single family homes in Electoral A also stand out, this probably warrants a more detailed geographic look to distinguish between dwellings in UBC/UNA/UEL (almost all single detached in this region are part of the UEL) and the rest more remote areas of Electoral A that hold about 20% of the regions occupied single detached homes.\nWithout further analysis we can’t say much about the rate of unoccupied single detached homes in the UEL. Bowen Island also has a high rate of unoccupied single detached homes, as well as temporarily occupied ones, possibly due to the presence of vacation homes on the island.\nIt is also instructive to view the data in terms of absolute numbers to understand the impact of each of the dwelling categories."
  },
  {
    "objectID": "posts/2018-01-25-empty-suites/index.html#comparison-with-single-detached-houses",
    "href": "posts/2018-01-25-empty-suites/index.html#comparison-with-single-detached-houses",
    "title": "Empty Suites",
    "section": "Comparison with Single Detached Houses",
    "text": "Comparison with Single Detached Houses\n\nWhat we see is that the share of unoccupied duplex units consistently and significantly higher across all regions. In absence of a compelling argument that main units in suited single family homes should be unoccupied at a higher rate than unsuited single family homes, this points to a significant portion of suited single family homes having empty suites.\nIf we assume that the main unit in unsuited and suited homes have equal probability of being unoccupied, and that a partially vacant duplex overwhelmingly has the main unit occupied and the suite unoccupied, we can estimate the ratio and total number of main units and suites that are unoccupied, out of all duplex units.\n\nReading the graph we should be aware that some of the empty suites are in duplexes with empty main units. If we believe it likely that in almost no cases the suite will be occupied when the main unit is unoccupied we can slice the data differently into duplex structures with both units unoccupied and partially occupied duplex units with main unit occupied but empty suite. As opposed to the previous treatment, we now base our graph on the toal number of structures (half the number of duplex units), not on the total number of duplex units."
  },
  {
    "objectID": "posts/2018-02-08-neighbourhood-level-census-data/index.html",
    "href": "posts/2018-02-08-neighbourhood-level-census-data/index.html",
    "title": "Neighbourhood Level Census Data",
    "section": "",
    "text": "The neighbourhood level custom tab the City of Vancouver pulls for every census has arrived on the open data portal today. We have not worked much with that data because the 2011 dataset excluded the NHS, but it’s worth revisiting with the 2016 data now available. (Hopefully the 2011 NHS data will get retroactively added, it’s a bit of a shame that it’s missing and CoV return rates were quite reasonable.)\nThe neighbourhood level data adds to the generally available census data in two ways:\n\nIt enables easy comparisons through time, since the neighbourhood geographies have been stable (whereas there have been changes to the census regions within the City of Vancouver between each of the censuses).\nIt gives a finer breakup of the City into regions with recognizable names. That makes it easy to utilize the data for non-map based visualizations and analysis.\n\n\nThe Data\nWhile a small dataset, getting it into working condition can be a bit of a pain.\n\nThe data comes transposed from the usual format census data is delivered in and that is useful for analysis.\nThe data comes in windows-1252 encoding instead of more universal formats like UTF.\nThe data is formatted as text, with comma thousand separators that need to be stripped and converted to numeric format.\nSome of the region names are not consistent over time.\nSome inconsistencies in the row and column naming.\n\nAs quirky as this is, the issues aren’t that big and I wrote a quick import function a while back that works for all four available datasets (and only needed minimal tweaking for the 2016 data). And yes, the code is included in the blog post in case others want to save themselves some pain.\nBefore we get started we should explain another quirk/feature of the data. The City of Vancouver assumes various roles within Musqueam 2, e.g. policing and some public infrastructure. I am not entirely sure what the exact relationship is (and would appreciate some pointers to a good summary), but for our case at hand it means that Musqueam 2 is included in the Dunbar-Southlands neighbourhood (but not in the City of Vancouver total that’s also included in the datasets). So summing data over all neighbourhoods comes up with a larger total than taking the CoV numbers, the difference being the counts from Musqueam 2. Something to be aware of.\n\n\nChildren under 15\nLet’s load in the data and search for the variables we are interested in. To start off, we will look at the population of children below the age of 15. Most of the work is locating the variables of interest. We have done a similar analysis before looking at finer age brackets but broader regions for the 2006, 2011 and 2016 censuses.\n\nThis gives a very vivid view of the changing landscape of children in the City, with only Downtown, Kitsilano, Fairview and the West End adding significant number of children and massive drops in the more eastern neighbourhoods.\nTo better understand the trend over time we can view the change in 5-year steps. \nWe see that between 2001 and 2006 the number of children in the neighbourhoods overall increased substantially, decreased dramatically between 2006-2011 and moderately decreased again 2011-2016. The general trend was that children were added in the central areas and dropped elsewhere.\n\n\nRenters\nRates of renters by neighbourhoods has been in the news lately, so we can use the opportunity to refine some of the numbers and add temporal data to understand the development over time. For this we will have to skip 2011 since the City did not provide the NHS data at the neighbourhood level.\n\nTo better understand the trends over time we can graph the development. The red dots indicate the 2001 share, green the 2016 share with the 2006 share in black. Trend lines in red indicate a decreasing trend, the ones in green an increasing trend. \nWe can map the overall percentage change to get an idea of the geographic distribution of the change in share of renter households. \nLastly we can also look at the total change in renter households in each area. \nThis shows a strong imbalance in which neighbourhoods have added renter households over this 15 year time period, with some neighbourhoods even losing renter households, in particular Strathcona (see update below) and Grandview Woodlands.\n\n\nUpdate (Feb 9, 2018)\nI was told that the drop in the number of rental households in some areas could have been contributed to by re-classification of housing from private to collective dwellings, in particular SROs and senior housing. According to the census dictionary SROs and senior homes should have generally been classified as collective dwellings, but that’s a judgement call. We have seen before that comparisons across time can be tricky as changes in definitions and enumeration methods can have large impacts. I have learned to appreciate all these little quirks in the census data and am grateful when people point me to any that I missed.\nThe City did not pull data on the number of collective dwellings, so we can’t directly test this. But we do have data on the number of people in collective dwellings and use that as a proxy.\n\nLooking at the total number of people in collective we immediately notice some data quirks in the 2001 and 2006 data. To assess the number of people in collective dwellings we took the total population in each area and subtracted the number of people in private dwellings. This procedure could yield small negative numbers in some cases due to random rounding, but the -105 and -45 we see in 2001 and 2006 should not be able to happen. So these estimates have some problems that we don’t fully understand and thus should be treated with caution.\nIgnoring these issues we notice that indeed there was a sizable increase in the population in collective dwellings in downtown and Strathcona that could have confounded our change in renter households. To zoom in on that we map the change in people in collective dwellings, understanding that these estimates have some problems.\n\nThis map is counting people, comparing back to our map of the change in renter households above we notice that the drop in renter households in Strathcona could indeed be due to re-classification of private dwellings to collective dwellings.\nAs always, the code that made these graphs is available on GitHub. This should make it easy for others to jump in and map or analyse different aspects of the neighbourhood level data. Curious to see what interesting things other come up with.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Neighbourhood {Level} {Census} {Data}},\n  date = {2018-02-08},\n  url = {https://doodles.mountainmath.ca/posts/2018-02-08-neighbourhood-level-census-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Neighbourhood Level Census\nData.” MountanDoodles (blog). February 8, 2018. https://doodles.mountainmath.ca/posts/2018-02-08-neighbourhood-level-census-data."
  },
  {
    "objectID": "posts/2018-02-24-taxable-dwelling-density/index.html",
    "href": "posts/2018-02-24-taxable-dwelling-density/index.html",
    "title": "Taxable Dwelling Density",
    "section": "",
    "text": "Vancouver pushed out a heatmap of dwelling units that have so far failed to declare their empty homes tax status.  With everyone eagerly awaiting data on the empty homes tax declaration we wonder what can be learned from the map.\nTurns out not much, the city did not normalize by the number of dwelling units subject to the tax. So to first order, this is just a heatmap of where people live, which xkcd coined pet peeve #208.\nMost people of course will read this as a map of an indication where people may not live because the home is empty, a textbook case of poor visualization. We don’t have the data of the undeclared properties, but we do know roughly which the residential properties subject to the tax are. And we can map these to compare how close the city’s map gets, while understanding that there will be some statistical noise in the city data.\n\nAs we can see, it gives a pretty good match of the city data, modulo the choice of finer granularity in the map the city did. As always, the code that made this post is available on GitHub. Go grab it if you want to refine the dwelling density map.\n\nPet Peeve #209\nAnd while I have your attention, the 2016 Census did not have “more than 25,500 empty homes in Vancouver” as frequently claimed, but 21,820. The census counted 25,502 (yes, exactly 2 “more” than 25,500) homes not “occupied by usual residents”, the difference between those two numbers being temporary or foreign residents. And about 4k of the actually unoccupied homes are secondary suites, as we have discussed at length before, and the tax does not apply to them. Then we should consider that the empty homes tax does not apply to homes that were recently completed or sold (and those also tend to show up as more heavily unoccupied in the census), plus moving vacancies. So anyone floating the 25k number in conjunction with the empty homes tax is either woefully ignorant or purposefully misleading at this point.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Taxable {Dwelling} {Density}},\n  date = {2018-02-24},\n  url = {https://doodles.mountainmath.ca/posts/2018-02-24-taxable-dwelling-density},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Taxable Dwelling Density.”\nMountanDoodles (blog). February 24, 2018. https://doodles.mountainmath.ca/posts/2018-02-24-taxable-dwelling-density."
  },
  {
    "objectID": "posts/2018-03-12-transit-data/index.html",
    "href": "posts/2018-03-12-transit-data/index.html",
    "title": "Transit Data",
    "section": "",
    "text": "The other day I was catching a bus home later at night, which made me acutely aware that I should not take the frequent daytime transit in Vancouver for granted. On the ride home I decided to dig into this and grab some transit data. We have played with transit data before, but since this was going to be the second time it was high time for a quick R package to standardize our efforts and simplify things for the next time around. Or for anyone else interested in this. And I can’t think of a better way to use the time during a bus ride than writing an R package to access transit data!\nIt’s easy to write a quick package that gets the job done, but it took me a couple of days until I found some time to clean it up and write a short post, so here we go. This post is meant less to be informative about transit in Vancouver and more a mini-introduction of the R package in case others are interested in using this. It still has some pretty graphs and late-night bus riders might appreciate seeing why that bus they were waiting for never came.\nAs an example we are looking at transit service in the City of Vancouver. There are a number of ways to query transit data, we will specify a bounding box based on the geo polygon for Vancouver that we grabbed using our cancensus package.\n\nTransit Routes\nTo start off we pull the route data for the City of Vancouver using the bounding box from our polygon\nand plot it. \n\n\nTransit Stops\nNext we query all transit stops in the bounding box and clip them to the City of Vancouver and identify the high frequency B-line and Skytrain stops and visualize all 1,956 of them.\n\n\n\nService Frequency\nTo assess service frequency we pull all departures between 9am and 10am on March 13th and plot the frequencies by stop.  It’s kind of mind-boggling to think that in that hour transit services in Vancouver rack up 42,988 departures.\n\n\nAll on one map\nWe can join the frequency on the stations and get a geographic overview of transit frequency by stop, binning the frequency for easier readability. \n\n\nCompaing to evening service\nTo round up the example we want to compare this to transit departures midnight to 1am. \nAs expected, the frequency is significantly lower at night, but there are still 11,516 departures. We can again plot the geographic distribution, where we can observe that some local lines fade to grey with zero departures.\n\nAs always, the code that made this post is available on GitHub, go grab it if you want to reproduce the graphs, run some interesting stats with the data or re-run the analysis for a different region. Transitland has a pretty good repository of transit feeds from all over the world that are much more suitable for data analysis and visualization compared to the GTFS. And if the transit provider you are intersted in is not on the list you can always add it.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Transit {Data}},\n  date = {2018-03-12},\n  url = {https://doodles.mountainmath.ca/posts/2018-03-12-transit-data},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Transit Data.”\nMountanDoodles (blog). March 12, 2018. https://doodles.mountainmath.ca/posts/2018-03-12-transit-data."
  },
  {
    "objectID": "posts/2018-05-01-a-modest-school-tax-proposal/index.html",
    "href": "posts/2018-05-01-a-modest-school-tax-proposal/index.html",
    "title": "A modest school-tax proposal",
    "section": "",
    "text": "We have written about the extra school tax before, but in the meantime the discussion about the extra school tax has heated up considerably, cumulating in David Eby cancelling his town hall today. Over the years I also have been spending some time thinking about how people got to the privilege of paying the extra school tax, for example here and here.\nThere is a slight dissonance with people claiming that their homes should not be taxed because they are their homes and not their investment. But then their homes really only qualify for the tax because they have proven to have been an extremely lucrative investment. Of course one could use these investment gains to easily offset the extra school tax. Should be no issue since homes are not meant to be an investment in the first place, right? Apparently not, people like to have their cake and eat it too.\n\nEarning the privilege to pay the school tax\nSo let’s look at who actually pays the school tax. I don’t have much time today and will squeeze this into my lunch time, so for simplicity we are just focusing on single family homes here, which make up the vast mavority of properties the tax applies to.\n\nThere are 19,677 single family homes (that passed our rough filter) that pay the extra school tax, they are almost exclusively on the west side. This is missing some high-end condo buildings as well as duplexes. And some other properties that fell through our makeshift SFH filter, like homes with a stratified boat ramp on Deering Island. For a more complete list refer to the previous post on this.\nThe extra school tax is new, this is the first year it applies. But it’s instructional to see what things would have looked like if the tax did apply in e.g. 2006.\n\nNot that many, 514 to be precise. So how exactly did people end up with the privilege of being able to pay the extra school tax? Mostly through twiddling thumbs, so simply sitting on their land and doing nothing. How lucrative is thumb-twiddling you ask? Very lucrative. Don’t believe me? Just look at the average annual inflation-adjusted land value gain.\n\nTo recap, the average annual inflation-adjusted land value gains for SFH that the extra school tax applies to ranged from $75k to over $1m. That’s the gains due to increases in land value, so this does not include value gains because of upgrades to the buildings. Inflation adjusted. Averaged over 12 years. So at a minimum, properties increased by $75k per year on average. That’s a lot of avocado toast right there.\nWhich naturally leads us to compare the extra school tax to the average annual value gains. This gives us some perspective on the extra school tax. To be precise, we take the quotient of the average annual inflation-adjusted land value gain between 2006 and 2018 by the extra school tax. We interpret this as how many years of extra school tax could be paid for by a single year of average land value gains.\n\nSo at a minimum, people’s annual inflation-adjusted land value gains exceeds the school tax by a factor of 5 (actually it’s 6, but I like round cutoffs). At the high end it’s more than 1,000 times, but that’s mostly a function of people barely breaking the $3m barrier. This observation leads us to our modest proposal.\n\n\nA modest school-tax proposal\nIf you don’t want to pay the extra school tax, how about partnering up with someone who will pay the extra school tax for you for the next 10 years in exchange for the past 10 years of inflation-adjusted land value gains?\nIf you are over the age of 55 there is of course also the option of simply deferring your taxes. At 1.2% simple interest. Which sounds like a deal too good to be true. Yet it is.\nNot over 55? You can still get a similar deal as a family with children, at a somewhat higher interest rate of 3.2%. Don’t qualify for that either? Try the hardship program!\nThat won’t work for you either? You can leverage some of those past value gains by getting a HELOC. Oh, you already did that to finance some expensive hobbies and maxed out your HELOC? That would be a problem I guess. Not sure how many people are in that category, I have not heard of anyone speaking out. I suspect however that there aren’t that many, and that the public sympathy for cases like this will be muted.\nThere are certainly people that have bought fairly recently and don’t have those years of land value gains to fall back on. So there are definitely some edge cases where this logic does not apply to. But I hear homes aren’t investments. Property values, interest rates, and taxes and go up or down. And the main argument that is getting pushed these days, that people have lived in these homes (which aren’t “investments”) for many years and don’t have the income that matches the home values and therefore can’t or shouldn’t pay the tax, seems quite dishonest to me in light of all the thumb-twiddling gains they have reaped and the tax deferral options they have. While many people are struggling to just rent a somewhat adequate place.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {A Modest School-Tax Proposal},\n  date = {2018-05-01},\n  url = {https://doodles.mountainmath.ca/posts/2018-05-01-a-modest-school-tax-proposal},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “A Modest School-Tax Proposal.”\nMountanDoodles (blog). May 1, 2018. https://doodles.mountainmath.ca/posts/2018-05-01-a-modest-school-tax-proposal."
  },
  {
    "objectID": "posts/2018-05-16-teardowns-and-emissions/index.html",
    "href": "posts/2018-05-16-teardowns-and-emissions/index.html",
    "title": "Teardowns and Emissions",
    "section": "",
    "text": "Last year we took a detailed look at Single Family teardowns in Vancouver, that is houses in RS or “Single Family” zoning that got torn down. We focused exclusively on those homes in RS zoning because these have to be replaced by another, often bigger, Single Family home. Using historical data we build a probabilistic model to predict future teardowns in Vancouver. If you haven’t taken the time yet to read through the data story, you probably should do that right now. The rest of this post assumes the reader is familiar with the concepts explained in the data story.\nNow the second part of this work, taking our understanding of the land economics of teardowns and our teardown predictor, and investigated the impact on CO2 emissions. There are two opposing mechanisms at work. A new home starts out in a deep carbon hole dug by the embodied energy associated with the teardown/rebuild process. But these homes are more carbon efficient than the ones they replaced, so over time they will come out net carbon-positive. In simplified form the question is how long does it take to dig out of the hole and will these new homes stay around long enough before the teardown cycle again rolls over them?\nOur paper, co-authored with Joe Dahmen and Misha Das that just got published in Energy and Buildings shows that it’s complicated."
  },
  {
    "objectID": "posts/2018-05-16-teardowns-and-emissions/index.html#tldr",
    "href": "posts/2018-05-16-teardowns-and-emissions/index.html#tldr",
    "title": "Teardowns and Emissions",
    "section": "TL;DR",
    "text": "TL;DR\nSingle Family homes built to current emission standards will not survive long enough in order to pay back their carbon debt due to teardown/rebuilt. Homes built to CoV 2025 emission standards will generally survive long enough to come out net carbon positive, although this is only true for the first iteration of the teardown cycle.\nThe teardown index links carbon emissions to land value changes. For every 1 percent increase in land values we expect to see an additional 130 thousand tons of emissions. However, a flat or even a falling market does not mean a stop in teardown-related emissions, much of the dynamic is already baked in through decades of continued land appreciation.\nCaution should be exercised when trying to generalize the findings to other cities, the results depend to a large degree on Vancouver’s land economics, as well as the specific emissions profile of the current building stock and the emissions regulations moving forward.\nThe main takeaway is that accounting for embodied carbon is an essential step in understanding overall emissions, and should be part of any policy that aims to reduce emissions."
  },
  {
    "objectID": "posts/2018-05-16-teardowns-and-emissions/index.html#growth",
    "href": "posts/2018-05-16-teardowns-and-emissions/index.html#growth",
    "title": "Teardowns and Emissions",
    "section": "Growth",
    "text": "Growth\nWhat breaks many of our assumptions is that Vancouver is growing. Quite substantially so, current projections add about 1 million people to our 2.5 million region by 2041, 130,000 of which are projected to settle in the City of Vancouver. Growth projections are hard (and always wrong), and may well turn out differently, especially in regard to where in the region the growth will land. Current plans, with a much lower growth rate in the City of Vancouver compared to the rest of the metropolitan region is sure to put more pressure on housing in the centre of the region.\nIn terms of emissions this means that we are looking at a scenario where we have to create new dwelling units to accommodate that growth. So next to worrying about how we maintain, renew and upgrade our existing building stock and what the implications on emissions are, we also need to consider how we will accommodate the population growth and it’s impact on emissions. Which gets complex very fast.\nThe landscape this is set against is that currently 67% of our residential land use is zoned RS, a share that increases to 79% when we add in the RT zones and First Shaughnessy that have similar densities. if we add in land taken up by single family or duplex homes outside of these zones the share jumps to 81%, but all of these areas combined house less than half of the population. On a basic level, the question that this opens up is how we want to grow as a city (and a region), squeeze new housing into a few sites outside of or on the fringes (along the arterials) of RS, RT and FSD zoning, or allow denser housing forms within what Nathan Lauster called the Great House Reserve.\nVancouver has strong population growth and most sites already have buildings on them. In that situation we will have to densify by tearing down some buildings. What really matters from a systems perspective is our ability to add new units while keeping emissions as low as possible. So we need to minimize the ratio of demolitions to new construction."
  },
  {
    "objectID": "posts/2018-05-16-teardowns-and-emissions/index.html#rs-zoning",
    "href": "posts/2018-05-16-teardowns-and-emissions/index.html#rs-zoning",
    "title": "Teardowns and Emissions",
    "section": "RS zoning",
    "text": "RS zoning\nVancouver Single Family (RS) zoning is more complex than the name might suggest. Before we dive deeper into the teardown and construction data we should take a moment to survey the current landscape. Current RS zoning allows for a basement suite and a laneway house, according to 2016 census numbers and BCA roll data about 42% of Single Family homes (inside and outside of RS zoning) have secondary suites.\nWe finally have detailed permit data available for the City of Vancouver, but only for 2017 onward. While permit data does not directly correspond to what is happening on the ground, it gives us a much better proxy than what we had before to what is actually getting built.\n\nThis shows that the landscape of building in RS is a lot more complicated than just teardown/rebuilts, we build almost as many laneways as we are tearing down and rebuilding main houses. Laneways are their own detached houses on single family lots (and are counted as separate “Single Detached” houses in the census, assuming the census found them) and typically have between 700 and 1,000 sf of living space.\nLooking at the permit value we should point out that this reflects the building value of the newly built home, our teardown data story has more detailed data on how the building values of new builts relate to underlying land values. The permit value indicate a clear bias of secondary suites toward less expensive construction. Folding in what we know about how this relates to land values, namely that permit value tends to be proportional to land value, explains some of the clear geographic bias of suites in our building stock that is evident in census data.\nSuites are a bit of a Schrödinger’s cat of our building stock, and suites are the most likely type of housing to register as unoccupied in the census as we have explained before. The pure existence of a suite does not mean we have actually added a dwelling unit, the suite may be “offline” or absorbed within the main unit. Suites are also hard to track through time, most estimates rely on census counts that are better at discovering unpermitted suites. But changes in the count of suites in the census could stem from new suite conversions or new builts with suites, as well as the census simply getting better at finding suites with each census. Permit data may indicate that there may be a 4:3 equilibrium in suited vs unsuited homes, but this does not take into account what these homes replace, how changing market conditions will effect this ratio (given the bias against higher valued properties), and the number of suite conversions or reversions.\nGoing by permit data we have been starting 1,247 houses in RS zoning in 2017, where only the 575 laneway homes and possibly a slight increase in suites have been actually adding dwelling units. At the same time, the population in RS zoning has actually stagnated or even slightly dropped between the 2011 and 2016 censuses, the addition of new dwelling units was not large enough to counter-act the trend to smaller household sizes. All the while the City of Vancouver overall grew by 4.6%, and all that growth landed outside of the RS zones.\nWhat ultimately matters is less the built form on the ground, but more how people actually use these homes. The following graph shows the “Suite Index” in Vancouver. Broadly speaking the suite index shows the share of single family lots that are suited, but it refines that concept to only focus on those that are actually occupied. So we count a suited home with an unoccupied suite only as half. Areas with fewer than 50 (occupied) single family lots are greyed out.\n\nThe bias of (occupied) suites in areas of lower land values is striking, and this helps explain that the west side has been losing population between 2011 and 2016, and in fact the east side has seen a small (~0.5%) population gain during that time frame."
  },
  {
    "objectID": "posts/2018-05-16-teardowns-and-emissions/index.html#emissions",
    "href": "posts/2018-05-16-teardowns-and-emissions/index.html#emissions",
    "title": "Teardowns and Emissions",
    "section": "Emissions",
    "text": "Emissions\nNow let’s tie all of this back to emissions. Vancouver is growing, and we need to build more homes. And building homes causes emissions. So do the induced transportation patterns. We are probably better off if people live close to their jobs and walk, bike or take transit to work. Or at least drive shorter distances. Embodied energy in buildings can be very roughly approximated by the weight of the building. Concrete is heavy, and emits a lot of carbon in the curing process. From an emissions perspective we want to move away from concrete, but concentrating growth in the relatively small area outside of RS, RT and FSD in Vancouver means that we have to build taller buildings and generally rely more on concrete.\nAt the same time we should be focusing our building activity on actually adding units. Tearing down a single family home and building a 6-plex in it’s place will accrue a fair amount of embodied carbon debt, but tearing down the home and replacing it with yet another single family home, and accommodating the growth of the remaining units by adding a floor to a highrise accumulates a larger carbon debt.\nThe single family teardown cycle is chiefly responsible for Vancouver’s ineffectiveness of adding dwelling units. Vancouver has been seeing a fair amount of construction, but CMHC estimates that for each five units built in Metro Vancouver one gets torn down. StatCan has published a new data series starting at the beginning of 2018 with monthly permit data, where we can roughly reproduce this relationship, keeping in mind that we only have two months of data right now, so we should expect quite some volatility.\n\nComparing five metropolitan areas we clearly see the relatively high rate of demolitions like a middle finger held to the face of record construction levels. The primary target for reducing embodied carbon emissions is to shrink the size of this middle finger. And at the same time transition to lower carbon building materials and built form that accommodates them. While this is based on only 2 months of data, the Metro Vancouver demolitions to completions ratio in this graph matches longer term data that we now turn to.\nTo better understand the nature of the middle finger, we can take annual demolitions data from the Metro Vancouver Housing Databook and compare that to completions data from CMHC. The demolitions data originates mostly from Statistics Canada and has some data quality issues, but it’s the best we have.\n\nWe can plot the quotient of demolitions to completions to better understand how these two relate.\n\nThe 10 year average is indicated by the dashed horizontal lines, Burnaby and Vancouver are both above the Metro Vancouver average. (A bit of caution should be exercised for Richmond, demolitions data from Richmond comes directly from the City of Richmond and not via Statistics Canada and the difference in methods may cause problems when comparing data.)\nFrom here we can take a closer look at what is causing the high demolitions to completions ratio of about 1:5 in the City of Vancouver by drilling into what types of housing get completed and torn down.\n\nWe see that ground-oriented demolitions and completions tracked very well until about 2011, the difference can be explained by Laneway completions as the following graph shows, townhomes and rowhouses might have a small impact too.\n\nThe majority of demolitions in the City of Vancouver were single detached homes to be replaced by another single family home. This part of our construction is unproductive in the sense that it does not add to our dwelling stock. These are chiefly responsible for our “demolitions middle finger”, although the most recent uptick in apartment demolitions in Vancouver is something to closely watch."
  },
  {
    "objectID": "posts/2018-05-28-school-tax-calculator/index.html",
    "href": "posts/2018-05-28-school-tax-calculator/index.html",
    "title": "Extra School Tax Calculator",
    "section": "",
    "text": "One thing the extra school tax debate has brought to light is how challenging 8th grade math is for people, especially those living in homes valued over $3M. Journalists also seem to have trouble putting outrageously wrong statements into perspective, so here is a simple calculator on how much extra school tax people will owe depending on their home value, years until home is sold, expected changes in the real estate market, interest rates on deferred (through government program or second mortgage) taxes.\n8th grade math should really be something all of us are comfortable with. (And it really should be 6th grade math.) Another good reason to invest more into our K-12 education system. Incidentally only 35% of which is currently funded by school taxes, although that’s not the main reason why increasing school taxes, which is just the name of the provincial portion of the property taxes, is a good idea.\nWe have talked about the school taxes before, once just running the numbers for the City of Vancouver to see who is affected and once relating the school tax to land value against. It seems we need yet another post to simplify the math down to moving a couple of sliders.\n\nAssumptions\n\n\n\nHome Value\n\n\n\n\n\n\n\n\n\nAnnual change in property values\n\n\n\n\n\n\n\n\n\nDeferral simple interest rate\n\n\n\n\n\n\n\n\n\nYears of deferral\n\n\n\n\n\n\n\n\n\n\nResults\n\n\n\nAnnual extra school tax now\n\n\n\n\n\n\nTotal Home Value in 10 years\n\n\n\n\n\n\nAnnual extra school tax 10 years\n\n\n\n\n\n\nTotal Deferred Extra School Taxes in 10 years\n\n\n\n\n\n\nHome Value Minus Deferred Taxes in 10 years\n\n\n\n\n\n\n\nAn example\nAn upfront warning, I have a really hard time to dial down the snark level when talking about prominent examples from the news. You best not read on if that’s potentially offensive to you.\nAs an illustrative example consider the plight of David Tha. He bought his home in 1987 for $370,000 and it’s assessed at $6.75M now. That’s an 18.2 fold increase over 31 years, or an annual appreciation of 9.8%. He says he has done nothing to earn this increase in value, other than applying “a few coats of paint” over the years.\nTha does not want to defer his taxes as that would be “simply deferring the injustice”, and he notes that the interest rate for the deferral program is below prevailing interest rates so “that puts the burden on all B.C. residents”. Which is “something he’s against in principle”. Tha is quite the altruist when it comes to not taking advantage of government subsidies, not so much when it comes to his housing wealth that he says he did essentially nothing to earn.\nSo how does Tha plan to deal with the tax? Tha’s response bears quoting in full.\n\nThey’ve disconnected their landline and signed up for an internet-based phone service. Tha, who is 72, is trying to figure out a way to attach an antenna to their house so they can cut cable and yet still watch television. They’re eating less meat to cut down on their grocery bills and foregoing travel plans, all to try to save the money that will go towards their higher tax bill.\n\nWe have to conclude that he is very principled, but I am still not convinced that the single mom trying to make ends meet renting an East Van basement suite will be terribly sympathetic to Tha’s self-imposed budget constraints.\nBut Tha’s plight does not end here.\n\nThey also don’t want to put the burden on their daughter, a nurse to whom they plan to bequeath the house. Not only will she have to pay the deferred taxes but if the house continues to increase five per cent a year in value, and the surtax remains in place, Tha calculates the taxes will be $67,000 a year in 15 years, well beyond her ability to pay.\n\nThe math is of course ambitious. First off, if his house had really “only” appreciated by 5% annually since he bought it it would now be worth $1.68M and he would not have to pay the extra school tax. Next, employing some math debugging skill, he seems to believe that it’s ok not to compound the appreciation and is in the same camp as many homeowners in not understanding how regular property tax mill rates are calculated by assuming the mill rate is fixed as overall property values go up. And computed the total rate based on these assumptions, not just the extra school tax. Glad to know he is retired.\nSo let’s fix this and set out calculator to $6.75M home value, 5% real estate appreciating rate, 1.2% simple interest and 15 years. We see that in 15 years Tha’s daughter will be in the very uncomfortable position of inheriting a $14M home owing $390k in deferred extra school tax, a net value of over $13.5M. At that point in time the annual extra school tax amounts to $42k and she will have to make up her mind if she wants to keep deferring taxes, which she may have to finace through a second mortgage at higher interest rates (although still lower than Tha’s projected rate of real estate appreciation), or sell her home for around $14M.\nOf course Tha could sell right now and downsize, this way he could avoid having to skimp on meals and cut his cable TV. But…\n\nAnd where would they move? Prices are expensive everywhere in the city, he says. Their daughter lives with them so she’d have to find a place to live, too.\n\nVancouver is indeed expensive, but with $6.7M in equity, almost all of which is untaxed capital gains, it is very hard to see how they could find a reasonable place to live to fit their middle class lifestyle.\n\n\nBottom line\nThere probably are some cases of people hit by the extra school tax that would illicit my sympathy. Sadly I have not seen these pop up in the local media, the most vocal voices seem to mimic Tha’s line of reasoning. And to be fair, Tha’s math, while wrong, is quite a bit better than what we have seen from others.\n\n“He told Frank, who’s never deferred his taxes and is living his retirement very humbly, that he should defer his taxes and live comfortably so his kids can have that $7 million home,” says Yvonne Williams, who left the meeting after Eby argued homeowners can defer the tax “Well if Frank lives to 95, there’s gonna be no money left in that house and the he’ll have to surrender it to the city.”\n\nSince Frank is retired let’s assume he is 65 years old and set the calculator to $7M, no further property appreciation, a 1.2% deferral rate and 30 years. We see that he will owe $490,000 in deferred extra school tax by the time he turns 95, leaving him with a tidy $6,510,000 in net assets. We are guessing that Frank has lived in this home for a while, the accummulated deferred extra school tax will be a drop in the bucket compared to the value gains his home as seen over the past years. I find it hard to relate to how this could constitute any form of cruel hardship. And I am comfortably housed, I don’t want to imagine how someone struggling to pay rent in this city is supposed to by sympathetic to this.\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Extra {School} {Tax} {Calculator}},\n  date = {2018-05-28},\n  url = {https://doodles.mountainmath.ca/posts/2018-05-28-school-tax-calculator},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Extra School Tax Calculator.”\nMountanDoodles (blog). May 28, 2018. https://doodles.mountainmath.ca/posts/2018-05-28-school-tax-calculator."
  },
  {
    "objectID": "posts/2018-06-14-interprovincial-migration/index.html",
    "href": "posts/2018-06-14-interprovincial-migration/index.html",
    "title": "Interprovincial Migration",
    "section": "",
    "text": "CANSIM switched over to the New Dissemination Model (NDM) this past weekend. What this means is that we now have better organized CANSIM data. Yay. But it also broke my R package to easily access and process cansim data. Not so yay. Luckily it was an easy fix to switch things over to the NDM, and the cleaning of data gets even easier. And I also build in functionality to access tables through the old trusty cansim numbers. But unfortunately there was no way to automatically keep the table format the same. Some some adjustments are needed for people wanting to run some of my previous posts that utilize cansim data.\n\nRefreshed cansim R package\nThe refreshed cansim R package is just as easy to use as the old one, and has some added functionality. As an example we will use interprovincial migration data from the last quarter and visualize it as a chord diagram. We ran across this in this twitter thread and thought it would make for a good quick demo. For illustrative purposes we include the code in the post instead of hiding it like we usually do.\nFirst we look for tables with interprovincial migration that have origin and destination geographies using the built-in list_cansim_tables function (thanks Dmitry for spotting a bug and fixing it!). The first time it is called it will take quite a bit of time as it is scraping through cansim table pages. Hopefully StatCan will provide better APIs for this. Make sure you set the options(cache_path=\"some path\") option so that the data will be stored across sessions.\nlibrary(tidyverse)\n#devtools::install_github(\"mountainmath/cansim\")\nlibrary(cansim)\n#devtools::install_github(\"mattflor/chorddiag\")\nlibrary(chorddiag)\n\nlist_cansim_tables() %&gt;% \n  filter(grepl(\"interprovincial migrants.*origin.*destination\",title))\n## # A tibble: 2 x 6\n##   title           table  former  geo    description           release_date\n##   &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;                 &lt;date&gt;      \n## 1 Estimates of i… 17-10… 051-00… Provi… Quarterly number of … 2018-06-14  \n## 2 Estimates of i… 17-10… 051-00… Provi… Annual number of int… 2018-06-11\nWe see there are two tables, one annual and one quarterly. Let’s use the quarterly data.\ntable_id &lt;- list_cansim_tables() %&gt;% \n  filter(grepl(\"interprovincial migrants.*origin.*destination.*quarterly\",title)) %&gt;%\n  pull(table)\n\nmigration_data &lt;- get_cansim(table_id) %&gt;% normalize_cansim_values()\n## Accessing CANSIM NDM product 17-10-0045\nThe normalize_cansim_values function from the cansim package cleans the data by expressing things as numbers instead of e.g. “thousands”, normalizing percentages to be ratios and making inferences about the date format and try to automatically convert dates into R date objects.\nTo make the chord diagram we cut the data down to the last quarter, clip out the origin-destination matrix and feed it to the chord diagram.\nplot_data &lt;- migration_data %&gt;% \n  filter(Date==(.)$Date %&gt;% sort %&gt;% last) %&gt;% # only get the last available quarter\n  rename(Origin=GEO,Destination=`Geography, province of destination`) %&gt;%\n  mutate(Origin=sub(\",.+$\",\"\",Origin), Destination=sub(\",.+$\",\"\",Destination))\n\nchord_matrix &lt;- xtabs(VALUE~Destination+Origin, plot_data)\n\nchorddiag(chord_matrix,groupnameFontsize=10,\n          showTicks=FALSE, type=\"bipartite\",\n          groupnamePadding=5,\n          categorynamePadding=150,\n          width=800, height=600,\n          groupColors=viridis::viridis(nrow(chord_matrix),option=\"magma\"))\n\n\n\n\n\n\nThe chord diagram is interactive, which makes it easy to read off the various values as needed.\nThat’s it. Feel free to grab the code and play with the data. Analyzing CANSIM data has never been this easy. As always, the code lives on GitHub if you want to download it instead of copy-pasting it out of this post.\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Interprovincial {Migration}},\n  date = {2018-06-14},\n  url = {https://doodles.mountainmath.ca/posts/2018-06-14-interprovincial-migration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Interprovincial Migration.”\nMountanDoodles (blog). June 14, 2018. https://doodles.mountainmath.ca/posts/2018-06-14-interprovincial-migration."
  },
  {
    "objectID": "posts/2018-07-13-aboriginal-overrepresentation-in-correctional-services-and-police-checks/index.html",
    "href": "posts/2018-07-13-aboriginal-overrepresentation-in-correctional-services-and-police-checks/index.html",
    "title": "Aboriginal overrepresentation in correctional services and police checks",
    "section": "",
    "text": "In the past weeks I got interested in several news stories on aboriginal youth admissions to correctional services, adult incarceration rates and frequency of getting carded. I have this habit that when something interests me I go grab the original data and take a look myself. Having done this three times on related issues within a fairly short timeframe I decided to throw my code snippets together into a blog post.\n\nTL;DR\nThe takeaway is that no matter how you look at things, aboriginal people are overrepresented in our correctional systems, as well as when it comes to just getting carded on the street. This clearly shows that the long shadow of Canada’s ugly history towards aboriginal people is still very present in today’s experience of aboriginal people. It points to sytemic failures within the Canadian system lasting to this day, much work is still to be done along the path to reconciliation.\n\n\nYouth admissions to correctional services\nThis is the first article I read, and in many ways the most troubling. If we can’t reduce youth admissions to correctional services, it will be even harder to reduce adult incarceration rates. Indeed, another report on this topic notes that\n\nSinclair recalls that in a survey conducted in 1991, more than 70 per cent of adult inmates in the federal system had prior experience with the child welfare system.\n\n\nHere we added the share of the youth (under 20) population with aboriginal identity in 2016 as horizontal black lines. We see that the the frequency of admissions of youth with aboriginal identity is consistently above the aboriginal share in the general population in some provinces.\nWe can split the data into groups of correctional services.\n\nThis shows a discrepancy in the type of correctional services for aboriginal youths.\nWe can normalize the data by dividing by the share of aboriginal youth in the overall youth population in each region in 2016 to show the “factor of overrepresentation”. The share of aboriginal youth in the population may have changed over time, this analysis could be refined by folding in older census data.\n\nThis shows alarmingly high rates for British Columbia, as well as Alberta, although Alberta is missing data for recent years. The overall Canadian numbers are skewed when looking at sub-types like Provincial director remand that are dominated by a few provinces with higher aboriginal share.\nLooking at Manitoba in particular\n\nwe notice that the factor of overrepresentation is highest for secure custody and lowest for community sentences, which should receive more scrutiny.\n\n\nAdult custodial admissions\nJust like before we can look at the share of admissions to custodial services that have aboriginal identity, which has been climbing Canada-wide as reported by Angela Sterritt.\n\nSplitting this out by type of admission\n\nwe again notice disparate shares in some provinces. We can again normalize the data by the share of aboriginals in the adult (20+) population to examine the factor of aboriginal overrepresentation.\n\nHere we see that Alberta and British Columbia are again the provinces with the highest aboriginal overrepresentation, and we also notice that the the overrepresentation has been getting worse over time in some provinces.\nThese graphs contain a lot of information and can be messy and difficult to read. To clean things up a bit we can focus on the last available year only.\n\n\n\nVancouver Street Checks\nLastly we turn to Vancouver street checks. It was reported yesterday that “1 in 5 women ‘carded’ by Vancouver police in 2016 were Indigenous”. The Union of BC Indian Chiefs and (UBCIC) and the BC Civil Liberties Association (BCCLA) have filed a complaint with the Office of the Police Complaints Commissioner (OPCC) over data suggesting Indigenous people were being disproportionately street-checked.\nThis complaint is based upon data released via an FOI and posted on the Vancouver website. It’s in PDF format, so it requires a bit of scraping and massaging to get it into processeable form.\nThe data contains a breakdown of street checks by Ethnicity and Gender and we see how female aboriginals are disproportionally represented in the data.\n\nWe can use data from the 2016 census for the City of Vancouver, as well as Musqueam 2 which is also part of the VPD jurisdiction, to compute the factor of overrepresentation like we did before.\n\nThis shows that aboriginal women are overrepresented by a factor 10, although there was a slight drop in recent years. Male aboriginal overrepresentation has been consistently lower, but trending slightly upward. People will not be surprised that black males are the next-most overrepresented group.\n\n\nNext steps\nThese numbers are disturbing, and this analysis is just scratching the surface. It deserves more detailed look province by province. The disparate rates for different custodial services should also be examined more closely, and in this post we have not explored the full breakdown of custodial services that is available in the data.\nAs always, the code for the analysis is available on GitHub, the code automatically pulls in all the data needed and reproduces the analysis. This should make it fairly straightforward for anyone to copy, adapt and refine the results.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Aboriginal Overrepresentation in Correctional Services and\n    Police Checks},\n  date = {2018-07-13},\n  url = {https://doodles.mountainmath.ca/posts/2018-07-13-aboriginal-overrepresentation-in-correctional-services-and-police-checks},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Aboriginal Overrepresentation in\nCorrectional Services and Police Checks.” MountanDoodles\n(blog). July 13, 2018. https://doodles.mountainmath.ca/posts/2018-07-13-aboriginal-overrepresentation-in-correctional-services-and-police-checks."
  },
  {
    "objectID": "posts/2018-07-22-fact-checking-vancouver-s-swamp-drainers/index.html",
    "href": "posts/2018-07-22-fact-checking-vancouver-s-swamp-drainers/index.html",
    "title": "Fact-checking Vancouver’s Swamp Drainers",
    "section": "",
    "text": "Down south of the border, a politician who shall remain nameless campaigned on “draining the swamp” of Washington D.C., trafficked in countless conspiracies, and lied his way into office. His lies painted a picture of a United States turned dark, corrupt and menacing. He promised to fix it, Making American Great Again, mostly by shutting down globalization and kicking out the immigrants.\nIn Canada, we like to think we’re immune to this kind of rhetoric. But a strain has made its way into discussions concerning Vancouver, where the intersection of real estate, politics, and globalization are increasingly portrayed as a swamp in need of draining. We don’t believe most of those portraying Vancouver as swamp-like are intentionally lying (and in real life they surely favour the preservation of environmentally sensitive wetlands). Nevertheless many commenters are muddying the discourse with poorly sourced claims as a means of scoring political points and attacking various aspects of globalization.\nIt’s tricky to track down the spread of all the false claims out there. Fortunately a bunch of them were concentrated in a recent piece on “Dirty Money” in Macleans by Terry Glavin that views Vancouver as “a case study in the dark, broken and ugly side of globalization.” Recognizing that getting facts and interpretations right is often difficult for even the most well-intentioned, let’s work toward correcting a few misperceptions, line by line:\n\n“At least 20,000 Vancouver homes are empty, and nobody’s really sure who owns them.”\nVariations of similar statements permeate the media, with various degrees of factual accuracy. The most common misrepresentation is to refer to the 25k homes not “occupied by usual residents” as “empty”, which the above quote avoids by using an appropriately lower number.\nThe main issue with the above quote is that it’s portraying those “at least 20,000” homes as problematic vacancies, neglecting that that count includes moving vacancies around census day, empty suites (about 4000 of them), and units in buildings that completed around census time and did not have the time to fill in yet.\nAccounting for these types of vacancies, we arrive at the ballpark of the Ecotagious Study based on BC Hydro data that found between 10,800 (for year-long vacancies) to around 13,500 (for four-month vacancies) and now the 8,481 empty homes through the empty homes tax declarations, although some of those empty homes found via the EHT are outside of the universe Ecotagious reported on.\nWhen quoting these numbers, the key question is what are the numbers supposed to be used for. If it’s to highlight “problematic” vacancies, then the Ecotagious numbers probably get us the best estimate for that point in time. Since then the number has likely dropped due to Empty Homes Tax pressure, we will have to wait until the repeat of the Ecotagious study to get confirmation on by how much.\nAnd the reason we don’t know who owns them is not for some nefarious reason but simply because the methods we have for estimating empty homes (other than the ones caught by the Empty Homes Tax) do not allow for the identification of units.\n\n\n“Another 25,000 residences are occupied by homeowners whose declared taxable household incomes are mysteriously lower than the amount they’re shelling out in property taxes, utilities and mortgage payments.”\nThat’s plain false, we have looked at this before. The 2016 census counted only 8,940 owner households with higher shelter costs than income. An additional 14,510 renter households paid more than their income in rent and utilities, making for a total of 23,450 households in the City of Vancouver that had higher shelter cost than income, most of which were renter households.\nThe wording of the sentence, followed by the next talking about tax avoidance in British Columbia real estate, seemingly suggests that the majority of these 23,450 households were cheating in some way. Let’s take a closer look at these households with shelter cost higher than income.\nOne of us (Jens) is partially responsible for bringing this stat into circulation and failing to provide more extensive context from the get-go.\n\nLooking more closely, we see that the bulk of these households are non-census-family households, probably roommates in many cases. Students likely account for a lot of the data. Single parents are also common. While there are some indications of irregularities in the data worth investigating further, broadly suggesting all these households are tax cheats is irresponsible.\n\n\n“Non-residents own roughly $45 billion worth of Metro Vancouver’s residential properties, and non-residents picked up one in five condominiums sold in Metro Vancouver over the past three years.”\nThe first part is fairly accurate, CHCP reports that $43 billion worth of residential properties in Metro Vancouver were owned by non-residents. Of course that’s less than 5% of the total value of $884.50 billion.\nThe second part is a prime example of making statements without understanding the data. We don’t have data on non-resident buyers, presumably referring to buyers with primary resident outside of Canada at the time of the sale.\nConsidering similar statements in an earlier article by the same author, our best guess is that the author was referring to non-resident owners of condos that were built between 2016 and late 2017. Owners of recently built condos could be taken as a proxy for buyers if one makes some assumptions on resales.\nExcept the ratio of condo units built between 2016 and late 2017 that were held by non-resident owners is one in 7.1 for Metro Vancouver, and for the City of Vancouver that the previous article was referring to the ratio is one in 6.5. (CANSIM 33-10-0003)\n\nIn summary it seems the original statement is the product of playing loose with definitions, Metro vs City mixup and aggressive rounding to pump up the numbers.\n\n\n“But Transparency International reckons about half of Vancouver’s west-side residences are owned by mystery trusts or shell companies.”\nBig if true, a claim so outrageous that it needs data to back it up. It seems that this is based on a transparency international report that the author also referred to in a February column, where the author characterized this as “Transparency International estimates that perhaps half of Vancouver’s high-end residences are now owned by shell companies or trusts”. Now this has morphed into “about half of Vancouver’s west-side residences”. It’s good to remember what the Transparency International study actually did, it looked at the 100 most expensive properties in Metro Vancouver and found that 46 of these were owned by companies or trusts (not all of which have opaque ownership).\nVia StatCan’s CHSP (CANSIM 39-10-0003) we now know that 5.61% of Metro Vancouver’s residential properties are owned by companies or trusts (or “non-individuals”), roughly in line with most other Canadian metropolitan areas in BC and ON as the following graph shows. Needless to say, the 100 most expensive properties on Vancouver’s west side are likely quite distinct from the rest.\nEven after adding the non-resident owners to the non-individual owners, Vancouver still looks a lot like most other metro areas. In fact, the only metro area that really stands out is London, ON. Otherwise it’s the non-metropolitan portions of BC and ON that have the highest representation of company and trust ownership structures.\n\n\n\n“In Metro Vancouver, homeownership costs amount to 87.8 per cent of a typical household’s income”\nIt does not. Most people spend far less, as the following graph on share of income spent by owners on shelter costs demonstrates.\n\nThe author appears to be conflating running shelter costs of owner households with the RBC affordability metric which compares the cost of financing the typical home for sale in the region to the typical household income. The latter metric may (imperfectly) reflect some of the difficulty now facing those wishing to jump from renting to owning, but has little bearing on how much typical households currently spend in either category.\n\n\n“Vancouver has also become a major global hub for organized crime networks based in China.”\nHere the author immediately pivots to the opioid crisis and the suspicious transactions identified in the recent money laundering report concerning lax oversight of casinos, attempting to link these to broader affordability issues and to globalization. To be clear, both the opioid epidemic and money laundering are serious issues in their own right. The fentanyl crisis has killed way too many British Columbians. As a recent report by Sandy Garossino notes, the criminal organizations associated with money laundering through BC Casinos have also claimed multiple lives. We should be outraged by the crisis and the crime ring, but it’s wrong, as Garossino adds, that this, “mainly bugs us because we figure it’s driving up the cost of housing in Vancouver.” The opioid epidemic demands more sustained attention than it’s likely to receive as a prop for tarring globalization. That’s not at all what it’s about. It requires a comprehensive re-think of our health care systems, pain management strategies, and criminalization of drug use, and the biggest villain in the story so far appears to be a major American pharmaceutical company. As for money laundering, further reporting on its role within the real estate sector has been promised by the Attorney General, but so far it’s not clear that shady practices - while certainly present - have had much to do with driving up real estate prices. As multiple commenters have noted, even if all the $100 million so far reported to have been laundered in our casinos over 10 years was re-invested in real estate, it would represent at most tiny fraction of total real estate transactions. Property transfer tax data shows that Metro Vancouver averaged $5.2bn worth of residential real estate transactions each month in 2017, dropping to $4.5bn during the first 5 months of 2018. There are real reasons to be outraged over the opioid epidemic and money laundering. But the link between these issues and affordability remains tenuous, and insisting upon the link in the absence of further reporting diminishes the importance of the documented damage they’ve already generated without pointing toward any good solutions for affordability, the opioid crisis, or tackling money laundering.\n\n\n“Freeland could have been describing Vancouver: ‘Median wages have been stagnating, jobs are becoming more precarious, pensions uncertain, housing, child care and education harder to afford.’”\nThis is plain false. To its credit, back in February the NDP government moved to make childcare much more affordable for British Columbians. Why ignore this progress? Moreover, Vancouver has seen strong jobs and income growth. To gauge wage growth, we look at full-time employment income for couple families, lone parent families and unattached individuals and compare the trajectories to Metro Toronto.\n We see that Vancouver CMA has overtaken Toronto for non-family individual income and lone-parent median income, and almost closed the gap on couple family income.\n\nThis shows how Vancouver’s labour force participation rate has increased with respect to Toronto while the unemployment rate decreased. Lastly we can look at the regional job vacancy rate for the respective economic regions to see how Vancouver’s job market is much stronger than the labour force is able to fill.\n\n\n\nAfterword\nIn our hyper-polarized environment it is probably not enough to simply point out factual errors without further comment. So we take this opportunity to state that we strongly support stricter oversight and enforcement of money-laundering, as well as implementing measures to increase transparency in property ownership. We are also gravely concerned about Vancouver’s affordability problems. We’ve supported a number of housing policies recently put forward by local governments, including the empty homes tax, the school tax, and the boost to social housing investments, all aimed at fixing regulation and providing more housing to those most in need. It’s important to separate out what governments are doing right from where they might be failing. This is where swamp imagery fails us, blending everything together and dragging it all into the mud.\nWe think fixing our affordability problems is going to involve making tough choices and policy tradeoffs, and we should approach them with a clear sense of what’s at stake rather than mixed up facts, vague swamp-ish imagery and the sense it can all be blamed on the dark, corrupting forces of globalization. We’ve all seen where that last route can take us.\nAs usual, the underlying R Notebook for this post that includes all the code for the graphs and numbers in this post is available on GitHub. Feel free to download it to reproduce the analysis or adapt it for your own purposes. Hopefully this kind of transparent and reproducible analysis can help establish a shared base of facts. And reduce the amount of guessing needed to make sense of people’s numbers and statements.\n\n\nUpdate (July 27, 2018)\nSeveral people have pointed out via Twitter and comments that inflation-adjusted income growth might be a better metric to use. And that’s a good point. In the context of housing we often have nominal housing prices in mind, so nominal income can be a good metric in this context. But inflation-adjusted incomes add another important perspective.\n\nHere we used Canada-wide inflation estimates, Vancouver’s income growth looks even stronger when normalizing by CMA-specific CPI. (Digging into the reasons for this would probably make another interesting blog post.) The two graphs show the inflation-adjusted incomes, as well as change in adjusted incomes indexed to 2000. We can clearly see the growth in all categories, both in absolute terms, as well as in relative terms compared to Toronto. Despite this, the notion of “stagnant incomes” in Vancouver is quite pervasive in news stories.\n\n\nPS\nThis was written jointly with Nathan Lauster and cross-posted on his blog.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{lauster2018,\n  author = {Lauster, Nathan and von Bergmann, Jens},\n  title = {Fact-Checking {Vancouver’s} {Swamp} {Drainers}},\n  date = {2018-07-22},\n  url = {https://doodles.mountainmath.ca/posts/2018-07-22-fact-checking-vancouver-s-swamp-drainers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLauster, Nathan, and Jens von Bergmann. 2018. “Fact-Checking\nVancouver’s Swamp Drainers.” MountanDoodles (blog). July\n22, 2018. https://doodles.mountainmath.ca/posts/2018-07-22-fact-checking-vancouver-s-swamp-drainers."
  },
  {
    "objectID": "posts/2018-08-10-taxing-property-instead-of-income-in-b-c/index.html",
    "href": "posts/2018-08-10-taxing-property-instead-of-income-in-b-c/index.html",
    "title": "Taxing property instead of income in B.C.",
    "section": "",
    "text": "Vancouver has low property taxes and high income taxes. Seattle is the opposite. What would it look like if British Columbia was more like Washington State? If we got rid of personal provincial income tax and recovered the revenue by raising the provincial portion of the residential property tax, a.k.a. the “school tax”.\nThe tax policy of British Columbia, when compared to Washington, is sending the message that it’s a great place to come and invest in property with it’s low property tax rate, but not such a great place to live and work with it’s higher income tax rate. There have been a number of initiatives to try and change this, from the BC Housing Affordability Fund that aims to raise property taxes for those that don’t work here, to the Extra School Tax on properties over $3M that raises property taxes on expensive properties and follows arguments for a progressive property tax, to the “speculation tax” that is being discussed and the details are getting fleshed out right now.\nBut there has been little public discussion about the simplest solution that is virtually impossible to avoid: Get rid of the provincial income tax and shift the entire tax load to property taxes. That would discourage using property as investment, soften price volatility and give people that work and pay taxes in BC an edge. This tax setup may seem a little radical to some in Vancouver, but it’s totally normal to people in Seattle.\n\nThe numbers\nLet’s look at the numbers. The current BC budget for 2018∕2019 estimates a personal income tax revenue of $9.53 billion. The total 2018 property tax assessment for residential (Class 1) properties in BC was $1,445.51 billion for the 2018 tax year. So to offset the foregone income tax we would need an additional flat property tax rate of 0.66%. Or possibly choose something more complex like a progressive property tax. And we might want to exclude purpose-built rental buildings from this, that would certainly help get more of those built. (Although property taxes on rental buildings are tax-deductible, so the net effect on these is softer.) For now, let’s go with the flat tax.\nThe property tax rate in King County is 1.02%. How would that compare with British Columbia’s rates? Let’s add in the “hypothetical income tax relief” portion of the property tax.\n\nThis shows that if we followed Washington State’s example and nixed our provincial income tax and recovered the lost revenue from property taxes, most of the larger cities would end up with a tax rate similar to Seattle’s.\nBut how about the smaller cities? Let’s take a look at the highest taxed municipalities.\n\nWe see that the spread is not that large, although a few of the very small municipalities have substantially higher tax rates. So substituting the income tax revenue through increased “school tax” brings essentially all of British Columbia into the range of King County’s property taxes.\n\n\nThe effect on homeowners\nLet’s look at what this translates into on the ground. For Metro Vancouver we had an average owner-occupied home value of $1,005,920 in 2016, and average annual shelter costs, including mortgage payments, property taxes and regular home maintenance, but also utilities, of $19,464. (Averages are better suited than medians for this kind of calculations, but that choice does not make much of a difference for the results). This shows that the average shelter cost would go up by $6,635, an increase of the average running shelter cost by 34.1%.\nThat looks like a substantial increase, but we need to remember that this is offset by getting rid of the provincial income tax. The average effective provincial tax rate was 5.61%, so people will have on average 5.61% more after-tax income to take home. Homeowners in Metro Vancouver had an average household income of $115,760, resulting in a lowballed average increase in after-tax income of $6,490.43. That number is lowballing it since Metro Vancouver homeowners tend to have higher incomes than the average British Columbia household, so they will have a higher effective provincial tax rate and thus higher savings.\n\n\nWinners and losers\nWhenever there is a change in the tax system there will be winners and losers. The hypothetical tax change is revenue neutral, so on average everyone comes out even. Except homeowners that don’t live or don’t pay taxes in BC, those will pay extra. So the BC taxpaying resident will come out on top. On average, and that is also supported by our rough analysis using census data above.\nSome will pay extra, in particular homeowners in high-value homes with low income, including some seniors. Others would end up on top, for example high earners in lower value housing or renting.\nIt would probably be worthwhile to dig down further and understand who the winners and losers would be after such a policy change.\n\n\nImpact on property values\nBut higher property taxes depress property values. But by how much? On the other hand, increasing after-tax income puts upward pressure on home prices. Estimating and comparing these effects is tricky business. But we can give a simplified heuristic argument to get a rough idea of the possible effects.\nWe can estimate the effect of raising property taxes on home values by computing the present value of the additional future tax payments. Using a discount rate of 3.5% we get a present value of 12.1% of the home value assuming a 30 year revenue stream, or if we assume perpetual payments we get 18.8% of the home value. That gives us a range by which this could lower home values.\nFor people working and paying taxes in BC this would be offset by a the ability to spend more on housing due to lower income taxes. Estimating the effect of that is a bit harder as it requires a lot more assumptions. Perhaps a simple way to estimate it is to compare (a portion of) the present value of the tax savings and compare that to home values. If we dedicate the entire tax savings to housing we an upward pressure on home prices by 11.9%, if we only dedicate a third of the tax savings to housing and spend the remainder on other things we get an home price increase of 3.96%. So rising after-tax income would somewhat offset the downward pressure from property taxes.\nOn balance the tax shift will likely exert a little downward pressure on home prices. Which in turn would require rising the hypothetical property tax relief rate a bit. But simple math shows things would stabilize pretty quickly.\nAgain that’s an over-simplified view on the issue, and things are never that simple. Especially when it comes to Vancouver real estate. I will leave it to the economists to refine this appropriately.\nAs always, the code underlying this post is available on GitHub, feel free to grab it and adapt four your own purposes.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{von bergmann2018,\n  author = {von Bergmann, Jens},\n  title = {Taxing Property Instead of Income in {B.C.}},\n  date = {2018-08-10},\n  url = {https://doodles.mountainmath.ca/posts/2018-08-10-taxing-property-instead-of-income-in-b-c},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBergmann, Jens von. 2018. “Taxing Property Instead of Income in\nB.C.” MountanDoodles (blog). August 10, 2018. https://doodles.mountainmath.ca/posts/2018-08-10-taxing-property-instead-of-income-in-b-c."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html",
    "title": "Airbnb and STR licences",
    "section": "",
    "text": "We are getting close the end of the STR grace period ending August 31st, so it’s time to take a look at compliance of the STR operators. For today we will just look at Airbnb, which is the largest STR listings platform. Other larger listings platforms are VRBO or Craiglist."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html#how-did-we-get-here",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html#how-did-we-get-here",
    "title": "Airbnb and STR licences",
    "section": "How did we get here?",
    "text": "How did we get here?\nThe concern over short term rentals is a fairly new phenomenon. As recently as 2013 a local columnist praised the benefits of Airbnb in generating higher incomes and side-stepping the resiential tenancy act. In the following years potential negative impacts of Airbnb slowly entered the public debate, Karen Sawatzky’s excellent master’s thesis provided the foundation for much of the debate, her slides from the PHRN in 2015 give a good overview over the main points.\nThis led to a City of Vancouver council motion in April 2016 to direct staff to study the “impact of short-term rentals on housing stock”, “options to mitigate negative impacts”, “steps that other cities are taking to address these issues”, and to “seek co-operation from Airbnb and other listing services, including detailed data on listings, to ensure an accurate assessment of the issue”, and “to consult with the Renters Advisory Committee, the tourism industry, and others as needed on the City’s response to this issue”.\nLots of “process” and two years later this resulted in the data sharing MOU with Airbnb, with Airbnb agreeing to reject listings without a licence number after August 31st, 2018 and sharing data on hosts, licences and addresses with the city thereafter."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html#the-rules",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html#the-rules",
    "title": "Airbnb and STR licences",
    "section": "The Rules",
    "text": "The Rules\nTo recap, the STR rules in the City of Vancouver defines a short term rental (STR) as a unit rented out for less than 30 days at a time. To operate an STR legally within the City of Vancouver the operator needs to have an STR licence. After August 31st any operator operating an STR without a licence faces fines up to $1,000.\nThe STR business licence needs to be displayed on all listings."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html#airbnb",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html#airbnb",
    "title": "Airbnb and STR licences",
    "section": "Airbnb",
    "text": "Airbnb\nThe City agreement with Airbnb makes enforcement a lot easier. The rules require the display of a licence on each ad, and failing to display a valid licence is an easily enforceable offence, with Airbnb sharing host, licence and address information. The City won’t have to check if a suite actually got rented out, the mere act of advertising it is an offence.\nThe licence information, if provided, is already displayed on Airbnb listings. That gives us the opportunity to gauge compliance at this point in time, a week and a half before the grace period runs out. And we can compare that with the Business licence data from the City.\nThe data is not perfect, it takes a bit of effort to convince the Airbnb website to divulge the information we require for this, and listings are in constant flux. But it gives a reasonable good indication of where we are at at this point in time.\n\nCompliance\nOur first question is compliance, how many listings have submitted a valid licence.\nOur scrapes picked up 4,768 listings within the City of Vancouver proper. (Read further below about some caveats regarding the geography).\nWe have looked at STR business licence data before, which makes it easy to fold it into the Airbnb data. The city dataset gets updated on a daily basis, so we are assuming the licence data from the city is up-to-date. It contains 218 licences marked as “pending”, so these are intermediate licences still making their way through the bureaucracy.\n\nInterestingly, 10 of these pending licences already found their way onto Airbnb. Concerning is the appearance of 10 licences marked as “Inactive” or “Gone out of business” in listings. Similarly, there are 62 licences that are formatted like City licence numbers but have no match in the City dataset. Then there are 199 entries with the word “Exempt” in the licence field, with some noting in the description that they only rent for periods greater than 30 days, plus another 51 listings with other text in the licence field that does not match the City licence format. Most of those claim to be exempt because they rent for longer than 30 days or pledge to apply for their licence soon.\n\n\nExempt licences\nThe obvious thing to check for listings that claim to be exempt is what minimum stay is set within Airbnb, that is the minimum number of nights that need to be booked, and then compare that to the licence status of the listings.\n\nThere are a surprising (to us) number of long-term listings, so listings with a minimum stay of 30 days or more. While listings with minimum stay of at least 30 days have higher proportion of missing and “Exempt” listings, the majority of listings claiming to be “Exempt” are not listed as long-term rentals on Airbnb. The same is true for listings with missing licence information. We might be missing some other type of rental that are exempt from getting a licence, or the enforcement department will be busy next month.\nAs an aside, there may be some legal wiggle room to set the minimum stay to 29 instead of 30, as the minimum stay counts nights not days.\n\n\nMultiple licences\nThe vast majority of the 2,065 STR operators have a single licence, but some have more than one active license. \nSome of these cases are operators that advertise several rooms or spaces in shared rooms in their house. But another is what appears to be renting out main unit, basement suite and laneway house as three separate units with operators living off-site. Again, we suspect that these cases will be subject to enforcement soon.\n\n\nMultiple listings\nWe can also slice the data the other way around and look for operators with more than one listing on Airbnb.\n\nIn total 683 operators have more than one listing, most of those have two, but one operator has 36. Some of these will be people renting out several rooms within the primary residence that they live in. Not sure what the rules say about that. But looking through the data it’s obvious that there are also people operating multiple units. And people operating hostel-like accommodation. Some may be hotels or bed and breakfast operators cross-listing, but browsing the data it is clear that that there are plenty that don’t fit these categories. I am curious to see how they will fare after the grace period runs out."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html#multiple-listings-with-same-licence",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html#multiple-listings-with-same-licence",
    "title": "Airbnb and STR licences",
    "section": "Multiple listings with same licence",
    "text": "Multiple listings with same licence\nLastly we can look at issued licenses and check for how many listings they were used.\n\nIt turns out some licences are used for several listings, up to 8 in some cases. I am not entirely clear what the rules are when one operator lists several rooms (or just several beds) in their house, but I was under the impression that one could not rent out several “entire home/apt” units under the same licence.\nFiltering down to listings that are entire homes or apartments, we still get a sizable number of licences used on more than one listing.\n\nThat seems problematic."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html#geography",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html#geography",
    "title": "Airbnb and STR licences",
    "section": "Geography",
    "text": "Geography\nJust for the fun of it we can also look at the geography of listings. We are already familiar with geographic distribution of listings via the work of Inside Airbnb, so there is not much new to be learned here. But for completeness we show the listings by licence status. Locations are only approximate."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html#jurisdictions",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html#jurisdictions",
    "title": "Airbnb and STR licences",
    "section": "Jurisdictions",
    "text": "Jurisdictions\nThe last point we want to touch on is that of jurisdictions. Vancouver is a case where address and jurisdiction don’t match. Everyone in the City of Vancouver has a Vancouver address, but not everyone with a Vancouver address lives in the City of Vancouver. The latter category is comprised of Musqueam 2, UBC, UNA and the UEL. Musqueam 2 are covered under some City of Vancouver planning procedures, but it is not clear to us if the Airbnb rules apply in Musqueam 2. The rules don’t apply in UBC, UNA or UEL homes.\nFor us that brings some difficulties in reliably identifying listings within the City of Vancouver. The following map gives an overview on matching listings to jurisdictions, either by geography and name, by geography only, name only, or neither.\n\nWe went by geography only for all of the above analysis, and looking at the map it seems that we got some cases wrong along Boundary Road and possibly near Blanca in Little Australia. We are also missing some cases located (possibly by accident) within False Creek. We could use the neighbourhood field from Airbnb to help decide some of these edge cases, but that field is not always populated and does not distinguish Musqueam 2 (although maybe we should include that area anyway). For this post we will live with those misclassifications."
  },
  {
    "objectID": "posts/2018-08-21-airbnb-and-str-licences/index.html#next-steps",
    "href": "posts/2018-08-21-airbnb-and-str-licences/index.html#next-steps",
    "title": "Airbnb and STR licences",
    "section": "Next steps",
    "text": "Next steps\nWe will revisit this in early September after the grace period expires. At that point every listing will have to have the licence number field populated on Airbnb to stay visible. Unless we see a huge spike in business license applications we should either see a lot of listings coming offline or a slew of bogus licence numbers appear. The latter option will likely turn out costly for operators choosing to go that route, as Airbnb will begin sharing data on hosts, addresses and licence numbers for CoV to compare with their dataset. The September/November time frame is when we will learn how effective Vancouver’s Airbnb regulation really is. Ether way, the bylaw officers will be busy."
  },
  {
    "objectID": "posts/2018-10-16-naked-buildings-map/index.html",
    "href": "posts/2018-10-16-naked-buildings-map/index.html",
    "title": "Naked Buildings Map",
    "section": "",
    "text": "The other day the New Your Times did a really fun story on buildings in the US, and I was chatting about that with someone at the VPL last night. Which reminded me that I wanted to replicate that for Vancouver. So here we go.\nThere are several datasets for building data, and we have worked quite a bit with the City of Vancouver LIDAR generated data of 2009. We have mapped this on several occasions in the past, for example to show building heights, or just residential buildings to map building values, and have used the building heights data in Vancouver and Toronto to make building heights profiles by distance from downtown.\nRobert White did a really nice building map too.\nAnother great source, in many ways better for this project, is OSM building data. It’s not very good at 3D data, but the footprints are pretty up-to-date.\nI really liked the simplicity of the NYT map, so why not make yet another building map? These kind of “naked maps” can be quite revealing, and it reminds of the naked bike maps from the WaPo WonkBlog that we also imitated, although it seems that my live maps did not survive my blog update. The WaPo even made a quiz to have people guess cities by the naked bike maps."
  },
  {
    "objectID": "posts/2018-10-16-naked-buildings-map/index.html#vancouver-building-footprints",
    "href": "posts/2018-10-16-naked-buildings-map/index.html#vancouver-building-footprints",
    "title": "Naked Buildings Map",
    "section": "Vancouver building footprints",
    "text": "Vancouver building footprints\nThis is a good opportunity to use the excellent mapdeck package that provides R bindings to deck.gl. The maps are interactive, you can zoom and pan, but they only cover the City of Vancouver. It may take a moment for the data to load.\n\n\nIf you want to enjoy this in detail, head over to the fullscreen version.\nIf you want some context, here is another one with streets and street names."
  },
  {
    "objectID": "posts/2018-10-16-naked-buildings-map/index.html#osm-buildings",
    "href": "posts/2018-10-16-naked-buildings-map/index.html#osm-buildings",
    "title": "Naked Buildings Map",
    "section": "OSM buildings",
    "text": "OSM buildings\nWe do have another source of buildings: Open Street Map. And that data comes tiled on most basemaps. With modern vector maps it’s easy to make a map with just the buildigns, but the disadvantage is that the buildings are only visible when zoomed in quite closely, vector tiles at higher zoom level typically don’t come with the buildings data.\n\nThe problem here is that the buildings disappear when zooming out, that’s because the data is served as vector tiles, and buildings are omitted for higher zoom levels.\nOne way around that is to download the buildings from osm first. We quite like Geoff Boeing’s OSMnx package, so that gives us a chance to try out the better python integration in R notebooks in RStudio 1.2. Things are still not as smooth as one would like, and I could not figure out how to pass binary geographic data from python to R, but transforming to geojson gets the job done.\n\n\nAgain, you can also view this map fullscreen. The advantage of grabbing the data form OSM is that this immediately generalizes to everywhere where we have OSM building data.\nAs always, the code for this post is on GitHub. If you want maps for other cities in Canada or elsewhere in the world, just grab the code and modify it to your liking."
  },
  {
    "objectID": "posts/2018-10-22-toronto-wards/index.html",
    "href": "posts/2018-10-22-toronto-wards/index.html",
    "title": "Toronto wards",
    "section": "",
    "text": "Vancouver had elections on Saturday, today Toronto had their elections. And as opposed to Vancouver, Toronto has wards. Which makes things more fun, as we can look at census data for each ward to understand how people voted in the ward. We ran a very similar type of analysis the other day for Vancouver, so this is an easy add.\nThe Toronto Open Data catalogue has data for the ward boundaries and a custom tab with census data. All we need to do is download the data and link them. To check that it works we start with a population density map by wards.\nNow to more interesting things. Results aren’t all in yet, so let’s look at eligible voters, so citizens aged 18 and up. The only problem is, the city did not pull that variable for their custom tab.\nWhat we need is to estimate eligible voters in each ward from regular census data. Enter TongFen."
  },
  {
    "objectID": "posts/2018-10-22-toronto-wards/index.html#tongfen",
    "href": "posts/2018-10-22-toronto-wards/index.html#tongfen",
    "title": "Toronto wards",
    "section": "TongFen",
    "text": "TongFen\nThe TongFen R package that automates the process of estimating census data for arbitrary regions. And we can use the dotdensty package facilitates using Dissemination Block population and region data to refine the estimate.\nBefore we get serious, let’s take this for a test ride and look at some census data that the City of Toronto did pull for the wards. And compare it to our TongFen estimates.\n\nThis shows that the difference between the estimates and the variables are quite sizable, up to a little over 1%, for most variables and up to 3% for non-permanent residents. There are only somewhere between 1230 and 9920 non-permanent residents per ward, so only fairly small errors are needed to produce a 3% error.\nBut if we are interested in derived quantities, like the share of non-permanent residents, then we should expect better results. I will spare you the math, but we expect some of the errors to divide out. And we can see this if we look at the difference in the percentage shares.\n\nThe come out consistently less than a quarter percent. That’s pretty good and gives us confidence that we can use TongFen to estimate the share of eligible voters in wards.\nFirst let’s look at what fraction of eligible voters lives in each ward.\n\nWard 3 is definitely the winner, with 1.53 times the number of eligible voters than Ward 16.\nI picked up that there are some discussions in Toronto to allow permanent residents to vote. And Doug Saunders argued this point earlier this year So let’s check what percentage of the adult population in each ward is not eligible to vote. (There is a slight issue in that citizen’s are only reported for the population in private households that we will gloss over here.)\n\nThis shows that this is not just a question about residency and allegiance, but also about geography. In Ward 18 a fill 28.9% of adults aren’t eligible to vote, whereas in Ward 25 only 9.70% aren’t eligible. This includes non-permanent residents, for which we don’t have age data. If we assume all non-permanent residents are adults, so discounting school-aged non-permanent residents, we get slightly lower numbers as the following graph shows.\n\nAs expected this does not change things by much."
  },
  {
    "objectID": "posts/2018-10-22-toronto-wards/index.html#more-tongfen",
    "href": "posts/2018-10-22-toronto-wards/index.html#more-tongfen",
    "title": "Toronto wards",
    "section": "More TongFen",
    "text": "More TongFen\nFor geeks only, a little more details on TongFen. We mentioned at the beginning that we go down to census block data to derive the estimates. It’s a good exercises to compare what happens when we only use Dissemination Area or even Census Tract level data to derive the estimates. Let’s do a quick comparison.\n\n\n\n\nWe see that census tracts can lead to very large errors. When using variables that have stronger spatial gradients this will be even more pronounced. That’s why trying to validate TongFen estimates is important. And of course doing the little extra work to take advantage of of Dissemination Block data. Incidentally, we suspect that the botched numbers that prompted our recent Vancouver eligible voter fact-check may have been the product of a poorly executed TongFen."
  },
  {
    "objectID": "posts/2018-10-22-toronto-wards/index.html#next-steps",
    "href": "posts/2018-10-22-toronto-wards/index.html#next-steps",
    "title": "Toronto wards",
    "section": "Next steps",
    "text": "Next steps\nThat’s it for tonight, I might do another post once the election data comes in. Or maybe someone that understand Toronto better is interested in grabbing my code and doing a more in-depth analysis."
  },
  {
    "objectID": "posts/2018-11-28-moving-penalty/index.html",
    "href": "posts/2018-11-28-moving-penalty/index.html",
    "title": "Moving Penalty",
    "section": "",
    "text": "Aaron Licker asked a good question about this very interesting dataset.\nUnfortunately it is not obvious where to get the raw data, but Keith Stewart at the Vancouver CMHC office was kind enough the share the dataset. So read on to follow my quick look at the data, or just download it if you want to tinker yourself. (French version here)."
  },
  {
    "objectID": "posts/2018-11-28-moving-penalty/index.html#moving-penalty",
    "href": "posts/2018-11-28-moving-penalty/index.html#moving-penalty",
    "title": "Moving Penalty",
    "section": "Moving penalty",
    "text": "Moving penalty\nThe data is interesting because it gives an estimate of the “moving penalty”, that is on average how much more rent a person that is currently renting would have to pay to move to a different purpose-built rental unit in the same CMA.\nOf course the exact amount is location specific, and the CMHC estimates for the rents for “vacant” units are not very precise, especially in areas with low vacancy rates. And we don’t know much about the quality and location of the vacant units. The vacancy rate varies geographically across CMAs. For example the City of Vancouver has a lower rate than the CMA average, so this will lead to the vacancy units over-sampling the outer (and less expensive) areas, so the true penalty to move in Vancouver might be higher.\nUnderstanding some of the data caveats, here are the moving penalties by CMA.\n\nThis suggests that in Halifax people could lower their rent by almost 10% if they are willing to move, whereas in Victoria people should expect a 30% rent hike if they need to move. Again, these are only estimates, and quality and location of units can have sizable effects on this representation. It could well be that the rentals that are currently vacant in Halifax are simply not very desirable and Halifax has no need for less desirable (and cheaper) rentals.\nThe same cannot be said for Vancouver, where council is currently in the second day of a public hearing about strengthening renter protections in case of extensive building renovations. Renovations improve the quality of rentals, which is good for renters. But they also increase rents, which is bad for renters. On top of that, in a rent controlled environment with vacancy decontrol like in Vancouver, renters may pay a strong moving penalty that is independent of the quality of the unit and just a function of the time they have lived in their current unit. At the centre of this are “renovictions”, where the landlord conducting extensive renovations that requires the termination of the leases. It is not always clear if these renovations are matter of necessity to prolong the life of the building (which is quite old in many cases), or if it primarily motivated by a combination of the desire to circumvent rent control and capture higher rents for higher quality renovated units.\nOf note is that the moving penalty is generally higher in Provinces with rent control.\nTo better understand this we can correlate the moving penalty with the vacancy rate that we looked at earlier today.\n\nThe scatterplot with all bedroom types gets a bit messy, but things get much clearer if we correlate the variables separately by number of Bedrooms. This shows that the moving penalty anti-correlates with the vacancy rate.\nTo round things off, we can also look at the fixed-sample rent change. Here we expect a high pressure to raise rents in areas with a high moving penalty, which would lead to higher fixed-sample rent increases.\n\nThis graph shows that fixed-sample rents indeed correlate with the moving penalty. At some point it is probably worth looking into these relationships more closely.\nAs always, the code for the analysis is available on GitHub for anyone to reproduce, adapt or appropriate for their own purposes."
  },
  {
    "objectID": "posts/2018-12-17-how-are-condos-used/index.html",
    "href": "posts/2018-12-17-how-are-condos-used/index.html",
    "title": "How are condos used?",
    "section": "",
    "text": "(Cross-posted at HomeFreeSociology)\nCondominium apartments are fascinating! At their heart lies a relatively recent legal innovation enabling individual ownership of units in multi-unit developments. Since their arrival, condominium apartments have become places to build homes, sources of rental income, sites of speculative real estate investment, and experiments in private democratic government. They’re also in the middle of many on-going debates about housing and the future of cities in Canada and around the world. In 2018, we formed a team to study condominium apartments and how they were being used in order to better inform public and academic debates. Team members include data analyst and mathematician Jens von Bergmann, sociologist Nathanael Lauster, and law professor Douglas Harris. We recently presented some preliminary findings at the National Housing Conference in Ottawa and we’re looking forward to continued research collaboration.\nHere we make public some basic information about the development and use of condominium apartments across different metropolitan areas in Canada.\nThe first thing to note is that the legal architecture of condominium is deployed across a broad range of structure types. In addition to apartments, developers commonly use the condominium form to subdivide row houses, and occasionally single-detached houses (as in some gated communities). Nevertheless, condominium is used most commonly to subdivide ownership in low-rise and high-rise apartment buildings, and that’s what we focus on here.\nThe next thing worth noticing is that condominium is much more common in some metro areas than others. Vancouver jumps out for the proportion of its apartments - and housing stock overall - owned within condominium. Calgary and Edmonton also rely heavily on condominium to subdivide apartment buildings, although these sprawling metro areas are dominated by single-detached houses, much more so than Vancouver, reducing the overall prevalence of condominium.\nWe know that condominium apartments are exceptionally flexible forms of housing, but how are they being used across different metro areas? What proportions are owner-occupied? Rented? Occupied temporarily? Unoccupied?\nWe couldn’t extract data to answer the last two questions from the census because condominium status is recorded by respondents. However, using a variety of datasets, we figured out a transparent and replicable (if somewhat complicated) method for estimating temporarily occupied and unoccupied condominium units.\nThe answers to these questions about how condominium apartments are used speak to important elements in popular discourse and public debate. Since provincial governments introduced a statutory form of condominium in the late 1960s, developers have built condominium buildings rather than purpose-built rental apartments across much of Canada. Does this also mean that the proportion of owner-occupiers increases while that of renters decreases in cities where condominium developments proliferate? Or do owner-investors rent out their condominium units, augmenting the existing rental stock?\nOur findings on how condominium apartments are used are really interesting! In all the metro areas we analyzed, the modal use of condominium apartments is owner-occupation. As a result, it appears that condominium apartments are enabling more homeowners to live in increasingly dense cities.\nHowever, condominium apartments also make up a substantial proportion of the rental stock in many metro areas. While many condominium apartments are rented, relatively few show up as vacant (i.e. empty but listed as “for rent”) at any given point in time. Here we distinguish these rare vacancies, which are good for renters, from unoccupied condominiums. In tight markets such as Vancouver and Toronto we see effectively non-existent condominium apartment vacancy rates, comparable to purpose-built rental vacancy rates.\nThe least common use of condominium apartments is as a temporary residence (where owners declare their principal residence as somewhere else in the census, but occupy the unit occasionally).\nFinally we get to the “empty condos,” or those that show up as unoccupied in the census. Overall, we estimate that between 10% to 23% of condominium apartments were unoccupied in 2016, depending upon the metropolitan area. We don’t know why so many condominium apartments appear to be unoccupied, but it likely relates to their newness and to their inherent flexibility as property. Flexibility can show up in the census as “unoccupied” directly, as when owners use condominiums as second homes, and indirectly, as when condominium apartments are left empty in order to facilitate transactions between uses. We suspect that condominium apartments may cycle more frequently than other forms of property between different uses and occupants, thus creating transition periods without occupants and inflating the proportion of unoccupied units. For instance, condominium apartments can more plausibly be re-claimed for owner’s use than purpose-built rental apartments, cycling in an out of rental supply and potentially creating less stable rental housing.\nStrikingly, Vancouver and Toronto stand out as having the lowest proportion of unoccupied condominium apartments, a finding that may be somewhat counter-intuitive given the public attention that vacant units have received, rightly or wrongly, in both cities. When metropolitan areas rely upon condominium apartments as a key form of new housing supply, they should take the flexibility of the form into account. However, it appears that the proportion of unoccupied units in the housing stock will rise as the proportion of condominium apartments in the housing stock increases because condominium apartments are more likely to be unoccupied than purpose-built rentals, a pattern also noted with respect to other flexible housing forms, such as secondary suites (especially basement suites, which show up as units in a “duplex” in the census). This means that even though a smaller proportion of condominium apartments are unoccupied in Vancouver than elsewhere in Canada, a larger proportion of Vancouver’s housing stock shows up in the census as unoccupied.\nIn Canada’s three largest metropolitan areas, a pretty simple rubric applies: for every ten condominium apartments built, six are owner-occupied, three are occupied by renters, and one is unoccupied. In Calgary and Edmonton, add a renter and take away an owner-occupier. The data for the other cities we surveyed is available in the graphic above. As a bonus, we also provide a comparison with estimations from 2011 data to show changes over time in the graphic below.\nIn Vancouver, where condominium apartments have been an established part of the housing market for longer than in the rest of the country, there is very little change in the occupancy pattern between 2011 and 2016. In other big metropolitan areas, it appears that condominium apartments are increasingly used as rental stock. In most cases, the proportion of empty condominium apartments appears to be decreasing, something that may reflect the lingering effects of the 2008-09 property market crash. However, this is all very preliminary. But we’ll keep looking at the details as we proceed!"
  },
  {
    "objectID": "posts/2018-12-17-how-are-condos-used/index.html#methods",
    "href": "posts/2018-12-17-how-are-condos-used/index.html#methods",
    "title": "How are condos used?",
    "section": "Methods",
    "text": "Methods\nWe mixed two data sources to arrive at these estimates–the Census and the CMHC Rental Market Survey–and that made coming up with the estimates a little more complicated. There are several assumptions that go into the estimates, and there are several issues with mixing the data that we set out below.\n\nOverview\nWe cut the condominium stock into five different categories. The numbers of units occupied by owners and renters are straight-up census estimates from 98-400-X2016219 and 99-014-X2011026. To estimate the unoccupied units and the units occupied by temporary residents we used a custom tabulation of Structural type by Document type. We received this cross tabulation from Urban Futures, which one of use has worked with before on secondary suites. Both of those variables–the categorization of the dwelling type as well as the decision to label a unit without a census response as empty or occupied by someone who did not respond–is made by the enumerator. This allows us to ascertain the structural type of unoccupied units, and we can also get that information for units that are temporarily occupied.\nSo, we know how many apartment units were classified as unoccupied or temporarily occupied. To estimate how many condominium units fall into that category we need to make some assumptions. First, we assume that the apartment stock consists of three distinct type of units: condominium units, purpose-built rental units and non-market housing units. That’s not quite accurate. For example a single-family home with two secondary suites will be classified as an Apartment, fewer than five storeys if the census found the suites. These do exist in Vancouver, and elsewhere, but their numbers are small.\nGiven those three types of apartment units, we need to understand how many of the unoccupied and temporarily occupied units fall into each category. The CMCH Rental Market Survey has annual estimates of vacancy rates and universe size for the purpose-built rental stock. We take those estimates, only counting apartment units, to attribute unoccupied units to the purpose-built rental stock. In Vancouver, with its extremely low vacancy rates, this is a fairly small number. In Halifax, that number is comparatively larger. Further, we assume that the non-market units have a vacancy rate of zero, so that there are no empty non-market units. What’s left over we assign as empty condominium apartments.\nFinally, we use the estimate of vacant condominium apartments and those on the rental market from the CMHC Secondary Market Rental Survey, using their estimates of the condominium vacancy rate and the condominium rental universe. The vacancy rate is not available for all years and all CMAs. We have marked the CMAs with an asterisk in case the data was not available and back-filled it with our estimate of the condo rental universe and the Rms vacancy rate. We have seen previously that the Rms vacancy rate tracks the secondary market vacancy rate reasonably well.\nAttributing the temporarily occupied units gets even harder, but the numbers are smaller so getting things a little wrong has less impact. Here we again assume that no temporary residents live in non-market housing, and we assume they are equally likely to live in a condominium apartment (as owner or renter) or rent in purpose-built. That is a bit of a judgement call, but the details of these assumptions don’t make much of a difference to the numbers, and we invite people to grab the code if they would like to adjust the assumptions.\nThere are several issues when mixing CMHC Rms data with census data. For one, both are point-in-time estimates for slightly different times. The census is pegged in early May, the Rms for October. There may be fluctuations in temporary and unoccupied units, in particular in areas dominated by universities such as Waterloo, with the census being outside of the regular semester and the CMHC survey within.\nNext comes the geographic problem, with CMHC switching to new census geographies at the end of the year, so the rental universe still reflects the previous census geography. Montreal is one such example where the CMA changed 2011 to 2016 as we have explained before. That leads to problems when estimating the rental universe, but the effect is moderated when focusing on the empty units.\nAnother issue is that the definition of apartment that CMHC uses differs slightly from the census.\nFinally, for estimating the vacant condominium apartments that were on the rental market we used the CMHC rental condo universe estimate and not the one we derived from the census. There appear to be some differences in how CMHC and the census estimate rented condo units, with CMHC relying on surveys of property managers. In BC that likely involves tallying up units for which Form K was filed, likely leading to CMHC under-estimating strata rentals.\nIt is instructional to compare the two different estimates.\n\nWith the exception of Hamilton, the census condominium rental estimates are higher, in some cases substantially so. To shed more light on this we also compared the estimates of overall condominium apartments.\n\nWe looked at two separate census estimates: the occupied (by permanent residents) units that come straight from the census by filtering occupied units for apartments that are stratified, and the overall condo estimate that we derived by adding in vacant and temporary units. With the exception of Montréal the census estimate of occupied units only comes quite close to the CMHC condominium universe estimate. The differences are worth looking into in more detail at some point.\n\n\nWaffle graphs\nTo communicate the makeup of condominium apartments we settled on a custom version of a waffle graph. Displaying proportions on a square grid makes it easier to read them compared to pie charts or tree graphs. The 10x10 layout rounds numbers to percentage points, which is the appropriate level of accuracy given the uncertainty in the data and is intuitive to understand. When rounding to the nearest percentage, the numbers don’t always add up to 100. So we don’t do traditional rounding but round with the constraint that the total adds up to 100 while minimizing the \\(l_\\infty\\) error.\nThis does introduce potential problems when comparing across time or across geographies, where theoretically we could see an increase in the number of squares in one category although the actual estimated share dropped. This will only happen under very specific circumstances, and we checked that this did not occur in our graphs."
  },
  {
    "objectID": "posts/2018-12-17-how-are-condos-used/index.html#reproducibility",
    "href": "posts/2018-12-17-how-are-condos-used/index.html#reproducibility",
    "title": "How are condos used?",
    "section": "Reproducibility",
    "text": "Reproducibility\nThe code underlying this post is available on GitHub, as are the parts of the custom tabulation for 2016 and 2011 used in this post. Part of the Statistics Canada data we used requires conversion from XML into more manageable data format which, for performance reasons, requires python to be installed next to R that runs the rest of the code."
  },
  {
    "objectID": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html",
    "href": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html",
    "title": "The Fleecing of Canadian Millenials",
    "section": "",
    "text": "A couple of days ago the New York Times published an opinion documenting several aspects of how US Millenials are getting fleeced. Generation Squeeze has been doing a good job of highlighting what has changed for millennials compared to people their age in the past. The NYT article had some interesting data, and two charts in particular drew my interest, the change in median income by age group and the change in median net worth. So I decided to replicate them with Canadian data to see how they compare.\nStraight-up comparisons across countries are always tricky, but since we are mostly looking at changes, issues due to slight differences in methods tend to divide out."
  },
  {
    "objectID": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#income",
    "href": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#income",
    "title": "The Fleecing of Canadian Millenials",
    "section": "Income",
    "text": "Income\nFor income we turn to individual income of Canadians by age groups.\n\nTo compare with US data, we index it at the start of our time series in 1976.\n\nIt is quite striking how dramatically the income of seniors has increased. This pattern is also reflected In US data, but is not as pronounced. In US data, the 25 to 34 year old cohort came out even and all older age groups saw an inflation adjusted increase over the start year, whereas in Canada all age cohorts below the age of 45 came out negative. We should note here that indexing time series is sensitive to the start year, and the US data started a couple of years earlier. But this is unlikely to explain much of the discrepancy."
  },
  {
    "objectID": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#net-wealth",
    "href": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#net-wealth",
    "title": "The Fleecing of Canadian Millenials",
    "section": "Net wealth",
    "text": "Net wealth\nNext up is net wealth. The only measure of net wealth we have in Canada is the Survey of Financial Securities, and we have only four time points. It is not as robust as our income data, so we have to be a little more careful, especially when indexing the data.\nAgain we start out with total net wealth over time. Splitting up net worth of families to individual family members is hard, so we report the net worth by economic family type, where we group by age of the primary household maintainer. Treating economic families and unattached individuals separately avoids bias due to compositional issues.\n\nThis suggests that the Canadian net worth picture is decidedly different from the US numbers where net worth declined for all age groups below 55.\n\nTo understand changes over time we move to indexed data, although that leads to quite large confidence intervals. When looking at economic families we see through the board gains in net value. For unattached individuals the best-guess medians only show a decline for the 45 to 54 year range, although the confidence intervals are quite large so that this age group may well come out positive, or other age groups also may show a negative trend."
  },
  {
    "objectID": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#vancouver-toronto-montreal-and-calgary",
    "href": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#vancouver-toronto-montreal-and-calgary",
    "title": "The Fleecing of Canadian Millenials",
    "section": "Vancouver, Toronto, Montreal and Calgary",
    "text": "Vancouver, Toronto, Montreal and Calgary\nWe can look for regional variations, picking out the larges four CMAs.\n\nIncome\n\nThere are some regional variations, but the big picture is similar, confirming that this is largely a Canadian and not a regional story.\n\nToronto and Vancouver have negative income growth for all age groups except seniors over our timeframe, Calgary and Montreal manage to squeeze in two more age groups with positive income growth.\n\n\nWealth\nCMA level data has smaller sample sizes and larger confidence intervals and some suppressed data. So we focus on economic families that have better data, and drop age groups with missing values.\n\n\nOnly Toronto has small enough confidence intervals to be reasonably certain that net worth grew across age groups, but indexed growth for all CMAs is consistently above zero, with the exception of the 55 to 64 year old age group in Calgary. From this we can’t conclude that there are significant regional differences in the change of net worsth across these CMAs."
  },
  {
    "objectID": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#next-steps",
    "href": "posts/2019-01-31-the-fleecing-of-canadian-millenials/index.html#next-steps",
    "title": "The Fleecing of Canadian Millenials",
    "section": "Next steps",
    "text": "Next steps\nIn summary we see some parallels and some differences compared to the US. It would be interesting to dig deeper into these differences, or to pull out different components of net wealth, and treat housing separately. Maybe even treating primary and secondary residences separately. The code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes."
  },
  {
    "objectID": "posts/2019-02-04-vancouver-election-individual-ballots/index.html",
    "href": "posts/2019-02-04-vancouver-election-individual-ballots/index.html",
    "title": "Vancouver election individual ballots",
    "section": "",
    "text": "The City of Vancouver made individual ballot data available. So the vote choice on each individual ballot. It’s been out for a couple of days now, and I have been quietly hoping someone else would write something up. But I got too curious, so here is a super-fast write-up. We focus on mayor and council votes only. There were 21 candidates for mayor and 71 for council to choose from, and each voter was allowed to vote for (at most) one mayoral candidate and up to ten council candidates on their ballot."
  },
  {
    "objectID": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#number-of-votes-per-ballot",
    "href": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#number-of-votes-per-ballot",
    "title": "Vancouver election individual ballots",
    "section": "Number of votes per ballot",
    "text": "Number of votes per ballot\nThere were 175,399 valid ballots by people eligible to vote for mayor and council. The first question we have is how people were using their votes. Did people make use of all their 11 possible votes for mayor and council candidates, or did the vote for fewer, either because there weren’t 11 they liked on the list (unlikely, given the large choice), or because they did not want to go through the troubles of evaluating all individual candidates and just went with one party only, or because they were ‘plumping’ their votes. Plumping refers to people voting for a few first-choice candidates and denying their second-choice candidates a vote in case they might narrowly overtake their first-choice candidates in the overall counts.\n\nWe see that a slight majority of people 53.8% made use of all their votes, the other ballots had fewer than 11 mayor and council votes. To take a better look into that we are showing the party makeup of all votes by number of total votes per ballot. With our strong showing of independent mayoral candidates we focus only on council votes for this.\n\nWe see that the ballots with exactly 8 council votes are dominated by votes for the NPA – which incidentally ran 8 candidates."
  },
  {
    "objectID": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#party-votes",
    "href": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#party-votes",
    "title": "Vancouver election individual ballots",
    "section": "Party votes",
    "text": "Party votes\nThis brings us to the next section, where we investigate the role parties played in the election. The previous graph leads us to look into single party ballots, that is ballots where only a single party got votes.\n\nThis shows that NPA lead in the number of single-party-only ballots, but with 8 candidates they also have the advantage of single-party-only ballots not “wasting” too many votes. Compared with e.g. One City, where single-party-votes “wastes” 8 votes as there were only two candidates. It’s interesting to see that quite a few ballots only had independent candidates.\nWe can widen the scope slightly to look at combination of parties.\n\nA more comprehensive view that also takes into account the number of votes each party got can be gotten by looking at correlations across all parties.\n\nThis shows that NPA votes anti-correlate with every other party, with Coalition Vancouver and YES Vancouver showing the lowest negative correlation. We also see a emergence of a weak voting block made up of COPE, Greens, One City and Vision. Coalition Vancouver also anti-correlates with all other parties, although having almost neutral correlation with Pro Vancouver and YES Vancouver. Independents show the highest correlation with One City.\nThe ordering of the parties in the plot was done by hierarchical clustering on the correlations, and it interesting how well it picks out adjacencies in the parties. Independents scatter a bit, showing some correlation with One City and (to a weaker extent) Vision, and also some weak correlation with Pro Vancouver and YES Vancouver."
  },
  {
    "objectID": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#candidates",
    "href": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#candidates",
    "title": "Vancouver election individual ballots",
    "section": "Candidates",
    "text": "Candidates\nSo parties matter, but people vote for candidates and not parties. And no party got all their candidates elected, so people did pick and choose among party candidates. Looking at the top 30 council candidates (by total votes) and top 5 mayoral candidates, we can correlate the votes on each ballot just like we did with the parties. Apologies to the candidates that got dropped off, the good news is that it only takes a one-line change in the code to change that in case someone wants to slice the data differently.\n\nAs to be expected, the hierarchical clustering again brings out the parties even though we did not use party affiliation to make this graph. The fact that there was only one vote for mayoral office shows in anti-correlations between all mayoral candidates. The clustering into parties is not perfect, and it is interesting to investigate differences within the party blocks, as well as between candidates that don’t follow party lines. Even though the clustering did not place One City next to the COPE-Greens-Vision block we can still see the strong correlation. Some independent candidates also show correlation with that block, in particular Sarah Blyth and Shauna Sylvester and to a weaker extent Adrian Crook, Taqdir Kaur Bhandal, and Wade Grant, who cluster close to One City, but also correlate with the COPE-Greens-Vision block. NPA anti-correlating with everyone else points to lots of full-slate votes with the few votes there are to spare spreading out. Erin Shum seems also correlates with these particular independent candidates, but also with the Coalition Vancouver block but not the COPE-Greens-Vision or One City block that these other independent candidates clustered around.\nWe can use the data to identify adjacencies of independent candidates with parties.\n\nWe see Wei Qiao Zhang standing out as Vision-aligned, which is to be expected as the ballot showed him as a Vision candidate, although Vision did disavow him shortly before election day. Sarah Blyth seems strongly adjacent to One City, as well as the other Liberal parties."
  },
  {
    "objectID": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#the-new-urbanpreservationist-scale",
    "href": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#the-new-urbanpreservationist-scale",
    "title": "Vancouver election individual ballots",
    "section": "The new urban/preservationist scale",
    "text": "The new urban/preservationist scale\nThe Cambie Report introduced the urbanist/preservationist axis of Vancouver municipal politics, next to the usual Left/Right social-economic axis. It’s an interesting new way to look at things, Nathan has a good rundown. The Cambie Report crowd-sourced the scoring of the parties and major independent mayoral candidates and relased the data. That makes it easy for us to use this for analysis.\nUnfortunately, we don’t have data for the independent candidates, which means that there is a lot of missing data in the analysis in this section Moreover, candidates within each party may also scatter a bit around the average party position, which we won’t be able to pick up on. Maybe someone will collect scores for the independent candidates and even individualize party scores.\nThis means we are filtering the voting data down to only the ones that the Cambie Report has scored, that is party candidates as well as the two major independent mayoral candidates, Shauna Sylvester and Kennedy Stewart. Fortunately this covers all people on council right now, so it is maybe not as bad as it seems.\nWhile we now have scores for each individual candidate, it is not clear how to aggregate up the votes on a ballot to assign scores to each ballot. We will fill this with some heuristics:\n\nVotes for mayoral candidates carry double weight, votes for unscored mayoral candidates are discarded.\nScores for party-affiliated council candidates are added up and scaled up as if there were 10 votes.\n\nIn total this scheme gives the sum of 12 scores if the ballot also had a vote for a scored mayoral candidate or 10 otherwise, so we normalize the result by dividing by 12 or 10. Alternative ways would be to count the absence of votes with a negative weight, but it is not clear how much sense that makes as we can’t distinguish e.g. a candidate that just missed the first 10 that fit on a ballot or if the candidate got left off a ballot because the voter had a strong aversion.\n\nUsing the methods we chose it does not appear that there is a strong urbanist cluster visible in the data. Partially that is due to the urbanist vote spanning across several parties that also strongly correlate with preservationist parties. And looking at recent council votes, it does not seem that councils vote in block on these issues. Taking the Broadway subway extension vote, squarely an “urbanist” issue, the (only) COPE councillor together with one NPA councillors voted against, whereas all three Green councillors voted in favour - a move that was at least partially attributed to Greens recognizing the shifting makup of their base. It will take more time to observe if the crowd-sourced scores still hold after the election, and if having just one score for e.g. NPA candidates is appropriate.\nGiven our methods we still have a fairly high correlation of urbanist/preservationist and Liberal/Conservative scores of ballots (coefficient of 0.68) as is also evident from the graph."
  },
  {
    "objectID": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#next-steps",
    "href": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#next-steps",
    "title": "Vancouver election individual ballots",
    "section": "Next steps",
    "text": "Next steps\nThere are plenty of ways to refine this. One can look into Park Board votes or ballot questions, or analyse data by voting place or voting type (general voting, advanced voting, mail, …). Refining the Cambie Report scores would certainly be interesting. For those interested in expanding on the analysis, or folding in new data, or looking at some of the lower-ranking candidates that got dropped off the list, the code is available on GitHub as usual."
  },
  {
    "objectID": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#update",
    "href": "posts/2019-02-04-vancouver-election-individual-ballots/index.html#update",
    "title": "Vancouver election individual ballots",
    "section": "Update",
    "text": "Update\nChad Skelton did a really nice interactive looking at the votes. He used a different metric to show adjacency, for each pair of candidates A and B he computed what share of candidates that voted for A also voted for B. That’s a much more interpretable metric than my correlations, and may be better at picking up what’s going on with candidates that did not get that many votes. Overall, we don’t expect much different results, but it is worth exploring how these metrics differ.\nLet’s first compute the vote matrix based on likelihood. Just to keep things simple, we cut down by the\n\nThe matrix looks quite similar to the correlation matrix, with the advantage that it’s not symmetric and the asymmetry reveals more information. For example, people that got a high number of votes tend to have greener columns than rows.\nThe order of candidates is slightly different from our correlation plot, that’s due to us using hierarchical clustering on the likelihoods in this case. The result of the clustering is interesting in itself, the hierarchical clustering gives us information about the voters’ opinion on adjacencies of candidates. To conclude the post, we will give more details on the clusterings. And this time around, we will show the data for all candidates.\nFirst up, clustering based on the likelihoods.\n\nThe “height” gives us information about the separation between different groups. NPA did best at distinguishing itself from the other candidates, as is evident by all candidates coming together at low height. As discussed earlier, this is favoured by the NPA running 8 council candidates plus one mayoral candidate, so people voting for the full slate have little opportunity to establish other adjacencies.\nWe can compare this to clustering based on correlations.\n\nThe picture is quite similar, with again NPA coming together out at very low height. The order of the branches, so which side is on the left and which is on the right, is not very important, it’s the branching points that matter. This graph places Sarah Blyth adjacent to Shauna Sylvester and then the Greens, whereas the likelihood based graph places Sarah Blyth next to One City, and then Kennedy Stewarts and COPE.\nIt would be interesting to explore these subtleties further."
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html",
    "href": "posts/2019-02-21-planned-displacement/index.html",
    "title": "Planned displacement",
    "section": "",
    "text": "When we (Denis and Jens) got together for coffee the other day, Denis showed off some maps of renter density in the frequent transit network that he was working on. The idea immediately clicked and we decided to work this out together. Motivated by the issue of renter demoviction caused by the 2017 Metrotown Plan, we set out to quantify how one could plan for displacement on a regional level, instead of treating it as an unwelcome consequence of development at the lot level. Denis has some great context and explanations on his blog, be sure to check it out too!"
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#where-should-new-housing-go",
    "href": "posts/2019-02-21-planned-displacement/index.html#where-should-new-housing-go",
    "title": "Planned displacement",
    "section": "Where should new housing go?",
    "text": "Where should new housing go?\nDisplacement can be traumatic. Local news outlets like The Tyee have shed light on the human impact of displacement experienced by people in Metro Vancouver. But there has been little public discussion about how to incorporate the spectre of displacement into regional planning.\nAs Metro Vancouver is growing, we have a choice where and how to accommodate growth. Being mostly built out at this point, and having policies in place to protect natural, agricultural, and industrial land, growth will in many cases take the form of densification on existing residential land. This may lead to displacement, but the risk of displacement will vary enormously from lot to lot.\nA new resident’s access to jobs, schools, and amenities will also vary enormously from lot to lot. Concentrating development on the distant edge of the region would solve the displacement problem, but would also limit those new residents’ access to the places they need to go, and would force them to be dependent on a car.\nGrowth should be focused in areas with many transportation options - frequent transit, good bike routes, carshare vehicles, and amenities within walking distance.\nBut are there places in the region with abundant transportation that can be developed without displacement?\nLet’s unpack these two notions: abundant transportation and displacement."
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#abundant-transportation-frequent-transit",
    "href": "posts/2019-02-21-planned-displacement/index.html#abundant-transportation-frequent-transit",
    "title": "Planned displacement",
    "section": "Abundant Transportation (Frequent Transit)",
    "text": "Abundant Transportation (Frequent Transit)\nCanadian cities can generally be divided into two zones: areas where a car is absolutely essential for daily life, and areas where there are other options. Those other options (active transportation, transit, carshare) tend to flock together, being highly complementary in nature. A quick look at Modo’s map of vehicle locations confirms the idea that carshare needs transit-, walking-, and cycling-friendly locations to survive. Same thing goes for all the other modes. As options improve in these areas, a positive feedback loop leads to further improvement.\nFor the purpose of this post, we are focusing on places served by frequent transit because it is the most widely-used and most easily-definable of the bunch. We think it can be used as a proxy for places that have an abundance of transportation options. The code is available in GitHub to those who may want to analyse other measures of transportation abundance like Walkscore, carshare networks, or measures of accesibility.\nThere is no universal definition for frequent transit. We are working with an approximation of TransLink’s Frequent Transit Network definition.\nOur definition of Frequent Transit Coverage\nBetween 6am-9pm Weekdays, 7am-9pm Saturdays, 8am-9pm Sundays & Holidays:\n\nTransit riders wait no more than 16 minutes, 80% of the time\nTransit riders wait no more than 20 minutes, 95% of the time\nMaximum wait during this period is 30 minutes\nTransit riders are willing to travel 400m to a bus stop, 600m to a B-Line stop, and 800m to a rapid transit stop\n\nTo draw a coverage area around the transit stops, we assume that riders are willing to travel 400m to a bus stop, 600m to a B-Line stop, and 800m to a rapid transit stop.\nFor this post, we have not yet incorporated the expansions funded by TransLink’s Ten Year Vision like increased frequency, new B-lines and the Millennium Line extension. Anyone interested in including this is invited to grab the code and make the appropriate adjustments.\nAt the same time, we will keep parks and green space, agricultural land, institutional land (like schools), industrial land, and roads off-limits to development. So we focus on residential and mixed-use land use with frequent transit service.\nTo compute the frequent transit coverage we pull the transit data from the Transitland datastore using our transitland R package and compute the required metrics for each stop.\n\nWe are looking for stops that fit our metric for weekdays as well as weekends.\nApplying our metric leaves us with a map of frequent transit coverage.\n\nAgain, the frequent transit network can change over time, and it most certainly will expand and intensify in the near future. For this post we will stick with the current network."
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#exposure-and-vulnerability",
    "href": "posts/2019-02-21-planned-displacement/index.html#exposure-and-vulnerability",
    "title": "Planned displacement",
    "section": "Exposure and vulnerability",
    "text": "Exposure and vulnerability\nWhen planning for displacement, we have two goals in mind:\n\nminimize exposure to displacement, and\nminimize vulnerability of the exposed population.\n\nWe measure exposure by population density. Vulnerability is a more complex metric, for this post we use tenure as a first-level approximation.\n The effect of the displacement depends strongly on tenure. Strictly speaking, owner households only experience voluntary displacement, as they can choose to sell their property or to stay put. The immediate neighbourhood around them may change though, so their familiar surroundings may get displaced.\nTenant households have no choice in this. If the building owner sells their building to make space for denser development, they lose their home and often won’t be able to find a comparable rental accommodation nearby.\nHomeowners may feel negative impacts from neighbourhood change, but it cannot be compared with the much more impactful hardship of eviction.\nWe have excluded First Nations Reserves from our analysis as it was not clear to us how to our vulnerability metrics would apply to those regions and because some of census data was suppressed."
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#methods",
    "href": "posts/2019-02-21-planned-displacement/index.html#methods",
    "title": "Planned displacement",
    "section": "Methods",
    "text": "Methods\nWith our simplistic definitions, measuring exposure and vulnerability is straight-forward. All we need is the frequent transit coverage area, land use data and census data. And a way to wrangle it all together. Fortunately we have worked with all these pieces before, so it just requires our cancensus package to access the census data and the Metro Vancouver land use data that we worked with before.\nTo tie things together we will need to estimate data for our custom geographies using our tongfen package, together with proportional re-aggregation based on dissemination block population, dwelling and household counts as this gives the most reliable results as we explained here."
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#net-and-gross-density",
    "href": "posts/2019-02-21-planned-displacement/index.html#net-and-gross-density",
    "title": "Planned displacement",
    "section": "Net and gross density",
    "text": "Net and gross density\nWe want to explore renter density within the frequent transit coverage area. For that we want to distinguish between two different types of density, net density and gross density. Gross density is based on the total area of a region, net density is just based on the area of the residential lots, so it ignores road right of ways, parks, schools, and other non-residential land uses. Net density is the concepts that we will be using here, as the property lots is what matters in our context."
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#displacement-vulnerability",
    "href": "posts/2019-02-21-planned-displacement/index.html#displacement-vulnerability",
    "title": "Planned displacement",
    "section": "Displacement vulnerability",
    "text": "Displacement vulnerability\nSo where within the frequent transit coverage area do people live? Where do renters live? All that is left to do is to estimate the net renter density for all residential (or mixed use) areas within the frequent transit network.\nOverall our frequent transit network covers 25,327 acres using net residential density, only counting residential or mixed land use.\n\nMost of our frequent transit network is comprised of population densities around 10 to 25 people her acre.\nPlanners usually look at dwelling units per acre as a metric, so let’s look at what our frequent transit network looks like in terms of this. Pricetags did an overview on Jane Jacob’s view on dwellings per acre a while back, Jacobs called 6 or fewer dwellings per acre very low density which “can make out well in the suburbs”, between 10 and 20 dwelling units per acre can “yields a kind of semisuburb”, then the “in between densities” until “the point at which lively diversity and public life can arise” which she pegs at starting at around 100 dwelling units per acre, although she modifies that with “As a general rule, I think 100 dwellings per acre will be found to be too low.” Just for reference, 100 dwelling units per acre is roughly the density found in the Olympic Village.\n\nUsing Jacob’s cutoffs, we see that almost half of our frequent transit network has very low dwelling densities suited for suburbs. The next largest chunk still sits below the 10 to 20 dwelling unit “semisuburb”. The “in between” densities take up most of the rest, with only a tiny fraction of our frequent transit network reaching densities “at which lively diversity and public life can arise”.\nMore important for us when looking at displacement is renter density, as renters are the ones most vulnerable when displaced.\n\nHere we see that many areas have fewer than one renter per acre. The bottom brackets are spaced quite closely as to give more detail on the distribution. When minimizing displacement vulnerability we probably want to avoid re-developing areas with higher number of renters. Looking at the graph, that should not be very hard.\nSo where are these areas with low renter density? Time for a map. Anticipating that people may want to zoom in and view things more closely, we made it interactive.\n\n\nView Fullscreen"
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#planned-displacement",
    "href": "posts/2019-02-21-planned-displacement/index.html#planned-displacement",
    "title": "Planned displacement",
    "section": "Planned Displacement",
    "text": "Planned Displacement\nWe can now use this map to examine the parts of Metrotown affected by demoviction, for example. With 20 to 40 renters per acre, it’s one of the most renter-dense neighbourhoods in the region. Meanwhile, lots across the street hold 1 or 2 renters per acre, and they are not subject to redevelopment plans. One can’t help but think that renters are being directly targeted, despite how much more vulnerable they are to eviction. (In this particular case, local activists composed a “People’s Plan” recommending that denser development be permitted in the area south of Metrotown that has few renters. The People’s Plan was part of the inspiration for this post.)\nThe map shows many other examples of great locations for “development without displacement”. There should no longer be any doubt that we can protect renters and increase housing supply at the same time."
  },
  {
    "objectID": "posts/2019-02-21-planned-displacement/index.html#upshot",
    "href": "posts/2019-02-21-planned-displacement/index.html#upshot",
    "title": "Planned displacement",
    "section": "Upshot",
    "text": "Upshot\nThis is just a start to open the conversation about how discussions around displacement could enter planning our growth. We could keep going forever on this, refine the vulnerability model, allow for changes in the transit network, etcetera. We could combine stops for those counts at transit nodes.\nAs always, the code for the analysis and visualizations is available for anyone to grab and adapt this for their purposes."
  },
  {
    "objectID": "posts/2019-03-02-airbnb-updates/index.html",
    "href": "posts/2019-03-02-airbnb-updates/index.html",
    "title": "Airbnb updates",
    "section": "",
    "text": "About half a year ago I did a post on Airbnb data back when enforcement of the Short Term Rental (STR) regulation came into full effect starting September 2018 and have not really writing things up since then. Probably time for an update post. What has happened since, and what have we learned?"
  },
  {
    "objectID": "posts/2019-03-02-airbnb-updates/index.html#overview",
    "href": "posts/2019-03-02-airbnb-updates/index.html#overview",
    "title": "Airbnb updates",
    "section": "Overview",
    "text": "Overview\nLet’s take a look how listings evolved.\n\nAfter the initial purge of listings before the start of enforcement, not much changed. The initial purge removed a bunch of listings, and flipped listings without a licence number to a minimum stay of 30 days.\nThe number of listings has been slowly creeping up, with a couple of dips along the way.\n\nViewed by minimum stay, the minimum number of nights a listing can be booked through the Airbnb website, we see that the vast majority of the listings with missing licence information are “long term” (30 days or more) listings. These are not covered by the STR rules. Some listings are exempt as they are cross-listings of regular hotels or B&Bs, but we have no way to verify the legitimacy using our data without going through the listings one by one. Some other listings have licence numbers that are malformed, or have the correct format but have no match in the City of Vancouver STR licence database."
  },
  {
    "objectID": "posts/2019-03-02-airbnb-updates/index.html#short-term-rentals",
    "href": "posts/2019-03-02-airbnb-updates/index.html#short-term-rentals",
    "title": "Airbnb updates",
    "section": "Short term rentals",
    "text": "Short term rentals\nFor the rest of the post we focus exclusively on short term rentals, that is listings with minimum stay of fewer than 30 days.\n\nCurrently 66.8% of the STR listings are Entire home/apt with almost all of the the remainder being private rooms. The number of Entire home/apt has been slowly increasing while the number of private rooms has been fairly stable.\n\nTo better understand the flow of new listings coming online and old listings getting discontinued, we colour by time the listings first came online.\n\nThis shows that there has been a steady decline in listings from before the Sept 1, 2018 deadline, with new listings coming online to take their place. It is not clear if or to what extent the new listings coming online are just re-brandings of the same physical home that went offline earlier, or if these sets of homes are disjoint."
  },
  {
    "objectID": "posts/2019-03-02-airbnb-updates/index.html#compliance",
    "href": "posts/2019-03-02-airbnb-updates/index.html#compliance",
    "title": "Airbnb updates",
    "section": "Compliance",
    "text": "Compliance\nDetermining compliance is hard. Part of that is that I am still not entirely clear on the STR rules. It is my understanding that short term rentals are allowed if\n\nthe host has a valid licence for the listing,\nthe host rents out at most one listing,\nthe home is the primary residence of the host, and\nthe listing is either the hosts primary residence, or it is a room (or section) of hosts’ home.\n\nIn particular, hosts are not allowed to rent out multiple listings, and each licence can only be used for one specific booking at a time. Moreover, owners of a property cannot STR a property unless it is their own primary residence, renting it out for half a year long term so it is someone else’s primary residence and then renting it out short term for the rest of the year violates the rules.\n\nValid licences\nAdvertising a short term rental in the City of Vancouver requires a licence. The city has been handing out licences since the rollout about a year ago.\n\nThe graphs shows the initial spike when the city started the licence process, although the bunching at the first date may well be an artifact of the dataset lumping all previous licences together to the date when the dataset first started. The next spike we see coincides with the STR enforcement coming into full effect September 1, 2018. Then we see a spike in December, continuing into January, renewing the 2018 licences for 2019.\nThis process is still underway, only around half of all licences have been renewed for 2019 as of now.\n\nIt’s unclear to me what the cutoff is by when people have to have their 2019 licence in place, whether it’s January 1st, or if there is a grace period. Some don’t seem to be in any rush to get their 2019 licence.\nThe valid format licences are licences that are formatted like proper City of Vancouver STR licenses, but don’t match anything in the city licence database. These could be cases of typos or made-up licences, we were too lazy to run a fuzzy-match to identify possible typos.\nLicence with invalid format are entries in the licence field that don’t come close to match the STR licence format and can be quite creative at times. Licences marked as exempt are hosts self-declaring exempt status. Hotels or registered B&Bs are exempt from the licence requirement, and it requires manual work going through the listings to judge if they likely fall under these categories, which we have not done.\nSome listings have pending, inactive or cancelled licences. I am unsure if listing with pending licences complies with regulations, but using licences marked by the city as inactive or cancelled surely won’t.\nThe cases with missing licences are a mixture of regular hotels or B&Bs cross-listing on Airbnb, as well as people evading the licence field requirement by tagging their listing as being in a different municipality (sometimes using a different language for the city name) while geo-locating in Vancouver as has been reported before.\n\nSpot checks show that listings with field correctly labelled as “Vancouver” seem to be mostly hotels that are cross-posting, except for the surge of these in January. The ones that have different text in the city field seem to be individuals that appear to have found a way around the Airbnb licence requirements, possibly inadvertently so.\n\n\n\nMultiple listings per host or licence\nI am guessing that hosts having multiple listings or there being multiple listings on the same licence is ok as long as hosts only book one listing per host and/or licence at any given time. For example, a host has several rooms in their home that they list as STR, dynamically deciding which one to rent out on demand but never booking more than one simultaneously. This is probably in concordance with legislation, but it is hard to verify that only one of the listings is booked at any given point in time and it opens the doors to abuse.\nThe situation becomes much clearer when there are several entire home/apt units listed by a single host or on a single licence, as a host can’t possibly have more than one primary residence.\n\nLooking at timelines we note how the initial purge dramatically decreased the aggregate number of STR listings in excess of one listing per host. When only looking at listings with a valid licence, the aggregate number of listings in excess of one listing per licence is lower and did not see a drop with the deadline coming in, mostly because people were still scrambling to obtain a licence. Both timelines hint at a slight decline in the recent months, but it takes some imagination to paint that as a success for compliance.\nMost of the hosts or licences that have more than one listing list multiple private rooms, followed by hosts or licences simultaneously advertising two or more entire home/apts. These, together with hosts or licences with two more more entire home/apts as well as one or more private room(s), are probably the most obviously problematic listings, as each host can have at most one primary residence. Taken together, there are 183 hosts with valid licence and multiple entire home/apts listed, totalling 274 listings above the one entire home/apt per host that the rules might allow.\nThis seems to be a persistent problem.\n\nCounts per licence are lower because there are hosts using multiple licences. Licences with more than one entire home/apt strikes me as a clear violation of STR rules, what is less clear to me is if Airbnb “hosts” can act as agents for individual STR landlords and manage the airbnb presence for them. If allowed, this would mean some of the multiple entire home/apt listings per host may be compliant with STR rules.\n\n\nNon-primary residence entire home/apt\nFor an entire home/apt to be used as a STR, it needs to be the hosts primary residence, For example if the host lives at the home most of the times, but is out of town over the weekend, or away on vacation, and uses it as a STR during those times, that’s in perfect compliance with the STR rules.\nIf however the host uses the home full-time for Airbnb, or maybe uses it for half of the year and then rents it out long-term for the other half of the year, this would be in violation to Airbnb rules.\nThis is where things get tricky. We don’t actually know how long a home gets rented out. Airbnb does not share that data, although some jurisdictions have managed to obtain court injunctions to force Aibnb to release bookings data. For now all we can do is make some educated guesses based on the availability indicated in the calendar, as well as the number of reviews left on Airbnb. Calendar availability is quite tricky, we don’t know why some days are blocked and others are open. Are days blocked because the listing is booked or because it serves as primary residence during that time? When there are large open periods in the calendar available for booking we could infer that all of them could be booked out. But some hosts may set a high price and wait for some people to pick a selection of time slots and go on a mini-vacation during those times only. Which could be in compliance with the regulation, depending on how often this occurs.\nReviews are a clearer indication of an actual booking. Not every Airbnb visit will result in a review, but if a new review is posted there was a visit sometime before.\nTo this end, we look at STR listings that are entire home/apt with valid licences that were online on or before Sept. 1, 2018 and also in February 2019, and we count the number of reviews they have gained in this time period.\n\nWe see a lot of listings with very few reviews, which is consistent with use as a principal residence. But there are also listings that accumulated more than 20, more than 40 and even more than 60 reviews in this six month time period. Depending on the average length of stay, and the rate of guests leaving reviews, these numbers are indicative of full-time STR use.\nLooking at the cumulative distribution helps understand how many listings have at least 20 or 30 reviews.\n\nWe see that there are 405 listings with at least 20 reviews, where it becomes questionable whether these listings are really primary residences of the host.\nTo take this even further we mix in data from we started to collect data. Murray Cox, who is running Inside Airbnb has made his scrapes available. He has earlier scrapes, and we can fold them into our analysis,\nTaking the scrape from April 11, 2018, we can compare that to our latest scrape for a roughly 10 months span. We get 32,980 listings that were active on both dates and are listed now as entire apartment short term rentals with valid licence.\n\nAgain, a good portion of listings appear to comply with the STR regulation, but it is really hard to see how listings at the higher end of the review count spectrum can be principal residences."
  },
  {
    "objectID": "posts/2019-03-02-airbnb-updates/index.html#long-term-and-short-term",
    "href": "posts/2019-03-02-airbnb-updates/index.html#long-term-and-short-term",
    "title": "Airbnb updates",
    "section": "Long term and short term",
    "text": "Long term and short term\nAnother possible violation of the requirement that homes be the principle residence of the host is scenarios where the a property is rented out full time, half the time as long term rental (which avoids the empty homes tax for the property), and then renting them out short term for the rest of the time.\nLooking at the second graph from the top of this post, properties with a valid STR licence listed on Airbnb as long term rentals stand out as curious. If people rent their property long term, they don’t need an STR licence. Moreover, properties listed for long-term rentals are unlikely to be primary residences of the host. And if they are not primary residences of the host, they can’t be rented out short term.\nSo let’s take a look how many of these properties were listed for short term rental at some point in time.\n\nA large chunk of these homes were only listed as long term rentals. But there is also some switching between long and short term rentals going on, which possibly skirts STR rules only allowing hosts to rent out their own primary residence."
  },
  {
    "objectID": "posts/2019-03-02-airbnb-updates/index.html#suites-and-laneways",
    "href": "posts/2019-03-02-airbnb-updates/index.html#suites-and-laneways",
    "title": "Airbnb updates",
    "section": "Suites and laneways",
    "text": "Suites and laneways\nSuites and laneway houses are not allowed as STR unless they have a long-term tenant that rents them out with the landlord’s permission. They can be hard to find in the data without manually looking though individual listings, but we can get a (lower bound) estimate by searching for terms like “laneway home” or “garden level suite” in the title and description of listings.\n\nA fairly conservative search aimed to minimize false positives identifies 103 secondary suites and 24 laneway houses that are currently listed as STR. Again, some of these only rent out occasionally and are in compliance if rented out by the long-term tenant of these units with permission from the owner.\n\nLooking at the number of reviews, we notice again that some of these rent out at high volume and are unlikely to be principal residences."
  },
  {
    "objectID": "posts/2019-03-02-airbnb-updates/index.html#upshot",
    "href": "posts/2019-03-02-airbnb-updates/index.html#upshot",
    "title": "Airbnb updates",
    "section": "Upshot",
    "text": "Upshot\nNot much has changed in the overview statistics since the full STR rule enforcement in September. The data should strong indications that a portion of hosts aren’t in compliance with the regulations. To go beyond some of the obvious cases or use this kind of overview data to inform audits, one would have to look deeper at individual listings.\nGiven the strong indication of ongoing abuse in overview data, as well as a flood of examples of individual cases that people browsing through listings have documented, it might be time for the city take another look at how to strengthen some of their existing regulations to increase enforceability. Regulations like this are are rarely one-shot wonders, but require some adjusting and tweaking to make more effective.\nAnd it would be useful to get clarity on how the STR rules apply. I have not micro-parsed the regulation, but read through the city overview pages and am still not clear on how some of the details work out, as is evident in this post. Creating more clarity might already help to weed out some of the offending cases. People generally don’t want to break the law. Some offenders may be unaware of some of the details and acting in good faith, others may be happy to interpret any confusion around the regulations to their advantage. Clarity of how the rules apply may help these groups to come into compliance. There are likely also people acting in bad faith and knowingly breaking regulations, only more effective enforcement can help in those cases. s"
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html",
    "href": "posts/2019-03-27-density-timelines/index.html",
    "title": "Density timelines",
    "section": "",
    "text": "In the last post we compared international city density patterns. While travelling and reading Alain Bertaud’s excellent book Order without Design I decided to slightly expand on the initial images and add bar graphs showing radial density to get an aggregate understanding of density patterns, as well as adding timelines to show how densities have developed over time. I am getting increasingly interested in modelling urban economics, and understanding and quantifying urban densities is a part of that.\nThis aims to reproduce some of the results of Chapter 4 in Bertaud’s book. There are differences in data processing, I am making use of the 250m GHS global population model for 1975, 1990, 2000 and 2015. To compute the population density in 1km wide annuli around city centres I decided to only count areas with densities higher than 1 person per hectare, thus eliminating large parks, bodies of water or otherwise uninhabited (or sparsely inhabitated) areas. This will lead to different results, especially in coastal cities or cities otherwise constrained. One may wish to modify this, as usual the code for this post is available on GitHub in case someone wants to modify the assumptions.\nThis time around we need to be a little more careful in correctly determining the **centre* of cities in order for the inner density annuli to make sense, so we will hand-pick the city centre locations instead of just grabbing them from the maps package."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#density-annuli",
    "href": "posts/2019-03-27-density-timelines/index.html#density-annuli",
    "title": "Density timelines",
    "section": "Density Annuli",
    "text": "Density Annuli\nTo quantify city densities we focus on 1km concentric annuli emanating from the city centres. Bertaud explains that the basic urban model suggests that the densities decrease exponentially from the city centre. While a crude simplification, this works surprisingly well across metropolitan areas. Distance serves as an easily available stand-in for travel time to access jobs and amenities. We also added concentric rings for every 5km to the overview plots to make it easier to related them to the bar graphs for the radial density.\nLet’s take a look at what this gives for various cities.\n\nVancouver\nst_sf(name=\"Vancouver\",st_sfc(st_point(c(-123.1244,49.2819)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=30)\n\nWe fit an exponential density model to the data, and are in particular interested in the gradient given by the exponent coefficient. Bertaud noted that the gradient typically flattens over time, but in Vancouver we see the opposite. In 1975 our model estimates a coefficient of -0.14, which gradually increases (in magnitude) to -0.19 in 2015. We also note how the actual density distribution is not very well approximated by our density model. Vancouver starts out with fairly high densities in the city centre that drop off very fast to suburban densities just a couple of kilometres out.\n\n\nParis\nst_sf(name=\"Paris\",st_sfc(st_point(c(2.3575,48.8554)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nHere we notice a very good fit to our density model, with slightly flattening density gradient over time."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#new-york",
    "href": "posts/2019-03-27-density-timelines/index.html#new-york",
    "title": "Density timelines",
    "section": "New York",
    "text": "New York\nst_sf(name=\"New York\",st_sfc(st_point(c(-73.9956,40.7295)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nNew York City, together with the surrounding New Jersey cities, shows a density pattern similar to Vancouver, just at a higher density level. Densities are very high around downtown Manhattan, dropping of fast to uniformly high densities between 100 and 150 people per hectare until about 12km out where densities drop off."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#shanghai",
    "href": "posts/2019-03-27-density-timelines/index.html#shanghai",
    "title": "Density timelines",
    "section": "Shanghai",
    "text": "Shanghai\nst_sf(name=\"Shanghai\",st_sfc(st_point(c(121.4857,31.2456)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nShanghai has undergone spectacular growth, fitting our urban density model fairly well throughout. We note how the density initially decreased on the far outskirts of the city, which is likely a function of how we cut off areas with density below 1 person per hectare. In the density plot we notice a couple of separate cities in the 35km to 40km band in 1973, which subsequently grow and accumulate lower density bands around them, dragging down the overall density."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#moscow",
    "href": "posts/2019-03-27-density-timelines/index.html#moscow",
    "title": "Density timelines",
    "section": "Moscow",
    "text": "Moscow\nst_sf(name=\"Moscow\",st_sfc(st_point(c(37.6226,55.7526)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nIn Moscow we can observe a city that evolved under a planning economy for a good portion of time, which can lead to odd density patterns where the centre is surrounded by a higher density band."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#brasilia",
    "href": "posts/2019-03-27-density-timelines/index.html#brasilia",
    "title": "Density timelines",
    "section": "Brasilia",
    "text": "Brasilia\nst_sf(name=\"Brasilia\",st_sfc(st_point(c(-47.9014,-15.7881)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nBrasilia, city that got designed by planners from the beginning rather than growing by market forces, shows peculiar density patterns."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#beijing",
    "href": "posts/2019-03-27-density-timelines/index.html#beijing",
    "title": "Density timelines",
    "section": "Beijing",
    "text": "Beijing\nst_sf(name=\"Beijing\",st_sfc(st_point(c(116.3912,39.9176)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nBeijing has similar growth like Shanghai, but the population in the central part is lower, possibly due to government, cultural and commercial functions crowding out residential space. We centred the map on the imperial palace."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#los-angeles",
    "href": "posts/2019-03-27-density-timelines/index.html#los-angeles",
    "title": "Density timelines",
    "section": "Los Angeles",
    "text": "Los Angeles\nst_sf(name=\"Los Angeles\",st_sfc(st_point(c(-118.2554,34.0441)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nLos Angeles is an example of a city with a consistently low density gradient. We notice some increase in density in the centre and some small changes in density patterns in some more outlying areas."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#barcelona",
    "href": "posts/2019-03-27-density-timelines/index.html#barcelona",
    "title": "Density timelines",
    "section": "Barcelona",
    "text": "Barcelona\nst_sf(name=\"Barcelona\",st_sfc(st_point(c(2.1767,41.3856)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=20)\n\nBarcelona stands out as having fairly high density throughout, that drops off dramatically once one leaves the central region.\n\nBangkok\nst_sf(name=\"Bangkok\",st_sfc(st_point(c(100.4979,13.7517)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nBangkok is another city that experienced strong growth, all the while maintaining a fairly constant density gradient.\n\n\nLondon\nst_sf(name=\"London\",st_sfc(st_point(c(-0.1132,51.5056)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=40)\n\nThe commercial and cultural uses in the inner city keep population density low in the central parts, with population density peaking around the 5km radius around the centre and declining from there on."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#tokyo",
    "href": "posts/2019-03-27-density-timelines/index.html#tokyo",
    "title": "Density timelines",
    "section": "Tokyo",
    "text": "Tokyo\nst_sf(name=\"Tokyo\",st_sfc(st_point(c(139.7536,35.6851)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=50)\n\nTokyo is a good example of the population density inversion we often see in city centres. We placed the centre into the imperial palace, the low density dip clearly visible in the maps. But that’s not what’s driving the low density at the centre, it’s the concentration of office buildings crowding out (nighttime) residential density. This also highlights some of the problems when focusing exclusively on (nighttime) population density and neglecting commercial density."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#taipei",
    "href": "posts/2019-03-27-density-timelines/index.html#taipei",
    "title": "Density timelines",
    "section": "Taipei",
    "text": "Taipei\nst_sf(name=\"Taipei\",st_sfc(st_point(c(121.5437,25.0416)),crs=4326)) %&gt;% \n  plot_density_facet(.,radius_km=30)\n\nIn large cities in can be hard to select the centre of the city. In Taipei we went with the Zhongxiao Fuxing station as the centre, which is probably more a personal preference than anything scientific. We can clearly notice the density bump around 25km out, where nearby cities of Taoyuan and Keelong kick in."
  },
  {
    "objectID": "posts/2019-03-27-density-timelines/index.html#upshot",
    "href": "posts/2019-03-27-density-timelines/index.html#upshot",
    "title": "Density timelines",
    "section": "Upshot",
    "text": "Upshot\nWe could go on forever, there are many other interesting cities to look at. The code for this post is available in case people are interested in other cities. Fair warning, the scripts will download a couple of gigabytes of data the first time the scripts are run, and the water layer in the maps expect a tile source with API keys to be set for the rmapzen package.\nIt would be nice to add a function that locates the centre for any given city simply by analyzing the population density. But that did not fit into our blog post time budget that was confined to our flight home. Unfortunately that also means there was no time to clean up the code or speed it up a bit.\nWe could also refine out model to account for cases where the inner city has lower (nighttime) population density, surrounded by a higher density ring. Similarly, we could add provisions where density increases again further out as we reach neighbouring cities as we have seen for example in case of Taipei. As a related project, one could try and delineate regions that function as separate metropolitan areas just by population density patterns."
  },
  {
    "objectID": "posts/2019-04-20-a-bedroom-is-a-bedroom/index.html",
    "href": "posts/2019-04-20-a-bedroom-is-a-bedroom/index.html",
    "title": "A bedroom is a bedroom",
    "section": "",
    "text": "A quick note following up on a discussion earlier today, where the question came up on how to compare single family with condo (or rental apartment) density. This point comes up a lot and becomes increasingly important as Vancouver densifies.\nIn Vancouver, some single family homes are heavily suited. Legally a single family lot can have the main unit, a secondary suite and a laneway house. Roughly a half of single family homes have a suite, and a couple percent have laneway houses. Some homes have more than one suite. There is also a strong geographic bias for suites, for existing suites as well as in permit data for new homes. For new SFH, a little over half of them come with suites. Of course having a suite does not mean the suite is rented out. In fact, suites are the most empty type of dwelling units in Vancouver, which should not be too surprising given their flexible character.\nWhen we replace single family homes with a condo or rental apartment, what’s an easy way to compare density before and after? An easy way to do this it to simply count bedrooms."
  },
  {
    "objectID": "posts/2019-04-20-a-bedroom-is-a-bedroom/index.html#a-bedroom-is-a-bedroom-is-a-bedroom.",
    "href": "posts/2019-04-20-a-bedroom-is-a-bedroom/index.html#a-bedroom-is-a-bedroom-is-a-bedroom.",
    "title": "A bedroom is a bedroom",
    "section": "A bedroom is a bedroom is a bedroom.",
    "text": "A bedroom is a bedroom is a bedroom.\nThe key takeaway is that average household size largely depends on the number of bedrooms, and to a lesser extent on tenure, but not so much on the structural type of a building.\n\nSo a 3 bedroom single detached home houses on average the same number of people as a 3 bedroom condo in a high rise building, a little fewer in owner-occupied units and a little more in renter-occupied ones.\nRemember that suited single family homes are classified as “Duplex” in the census. The 1-bedroom single detached houses, and some of the 2-bedroom houses, are likely laneway houses. The Row house category includes many townhouses.\nHolding structural type and number of bedrooms constant, household sizes of renter households tends to be noticeably larger than for owner households, which is in line with the notion that owners tend to be wealthier and wealthier households tend to consume more housing, so they tend to live in larger units with bedrooms to spare."
  },
  {
    "objectID": "posts/2019-04-20-a-bedroom-is-a-bedroom/index.html#upshot",
    "href": "posts/2019-04-20-a-bedroom-is-a-bedroom/index.html#upshot",
    "title": "A bedroom is a bedroom",
    "section": "Upshot",
    "text": "Upshot\nWhat this tells us is that if we want to compare density, or household sizes, of single family homes to other forms of housing, it’s generally enough to simply compare bedrooms.\nThis of course also ties into the discussions about empty bedrooms that we have looked into several times before.\nIf you want to play more with the numbers or check this for other cities, feel free to grab the code and adapt it for your own purposes."
  },
  {
    "objectID": "posts/2019-06-03-2001-census-data-and-tongfen/index.html",
    "href": "posts/2019-06-03-2001-census-data-and-tongfen/index.html",
    "title": "2001 Census Data (and TongFen)",
    "section": "",
    "text": "CensusMapper now has 2001 census data, the changes are live and functional and available for mapping and via the data API. We ran some basic verification of the import, and set the metadata for the variables. There may still be some quirks in need of getting ironed out, feedback is appreciated if anyone finds anything that does not look right.\nAt the same time we finally updated my TongFen package to also include DA level TongFen out of the box, in addition to the CT level that has been working for a while. TongFen is (what we call) the process of making data that originally comes on different geographies comparable, and the TongFen package does that in two ways. It allows for estimates on arbitrary geometries, and it allows re-aggregation of variables from different (Canadian) censuses on a common DA or CT tiling. The latter method does not just result in estimates, but exact counts (up to statistical rounding) at the expense of finding a coarser least common geographic denominator.\nThis last part is how TongFen got it’s name. Adding fractions involves two steps 1) converting the fractions to the least common denominator and 2) adding the numerators. The first step is hard, even saying it is hard. But in Chinese there is a simple word for that first step: TongFen (通分). Just having a simple word for this process can make it conceptually simpler.\nWhen working with geospatial data on different (but somehow comparable) geometries we face essentially the same challenge: finding the “least common geography”. The TongFen package is makes that step simple by completely automating it. From finding the least common geography to aggregating up the variables, paying attention to if the census variable is e.g. a simple count or a percentage or maybe the average income and aggregating the values appropriately and autonomously.\nSo let’s see how this works by comparing 2001 and 2006 dwelling data. With the purpose of the post being to showcase the new data and methods as much as to shed some more light onto how census methods effect how the census sees dwellings, we keep most of the code visible on the main page for this post."
  },
  {
    "objectID": "posts/2019-06-03-2001-census-data-and-tongfen/index.html#dwelling-type-data",
    "href": "posts/2019-06-03-2001-census-data-and-tongfen/index.html#dwelling-type-data",
    "title": "2001 Census Data (and TongFen)",
    "section": "Dwelling type data",
    "text": "Dwelling type data\nLet’s see how this works by exploring some of the changes in census methodology between the 2001 and 2006 census that has tripped up people in the past. The census changed the way it counts dwelling units, in particular it started hunting around for secondary suites.\nUsing the cancensus and TongFen that’s easy. We first search for the variables on “structural type of dwelling” and grab all the child variables. Then we grab the data for the City of Vancouver on a common 2001 and 2006 geography using the TongFen package.\nvectors_01 &lt;- search_census_vectors(\"structural type\",\"CA01\") %&gt;% rbind(child_census_vectors(.))\nvectors_06 &lt;- search_census_vectors(\"structural type\",\"CA06\") %&gt;% rbind(child_census_vectors(.))\nall_vectors &lt;- c(vectors_01$vector,vectors_06$vector)\nregions &lt;- search_census_regions(\"^Vancouver$\",\"CA01\") %&gt;% \n  filter(level==\"CSD\") %&gt;% as_census_region_list()\n\ndata_da &lt;- get_tongfen_census_da(regions=regions,vectors = all_vectors,geo_format = 'sf')\nWith the data at hand we compute the change in single detached and duplex dwellings, and discretise them for mapping purposes.\nchange_breaks &lt;- c(-Inf,-100,-50,-25,-10,10,25,50,100,Inf)\nchange_labels &lt;- c(\"Lost over 100\",\"Lost 50 to 100\",\"Lost 25 to 50\",\"Lost 10 to 25\",\"About the same\",\n                   \"Gained 10 to 25\",\"Gained 25 to 50\",\"Gained 50 to 100\",\"Gained over 100\")\n\nplot_data &lt;- data_da %&gt;% \n  mutate(duplex_change=v_CA06_123-v_CA01_116,\n         sd_change=v_CA06_120-v_CA01_113,\n         dwelling_change=Dwellings_CA06-Dwellings_CA01,\n         household_change=Households_CA06-Households_CA01,\n         unoccupied_change=dwelling_change-household_change,\n         unoccupied_01=Dwellings_CA01-Households_CA01,\n         unoccupied_06=Dwellings_CA06-Households_CA06) %&gt;%\n  mutate(sd_change_d=cut(sd_change,breaks=change_breaks,labels=change_labels),\n         duplex_change_d=cut(duplex_change,breaks=change_breaks,labels=change_labels),\n         unoccupied_change_d=cut(unoccupied_change,breaks=change_breaks,labels=change_labels))\nOverall StatCan reports that Vancouver lost quite a few single detached dwellings between these censuses, 17,045 in total (net) to be precise. Let’s map them to understand the geographic distribution of the lost single detached dwellings.\nggplot(plot_data) + \n  geom_sf(aes(fill=sd_change_d),size=0.1) +\n  change_map_theme +\n  labs(title=\"Change in occupied Single Detached 2001 to 2006\")\n\nThe bulk seems to be in Vancouver “single family” neighbourhoods, so what happened to them? The answer to that is simple, in 2006 the census started hunting for secondary suites. And a single detached house with a secondary suite is not “single” detached any more, but classified as a “duplex” in census language. Mapping the change in duplex units reveals that there is a reasonably good match.\nggplot(plot_data,aes(fill=duplex_change_d)) + \n  geom_sf(size=0.1) + \n  change_map_theme +\n  labs(title=\"Change in occupied 'Duplex' 2001 to 2006\")\n\nExcept it’s never quite that simple. For every single detached home where the census found a secondary suite, the census now counts two units instead of one. Eyeballing the map it does indeed seem like there may be more duplex units gained than single detached lost, but are there really twice as many?\nThat question comes down to two details: 1) The modifier in the map title that we have not yet talked about: We are only looking at occupied units, not all units. The best way to resolve this is to get a custom tabulation to look at structural type of dwellings for all dwelling units, not just the occupied ones. We have done that using 2011 data and found that secondary suites are the most likely type of dwelling to register as not occupied (by usual residents). 2) “Apartments, fewer than five storeys”. This is the census classification for anything with fewer than five storeys and more than two units. Including for a “single family” home with two secondary suites. If the census found homes with single secondary suites (“duplex”) it will also have found homes with two secondary suites.\nBut that’s for another time to untangle. To close this off let’s look at the overall change in units not occupied by usual residents.\nggplot(plot_data,aes(fill=unoccupied_change_d)) + \n  geom_sf(size=0.1) + \n  scale_fill_brewer(palette = \"PRGn\",direction = -1) +\n  map_theme +\n  labs(title=\"Change in unoccupied (by usual residents) 2001 to 2006\")\n\nWe do see a general increase in those units in the areas that saw single detached get reclassified into duplexes, but what stands out is the activity near new developments. Both areas where buildings were completed just before the 2001 census and that filled in by the 2006 census, as well as near buildings that completed close to the 2006 census date and have not filled in yet.\nUnfortunately we don’t have this data at the census block level, which makes it easier to identify single buildings that drive the change in unoccupancy numbers."
  },
  {
    "objectID": "posts/2019-06-03-2001-census-data-and-tongfen/index.html#summary",
    "href": "posts/2019-06-03-2001-census-data-and-tongfen/index.html#summary",
    "title": "2001 Census Data (and TongFen)",
    "section": "Summary",
    "text": "Summary\nWith 2001 data now available we get longer timelines to understand how our cities change. And the added ability for DA level comparisons in the TongFen package make it super-easy to analyse change on fine geographies. A general caveat here is that at DA level statistical rounding plays a role, as well as other issues with census data including geocoding problems.\nAs usual, the code for the post is available on GitHub for those interested in replicating or adapt this for their own purposes."
  },
  {
    "objectID": "posts/2019-06-09-vancouver-population-density-over-time/index.html",
    "href": "posts/2019-06-09-vancouver-population-density-over-time/index.html",
    "title": "Vancouver population density over time",
    "section": "",
    "text": "Canadian census data is freely available, alas not in a very convenient format for older data. Census data back to 1991 are available from Statistics Canada with an open data licence, digital geographic data is only available back to 2001. Older census data is available in digital format via paid subscription services from private entities with restrictive licences. But all data is available for free as open data in paper format.\nStuart Smith took it upon himself to go to the library and digitize (i.e. type into a spreadsheet) old census data all the way back to 1941. And he used the reference maps to trace back the areas to tongfen to modern census boundaries. Which it totally awesome. And Stuart has been making neighbourhood timelines using that data to great effect.\nWe decided to team up and take another look at the data to understand population growth in the City of Vancouver, as well as Musqueam 2 and the UBC/UEL/UNA area.\nWhen computing density there are always some choices to be made. Ideally we like to work with net density, that is only counting area taken up by residential lots. That works well for point-in-time density calculations like we did recently, but raises new questions when looking at time series because the designation of lots as residential changes over time. So we would either have to use changing geographies over time, or use some kind of union geography to aggregate residential land use over time. For this we decided to go a simpler route and just take out parks, but leave industrial and commercial land uses included. None of the effects the population growth in an area, but it does effect the population density.\nWith these caveats in mind, let’s take a look at an animated population density in Vancouver between 1941 and 2016.\nThis gives an overview of where Vancouver has added population – and where it hasn’t. We want to take a more systematic look at how this played out in the different areas."
  },
  {
    "objectID": "posts/2019-06-09-vancouver-population-density-over-time/index.html#classifying-growth-and-density-patterns",
    "href": "posts/2019-06-09-vancouver-population-density-over-time/index.html#classifying-growth-and-density-patterns",
    "title": "Vancouver population density over time",
    "section": "Classifying growth and density patterns",
    "text": "Classifying growth and density patterns\nLooking at the changes in population density, we wanted to dive a little deeper and cluster neighbourhoods by how they grew. Trying not to pre-impose our ideas we settled on simple unsupervised k-means clustering. We did play a little bit with the number of clusters, 6 seemed like a good number to get some nuance but still keep things simple. As variables we used growth between the first and last years in our series, and current density. We applied a log scale and normalized both before clustering.\n\nThe scatter plot shows that there us a large variation in the data, ranging from population decline for Area 6 that is part of our low-growth cluster, to tremendous growth for our high-growth cluster comprised of Areas 37 and 38 where population grew by a factor of 20.\nGenerally the clustering makes intuitive sense, although there are some edge cases. The labels correspond to the 1941 census tract boundaries, we kept them as labels for the neighbourhoods. There is a reference map further down.\nWe could have also used a more complex model to look at the whole time series for the clustering, but results were not much different and the simple clustering seemed to make sense when looking at the time series.\n\nWe can take these time series and condense them into our six growth and density archetypes.\n\n\n\nReferring back to the time series for the neighbourhoods, we can examine some of the ones that did not quite fit their archetype model. Neighbourhood 16, containing Fairview and South False Creek, hit a low in 1976 before growing quite strong. So overall the growth was only low when using 1941 as the starting point, but it would qualify as medium growth when starting from 1976.\nSimilarly, area 5 saw a decline until 1986, but Yaletown lead to strong population growth.\nArea 2 in the West End took off between the 1956 and 1961 censuses and reached a peak in 1971 that was only eclipsed in the last census, something we also see to some extent in Area 3.\nArea 30 is highest density area of the purple “medium density from medium growth” category, with Joyce-Collingwood marking it’s imprint even though it is located within a very confined part of that area.\nThe “low density and very low growth” areas stand out as being geographically disconnected, but thus is the fate of these parts of Dunbar, West Point Grey and Strathcona."
  },
  {
    "objectID": "posts/2019-06-09-vancouver-population-density-over-time/index.html#more-recent-history",
    "href": "posts/2019-06-09-vancouver-population-density-over-time/index.html#more-recent-history",
    "title": "Vancouver population density over time",
    "section": "More recent history",
    "text": "More recent history\nMemory is short, and not many Vancouverites have 1941 as a comparison in mind. While is great to take the long view on this topic, we also wanted to complement by just looking at the more recent history and using 1976 as a start year, roughly the half-way mark in our time series.\n\nHere the picture is quite different. Area 37 dropped back into a lower growth cluster, while Areas 5 and 16 joined 38 in the high-growth cluster.\nWe can take these time series and condense them into our six growth and density archetypes.\n\nUsing a more recent time frame paints a somewhat different pictures, albeit one that most are more familiar with."
  },
  {
    "objectID": "posts/2019-06-09-vancouver-population-density-over-time/index.html#finer-geographies",
    "href": "posts/2019-06-09-vancouver-population-density-over-time/index.html#finer-geographies",
    "title": "Vancouver population density over time",
    "section": "Finer geographies",
    "text": "Finer geographies\nThere is a tradeoff between the length of the timeline and how coarse our geographies are. 1976 is somewhat of an inflection point in terms of geographies, so it’s worthwhile to use this as a starting point for taking anther look using finer geographies.\nThis gives us more sub-regions to classify, but we can just run it though our established machine.\n\n\nThe slightly refined geographies add a little bit more detail, for example the mid-density centre of Kerrisdale pops out. Generally it paints a similar picture to the more coarser geography used above."
  },
  {
    "objectID": "posts/2019-06-09-vancouver-population-density-over-time/index.html#implications",
    "href": "posts/2019-06-09-vancouver-population-density-over-time/index.html#implications",
    "title": "Vancouver population density over time",
    "section": "Implications",
    "text": "Implications\nThis highlights how unevenly growth has been distributed in Vancouver. This pattern is not caused by people’s preferences, it is almost entirely shaped by policy that dictate which areas should accommodate growth, and which ones should not. One modest exception is the proliferation of informal dwellings in form of secondary suites that has lead so some growth and was only fairly recently legalized.\nWe don’t know how growth would have evolved without the imposing rigid zoning that does not just regulate nuisances by e.g. separating residential from industrial areas, but imposes very low density on the majority of city land designated for residential use. But we do understand how cities generally function. Cities exist because the proximity of people, jobs and amenities create synergies. And this leads to residential density genreally decaying exponentially with distance from the central business district and amenities. In Vancouver we notice the imbalance between the densities at the centre to densities only 4km out.\n\n\n\nVancouver densities\n\n\nThe strong differential also builds up very visibly in the 3D animated map near the top, and is indicative of a massive planning failure that forces artificially low density in large swaths of the city while squeezing growth into small central areas as well as areas far outside of the City of Vancouver in other parts of Metro Vancouver. The consequence is a loss of social welfare, forcing people into longer commutes and into lower-amenity areas."
  },
  {
    "objectID": "posts/2019-06-09-vancouver-population-density-over-time/index.html#the-data",
    "href": "posts/2019-06-09-vancouver-population-density-over-time/index.html#the-data",
    "title": "Vancouver population density over time",
    "section": "The data",
    "text": "The data\nStuart made the transcribed census data available, you can download it here. Or grab the code to the post and play with the data that way."
  },
  {
    "objectID": "posts/2019-06-15-census-custom-timelines/index.html",
    "href": "posts/2019-06-15-census-custom-timelines/index.html",
    "title": "Census custom timelines",
    "section": "",
    "text": "After our recent posts on multi-census comparisons I was pointed to a semi-custom tabulation for census timelines back to 1971 for Vancouver and Toronto. That’s data for the 1971, 1981, 1986, 1991, 1996, 2001, 2006 and 2011 censuses on a common 2016 DA geography for the two CMAs. This is really cool, not just that it eliminates the need to tongfen the geographies, but in particular because Statistics Canada does not even haven publicly available geographic boundary files for censuses before 2001.\nIt also ties in nicely with recent work we did with Stuart Smith that took hand-transcribed census data all the way back to 1941 and looked at population change on a common geography. This data does not go back quite as far, but it comes on a much finer geography."
  },
  {
    "objectID": "posts/2019-06-15-census-custom-timelines/index.html#the-data",
    "href": "posts/2019-06-15-census-custom-timelines/index.html#the-data",
    "title": "Census custom timelines",
    "section": "The data",
    "text": "The data\nThe data comes in the much hated (by us at least) Beyond 20/20 format, which requires manual work to get it into a form where we can run our scripts. It comes in separate files for Toronto and Vancouver, and separate files for each year, and some years are yet again broken down into separate files (in two different ways). Which makes it quite annoying to assemble the data.\nIf that was not enough trouble, the spelling of the variable names is not consistent across the different extracts, so it requires manual adjustments. Moreover, there are some multi-row variable names that need special attention during the import. And while this is thoroughly annoying, this is unfortunately expected and quite the norm for custom tabulations from Statistics Canada.\nThe data only comes at DA and CMA level, so we had to aggregate up the data for the intermediate census geographies of CTs and CSDs to fit into the CensusMapper geographic hierarchy and allow for rapid analysis at different geographic levels. There are several problems with this though. Some variables, like medians, can’t be aggregated up. And other variables, like averages or percentages, need metadata in order to aggregate them up properly. CensusMapper added this metadata for the 2001 through 2016 censuses, but we have not added it for this custom tabulation. It’s a lot of manual work and we could not justify dedicating the required resources to this. Lastly, some data is suppressed at the DA level, and it will be missing from higher level aggregate counts too.\nUnfortunately the data is constrained to Vancouver and Toronto CMA, it would be amazing to have this data available nation wide. I see an opportunity for Statistics Canada to provide a consistent semi-custom tabulations for all of Canada, including for higher aggregation levels.\nTo make it easier for us to work with the data we have imported it into CensusMapper. As we haven’t yet added the metadata that would enable us to make it available for the general public to map, but that’s something that we are hoping to find the resources to do in the future.\nOne usual caveat with the data is that geocoding is hard, and we should expect some issues where StatCan incorrectly, and inconsistently, geocoded dwelling across the censuses. Geocoding errors will show up as one region suddenly losing population, while an adjacent region is gaining. This won’t happen often, but there are a lot of DAs in Metro Vancouver and it is bound to happen at times."
  },
  {
    "objectID": "posts/2019-06-15-census-custom-timelines/index.html#population-change",
    "href": "posts/2019-06-15-census-custom-timelines/index.html#population-change",
    "title": "Census custom timelines",
    "section": "Population change",
    "text": "Population change\nFor today we will just look at the most basic variable: Population. And map how it changed over time. People can view this data on CensusMapper. But flat 2D maps have difficulty to show the extent to which the population change across regions differs.\nRelative population change, that is percent increase in population, is also a challenging concept when dealing with regions that did not have any population in 1971. A better way to slice the data is to look at change in population density. Which introduces a problem when comparing one geography to another. Some regions contain large parks, others just housing (and some road space). The park space will weight down increases in population density compared to the area without parks.\nIt still gives a decent overview over how a region changed. Here is a map of population change around the City of Toronto 1971 to 2016.\n\nIt’s quite striking how unequal the growth has been distributed throughout the region. We aggregate the areas in each of our growth bins for just the City of Toronto to quantify this.\n\n\nLand use\nThe solution to the problem of some areas containing large non-residential land uses is of course to cut the regions down to residential lots, so switch from gross density to net density like we have done in our project with Denis Agar looking at population, dwelling and renter density in the frequent transit network. For Vancouver we can use the Metro Vancouver Land Use Data. For this map, we are keeping the industrial and commercial land use areas, as well as the areas marked as undeveloped or unclassified, as the land use data is a bit dated now and does not account for some of the recent residential growth in some of these areas.\nThis data is best explored in an interactive map, which we won’t embed here because it loads a large dataset and should probably not be explored on mobile. It’s totally worth it to take the time and play with this on a desktop though.\n\nExplore interactive population change map\nHere we allow to view the population density for each year, and we have the option to view the change in population density for any two years."
  },
  {
    "objectID": "posts/2019-06-15-census-custom-timelines/index.html#upshot",
    "href": "posts/2019-06-15-census-custom-timelines/index.html#upshot",
    "title": "Census custom timelines",
    "section": "Upshot",
    "text": "Upshot\nPopulation change is the obvious point to start the explorations of this rich custom tabulations. I am glad to have found it and am looking forward to exploring it more. And hoping that others will jump onto the bandwagon and use this to understand how Vancouver and Toronto have changed since 1971. As usual, the code for this post is available on GitHub for those that are looking for some pointers how to use this data."
  },
  {
    "objectID": "posts/2019-06-21-frequent-transit-zoning/index.html",
    "href": "posts/2019-06-21-frequent-transit-zoning/index.html",
    "title": "Frequent transit zoning",
    "section": "",
    "text": "Today I saw a particlarly uninformed tweet claiming that “the most important areas to densify are near transit and are mostly upzoned already”. I tend to agree with the first part, but the notion that our frequent transit network is “mostly upzoned already” is plain wrong. I suspect that a lot of other casual observers share the misconception. So I decided to take this as an opportunity for a quick post to quantify zoning in our frequent transit network.\nFor this I am taking a similar approach to the work with Denis Agar where we built the frequent transit network from actual transit schedules and cut it down to residential land use. In short, we defined the frequent transit network as areas around stops that are served by transit every 16 to 20 minutes during 6am-9pm Weekdays, 7am-9pm Saturdays, 8am-9pm Sundays & Holidays, refer to the previous post for details. This definition comes up with a similar coverage area as TransLink does for their definition of frequent transit network. For the purposes of this post we are throwing out areas covered by parks, roads and other transportation uses, cemeteries, ports and airports and only focus on residential, commercial, industrial and undeveloped/unclassified lots.\nWe then categorize the resulting area by it’s zoning, grouping them into low density (RS, RT, FSHCA, ..), multi-family (RM, CD, …), commercial, and industrial zoning. Some of our categories are a little rough and could probably use refining, especially when categorizing lots zoned Comprehensive Development into commercial or multi-family, but this seems to do a pretty good job for the purposes of this post.\nThis illustrates well how good the coverage of our current frequent transit network within the City of Vancouver already is. And as transit keeps improving, hopefully we will be able to fill in the remaining gaps.\nTo wrap this up we quantify the area taken up by each of our zoning categories within the frequent transit network.\nThis shows very clearly how far off the claim that areas in our frequent transit network are “mostly upzoned already” really is. Which is another manifestation of the loss of social welfare due to overly restrictive land use planning, which forces people to live further away from jobs and amenities. On the upside, the vastness of the low-density zoning in our frequent transit network provides a great opportunity, also known as the Great House Reserve, to move our region forward."
  },
  {
    "objectID": "posts/2019-06-21-frequent-transit-zoning/index.html#next-steps",
    "href": "posts/2019-06-21-frequent-transit-zoning/index.html#next-steps",
    "title": "Frequent transit zoning",
    "section": "Next steps",
    "text": "Next steps\nIf you are interested in refining this by e.g. fine-tuning the land use or zoning categories used, or maybe classify by land use instead of zoning, feel free to grab the code and make the appropriate changes."
  },
  {
    "objectID": "posts/2019-07-07-shaughnessy-townhomes/index.html",
    "href": "posts/2019-07-07-shaughnessy-townhomes/index.html",
    "title": "Shaughnessy Townhomes",
    "section": "",
    "text": "City of Vancouver council rejected the development application for 21 purpose-built rental townhouses in Vancouver’s exclusive enclave of Shaughnessy last week, and the owner is now proceeding with building a mansion on that lot instead.\nCouncillors gave a variety of reasons for the rejection. Some were voicing concerns of about the compatibility of hospice use with the 3 1/2 storey townhouse development next door, which seems far fetched as a quick look at St John’s hospice at UBC (the low building on the right in the picture here) shows. There the hospice wanted to locate directly next to a 19 storey condo tower, and condo owners fought tooth and nail to stop the hospice from getting built. The difference there is that the condo tower was their first and it was the condo tower that was arguing that these are incompatible uses, while in Shaughnessy the hospice was first and was arguing that the townhouses are incompatible use. In both cases, some design changes were made to the respective new developments to enhance privacy of the respective other side. Bottom line, hospice and residential developments can happily co-exist, those that claim otherwise are plain NIMBYing.\nAnother reason cited was construction noise that would impact the hospice. The city negotiated a good neighbour agreement that would pause construction during certain times of day to lessen the impact. No property, including the hospice, can reasonably expect a development bubble around their property. A good neighbour agreement is pretty much the best one can hope for. Now the hospice is getting a 12,000sf mansion development next door without a good neighbour agreement. To top things off, the hospice made it known during the public hearing that they are interested in buying the lot and developing a senior centre there. Apart from the obvious conflict of interest wrt stopping the townhouses, this also calls the hospice’s concerns about development related disruptions into question.\nOne green councillor argued they voted against the project because there wasn’t enough consultation, while the other two Green councillors said they voted against it because there was too much parking, parking that was added in response to community feedback during consultation.\nProbably the biggest concern that was that “the rents are just going to be too high for people who rent to afford”. Which is a point that comes up a lot with new developments and deserves some unpacking."
  },
  {
    "objectID": "posts/2019-07-07-shaughnessy-townhomes/index.html#20_families",
    "href": "posts/2019-07-07-shaughnessy-townhomes/index.html#20_families",
    "title": "Shaughnessy Townhomes",
    "section": "20 fewer families in Vancouver",
    "text": "20 fewer families in Vancouver\nThe bottom line of the council vote is that 20 fewer families will be living in Vancouver, or maybe 19 or 18 as the owner indicated he is planning to occupy the mansion with his extended family. Given Vancouver’s vanishing rental vacancy rate, it’s safe to assume that all these units will be occupied.\nWho are these 20 families? We don’t know, we don’t know their faces. Which is what makes it so easy for council to vote them down. But it’s not the 20 families that can afford the rents in these Townhouses that will get pushed out of Vancouver as a result, these families will simply bid up rents somewhere else in the city. It’s lower income families further down the income distribution that will get pushed out of Vancouver as a result of council’s decision. This is a process that has been going on for quite a while now, with the City of Vancouver bleeding low income households and gaining high income households faster than the rest of the region.\nIt bears repeating that it’s the low income people that are most impacted by the decision to deny those townhouses to get built, a general phenomenon recently described in detail using panel data in the US by Evan Mast and umpacked by Stephen Quinn a couple of days ago.\nIt’s probably worth to spend a bit of time to contextualize this for the Vancouver setting."
  },
  {
    "objectID": "posts/2019-07-07-shaughnessy-townhomes/index.html#affordability",
    "href": "posts/2019-07-07-shaughnessy-townhomes/index.html#affordability",
    "title": "Shaughnessy Townhomes",
    "section": "Affordability",
    "text": "Affordability\nTo start off, let’s look at the claim that the townhouses would be unaffordable. As Jill Atkey observed, a family moving into a 3 bedroom unit would need about $140k in pre-tax income to meet the standard affordability definition. Some people were quick to point to median household incomes as a proof that families won’t be able to afford them. But incomes, just like housing costs or rents, are a distribution. And needs of households vary.\nAnd in situations like this it’s important to remember the fine print in Canada’s core housing need metric, in particular the part that looks at suitability of the unit. It essentially says that when it comes to 3 bedroom units, we should be looking toward families with children and ask if it’s affordable to them, the question if the 3 bedroom unit is affordable to a one-person household is not relevant. In fact, an overhoused household that spends more than 30% of their income on housing but could afford a right-sized unit in their neighbourhood is not considered to be in core housing need. One can argue about the details of the metric, but probably everyone can agree that the target population for a 3 bedroom townhouse is a family with children and not a 1 person household.\nSo let’s look at incomes of families with children at home. In 2015 the median income was $111,636 in the City of Vancouver and $112,167 in the Metro region. So the median family with children won’t be able to afford these townhouses, but what portion of families will? We don’t have data on that exact question, but we do have a crosstab that allows us to check what portion had an income of $150k or more in 2015.\n\nIn the City of Vancouver 32.3% of all families with children at home had an income of at least $150k in 2015, for Metro Vancouver the share is 29.4%. So these townhouses are affordable to almost one third of families with children at home.\nRenter households tend to have lower incomes, as a second metric we can just look at renter households in market rentals.\n\nThe shares of families with children at home making at least $150k in 2015 are 18.3% and 12.6% for the city and the Metro area, respectively. That’s a significantly lower portion, but still a sizeable contingent of families, even at the $150k level instead of the more stringent $140k affordability cutoff. And that’s not accounting for these townhouses being centrally located with excellent access to transit which translates into transportation (and time) cost savings. And that family incomes have risen substantially since and will have increased, and with them the share to whom the townhouses would be affordable.\nThe simple reality that income is a distribution, and that there is a sizable portion of families that can afford to rent these townhouses, should not need pointing out. And of course we knew that since the townhouses would have gotten rented out. When someon is arguing that these won’t be affordable, they essentially say that they won’t be affordable to the population they care about. But that’s a very short-sighted argument, housing units are part of a complex housing system."
  },
  {
    "objectID": "posts/2019-07-07-shaughnessy-townhomes/index.html#system-effects",
    "href": "posts/2019-07-07-shaughnessy-townhomes/index.html#system-effects",
    "title": "Shaughnessy Townhomes",
    "section": "System effects",
    "text": "System effects\nSo lets assume council really did not care about families with children in the top 30% (or 15%) of the income distribution. As explained above, those families aren’t the ones that will get displaced from Vancouver as a result, the perverse effect of council’s decision is that it’s lower income families that get pushed out.\nWe need to think of housing as a system. Only allowing housing for you favourite clientele, or only allowing your favourite type of housing, is not going to solve our housing crisis. We do need more housing of all types, non-market housing as well as market housing. We simply don’t have the capacity to build enough non-market housing for everyone, we also need to build more market housing to lower the entry rents to market housing. The problem needs to be attacked from both sides. We understand well how vacancy rates relate to rent increases, and cities like Seattle show that a massive increase in building can indeed lead to lower rents last year, with now rents rising much slower than the national average and landlords giving out perks to attract renters before adjusting for the increase in quality that the influx of new units brings."
  },
  {
    "objectID": "posts/2019-07-07-shaughnessy-townhomes/index.html#upshot",
    "href": "posts/2019-07-07-shaughnessy-townhomes/index.html#upshot",
    "title": "Shaughnessy Townhomes",
    "section": "Upshot",
    "text": "Upshot\nNot building those townhouses does not reduce demand, it merely reduces the number of people than can live here, and increases competition for the remaining units. Of course 21 townhouses by themselves don’t make much of a difference in placating our latent demand for housing. And given how long it took council to debate this one project I am having my doubts that a reasonable number of smaller rental projects can get approved if things continue at this pace. Unless council is going for larger projects like some of the MIRHPP that are in the pipeline (and haven’t dropped out yet). Personally, I prefer the shorter “missing middle” type housing, including townhouses and lowrise, but I will take any rental housing Vancouver can get. It’s badly needed, and the MIRHPP projects include 20% of below market rental units as an added incentive. Hopefully that will soften some of the affordability concerns from those people that can’t wrap their head around the system effects. We shall see how council will handle the MIRHPP projects when they are coming up for a vote."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html",
    "title": "On Vancouver population projections",
    "section": "",
    "text": "Metro Vancouver’s population is growing. For planning purposes we want to understand how our population will be growing. For that we need projections.\nHere we need to carefully distinguish two related but distinct types of population projections.\nProjecting demand, or even just estimating current population demand, is complex. Demand is a function of a variety of factors, most importantly jobs, and amenities, as well as home prices and rents (in relation to incomes and wealth). Jobs and amenities, relative to other areas, determine the attractiveness of a region to in-migration (and the dissuasion to out-migration). Natural growth (birth minus deaths) is only a small factor in Metro Vancouver’s growth.\nThis attractiveness of a region sets an abstract upper limit for population demand. Actual population growth up to the abstract upper limit depends on the amount of available housing (relating to population demand via households). This is where we have to remember that demand is a function of price, and the abstract upper limit on population demand is built on the assumption that people can find housing at an acceptable price. In reality, the population demand is curbed through market prices for market housing, and waitlists for non-market housing. This is how population demand and population growth meet.\nFor planning purposes, defining demand through current market prices is tautological and not very useful. After all, population projections are often used to limit the amount of permitted housing. Which leaves us with the question: What is the cost of housing we should estimate population demand at?\nFor market housing, a good, albeit somewhat abstract, answer is Glaeser’s Minimum Profitable Production Cost (MPPC), that is the cost to add housing to a given market, which depends on construction cost (which will rise with density and could use some refinement), augmented by municipal service charges and possibly other charges, for example charges aimed to maintain livability, the cost of land (usually taken to be 20% of construction cost and service charges), and a developer profit. While the MPPC in Vancouver is high, the price of market housing is even higher than that. In the media this is often described as elevated land values. Land values of a housing unit is the residual value that buyers are willing to pay above what it costs to construct the unit (including development charges). That differential is negotiated through supply and demand of market housing.\nDemand for non-market housing is, by definition, not negotiated through price. The distribution of non-market housing is done via eligibility criteria and waitlists. Demand can be approximated by the length of the waitlist, although that will generally be a substantial under-estimate in real life situations where wait list tend to be so long that many eligible households don’t even bother applying."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#a-look-at-population-demand-in-vancouver-and-calgary",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#a-look-at-population-demand-in-vancouver-and-calgary",
    "title": "On Vancouver population projections",
    "section": "A look at population demand in Vancouver and Calgary",
    "text": "A look at population demand in Vancouver and Calgary\nIt is safe to say that Vancouver’s amenity value is comparatively high, but it is hard to quantify that. Quantifying the job market is much easier. For comparison we choose Calgary, also a city with high amenity value, although probably not quite as high even though it often features high in livability surveys. However, at this time Calgary facing a very different jobs market from Vancouver.\nCalgary has a healthy existing housing supply to accommodate mild population growth with a primary market rental vacancy rate of 3.9% and a private condo rental vacancy rate of 2.7%. At the same time Calgary is continuing to add new housing units at a good pace with apartment building starts for the first six months of 2019 only 2.5% below the corresponding 10 year average and completions and apartment units under construction up by 15.9% and 14.0%, respectively.\nEconomic indicators in Calgary are slowly improving, but current economic conditions show a relatively high unemployment rate of 7.0%, above the Canadian average of 5.5%. At the same time, Calgary has a job vacancy rate of 2.5%, below the Canadian average of 3.1%. That means that unless economic conditions change we expect that on balance people to move away from Calgary over the long term.\nThis means that Calgary’s housing market likely has a bit of a buffer at this point so that population demand and population growth are roughly the same right now, which simplifies things.\nIn Vancouver on the other hand, population demand and population growth are quite different. Vancouver’s unemployment rate sits at 4.0% while the job vacancy rate of the economic region is 4.9%. The current population can’t fill the existing jobs, and with current primary and secondary market rental vacancy rates at an anemic 1.0% and 0.3%, respectively, there is no way for people wanting to fill those jobs to move into the region. Current population demand is significantly higher than population growth. Dwelling growth becomes a hard cap on population growth.\nThe silver lining in this desperate situation is that apartment building starts in the first half of 2019 is a whopping 102.7% above the corresponding 10 year average and completions and apartment units under construction are up by 41.9% and 78.1%, respectively. One should note however that at current rates at fairly high completions it would take about 15 months to complete the number of dwelling units needed to make room for the 26,537 workers (at a healthy average rate of 1.3 workers per dwelling unit, and completely ignoring the need for moving vacancies) to bring Vancouver’s job vacancy rate down to the Canadian average. And that’s not accounting for jobs created through continued economic growth or knock-on effects of the economic activity associated with adding these workers and filling the jobs. Thus it will take a while to clear the backlog of undersupply, let alone catch up with latent demand due to delayed household formation, overcrowded households and bring the rental vacancy rate up to healthy levels.\nIn clearing the backlog, Vancouver’s prices and rents will have to adjust to meet this demand, with recent demand side measures like the City of Vancouver Empty Homes Tax and the vacancy tax component of the provincial Speculation and Vacancy Tax helping ensure that properties get occupied and thus aiding the process of price and rent adjustments.\nThis means that at least for the shorter term future Vancouver will have to arrange itself with a significant gap between population demand and population growth, and the associated market selection mechanism sorting out who gets to come/stay and who gets pushed or shut out of market housing, and our non-market mechanisms selecting how gets to occupy non-market housing units."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#planning-for-growth",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#planning-for-growth",
    "title": "On Vancouver population projections",
    "section": "Planning for growth",
    "text": "Planning for growth\nIn our highly regulated building environment, population projections (and dwelling projections) are turned into a high-stakes game. In Vancouver, we rely on the Metro Vancouver Regional Growth Strategy (RGS) to set population and dwelling growth targets, and distribute the growth across the municipalities within the region. Those targets are then used as a guideline for how much new construction to permit.\nAt the metro level, there are three possible outcomes:\n\nThe RGS overestimates population demand and we permit more housing than needed.\nThe RGS correctly estimates population demand and we permit exactly as much housing as needed.\nThe RGS underestimates population demand and/or we permit too little housing.\n\nIn the first case, developers will simply stop building excess housing since providing housing in excess of demand becomes unprofitable fast. The second case essentially never happens, projections are always wrong. The third case is the case of a housing shortage, which leads to people getting shut out of the region, price growth for ownership and rental housing, and a dampening of the economy due to labour shortages.\nThe costs for under-estimating growth are high (while the cost to over-estimate growth is low). Time to take a more careful look at the RGS."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#benchmarking-the-rgs",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#benchmarking-the-rgs",
    "title": "On Vancouver population projections",
    "section": "Benchmarking the RGS",
    "text": "Benchmarking the RGS\nTo illustrate this we can look at the (now dated) population projections by the Metro Vancouver Regional District, (now just called Metro Vancouver) from 2010.\n\nThe projections call for a slowing of the growth in the region over the years. In the beginning, dwelling growth exceeds population growth to account for shrinking household size, but curiously the model has almost identical dwelling and population growth rates for 2031 to 2041. Unfortunately there is no further explanation how the model was derived, but this seems to assume that household size stabilizes by the 2031-2041 period and all latent demand has been dealt with. Employment is projected to rise at a slightly slower rate than population, probably reflecting the shrinking portion of the population in the work force as the share of seniors continues to increase and the entry time in the labour market continues to get delayed due to increasing levels of education.\nLong term projections are hard, so let’s focus on the short term until 2021. And interpolate values for the 5 year intervals given by census years that allow us to check how the projections held up. For simplicity, we will assume the growth rates to be constant within each interval, the exact way we interpolate does not make much of the difference for what follows. Projections are always bound to be different from actual numbers anyway, what this post focuses on is the relative difference between the three metrics.\nThe Regional Growth Strategy was updated in 2017, and it is a worthwhile exercise to check how the updated projections compare.\n\nWe see that the updated projections sport slightly higher growth rates, and also remove the assumption of a stop to shrinking household size for the 2031-2041 period."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#how-to-measure-population-dwellings-and-employment",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#how-to-measure-population-dwellings-and-employment",
    "title": "On Vancouver population projections",
    "section": "How to measure population, dwellings and employment",
    "text": "How to measure population, dwellings and employment\nTo understand population demand we need to at least measure population, dwellings and jobs/employment.\nThe question how to measure these metrics is surprisingly less straight-forward than it seems. Ideally we want high-frequency data so we can closely monitor how our region is changing. Annual data is probably good enough for that. StatCan has a number of annual tables at the CMA level that can inform on employment and population growth, although the employment tables are based on place of residence and not place of work. Which makes a difference when there are changes in net commute patterns for Metro Vancouver. The census also counts population and employment, although one needs to adjust for the census undercounts.\nStatCan does not have good tables on the number of dwelling units, although CHSP data is starting to use administrative data to count residential properties. Folding in unit counts for each property gets us closer to dwelling units, but this still fails to account for some informal dwellings, like non-authorized secondary suites. BC Stats has annual estimates of dwelling units that can also be used. The census has (private) dwelling counts, but they generally under-count as the census inevitably misses dwelling units, as we also need to account for collective dwellings.\nFiner geography data, so data for municipalities within Metro Vancouver, are harder to get from the annual StatCan tables, although BC Stats has population estimates (and projections) at the municipal level, and custom tabulations are available at least for the larger municipalities. Census data is available on municipal and sub-municipal geographies, but the numbers still need to be adjusted for undercounts and net commute flows.\nSorting out these details takes more effort than what we are willing to spent on this post. We will focus on growth rates and differences over given time periods, what’s most important is to choose consistent metrics for this. Differences stemming from the particular choice of metric tend to divide out this way, and we will ignore related issues for this post as they are smaller than the effects we are interested in.\nThe RGS is unfortunately a bit vague on how exactly the base estimates were derived, stating that dwelling counts (just private or private and collective?) are based on the census, and population and employment (not jobs) estimates are derived from the census and were adjusted for census undercounts (probably via a custom-tabulation at the sub-metro level?)."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#comparing-metrics",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#comparing-metrics",
    "title": "On Vancouver population projections",
    "section": "Comparing metrics",
    "text": "Comparing metrics\nPopulation counts at the metropolitan level are easily available from StatCan table 17-10-0135 that is already adjusted for census undercounts, but is anchored on July 1st, almost two months after the census. For Metro Vancouver, that came out at 2,188,743, quite close to the 2,195,000 from the Regional Growth Strategy.\nNext up is the number of dwellings, those should be straight-up census numbers, but the RGS had 848,000 dwelling units and the 2006 census counted 870,992 private dwellings, not including collective dwellings. It’s unclear to me why these differ so dramatically.\nEmployment counts are harder than they should be. Moreover, employment is probably not the most relevant metric, it would be better to count jobs. Which is even harder. As the RGS employment estimates are based on the census, we can query the employment variable by place of work geography. We get a total employment of 1,113,465 people in Metro Vancouver, compared to 1,158,000 from the RGS.\nThe match is reasonably good, although our census numbers don’t include the census undercount yet. Undercounts are only available for the place of residence, not the place of work geography, but that difference will have a negligible effect. An alternative is to use Labour Force Survey data that is available annually, but only considers place of residence instead of place of work. For 2006 that estimate comes out as 1,139,800 for the (unadjusted) employment in Metro Vancouver in May 2006.\nTo move forward, we will focus on the growth of our measures rather than the absolute numbers, as differences in sources and methods tend to divide out that way. As this is something that should be monitored on an ongoing basis we will be relying on the annual StatCan tables for population and employment estimates, rather than the census estimates.\nHowever, when we need finer regional breakup we are confined to census estimates."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#checking-in-with-2011-data",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#checking-in-with-2011-data",
    "title": "On Vancouver population projections",
    "section": "Checking in with 2011 data",
    "text": "Checking in with 2011 data\nThe 2011 Census data gives us the first reality check of the data. We use the baseline 2011 estimates from the updated Metro Vancouver Regional Growth Strategy from 2017 as another point of comparison.\nWe compare the annual growth rates for the 2006-2011 timeframe as projected by Metro Vancouver in 2011 to the original 2006 base line and updated 2011 base lines, to the “actual” change based on Census (for private dwellings) and annual StatCan tables (for population and employment). Calling these “actual” is a bit ambitious, as we are glossing over details as we have explained above, but we will run with this for now.\n\nWhat this shows is that population came in roughly as projected, whereas employment fell a bit short (which should not be surprising considering financial crisis hit in that time window). What’s a real head-scratcher is the change in dwellings, where projections came a bit short of the actual, and the two Regional Growth Strategy baseline differ significantly. This points to a change in methods in the RGS.\nWe can also look at how our population, dwelling and employment estimates line up with the 2006 and 2011 baseline estimates from Metro Vancouver.\n\nThis shows that the Metro Vancouver Regional Growth Strategy did a decent job at projecting jobs and population, but came in systematically short of their dwelling targets. The smaller differences aren’t concerning and expected when using different metrics, but it would be prudent to better understand the apparent dwelling shortfall in the RGS. It would be good to better understand exactly how Metro Vancouver derives their estimates and projections."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#checking-in-with-2016-data",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#checking-in-with-2016-data",
    "title": "On Vancouver population projections",
    "section": "Checking in with 2016 data",
    "text": "Checking in with 2016 data\nThe 2016 census gives us another opportunity to check the projections against the data. We can use the projections from the original and the updated Growth Strategy and compare them to our estimates based on Census data and annual tables.\n\nWe see that the population grew faster than projected off the 2006 baseline, but a bit slower than projected off the 2011 baseline. Dwellings consistently grew slower than projected while employment consistently outperformed projections.\nAnother interesting takeaway from the data is to just look at the trend in our “Actual” data.\n\nThis shows that dwelling growth slowed somewhat after 2011, while population growth slightly increased and employment growth increased dramatically, recovering from the slower growth during the economic slowdown around 2008."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#regional-breakdown",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#regional-breakdown",
    "title": "On Vancouver population projections",
    "section": "Regional breakdown",
    "text": "Regional breakdown\nAnother aspect of the Metro Vancouver Regional Growth Strategy is the regional allocation of the growth. Here we are working exclusively with the updated Growth Strategy. For this we can’t get away with using annual population and employment data any more but have no choice but go with census data. This means we have to worry about census undercounts, but this is where things get tricky. Those estimates aren’t easily available at the municipal level, and one would expect significant regional variation that simply using metro estimates isn’t particularly informative. BC Stats has custom population estimates at the municipal level that include the census undercount, and StatCan probably has custom estimates as well. We will skip over this detail, the difference in census coverage rates between the census years should not impact our results by much.\nMoreover, we can’t lazily assume that place of residence and place of work (roughly) coincides like we did when looking at the Metro level, we will have to be careful about how we count employment. Again, a custom tabulation would be helpful for this, but we will make due with adjusting employment numbers by net commuter flow between municipalities, recognizing that we are missing flows between municipalities with fewer than 20 commuters.\n\nAgain, projections are just projections, and our census numbers do not account for census undercounts, although most of these will divide out when taking differences. And our employment estimates, especially for smaller municipalities, should be taken with caution because our commute flow data dropped flows below 20 commuters. Thus it makes more sense to focus on the more populous municipalities.\n\nThe City of North Vancouver is the only municipality that exceeded its projections on all three metrics. The City of Vancouver, Surrey, New Westminister and Burnaby all exceeded their employment projections, while at the same time falling short of their dwelling (and their population) projections. All of the other larger municipalities, with the exception of Maple Ridge, came in below their projections on all three metrics.\nUnfortunately Metro Vancouver did not detail how they arrived at their projections, so it is difficult to read too much into this discrepancy. But it provides a good opportunity to re-evaluate the metrics and assumptions they are based on. What we do see is a very good correspondence between dwelling and population growth, as is to be expected.\nOf particular interest is the relationship between these metrics, in particular the one between dwellings and population. It can be useful to break down population change into population change due to change in dwelling units, population change due to change in household size and population change due to change in the rate of unoccupied dwelling units as we have explained before, with each component mapped out across Canada.\n\nGenerally we expect population to grow slower than dwellings due to shrinking household size, but this is not always the case as the data shows. For example, household size increase in the City of North Vancouver, and the increase was large just enough to make up for a slight increase in dwellings not occupied by usual residents. Maple Ridge and the District of Langley on the other hand experienced a slight drop in household size, but this was more than made up for by a drop in dwellings not occupied by usual residents. New Westminster saw both, an increase in household size and a drop in dwellings not occupied by usual residents, which boosted the ratio. Our interactive map makes it easy to explore the reasons for these census ratios in the case of the population in private households only as well as in a more complex visualization for the entire population (which slightly changes the interpretation for Maple Ridge, presumably due to an increase in collective dwellings).\nWe should point out that for Electoral A there has been a significant number of reclassification from collective to private dwellings within parts of UBC student housing that register as unoccupied by usual residents in the census, as well as significant new construction, both of which add to “unoccupied” dwellings and drag down the ratios from the census shown in the graph above.\nTo quantify the relationship between population and dwelling growth rates we can fit a simple linear model to the RGS targets, as well as to growth rates derived from census data.\n\nCensus data gives a better fit than to the RGS projections, with an adjusted R2 of 0.849 for census data and 0.776 for the RGS. This points to room for refinement. One should be able to improve the model fit by accounting for collective dwellings, as well as number of units completed in the months before census day for each of the census years, which has a noticeable impact on the number of unoccupied units as we have explained before. A more detailed understanding of the number of bedrooms of the added (and lost) dwelling units, as well as tenure, is a great predictor of household size, information on the mix of bedrooms should give another significant boost to improve the model fit. Lastly, we need to recognize that change in wealth is another measure, as change in wealth is generally associated with consumption of more housing (and shrinking household size).\nThe takeaway is that the relationship between new dwelling units and population growth is very predictable, and in our supply constrained market we find that population growth is constrained by dwelling growth."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#planning-for-the-region",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#planning-for-the-region",
    "title": "On Vancouver population projections",
    "section": "Planning for the region",
    "text": "Planning for the region\nDwelling growth in our region is highly regulated. The Regional Growth Strategy is only a guiding document, but it is often used as a plan that allocates growth to the region overall and distributes it within the region.\nWe have already seen that there are significant latent population pressures, partially due to dwelling growth targets set out by the Growth Strategy not being met, and partially by these targets likely being too low. Here we want to focus on how the Growth Strategy is distributing that growth, and how that might relate to people’s preferences.\nWe have already seen above that the projections assume stronger growth in some of the outlying regions than in some of the central regions. To bring this out more, we compare the projected growth in the City of Vancouver to the rest of Metro Vancouver.\n\nThe Regional Growth Strategy is projecting that the City of Vancouver will grow slower than the rest of the region. This makes sense if we assume that, on balance, people would rather live further out than they do right now. But I all evidence points toward the opposite being true, that is that people would, on balance, rather live in the City of Vancouver than in more outlying municipalities. Allocating growth in direct opposition to people’s preferences serves to drive up prices and rents in the central areas, and to drive low income people out. Which is exactly what we have seen when looking at income data where incomes have rising much faster in the City of Vancouver than in Metro Vancouver overall, with the higher income brackets growing faster and low income brackets dropping faster in the City of Vancouver."
  },
  {
    "objectID": "posts/2019-08-01-on-vancouver-population-projections/index.html#upshot",
    "href": "posts/2019-08-01-on-vancouver-population-projections/index.html#upshot",
    "title": "On Vancouver population projections",
    "section": "Upshot",
    "text": "Upshot\nThe last two sections illustrate two main dangers of an overly restrictive planning regimen.\n\nWhen planners don’t manage to hit their dwelling targets, or set their dwelling targets too low, we end up with higher population pressures and hit labour market constraints.\nWhen planners don’t manage to allocate the growth within the region according to people’s preferences, it will create artificial price gradients and push people out of their preferred areas and lengthen commutes.\n\nBoth of these outcomes have strong negative consequences for individuals, as well as the for the region overall. This is why it is very important that planners pay close attention to early warning signals of dwelling shortages as well as within-region location preferences.\nMore transparent models would also be helpful, especially given how high the stakes are when the dwelling targets are set too low. It would be helpful if planners, maybe in conjunction with economists, could explain better what consequences different planning scenarios may likely have for the region, so the politicians and the general public can make better decisions on how the region should grow.\nIn our highly regulated environment it is prudent to develop better metrics that can be updated regularly and that help inform of region-wide housing shortages, as well as more localized housing shortages within the region. Recently Nathan Lauster and I set out to draw up an outline of how such metrics could be constructed, a task we are planning on following up on with more concrete metrics and code that implement them.\nAs usual, the code for the analysis is available on GitHub."
  },
  {
    "objectID": "posts/2019-08-19-running-on-empties/index.html",
    "href": "posts/2019-08-19-running-on-empties/index.html",
    "title": "Running on Empties",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nA spectre haunts housing policy. The spectre of empty homes. So how many empty homes are out there?\nUnfortunately, inept analyses of census data often leaves us with incomplete, or even worse, completely wrong answers to this question. When we get data on empty homes for a given city, they’re seldom put into comparative perspective. What’s worse, sometimes when they’re put into comparative perspective, they’re compared with the wrong data and picked up by credulous media, spreading misinformation. So let’s try to do it right!\nHere we want to compare some big metro areas and cities in Canada with similar metro areas and cities in the US. As a bonus, this comparison sheds some light on our incomplete data in Canada, and why empty homes have managed to become so central to Canadian housing discussions."
  },
  {
    "objectID": "posts/2019-08-19-running-on-empties/index.html#empty-homes",
    "href": "posts/2019-08-19-running-on-empties/index.html#empty-homes",
    "title": "Running on Empties",
    "section": "Empty homes",
    "text": "Empty homes\nIn Canada we only have one national measure of empty homes, the Census. It estimates the number of dwelling units that are not occupied on census day. It does not offer any insight to why those homes are not occupied. Nor is it part of the standard release data, for most censuses it is only available as a custom tabulation. However, the related number of homes not occupied by usual residents is part of the general census release data and available down to the census block level. It is given by the number of dwellings minus the number of households (aka “occupied dwelling units”), so it includes dwellings that are occupied by people who usually reside in a different “household.” To understand what that means we need to remind ourselves that the census counts people, and tries to count them only once. And each person belongs to exactly one household. This gets tricky for people that call several places their “home”, for example a student that rents an apartment near university but also lives with their parents during summer, or someone working in Fort McMurray for months at a time but lives with their family elsewhere during work breaks. These people may think of their family’s home as “home”, and the other place as “temporary”. In the census, the “temporary” home will be counted as “occupied by temporary residents” and not count as a “household,” as their main household is elsewhere."
  },
  {
    "objectID": "posts/2019-08-19-running-on-empties/index.html#canadian-numbers",
    "href": "posts/2019-08-19-running-on-empties/index.html#canadian-numbers",
    "title": "Running on Empties",
    "section": "Canadian numbers",
    "text": "Canadian numbers\nCanadian data is pretty simple. To start off we look at Canada’s major census metropolitan areas by their share of unoccupied dwellings. For context we also show the temporarily occupied units. We get a range of unoccupied households somewhere between roughly 2% and 10%, with most bigger metros hanging toward the middle, between 4% and 8% (or what the Lincoln Land Institute considers the desirable range of “reasonable vacancy”).\n\nWe can also look at municipalities, keeping in mind that the comparison across municipalities is inherently difficult as different municipalities play different roles within (or outside of) metropolitan areas. Here’s a selection of municipalities, including the boundaries for the old (pre-amalgamation) City of Toronto, just for kicks. Note that municipalities still tend to hang between the 4% to 8% reasonable vacancy range, but the high share of temporarily occupied homes in Waterloo stands out, likely a function of students making up a large share of the town’s population."
  },
  {
    "objectID": "posts/2019-08-19-running-on-empties/index.html#us-data",
    "href": "posts/2019-08-19-running-on-empties/index.html#us-data",
    "title": "Running on Empties",
    "section": "US Data",
    "text": "US Data\nUS data on unoccupied homes is available from multiple places. Here we use the American Community Survey as similar to the Canadian Census. (But see also the American Housing Survey for fun cross-referencing).\n\nUS data is great in that it adds important context to unoccupied units, specifying the reason the unit is unoccupied. This context is often completely absent from Canadian housing discussions. It clearly splits out the transactional vacancies, (units for rent or for sale), from moving vacancies (units sold or rented, but not yet occupied), from recreational vacancies (units for recreational, seasonal or occasional use), from other vacancies (not otherwise accounted for).\nThe range for US Metropoles is also much higher than for Canada, running 12% and higher in the seasonal vacation-oriented metros of Florida, Arizona, and Southern California. Just below these metros sit some of the rust belt metros (Pittsburgh, Detroit, St. Louis) that have lost population, resulting in higher “other vacancies” from homes left behind. Houston seems driven by a high proportion of dwellings available for rent. Overall the data show that many empty homes may be accounted for by these kind of transactional vacancies and moving vacancies, together comprising vacancies we might also think of as good vacancies insofar as they enable people to move between homes to find the best fit. Down toward at the bottom we see just under 5% in the Twin Cities of Minnesota.\nOverall, vacancies tend to be higher in the US than in Canada. As unoccupied dwellings rise much above 5%, they seem to be increasingly explained by recreational vacancies and other vacancies. A baseline of other vacancies remains largely unavoidable (e.g. homes under major renovations, tied up in court cases, etc.), and also appears to include people showing up as temporary residents in Canada. We can use ACS data on Usual Residence Elsewhere to provide figures similar to what we get in Canada, comparing all North American metros on roughly the same basis. Here we’ll just show the 14 biggest US metros along with the 6 biggest in Canada.\n\nOverall Canadian metros tend to have lower vacancy rates (combining unoccupied with temporarily occupied) than US metros. Seasonal destinations (Miami and Phoenix) - that also provide second homes for many Canadians - top the vacancy rates for large metro areas, followed by a diverse mix of large metros. Edmonton and Vancouver, though high for Canada, fit very comfortably in the low end for the US (running from Seattle to Boston), while Toronto, Calgary and Montreal occupy the bottom.\nWhat of the bad kind of vacancies, often associated with second or higher order homes for the wealthy or holding properties off the market for speculative purposes? Empty Homes Taxes and Vacancy Taxes in Vancouver and BC attempt to target just these kinds of dwellings, and so far they indicate that just over 1 in 9 unoccupied units end up getting taxed as second homes or otherwise vacant without defensible cause. Vacancy data from the US suggests that were such taxes imposed in places like Miami, that figure would likely be a lot higher. But Miami markets itself as a seasonal or vacation destination.\nVancouver’s Empty Homes Tax covers the City. BC’s speculation tax covers a region larger than cities or even any given metropolitan area. Just for kicks, let’s peek in on counties, a unit of governance in the US with no firm equivalent in Canada. Weirdly, counties can contain portions of cities, like New York County, which contains only the island of Manhattan within NYC. Sometimes counties are the same as cities, as seems to be the case for San Francisco county. Other times counties are a little larger, as with King County (containing Seattle). Sometimes they’re much larger, as with LA County. How heavily would vacancy taxes likely fall in these various counties? In the counties acting like metropolitan areas, including King County and LA County, overall unoccupancy rates are similar to Metro Vancouver. Vacation homes would likely be hit unless deemed ineligible for year round use. Some, but not all, other vacancies would likely be taxed. The vast majority of empty units probably wouldn’t remain empty long enough to trigger taxation. Counties containing Manhattan and San Francisco, with much higher seasonal use, would probably be hit much harder.\n\nAltogether, unoccupied dwellings are broadly similar between the US and Canada, with slightly more dwellings showing up as unoccupied in most metro areas to the south. Lots of municipalities, regions, and counties might profitably consider Empty Homes or Vacancy Taxes. But most unoccupied dwellings in most metros wouldn’t be much affected by them.\nAs usual, the code for the analysis is available on GitHub."
  },
  {
    "objectID": "posts/2019-09-06-job-vacancies/index.html",
    "href": "posts/2019-09-06-job-vacancies/index.html",
    "title": "Job vacancies",
    "section": "",
    "text": "A number I have been watching fairly closely is the job vacancy rate. It comes from StatCan’s Job Vacancy and Wage Survey (JVWS), and is updated quarterly. It is one of several surveys that complement the Labour Force Survey (LFS) that tends to receive a lot of attention. But the LFS is missing some important aspects of the labour market. I have been pushing JVWS data on numerous occasions, so I wanted to do a quick post to add a little more context. This is also a good opportunity to mix in data from the Survey of Employment, Payrolls and Hours (SEPH) and the Canadian Income Survey (CIS).\nThe LFS reports employment (and unemployment) numbers. Often this is used as a stand-in for the number of jobs, but the LFS is an imperfect proxy for two reasons.\nUnfortunately the JVWS is not available at the CMA level, only at economic regions. For Vancouver that means the economic region of the Lower Mainland-Southwest, which spans the census districts of Greater Vancouver, Fraser Valley and Squamish-Lillooet, and contains Metro Vancouver. The economic region for Toronto consists of Toronto, Peel, York and Durham which misses part of Metro Toronto and contains some parts outside of Metro Toronto.\nThe rate in the Lower Mainland-Southwest with a job vacancy rate of 4.90% immediately jumps out, it’s the only economic region with a job vacancy rate above 4%. We zoom into the area for geographic reference of the extent of the economic region.\nTo get a better understanding of the job vacancies, we take a look at the timelines at the provincial level.\nThere is a fair amount of seasonal volatility in the data, especially for Prince Edward Island and the other maritime provinces. The sharp diversion of BC from the rest of Canada is very obvious, with the rest of Canada in the 1.5% to 3% band while B.C. showed increasing rates that are now far above that of any other province.\nThe vacancies in British Columbia are driven by the Vancouver area, so economic region Lower Mainland-Southwest.\nLooking at the top 4 economic regions and Canada overall drives home the point how extraordinary the job vacancy rate in the Vancouver area is.\nThis rise in job vacancy over a fairly short period of time is testament of the growth of the BC economy, and it complements a parallel trend in LFS data.\nWe see that the Vancouver economic region of Lower Mainland-Southwest has the lowest unemployment rate. On the other hand, the economic downturn in Calgary and slow recovery is also clearly visible, where we focus on the same timeframe that JVWS data is available.\nLooking at participation rates, Calgary still stands out with participation rates significantly above the Canadian average. The other economic regions are tracking quite closely, with the Vancouver region gaining ground."
  },
  {
    "objectID": "posts/2019-09-06-job-vacancies/index.html#incomes-and-wages",
    "href": "posts/2019-09-06-job-vacancies/index.html#incomes-and-wages",
    "title": "Job vacancies",
    "section": "Incomes and wages",
    "text": "Incomes and wages\nThis begs the question on the impact on incomes of the employed population, as well as the wages offered for the vacant jobs.\nTo kick things off we consider the average hourly wage offered for vacant jobs.\n\nThe (real) average wage has increased in all areas except Montréal, but the increase in Calgary has been much steeper than that in Vancouver, showing that the job vacancy rate is not the main driver of changes in average wages offered. The type of vacant jobs is an obvious candidate to have a large impact on the wages."
  },
  {
    "objectID": "posts/2019-09-06-job-vacancies/index.html#survey-of-employment-payrolls-and-hours-seph-data",
    "href": "posts/2019-09-06-job-vacancies/index.html#survey-of-employment-payrolls-and-hours-seph-data",
    "title": "Job vacancies",
    "section": "Survey of Employment, Payrolls and Hours (SEPH) data",
    "text": "Survey of Employment, Payrolls and Hours (SEPH) data\nTime to fold in Survey of Employment, Payrolls and Hours (SEPH) data. Unfortunately that is only available at the provincial level, so it’s not the best match for our previous data that was focused on economic regions. But it should give a rough indication to how vacancies in different sectors contribute to overall vacancies and how they drive the average offered hourly wage. Here we resort to annual data as samples tend to get too thin for reliable finer temporal data. As a note of caution, this data excludes some sectors. More specifically\n\nIndustrial aggregate covers all industrial sectors except those primarily involved in agriculture, fishing and trapping, private household services, religious organisations and the military personnel of the defence services, federal, provincial and territorial public administration, as well as unclassified businesses. Unclassified businesses are business for which the industrial classification (North American Industry Classification System [NAICS] 2017 Version 3.0) has yet to be determined.\n\n\nWe singled out the 8 most prominent industries to make the graphs more readable. This identifies Health care and social assistance as the main driver of job vacancies Canada wide, while Calgary shows a different pattern with Accommodation and food services winning out. This sector is also featuring prominently in British Columbia. Job vacancy rates by sector add another piece of information.\n\nThe data quality suffers when splitting up job vacancies by sector, and it is not clear what impact job vacancies in unclassified businesses have. These patterns don’t seem sufficient to explain the difference in average wages offered for vacant jobs."
  },
  {
    "objectID": "posts/2019-09-06-job-vacancies/index.html#income",
    "href": "posts/2019-09-06-job-vacancies/index.html#income",
    "title": "Job vacancies",
    "section": "Income",
    "text": "Income\nA last clue might come from income of the employed population, complementing the wages offered for the vacant jobs. Here we chose to use Canadian Income Survey data and look at median total income of economic families.\n\nAgain, the recent recession in Calgary is clearly visible, but overall income levels remained well above the other metro areas. Vancouver has overtaken Toronto as we have pointed out several times before, with economic families in Toronto posting the lowest real income growth of our metro sample over that time period."
  },
  {
    "objectID": "posts/2019-09-06-job-vacancies/index.html#upshot",
    "href": "posts/2019-09-06-job-vacancies/index.html#upshot",
    "title": "Job vacancies",
    "section": "Upshot",
    "text": "Upshot\nThere is more to jobs data than just the LFS, and care should be taken not to confuse employment (by place of residence) with jobs. The job vacancy rate plugs one of the gaps between the two, the other is given by cross-boundary commutes.\nMetro regions don’t function the same way they initially did. Metro areas can’t be absorbed by other metro areas, so while e.g. Montreal has been growing geographically, Toronto is is quite constrained by water and neighbouring metro areas that show significant cross-boundary commute patterns. The same is true for Metro Vancouver which is bounded by water, mountains, and Abbotsford-Mission to the east.\nIt’s worthwhile to take a broader look every now and then and see how job vacancies and cross-boundary commutes (available through the census or at lower quality via triangulating various surveys) fit into the picture.\nThe code for the post is ovailable on GitHub for anyone that wants to explore this further."
  },
  {
    "objectID": "posts/2019-10-16-rents-and-vacancy-rates/index.html",
    "href": "posts/2019-10-16-rents-and-vacancy-rates/index.html",
    "title": "Rents and vacancy rates",
    "section": "",
    "text": "This post responds to a misconception about rental housing that has been making the rounds. Our housing crisis is fundamentally a rental crisis, so it’s important to keep the numbers straight so that we can better focus our energy and resources.\nThe misconception originate from the 2019 Vancouver Housing Data Book. The data book is a huge effort to compile and has a host of valuable information. Vancouver has been doing this for the second year now, and it is successively getting better. But there are bound to be some hiccups on the road, and these are two of them.\nAt issue here is not the data presented, the data is correct. But the choice of data, and how it is presented, lends itself to misconceptions."
  },
  {
    "objectID": "posts/2019-10-16-rents-and-vacancy-rates/index.html#expensive-rental-units-are-vacant-so-we-dont-need-more",
    "href": "posts/2019-10-16-rents-and-vacancy-rates/index.html#expensive-rental-units-are-vacant-so-we-dont-need-more",
    "title": "Rents and vacancy rates",
    "section": "Expensive rental units are vacant, (so we don’t need more)",
    "text": "Expensive rental units are vacant, (so we don’t need more)\nThis stat comes from page 183 of the Vancouver Housing Data Book. It shows that the vacancy rate for apartments renting over $3,750 is 8.7%. That’s out of the range of reasonable vacancy and at the bottom end of what the Lincoln Institute of Land Policy considers a Moderately High Vacancy. At that vacancy rate Canadian data shows that rents are sure to drop. By Vancouver standards, that vacancy rate is astronomically high. But that’s not the experience people have on the street. So what gives?\nThe key context for this number is that the is the $3,750 cutoff is the 99.8 percentile rent level in the city, or roughly the 116 most expensive units.\nLooking at the top 99.8 percentile is probably not very informative. Moreover, it’s generally a good idea to at least slice the data by bedroom type, and if possible also by location. Let’s take a look at rent quintiles by bedroom types in the City of Vancouver. That is we divide the purpose-built rental stock by bedroom type into five equally sized groups ordered by rent and check the vacancy rate in each of these groups.\n\nThe vacancy rate increases with the quintiles, with more expensive units being more likely to be empty. There are a number of reasons for this. The pool of people considering renting an expensive unit is naturally somewhat smaller than the pool of people considering to rent a unit priced in the middle quintiles, as there are fewer people able to pay those rents. But in Vancouver there are two other drivers that are more important for explaining the higher vacancy rates in the top quintile:\n\nRent control leads to occupied units renting on average at below-market rents. Because of vacancy decontrol that changes the instant the unit turns vacant, and the “rent” for the vacant unit, that is the advertised rent, will be much higher than what the unit previously rented for. A while back we looked into this in detail. This means that vacant units will have higher rents than occupied ones, and will accordingly skew toward the higher quintiles.\nWhen new buildings come online, it takes time to fully tenant the building. This is analogous to new condo buildings taking time to fill in, although the process is much faster in rental buildings. But it still takes time, especially in a rent controlled environment where landlords are willing to wait a little longer to fill the units rather than lower rents that will get locked in. And new units are generally more expensive and likely to fetch rents in the higher quintiles.\n\nLooking at the previous years, we see that there has been some variation in the vacancy rates across quintiles, with 2017 mirroring 2018 higher vacancy rates in the top quintile within each bedroom category.\n\nWe also notice quality concerns that have lead to removal of vacancy rates for 3 bedroom units in various quintiles and years, indicating that we probably should be careful interpreting the 3 bedroom vacancy rates.\nTo put that into relation to the numbers from the Vancouver Housing Data Book we take a look at the rent levels that break the rents into quintiles, and how these have evolved.\n\nWe again notice quality issues in the 3 bedroom rent cutoffs, due to the low number of 3 bedroom units in the universe. The lower cutoff of the top quintile 3 Bedroom rent in 2018 was $3,300, a bit below the overall 99.8 percentile cutoff of $3,750 reported in the Vancouver Housing Data Book.\nIn summary, it seldom makes sense to report rent levels without first slicing at least by bedroom type. And it is more useful to slice by smaller groups like quintiles instead of reporting values for the top 0.2% rent level."
  },
  {
    "objectID": "posts/2019-10-16-rents-and-vacancy-rates/index.html#geographic-distribution",
    "href": "posts/2019-10-16-rents-and-vacancy-rates/index.html#geographic-distribution",
    "title": "Rents and vacancy rates",
    "section": "Geographic distribution",
    "text": "Geographic distribution\nNext to bedrooms, geography is the other important variable to consider. Most renters care about roughly where in the city they are renting. For example, here are the vacancy rates for 1 Bedroom apartments in the first rent quintile by survey zone. The rent quintiles are taken over the entire City of Vancouver stock.\n\nWe see a large variation in the vacancy rate of 1 Bedroom units in the first city-wide rent quintile across survey zones. Part of that volatility can be explained by some survey zones having only relatively small number of units in the first rent quintile, as the following map shows.\n\nThe number of units is suppressed in some regions for some years due to quality issues. Marpole has the highest number of 1 bedroom units in the lowest rent quintile, whereas the downtown peninsula has relatively few, given the large number of 1 bedroom units there.\nTo confirm this last point, we plot the total number of 1 Bedroom units in each survey zone.\n\nThe share of the 1 bedroom units that are in the first rent quintile complete this picture.\n\nOn the flip side, where can plot the share of 1 bedroom units in the top rent quintile in each regions.\n\nThis underscores the relative attractiveness of central (and West Side) as opposed to Marpole (and East Side) locations.."
  },
  {
    "objectID": "posts/2019-10-16-rents-and-vacancy-rates/index.html#upshot",
    "href": "posts/2019-10-16-rents-and-vacancy-rates/index.html#upshot",
    "title": "Rents and vacancy rates",
    "section": "Upshot",
    "text": "Upshot\nLooking at the very extremes of the rent quintiles is likely not going to generate much insight, it will be clouded by data quality issues and not have much relevance for broader questions. As usual, the code for this post is available on GitHub for those interested playing further with the numbers. The CMHC Rms custom extract is also available on GitHub."
  },
  {
    "objectID": "posts/2019-10-22-elections-fun/index.html",
    "href": "posts/2019-10-22-elections-fun/index.html",
    "title": "Elections fun",
    "section": "",
    "text": "Canada is a large country, with some reasonably densely populated regions, and large areas that are sparsely populated. That makes it hard to map things. CensusMapper, our project to flexibly map Canadian census data, struggles with that. The choropleth maps can be quite misleading. The same problem comes up when mapping Canadian election data.\nThis map makes it virtually impossible to get a good reading of the distribution of votes. There are a couple of ways around this.\nFor example, one could break out the areas with electoral districts too small to make a visible impact on the map, or use a cartogram, like the following two examples taken from the Wikipedia page of the 2019 federal election.\nThe first keeps the overall geographic context, although the metropolitan areas that are broken out are hard to interpret unless one if very familiar with each region. The cartogram distorts the areas to give each electoral district the same amopunt of space, and thus gives a proportional view of the number of seats each party won. In this version, the labels and breaks help delineate familiar geographies, but it can be hard to properly place them on a map.\nTo bridge the divide between overall geography and emphasis on treating each district separately, one can also animate the cartogram between the familiar map view and the cartogram view. In the following example that we built as an observable notebook we move between a map of Canada and a cartogram where each electoral district is a dot with size given by the total number of votes cast.\nWe include a video clip of the animation for convenience, it’s just a screenshot from the interactive live observable notebook.\nThis still loses a lot of nuance. The colour is determined by who won the district, the animation reveals no information on how wide or narrow the margin of victory was. Or how the other candidates performed."
  },
  {
    "objectID": "posts/2019-10-22-elections-fun/index.html#winning-vote-share",
    "href": "posts/2019-10-22-elections-fun/index.html#winning-vote-share",
    "title": "Elections fun",
    "section": "Winning vote share",
    "text": "Winning vote share\nWith more than 2 candidates in each riding, one does not necessarily require a plurality of votes to win. Only 123 out of the 338 candidates won with over 50% of the vote in their district. The largest vote share any winning candidate got was 85.5%, the lowest was 28.5%.\n\nIt stands out that the top 36 spots were taken by the Conservatives."
  },
  {
    "objectID": "posts/2019-10-22-elections-fun/index.html#distribution-of-votes",
    "href": "posts/2019-10-22-elections-fun/index.html#distribution-of-votes",
    "title": "Elections fun",
    "section": "Distribution of votes",
    "text": "Distribution of votes\nWe can expand this view to show the vote share by party for each district."
  },
  {
    "objectID": "posts/2019-10-22-elections-fun/index.html#wasted-votes",
    "href": "posts/2019-10-22-elections-fun/index.html#wasted-votes",
    "title": "Elections fun",
    "section": "Wasted votes",
    "text": "Wasted votes\nWith our first-past-the-post system, we can also take a look at “wasted” votes. We define these as votes that have no bearing on the outcome. For winners it’s the vote margin by which they won, for the ones that did not win their district it’s the entirety of their votes.\n\nWasted votes is a somewhat artificial system that does not necessarily reflect how parties would have performed under a different voting system. We will need to take a closer look for that."
  },
  {
    "objectID": "posts/2019-10-22-elections-fun/index.html#first-past-the-post-vs-proportional-representation",
    "href": "posts/2019-10-22-elections-fun/index.html#first-past-the-post-vs-proportional-representation",
    "title": "Elections fun",
    "section": "First-past-the-post vs proportional representation",
    "text": "First-past-the-post vs proportional representation\nHow would the parties have fared under a proportional representation system (PR) instead of first-past-the-post (FPTP)? There are many different kinds of proportional representation systems out there, but they generally try to approximate a seat distribution that mirrors the overall vote share. For simplicity we will simply take the overall vote share as a proxy for what a proportional representation system might have yielded.\n\nThe Liberals are the big winner of FPTP, as is Bloc Québécois. The Conservatives fair equally well under either system, and NDP and the Green Party are the losers under the current FPTP system.\nWe can run this by individual province to see how well each province is represented in terms of first-past-the-post vs proportional representation.\n\nThis reveals that Alberta and Saskatchewan voters are a lot more diverse than the FPTP vote system may suggest, and the while the Conservatives got shut out of a couple of provinces, they still have sizeable support there. This kind of outcome is fairly typical for first-past-the-post systems, where the representation in parliament does not match the overall population well, especially when looking by province."
  },
  {
    "objectID": "posts/2019-10-22-elections-fun/index.html#upshot",
    "href": "posts/2019-10-22-elections-fun/index.html#upshot",
    "title": "Elections fun",
    "section": "Upshot",
    "text": "Upshot\nThere is endless fun to be had with elections data. As usual, the code for this post, including the pre-processing for the animation, is available on GitHub for anyone to adapt and dig deeper into elections data."
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html",
    "title": "Canadian Housing Survey",
    "section": "",
    "text": "The long awaited first batch of data from the Canadian Housing Survey came out yesterday. The Canadian Housing Survey (CHS) is a new survey that aims give a better idea of well housing needs of Canadians are met.\nRight now there are four tables publicly available, and we will give a quick tour of what’s out there, with a focus on Metro Vancouver, Toronto, Montréal and Calgary.\nThis post is meant as a quick overview of what’s available right now, the code is available on GitHub for anyone that wants to explore this further."
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#dwelling-and-neighbourhood-satisfaction",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#dwelling-and-neighbourhood-satisfaction",
    "title": "Canadian Housing Survey",
    "section": "Dwelling and neighbourhood satisfaction",
    "text": "Dwelling and neighbourhood satisfaction\nThe first table looks at satisfaction with of households with the dwelling units and neighbourhood they live in.\n\nIn this comparison, overall satisfaction with the dwelling is highest in Montreal and lowest in Vancouver. Satisfaction is generally higher in single-detached homes.\n\nAs one would expect, satisfaction is higher for owner households, and renters in subsidized housing have generally higher satisfaction with their dwelling units than renters in market housing.\nIt also gives a view at satisfaction with of households with the neighbourhood they live in.\n\nHouseholds in Montréal are on average most satisfied with the neighbourhood they live in while those in Toronto are least satisfied with their neighbourhood.\n\nAgain, owners show higher satisfaction with their neighbourhood compared to renters, but the situation for renters is reversed from dwelling satisfaction, with renters in market housing being in general more satisfied with their neighbourhood than those in non-market housing."
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#waitlist-status",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#waitlist-status",
    "title": "Canadian Housing Survey",
    "section": "Waitlist status",
    "text": "Waitlist status\n\nPredictably, only a very low share of owner-occupiers are on a wait list for social or affordable housing, and households already in subsidized housing have the highest share."
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#housing-suitability-and-adequacy",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#housing-suitability-and-adequacy",
    "title": "Canadian Housing Survey",
    "section": "Housing suitability and adequacy",
    "text": "Housing suitability and adequacy"
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#reasons-for-moving",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#reasons-for-moving",
    "title": "Canadian Housing Survey",
    "section": "Reasons for moving",
    "text": "Reasons for moving\nThe most exciting table is the reasons for the most recent move in the past 5 years, if the household moved. We have had that data form the US for a long time, but it was sorely missing in Canada.\n\n\nThe graph gets messy and people can report multiple reasons, but there are a few high-level takeaways.\n\nBritish Columbia has the highest shares of households forced to move by a landlord, a bank or other financial institution or the government.\nThe most frequent reason for moving across all of our regions is for a larger or nicer dwelling, followed by moving to a better location.\nIn Alberta people are becoming new homeowners at higher rates than in the other regions considered."
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#forced-moves",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#forced-moves",
    "title": "Canadian Housing Survey",
    "section": "Forced moves",
    "text": "Forced moves\nForced moves are especially concerning, so it’s worthwhile to look at these more closely. The exact nature is not further broken, these pertain to households “forced to move by a landlord, a bank or other financial institution or the government”.\n\nBritish Columbia takes the top spot with 4% of households moving during the last 5 years and doing so involuntarily on the last move, which is likely a function of the tight housing market with heavy reliance on insecure secondary market rentals, with condos and secondary suites easily being reclaimed by the owners. We also see evictions from purpose-built market housing, as well as people being forced to move from or within subsidized housing when their eligibility status changes.\nWe can also view this as a share of all moves, instead of as a share of all households.\n\nViewed as a share of movers, British Columbia households being forced to move on their most recent move jumps to an astonishing 10%. This is quite concerning and needs more attention. Micro data might help in understanding this further."
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#upshot",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#upshot",
    "title": "Canadian Housing Survey",
    "section": "Upshot",
    "text": "Upshot\nIt’s great to see this data finally making it out there, and hopefully micro data will become available too. It will take some time to weed through this in more detail, this can serve as a starting point. As usual, the code for this post is available on GitHub for anyone to grab to reproduce or to jump-start their analysis."
  },
  {
    "objectID": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#update",
    "href": "posts/2019-11-23-canadian-housing-survey-a-first-look/index.html#update",
    "title": "Canadian Housing Survey",
    "section": "Update",
    "text": "Update\nDon’t miss Nathan’s post that puts the Canadian CHS into context of the US CPS-ASEC data."
  },
  {
    "objectID": "posts/2019-12-09-fun-with-parking-tickets/index.html",
    "href": "posts/2019-12-09-fun-with-parking-tickets/index.html",
    "title": "Fun with parking tickets",
    "section": "",
    "text": "Almost three years ago I ran the numbers to identify “Vancouver’s most lucrative fire hydrant”.\nBeing a card-carrying Shoupista it’s high time for me to do an update. And looking back I can’t help but realize how my approach to data analysis, even about such trivial things as parking tickets, has changed since then. Back then I scripted makeshift analysis in a general purpose language. Nowadays I work in R or Python and am much more structured in my approach, with emphasis on reproducibility and transparency. And this included the entire pipeline, from data acquisition, to data cleaning, data analysis and visualization.\nIn this case, we are working with City of Vancouver Open Data, and data acquisition happens through my relatively new VancouvR package that ties into the new City of Vancouver Open Data API and is now on CRAN. Usually I hide the code from the post but make it available on GitHub in case people are interested, but this time around I am leaving some of the code blocks visible to showcase the VancouvR package and give people an idea what it looks like, and as advertisement for more people to come work with Vancouver data.\nFirst up, let’s check for parking ticket related datasets.\nBefore accessing the data it’s often useful to take a peek at the metadata to get and overview of what to expect from the dataset.\nWe are only interested in parking tickets related to fire hydrants. One great feature of the new City of Vancouver Open Data API is that we can do some basic summary statistics on their server, greatly simplifying and speeding up the analysis. The API accepts some SQL-like dialect that allows to specify basic where and group_by clauses, as well select statements that in our R package default to simply counting the number of rows.\nWe learn that 9510 non-voided or disputed tickets have been issued for “STOP WITHIN 5 METRES OF A FIRE HYDRANT”. Armed with that knowledge, we now query more detailed data on all these tickets. One hiccup is that the datasets for different time frames are inconsistently formatted, turning off automatic type-casting based on the inconsistent metadata makes it easier to work with the data.\nSo the City has issued around 3,000 tickets a year for parking within 5 metres of a fire hydrant. The last data entry we have for 2019 is from 2019-09-30, so there is still time for that number to grow. The Parking Bylaw calls for a $100 penalty for parking within 5 meteres of a fire hydrant, as measured along the curb from the closest point to the hydrant. So that comes out to about $300k a year in fines for blocking fire hydrants in the City of Vancouver.\nNext up, lets check the top 5 most heavily ticketed fire hydrants in our 9 year period.\nLooking at the list we immediately notice something odd. Numbers 2 and 4 appear to be the same block and street, just written differently. The addresses aren’t properly normalized, we will have to do some data cleaning work first. 麻煩! We hide the code for that behind a function call.\nThe clear winner is the one on the 2100 block of W 40TH AVE. Checking Google Street View, there is only one on the block. It was hard to find because – two cars blocked the view on it.\nNext up are the ones on the 1100 block of Haro St, closely followed by the 400 block on Keefer St.\nTo see how things have evolved over time we can check how they fared over the years.\nThe winner each year is from our overall top 5 list. Current front runner for 2019 is the one on the 5600 block of Ormidale St, which deserves a closer look.\nIt looks like the hydrant was a fairly low-key affair until 2014, when it dropped off the map and then took off around 2018. A quick check with Google Street View indicates that this sits in front of a new development. Checking through the timeline, the site shows a house in May 2009, which has been torn down by June 2012, although it is still possible to illegally park in front of it. By May 2014 and July 2014 the neighbouring house is gone too and there is a hole in the ground with heavy machinery digging a foundation, but cars can still illegally park there. In June 2015 and May 2016 it’s a full-on construction site with no options to park illegally any more. August 2017 marks the end of construction and the first people seem to have moved in. And the parking tickets start ramping up, with many more people trying to park on the street now.\nLastly, let’s get a high-level view on all the parking tickets issued throughout the city. But here things get a little ugly as we have to first geocode the blocks, and we will hide the code from now on.\nEspecially the fire hydrants that are attracting lots of tickets should probably receive a review by the engineering department. While people should pay more attention to where they park, there are some straight-forward ways to make things easier. Simply paining the curb red will probably fix this for most hydrants and make sure they are free of obstructions and easy to access in case of a fire."
  },
  {
    "objectID": "posts/2019-12-09-fun-with-parking-tickets/index.html#more-parking-tickets",
    "href": "posts/2019-12-09-fun-with-parking-tickets/index.html#more-parking-tickets",
    "title": "Fun with parking tickets",
    "section": "More parking tickets",
    "text": "More parking tickets\nFire hydrants are just one way to get a parking ticket. We can of course continue this and see what areas got the most overall tickets. And for what reason. We will concentrate on the tickets issued 2017 to 2019.\n\nThe distribution of tickets across the city is fairly consistent across years, with total ticket counts peaking in downtown, as well as the central Broadway corridor and in Kits.\nTo understand these patterns better, it is useful to look at the top reasons parking tickets have been issued.\n\nThe presence of parking meters clearly plays a role in parking tickets with a total of 596,944 out of the 1,074,220 infractions referencing some kind of violation involving a parking meter.\n\nThere seems to be a clear relationship between the number of tickets and the number of parking meters in each area. We can normalize the meter-related parking tickets by the number of meters in each area to give us a count on the average number of tickets per meter in each of the areas.\n\nThis shows a much more uniform pattern, with a clear outlier in Strathcona which might be worth looking into further.\nNow that we have some understanding of meter-related tickets, we can take a look at the remaining non-meter related tickets.\n\nThe remaining tickets distribute quite well over a range of categories.\n\nGeographically, highly ticketed blocks cluster in the downtown core and surrounding areas, and spill out along commercial corridors. One can’t help but notice the correlation with meter locations, possibly due to ticketing officers focusing their efforts on those areas."
  },
  {
    "objectID": "posts/2019-12-09-fun-with-parking-tickets/index.html#next-steps",
    "href": "posts/2019-12-09-fun-with-parking-tickets/index.html#next-steps",
    "title": "Fun with parking tickets",
    "section": "Next steps",
    "text": "Next steps\nThat’s a wrap for tonight’s quick run-through on how to use our new-ish VancouvR package to easily access Vancouver Open Data. As usual, the code is available for anyone to download and adapt for their own purposes."
  },
  {
    "objectID": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html",
    "href": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html",
    "title": "2019 CMHC Rental Market Survey",
    "section": "",
    "text": "Finally the new 2019 CMHC Rms data is out. As expected, the high-level numbers are pretty bleak. For Metro Vancouver the vacancy rate inched up a tiny bit from 1.0% in October 2018 to 1.1% in October 2019. In the City of Vancouver the vacancy rate similarly crept from 0.8% in October 2018 to 1.0% in October 2019.\nWith the slight uptick in vacancy rate, both areas saw somewhat lower rents increases, with the (nominal) fixed-sample rent increase in the year before October 2019 clocking in at 4.7% and 4.6% for the Metro Vancouver and the City of Vancouver, respectively. That compares to 6.1% and 6.0% in the year before that. But the vacancy rate is still very low and the rent increase is high."
  },
  {
    "objectID": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#rent-change-and-vacancy-rates",
    "href": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#rent-change-and-vacancy-rates",
    "title": "2019 CMHC Rental Market Survey",
    "section": "Rent change and vacancy rates",
    "text": "Rent change and vacancy rates\nTime to update our rent change vs vacancy rate graph that explores how those to variables relate throughout Canada. All we need to do is copy-paste the code from last year and re-run it to pull in the new data point.\n\nThat’s just one more data point compared to last year, but we tight, and generally tightening, vacancy rates across the country, with only Calgary and Edmonton exhibiting vacancy rates conducive to stable or slightly dropping inflation-adjusted rents. While Vancouver’s fixed-sample rent increase is not any more as extreme as in the previous three periods, Toronto’s climbing fixed-sample rent increase is troubling."
  },
  {
    "objectID": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#moving-penalty",
    "href": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#moving-penalty",
    "title": "2019 CMHC Rental Market Survey",
    "section": "Moving penalty",
    "text": "Moving penalty\nThe moving penalty is the difference the average renter can expect to pay if they were looking to, or were forced to move. This year we are opting for a different way to visualize the data that keeps the total rents in perspective, and also takes note of the quality of the CMHC rent estimates.\n\nWe notice big jumps in the “hot” rental market of the greater Toronto area and the BC metros. This should not be unexpected as a tight rental market, together with rent control with vacancy decontrol, is a recipe for a heightened moving penalty.\nHaving a higher moving penalty is problematic for many reasons, it reduces renter mobility, so renters may be less inclined to downsize or move for a new job. And it incentivises landlords to evict tenants to bring rents in rent controlled units up to market rents. The combination of lower overall mobility, together with landlord incentives to evict, may explain part of the high number of “forced moves” in BC that we have discussed before.\nIn general, the sample of the units with “vacant rents” is comparatively small, especially in markets with high vacancy rates. This may affect the overall quality of the estimates, and especially numbers with lower CMHC quality indicators should be viewed with extra caution.\nAs discussed before, the mix of vacant units may not be representative of the mix of occupied units so this is not a clean apples-with-apples comparison. But it still gives us a decent overall overview of the moving penalty."
  },
  {
    "objectID": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#update-2020-01-17",
    "href": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#update-2020-01-17",
    "title": "2019 CMHC Rental Market Survey",
    "section": "Update (2020-01-17)",
    "text": "Update (2020-01-17)\nDue to popular demand, here is a quick update of the original moving penalty graph from last year."
  },
  {
    "objectID": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#upshot",
    "href": "posts/2020-01-15-2019-cmhc-rental-market-survey/index.html#upshot",
    "title": "2019 CMHC Rental Market Survey",
    "section": "Upshot",
    "text": "Upshot\nThere is still lots of things to look at in this Rms release, this is just scratching the surface. But it will have to do for a quick morning post. As usual, the code is available on Github for those wanting to reproduce the analysis or build on it. The data with rents for vacant units is a custom extract kindly provided by Keith Stewart at CMHC, and I posted it on online if others also want to play with it."
  },
  {
    "objectID": "posts/2020-01-26-unoccupied-dwellings-data/index.html",
    "href": "posts/2020-01-26-unoccupied-dwellings-data/index.html",
    "title": "Unoccupied dwellings data",
    "section": "",
    "text": "Cities like Vancouver and Toronto talk a lot about unoccupied dwellings. We have a whole category for empty homes themed posts on this blog. Do we need one more? Probably not, except that we were able to open up an empty-homes related cross-tabulation that we needed through current work for CMHC. Yay, and big thanks to CMHC for making this available to the general public. Open data FTW!\nPossibly more useful is the classification of the entire building stock by structural type that this data contains, when in the past many have used the classification of the stock occupied by usual residents as a proxy that comes with the standard release census data.\nGiven how important housing has become, and how much interest unoccupied dwellings has been gathering across Canada, we want to quickly introduce the dataset, show how to access it via CensusMapper for mapping and via the CensusMapper APIs and through cancensus for analysis."
  },
  {
    "objectID": "posts/2020-01-26-unoccupied-dwellings-data/index.html#upshot",
    "href": "posts/2020-01-26-unoccupied-dwellings-data/index.html#upshot",
    "title": "Unoccupied dwellings data",
    "section": "Upshot",
    "text": "Upshot\nThat’s it for today. The data does not add much new insight, but it does at least give more accurate accounting of the number of unoccupied units in each region. Maybe that will reduce the number of times people will use dwellings not occupied by usual residents as a proxy for unoccupied units, denying students and other that think of the main residence being elsewhere their place in this city.\nMore data on housing is generally a good thing, and hopefully opening this up will enable others to dive deeper into the issues. As always, there are numerous caveats to keep in mind when dealing with this data. In particular, the re-classification of dwellings 2001-2006 had a profound impact on overall estimates of unoccupied dwellings. In areas with secondary suites like Vancouver, this re-classification had ripple effects through the other census periods, somewhat muted in 2006-2011 but again clearly visible 2011-2016 as the census discovered more suites. This dataset makes it easier to tack down causes of changes in unoccupied dwellings, this is one of them.\nDwellings occupied by foreign residents or temporarily present persons is a category that is notoriously hard to track. These are people that think of their primary residence being elsewhere. We know that students make up an important constituency of these, in some areas like for example Waterloo with it’s high student population and strong off-campus student housing market the impact can be significant. The 2006 census saw a jump in dwellings occupied by foreign residents or temporarily present persons, with a decline again in 2011. Caution is advised when interpreting these numbers.\nThese are just two bigger caveats to keep in mind, in general one will find more quirks to deeper one works with the data. Looking forward to seeing this data getting put to use."
  },
  {
    "objectID": "posts/2020-02-02-disaster-response-maps/index.html",
    "href": "posts/2020-02-02-disaster-response-maps/index.html",
    "title": "Disaster response maps",
    "section": "",
    "text": "To help deal with the response effort in the recent Newfoundland and Labrador show storm, StatCan created vulnerability maps of the most affected areas. It’s great to see StatCan putting their data to use. After seeing this fly by on twitter and then flagged by Simon on Linkedin I had some thoughts on this that might be worth a quick blog post.\nI am biased in how I think of these kind of disaster response maps via the tools that I have developed for working with StatCan census data, namely CensusMapper interactive mapping platform and the the cancensus R package.\nStatCan made four (PDF) maps, they mapped the share of seniors, share of people living along, share of buildings in disrepair and the Residential Instability Index. All of these are built on readily available census data, and the first three maps can be created as Canada-wide interactive maps in CensusMapper by anyone within minutes, as Simon pointed out. I have not imported the Canadian Index of Multiple Deprivation that underlies the maps, mostly because it was only done at the DA level hasn’t been pre-computed for higher aggregation levels that would be necessary for seamless mapping in CensusMapper. But there are other reasons which we will get to a little later.\nWeb maps aren’t always the best solution, paper maps have real value in a disaster region with possibly limited internet connectivity. Using the cancensus package we can create reproducible versions of the PDF maps that StatCan built. However, reproducibility is not that important, but these maps are very adaptable as a fringe benefit. We can easily produce the same maps for any other region in Canada by simply changing one line of code to shift the window of what we are mapping.\nLet’s step through this process and see how to do this in CensusMapper and also using cancensus"
  },
  {
    "objectID": "posts/2020-02-02-disaster-response-maps/index.html#censusmapper",
    "href": "posts/2020-02-02-disaster-response-maps/index.html#censusmapper",
    "title": "Disaster response maps",
    "section": "CensusMapper",
    "text": "CensusMapper\nEveryone can make “simple” maps on CensusMapper. Simple maps are maps based on single census variables, or shares built off of single census variables. People can save maps and share them with others after singing up for a free account. Here is how this works:\n\n\n\nCensusMapper maps are automatically Canada-wide, the geographic aggregation level that data is displayed at adjusts according to the zoom level. Maps for other census variables can be made the same way."
  },
  {
    "objectID": "posts/2020-02-02-disaster-response-maps/index.html#cancensus",
    "href": "posts/2020-02-02-disaster-response-maps/index.html#cancensus",
    "title": "Disaster response maps",
    "section": "Cancensus",
    "text": "Cancensus\nThe cancensus package ties into the CensusMapper data APIs to facilitate targeted data import, analysis, and visualization in R. The CensusMapper API tool provides a GUI interface for region selection and variable discovery.\nlibrary(tidyverse)\nlibrary(cancensus)\ncensus_data &lt;- get_census(dataset='CA16', regions=list(CMA=\"10001\"), geo_format='sf', level='DA', \n                          vectors=c(disrepair=\"v_CA16_4872\",seniors=\"v_CA16_244\",\n                                    alone=\"v_CA16_419\",pop_private_hh=\"v_CA16_424\")) %&gt;%\n  mutate(share_disrepair=disrepair/Households,\n         share_alone=alone/pop_private_hh,\n         share_senior=seniors/Population) \n\nggplot(census_data) +\n  geom_sf(aes(fill=share_disrepair)) +\n  st_johns_theme +\n  scale_fill_viridis_c(option = \"magma\",trans=\"log\",\n                       labels=scales::percent,breaks=c(0.01,0.025,0.05,0.1,0.2),\n                       na.value = \"darkgrey\") +\n  labs(title=\"Occupied private dwellings in need of major repairs\",fill=NULL)\n\nWe can similarly produce the other two maps."
  },
  {
    "objectID": "posts/2020-02-02-disaster-response-maps/index.html#residential-instability-index",
    "href": "posts/2020-02-02-disaster-response-maps/index.html#residential-instability-index",
    "title": "Disaster response maps",
    "section": "Residential Instability Index",
    "text": "Residential Instability Index\nplot_data &lt;- census_data %&gt;%\n  left_join(get_multiple_deprivation_index_data(),by=\"GeoUID\")\n\nggplot(plot_data) +\n  geom_sf(aes(fill=as.character(`Residential instability Quintiles`))) +\n  st_johns_theme2 +\n  scale_fill_brewer(palette = \"Purples\",na.value=\"grey\") +\n  labs(title=\"Residential instability index\",fill=\"Quintile\")\n\nBut what’s behind the Residential Instability Index? It’s a superposition of several census variables:\n\nProportion apartment housing units\nProportion of dwellings that are owned\nProportion living alone\nProportion of residential mobility (same house as 5 years ago)\nProportion of the population that is married/common-law\n\nUnfortunately the StatCan user guide does not detail the proportions used to generate the index. Knowing these would make it trivial to map the index Canada-wide on CensusMapper. This is another reason why we have not added the Multiple Deprivation data, it does not add much new data. The only thing we are missing is the handful of weights and the quintile binning."
  },
  {
    "objectID": "posts/2020-02-02-disaster-response-maps/index.html#upshot",
    "href": "posts/2020-02-02-disaster-response-maps/index.html#upshot",
    "title": "Disaster response maps",
    "section": "Upshot",
    "text": "Upshot\nThe StatCan maps used for the disaster recovery effort can already be built by anyone within minutes. Using CensusMapper to generate interactive Canada-wide web maps, or using cancensus to generate print-quality maps. Using cancensus we also have the advantage of having reproducible code that can be easily adapted for any other region in Canada. All that’s needed is to switch out the region we pull the census data from and the bounding box we want to restrict the map view to. We could work a little harder to add annotations, or switch over to a desktop GIS software and spent time styling the map in more detail if that’s desirable. There is no added value to have StatCan do this work."
  },
  {
    "objectID": "posts/2020-02-02-disaster-response-maps/index.html#opportunities",
    "href": "posts/2020-02-02-disaster-response-maps/index.html#opportunities",
    "title": "Disaster response maps",
    "section": "Opportunities",
    "text": "Opportunities\nWhere StatCan could add tremendous value however is to generate a Deprivation Index that is built up from individual level census data to estimate vulnerability. Take the example of our first three maps, looking at senior population, people living alone and buildings in disrepair. Imagine one DA with a third of the population seniors, a third of the population living alone and a third of the dwelling units in disrepair. How vulnerable is the population in that area? It depends.\nIf most of the seniors live in extended families, most of the people living alone are students in apartments, and most of the homes in disrepair are taken up by working age couples, things aren’t too bad. Lots of people are vulnerable in some way, but probably have enough resources to deal with things to some extent.\nBut if most of the seniors are living alone in homes in disrepair, we have a real problem. The other thirds of the population are doing fairly well, but that one third is extremely vulnerable and probably need help fast.\nLooking at single variables and overlaying them in some way, including the way it’s done in the Multiple Deprivation dataset, can’t identify the severity of the vulnerability faced by some people, and severity really matters. But StatCan does have the means to build up an index that looks at severity, but leveraging access to individual level census data. We already do this for example in the definition of Core Housing Need, where we also separate out how many people fail to meet at least one of the housing standards, and how many fail to meet two or all three. Which gives a measure of severity.\nSimilarly, StatCan could look at how many people fail one vulnerability metric, and how many fail two or three or more at the same time. This gives us a much better picture of vulnerability that can guide recovery operations."
  },
  {
    "objectID": "posts/2020-02-02-disaster-response-maps/index.html#outdated-data",
    "href": "posts/2020-02-02-disaster-response-maps/index.html#outdated-data",
    "title": "Disaster response maps",
    "section": "Outdated data",
    "text": "Outdated data\nAnother point to mention is that the census data is now almost 4 years old. It still can serve as a decent proxy in areas that did not change too much, but using these maps we will be flying in the dark in areas where lots of new development came in. We are working on ways to make census data more up-to-date, more on that soon. Construction data, as well as annual tax data can both play a role here. But this will only work well for some metrics, and is harder to do for others. And our upcoming project only goes down to census tract level geographies, which is of limited use in disaster recovery efforts. This provides yet another opportunity for StatCan to show leadership and leverage their much better data.\n\nAs usual, the code for this post is available on GitHub if others want to adapt this for their own purposes."
  },
  {
    "objectID": "posts/2020-03-03-overnight-visitors-and-crude-travel-vectors/index.html",
    "href": "posts/2020-03-03-overnight-visitors-and-crude-travel-vectors/index.html",
    "title": "Overnight Visitors and Crude Travel Vectors",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nThe spread of Coronavirus is reminding us of just how often people travel around, especially as various locations become quarantined and international travel corridors get shut down. So let’s take a look at some basic data on travel patterns here of relevance to us here in Vancouver. Then we’ll put them back in the context of Coronavirus.\nTLDR: travel data is really interesting, don’t be frightened of travellers, and there’s still a lot we don’t know about coronavirus\nWe’ve looked at the movement of people before in terms of migration, immigration and commuting patterns. But these are movements that are either regularized, everyday, and routine (e.g. commuting) or shuffle people between one settled set of routines and another (e.g. migration). Travel data gives us something different, representing something more like the unsettled movement of people. People travel for work, to visit family, and of course, for tourism. The Tourism Industry is interested enough in travel data that they ask Statistics Canada to compile data for them. Stats Canada combines Canadian travel surveys and border crossing administrative data to get us a decent look at overnight stays. So it is that we get overnight stayer data for Vancouver!\nLet’s look at where people are visiting Metro Vancouver from. The Tourism Vancouver data has an interesting selection of countries available, with special breakdowns for Canada and the USA. More than a quarter of all overnight stays in Metro Vancouver are trips from elsewhere in British Columbia. Another quarter plus of trips arrive from elsewhere in Canada, with Ontario and Alberta leading the way. The USA accounts for just under a quarter of overnight visits. Altogether, Canada and the USA account for over 8 million of the roughly 10 million visits. Most American visitors to Metro Vancouver arrive from nearby neighbours down the Pacific Coast (WA, OR, CA), which together account for over half of travel from the USA. About as many people visit from all of Mexico as from nearby Oregon (140k).\nOf the slightly less than two million international visitors from beyond NAFTA borders, a little over half arrive from Asian/Pacific countries, with most of the remainder from Europe. China, the UK, and Australia, Japan, India, and Germany each accounted for more than 100k visitors in 2019, South Korea, Hong Kong, and Taiwan not far behind. Let’s put all these flows together on a map.\nOf some concern, lots of the places identified above have had recent outbreaks of Coronavirus. We’re still in early days of tracking the virus. And we know it’s already having major effects on travel. But can we look at current prevalence estimates and recent travel patterns to give some insights into crude vector risks for Metro Vancouver? Maybe. But it’s worth keeping in mind that everything is still pretty much up in the air in terms of what we know!\nFirst let’s look at up-to-date active confirmed Coronavirus cases drawing on data collected at Johns Hopkins.\nWuhan, of course, appears as the centre of the outbreak, and Hubei Province in China contains most of the active confirmed cases to date (as of March 03, 2020!) The number of cases is important to track, obviously, and the starting point for healthcare workers and epidemiologists alike. But focusing on these numbers can provide a misleading impression of how widespread the Coronavirus has become. So let’s come up with a crude estimate of prevalence instead of case numbers. Here we’re going to use active confirmed cases as our starting point. Another option is to track all confirmed cases, including those who have recovered (no longer testing positive) or died from coronavirus. But active confirmed cases might arguably give us a better sense of current spread.\nWe can plot the evolving nature of active confirmed cases in terms of prevalence estimates across places, effectively dividing total number of active confirmed cases by population for our data reported so far. Setting this to motion, we can track outbreaks by prevalence across time. Even just looking at active confirmed cases, we get a sense that recorded prevalence has recently stopped climbing for Hubei province. Meanwhile, outbreaks in South Korea, Iran, Hong Kong, and the nearby state of Washington continue to grow. Also worth noting, some countries (e.g. South Korea) seem to have a better handle on testing the virus, providing better confidence in their numbers. The numbers coming out of other locales (Iran and the USA) seem far less reliable, either because of inconsistent testing, untrustworthy reporting by officials, or both. This sets a real limit on what we can know so far.\nOverall it needs to be stressed that - given the numbers we have so far - the prevalence of coronavirus is still very low. Even in Hubei province, the centre of the outbreak, not much more than a single active confirmed case per thousand people has been confirmed. Comparing locations of cases to surrounding populations, most places around with the world with outbreaks still see only about one active confirmed case per hundred thousand people. Even setting aside the hyper-cautious mood around the world and its effects on travel, if you met a visitor from one of these places in Metro Vancouver, fairly unlikely that they would be a carrier. There’s little reason to be scared of individual travellers!\nBut what about travel patterns writ large? Surely even if any individual presents a very low risk as a vector, by sheer number, the masses of people travelling through Vancouver from places with coronavirus outbreaks represent a risk. Indeed, that’s how the coronavirus has spread so far. We can very crudely estimate this risk by setting a base likelihood that each individual traveller from a given outbreak location is coronavirus-free (1 - cases / population). In other words, we might use currently active confirmed cases as our measure of prevalence, estimating we can be 99.99975% certain that a given traveller from Washington State will not be a carrier for coronavirus. But what if a LOT of people travel from Washington? Then we exponentiate 99.99975% by the number of visitors (126,493 for the first three months of 2019 as a proxy) to come up with an estimate that none of these travellers carry the virus (we really should be drawing without replacement here, but this is a good approximation), with the complement giving a rough estimate of at least one visitor being a carrier. This comes out at 27% using our current estimates. This only considers Washington residents travelling to Vancouver and still neglects Vancouver residents travelling to Washington and getting infected there. And it relies on current active confirmed cases, it does not include active but not yet confirmed cases. And it assumes travel patterns similar to a year ago. Still, it provides us with a measure of vector risk to Metro Vancouver that combines risk of coronavirus with travel volumes.\nLet’s run with this for recent coronavirus outbreak data based on travel volumes similar to past years - EXCEPT excluding cases from Hubei province in China after January 23rd (when the quarantine went in place). What does our crude evolving overnight travel vector risk look like?\nHere we can see rapidly changing vector possibilities. Conditions are changing fast! Still, it’s hard to know how much to trust these numbers. Given what we understand about testing at the moment, it’s likely we’re still overstating the risk from high quality testing locales (South Korea), as well as understating the risk from places where testing has been poor (Washington) and places where we don’t have any visitor data at all (Iran). We’re also missing current data on how travel is changing as well as data on where people from Metro Vancouver are travelling, which is a big deal given that most of our cases so far represent returned travellers from abroad.\nHere is a still of the most recent snapshot as of the writing of this."
  },
  {
    "objectID": "posts/2020-03-03-overnight-visitors-and-crude-travel-vectors/index.html#upshot",
    "href": "posts/2020-03-03-overnight-visitors-and-crude-travel-vectors/index.html#upshot",
    "title": "Overnight Visitors and Crude Travel Vectors",
    "section": "Upshot",
    "text": "Upshot\nSo here are the big takeaways from our exercise: 1) Visitor data to Metro Vancouver is actually really interesting, even for those outside of the tourism business. 2) Don’t shun travellers from abroad! The likelihood of anyone you meet, even coming from an outbreak centre, being a carrier of coronavirus is very, very low. 3) The combination of travel patterns plus coronavirus prevalence gives us some interesting ways to model evolving vector risks in Metro Vancouver. 4) But it’s not clear how much we should trust our data. Travel patterns have surely altered, and we need better coronavirus testing fast, especially in places like Washington State.\nOverall, integrating travel data with coronavirus data may, if nothing else, help people and agencies prepare and plan better. Practically any planning is better than some of the ad hoc decisions being made out there, as when American Airlines suspended its flights to Milan only after pilots refused to fly there. For most people, the important thing is to listen to local health agencies, like the BC Centre for Disease Control, wash your hands, and be kind to those around you, wherever they come from.\nAs usual, the code for the post is available on GitHub in case anyone wants to refine or adapt it for their own purposes."
  },
  {
    "objectID": "posts/2020-03-03-overnight-visitors-and-crude-travel-vectors/index.html#update-2020-03-04",
    "href": "posts/2020-03-03-overnight-visitors-and-crude-travel-vectors/index.html#update-2020-03-04",
    "title": "Overnight Visitors and Crude Travel Vectors",
    "section": "Update (2020-03-04)",
    "text": "Update (2020-03-04)\nFor a look at how the professionals are joining international travel data to coronavirus data, see Gardner (et al) (now unfortunately outdated!)"
  },
  {
    "objectID": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html",
    "href": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html",
    "title": "Behaviour change in response to COVID-19",
    "section": "",
    "text": "With COVID-19 cases growing exponentially, Canada has introduced sweeping restrictions to curb the spread of the virus. People are asked to practice social distancing, work from home if possible, keep shopping trips to a minimum, keep a distance of at least 6 feet to people outside of their household, universities and schools have been closed, and travel has been restricted."
  },
  {
    "objectID": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#why-social-distancing",
    "href": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#why-social-distancing",
    "title": "Behaviour change in response to COVID-19",
    "section": "Why social distancing?",
    "text": "Why social distancing?\nJust in case it’s not clear what the problem is, let’s take a look at the trajectory we are currently on. John Burn-Murdoch has been doing excellent graphing of case trajectories at the Financial Times, we will adapt this method to understand Canada’s situation, focusing on other western countries as comparables.\n\nThe log scale graph is useful to capture the exponential nature of the growth of cases. All these countries lie roughly in the band of doubling cases every 3 days. “Flattening the curve” means lowering the slope of the trajectory, “planking” means to get the slope to be horizontal.\nCanada falls right into this band of doubling every 3 days, but is roughly 6 days behind the UK, 9 days behind the US and Spain, 11 days behind France and Germany and 17 days behind Italy. Some countries are showing evidence of modest flattening, for example the trajectory of the last 6 days in Italy is displaying a slightly lower slope (growth rate) than the previous trend. But that’s only a small consolidation, especially given how far out on the curve Italy is. None of these countries come close to “plank” their curve yet.\nLooking at deaths is another way to compare countries and see how we are fairing.\n\nStarting at lower cutoff of 10 deaths adds to the volatility of the graph, especially at the beginning. But countries further ahead seem to again approach the slope of doubling every 3 days. If Canada follows the same trend of doubling every 3 days, then Canada is 4 days behind Germany, 7 days behind the UK, and 12 days behind France.\nAfter initial seeding, the exponential growth of the virus is driven by local transmissions. In Canada we are now seeing internal travel restrictions, it makes sense to also take a look at local growth rates. Unfortunately, Canada does not publish official data, but the COVID-19 Canada Open Data Working Group has manually assembled Canadian data and made them available for others to use.\n\nDrilling down even further we can look at health regions.\n\nAs we drill down into smaller regions, the numbers get more volatile, but they help in understanding how the local growth progresses. We start at a fairly low cutoff of 20 and 10 cases to get decent length timelines.\nThe slope of these graphs corresponds to the growth rate of confirmed cases, the stabilize around a doubling every 4 days. Differences in slope can indicate different local rates of growth, but they can also be impacted by changes in testing. Confirmed cases only make up a portion of all infections, some go undetected. For example, group of US scientists estimated that about 85% of infections in Wuhan were undocumented. Detection rates will vary across countries, but a difference in detection rates that’s constant over time will not impact the growth rates in our log-graph. But when the detection rates change over time, it does impact the slope. This can happen when testing is increasing over time."
  },
  {
    "objectID": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#the-solution",
    "href": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#the-solution",
    "title": "Behaviour change in response to COVID-19",
    "section": "The solution",
    "text": "The solution\nTo deal with the virus we need to slow down the growth rate, ideally to zero. In absence of having a vaccine, our only way to do this is to change people’s behaviour. If people stay away from each other, wash their hands with soap regularly, and don’t touch their face, the growth rate can be slowed down significantly. That’s the idea behind social distancing. But changing behaviour is hard, and the ways people respond to calls for social distancing varies.\nAs we have seen in Europe, and are now seeing in Canada, government will impose successively more restrictive conditions when people don’t follow the social distancing guidelines. And all of this costs time. Time we don’t have."
  },
  {
    "objectID": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#the-laggy-steering-wheel",
    "href": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#the-laggy-steering-wheel",
    "title": "Behaviour change in response to COVID-19",
    "section": "The laggy steering wheel",
    "text": "The laggy steering wheel\nWe have seen early on in the example of Wuhan that measures we are implementing today will only become visible in the data in about two weeks in the future.\nIt should take 2 weeks for shutdowns/distancing to show up in the case numbers, if China is any guide. pic.twitter.com/5qyl0aZfxn— Noah Smith 🐇🇺🇸🇺🇦 (@Noahpinion) March 21, 2020\n\n\n\nChanges in behaviour today means the number of people getting infected tomorrow will be lower than otherwise. These infected people won’t develop symptoms until after on average 5 days, sometimes it can take up to 14 days. These people may watch the symptoms for a couple of days (hopefully self-isolating) until they request a test. Depending on the local protocol, they will get tested within a couple of days. Then it takes a couple of days for the results to come in and get reported (BC did not report any cases yesterday, we will have to wait until 10 am Monday to get the update for Sunday), and then make it into the data feeds and the newspapers for people to see. In other words, we don’t know how effective our behaviour change is until two weeks later.\nThe New York Times has a great article highlighting Italy’s journey to more effective social distancing. People tend to react to what they see right now and aren’t wired to react today to what we expect to see in two weeks time.\nThere are several ways to attack this problem. One is to do a better job explaining why we act now while the numbers still seem relatively low. Another is to reduce the time it takes for infected people to enter the official statistics. We can encourage people to seek testing earlier and increase the turnaround time for tests and set up live centralized data feeds for reporting new cases that makes it easy for news outlets to report in a timely manner.\nIn the meantime we should find alternative live measure of behaviour change. Italy has used cell phone mobility data to understand population-level compliance with social distancing."
  },
  {
    "objectID": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#traffic-as-a-live-measure",
    "href": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#traffic-as-a-live-measure",
    "title": "Behaviour change in response to COVID-19",
    "section": "Traffic As a Live Measure",
    "text": "Traffic As a Live Measure\nOne way to measure real-time behaviour change is to look at traffic. People have already been looking at change in congestion as a metric for how people change behaviour due to COVID-19, others have looked at changes in transit ridership.\nHere in Vancouver we can also look at congestion, but it is hard do estimate changes in traffic from changes in congestion. TransLink aggregate ridership data has a significant lag and only monthly temporal resolution, so it isn’t useful for this.\nA good way to understand change in traffic due to COVID-19 is to look at traffic loop counts. Surrey Open Data makes traffic loop counts available. We already looked at this to estimate the traffic impact of school dropoff and pickup traffic.\n\nWe see that overall 2020 traffic levels were higher than 2019 levels, but that reversed in the past week when 2020 traffic levels dropped.\nTo compare this more easily we can shift the 2019 data forward so that the respective spring break periods match up plot both on the same graph, focusing in on the past two weeks.\n\nHere we see how social distancing has evolved over time. We observe how the 2020 numbers change from exceeding 2019 traffic counts before the spring break weekend to increasing falling below them during the first week of spring break.\nWe can zoom into that even further by computing the percent change in traffic compared to last year and see how that changes over time.\n\nThere is quite a bit of volatility in the data, with a clear successive decrease in traffic volumes in the past week. Behaviour change takes time for people to follow through with, and increasing government restrictions facilitate this too.\n\nAn interesting observation here is that commuting (rush hour) traffic is reduced by at least as much as non-commuting traffic. On the surface this contradicts the observation by John Burn-Murdoch that rush hour congestion decreased more than discresionary (midday) traffic in some cities. But those two might be reconciled by the fact that reduction in “congestion”, as measured by TomTom, is quite different from measuring a reduction in traffic volumes. Removing even a small percentage of traffic at rush hour will lead to a much larger reduction of travel speeds than removing the same share at midday. The coming weeks will tell if this was just a part of the slow adapting to social distancing and midday travel will eventually drop by more than commute traffic."
  },
  {
    "objectID": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#upshot",
    "href": "posts/2020-03-22-behaviour-change-in-response-to-covid-19/index.html#upshot",
    "title": "Behaviour change in response to COVID-19",
    "section": "Upshot",
    "text": "Upshot\nSlowing the spread of the virus is extremely important. And we have to do this while the numbers are still relatively small and the health system is not yet completely overwhelmed. Change is hard when there is such a large gap between behaviour changes and when we will see the results. We need to closely monitor how well people’s behaviour changes at the population level. It’s only a proxy metric for how we are doing in fighting the virus, but we get immediate feedback to guide if governments needs to increase restrictions to further force behaviour change.\nSome behaviour change, or lack thereof, is obvious. The images and videos of people playing basketball or soccer, having parties at the beach, or failing to practice appropriate social distancing walking along the crowded seawall, all are making the rounds on social media, and are testament to that. It is understandable that it can be hard to extrapolate individuals’ behaviour to the population level, it’s hard to quantify the extent of it. We need more systematic ways of tracking this, relying on social media reports to bubble up and trigger piecemeal action is better than nothing, but it’s a far cry from a holistic response. That’s where metrics like traffic patterns, or even better, cell phone mobility data as used in Italy, become useful. They tell us at the population level how well social distancing is practised.\nNot everything can be monitored, and not everything needs to be monitored. A selection of several key metrics would probably suffice to get a good holistic picture.\nFor example, Taiwan will monitor compliance with self-quarantine rules for travellers and other people needing to self-quarantine by monitoring their cell phone location. These measures may seem overly intrusive, but they have, together with a host of other measures, enabled Taiwan to keep schools open and people going to work as normal.\nThere are countries that go even further and conduct individual level monitoring. Epidemological models of the virus tell us that interventions like social distancing will have to be kept up for very long times, otherwise we will again end up with rising cases. Democratic countries like Taiwan and Singapore have shown that there are other ways to keep the virus in check. Both countries have massive population hygiene education campaigns facilitating collective behaviour change, including measuring temperature prior to leaving home for school or work, temperature measurements before entering stores or large public spaces, submitting to intrusive surveillance of self-quarantine of all incoming travellers, and combining mobility data with aggressive (and intrusive) data-driven contact tracing. These intrusive measures buy other freedoms, kids can go to school, people can go to work and life functions somewhat normally.\nRight now we are sacrificing basic freedom of movement and assembly, freedom to go to school or to work. Taiwan and Singapore show that we can regain some of these by giving up other freedoms and submitting to some level of surveillance. Behaviour change is key, but on top of that we need to make tough choices what freedoms we are willing to give up and which ones to keep. At least until we have a vaccine available that can be safely deployed at scale."
  },
  {
    "objectID": "posts/2020-04-10-covid-19-data-in-canada/index.html",
    "href": "posts/2020-04-10-covid-19-data-in-canada/index.html",
    "title": "Covid-19 data in Canada",
    "section": "",
    "text": "With social distancing and travel restrictions in full swing in Canada, everyone wants to know if it’s working. And when and how we can start to loosen some of the restrictions. To answer these questions we need data.\nCanada has been extremely slow to make covid-19 related data available, only at the end of March did Canada get official government dataset for confirmed case counts and deaths by province and date. Shortly after Statistics Canada published individual case data with demographic and hospitilization information (since replaced by a new timeseries), but that data only covers about half of all cases in Canada. Why only half? Because Health Canada, the national health authority from which the StatCan data originates, can only publish data “for which a case report form has been received by the Public Health Agency of Canada from provincial or territorial partners”. And it is missing a lot of data.\nMoreover, the official data sources are about a day behind in reporting. Which brings us straight to the main issue when dealing with covid-19 related data."
  },
  {
    "objectID": "posts/2020-04-10-covid-19-data-in-canada/index.html#lag",
    "href": "posts/2020-04-10-covid-19-data-in-canada/index.html#lag",
    "title": "Covid-19 data in Canada",
    "section": "Lag",
    "text": "Lag\nWhen we want to understand the effect of social distancing and other measures, we want to know how it affects the infection rates. But we can’t measure these directly. Typically the first time we get a hunch of an infection is when someone develops symptoms consistent with covid-19. Typically this happens about 4 to 5 days after contracting the virus, but can sometimes take up to 14 days. That onset of symptoms date is the best date we have to see if social distancing is working, and it lags by 4 to 5 days. After experiencing symptoms, the infected person might seek to get tested, actually get tested, the test result will come out, and the result will get reported.\nThe time it takes from onset of symptoms to results getting reported can be reduced. We can encourage people with symptoms to seek testing fast, and we can try to test people as fast as possible and promptly report the results.\nI have not seen studies how long this process takes in Canada, and it probably does not even make sense to ask this question. And the reason is the same as Health Canada not having demographic data on about half of all cases. The Canadian health case system is very fragmented, and Canada has not established a unified framework how to deal with covid-19 cases. The lag between onset of symptoms and reporting of confirmed cases likely varies considerably across health authorities and provinces.\nOntario has released case data with the onset of symptoms (possibly estimate). Unfortunately the dataset does not contain the date each case was reported, so we can’t compute the lag directly for each case. But we can compare that to the data on reported cases by date.\n\nThe aggregate data still shows clearly that there is a significant lag between onset of symptoms and the reporting of confirmed cases. The data also shows how surges in reporting, as happened in Ontario on April 1st, may not reflect conditions on the ground. But reported cases is the data that’s most commonly available and closely watched by journalists and the public."
  },
  {
    "objectID": "posts/2020-04-10-covid-19-data-in-canada/index.html#testing",
    "href": "posts/2020-04-10-covid-19-data-in-canada/index.html#testing",
    "title": "Covid-19 data in Canada",
    "section": "Testing",
    "text": "Testing\nAnother variable in all of this is testing. Cases only show up in these statistics after people get tested. But the number of tests conducted, and the testing protocols, differ across provinces. And through time. If testing protocol stayed constant over time, and testing increased at the same rate at the spread of covid-19 so that the ratio of confirmed cases to all infected people stayed constant over time, and other factors like reporting lag also stayed constant, then growth of confirmed cases would equal the growth of true infected cases. Even if the total count of infected cases would be a lot higher than the ones confirmed positive.\nBut testing protocol changes, which introduces bumps in the growth of confirmed cases. And testing does not keep up with the growth of covid-19, which means the growth rate of cases likely under-estimates the growth rate of infections.\n\nDespite all this, growth rates of confirmed cases is still often used as a proxy for growth in overall infections. The main reason is that it’s the least laggy metric we have. Other metrics we have, like deaths or hospitalizations, lag more. And the impact of change in test protocol often leads to a distinct short temporary bump in growth rates. Falling behind in testing may have effects that are harder to detect, but effects may be less impactful that one might naively expect."
  },
  {
    "objectID": "posts/2020-04-10-covid-19-data-in-canada/index.html#upshot",
    "href": "posts/2020-04-10-covid-19-data-in-canada/index.html#upshot",
    "title": "Covid-19 data in Canada",
    "section": "Upshot",
    "text": "Upshot\nRight now we need to focus on social distancing to get the spread under control. But at some point we will want to start lifting restrictions and switch into containment mode. To determine when and how we should do this, and in order to be effective at containment, we need better data. It does not appear that Canada is set up for this. Hopefully that will change.\nAs usual, the code for the post is available on GitHub in case anyone wants to refine or adapt it for their own purposes."
  },
  {
    "objectID": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html",
    "href": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html",
    "title": "Census tract level T1FF tax data",
    "section": "",
    "text": "Recently we added a cross-tabulation of Structural type by Document type to CensusMapper for mapping and API use, including in our {cancensus} R package. Today we added another datasets to this.\nBoth of these datasets come through a project we are currently doing with CMHC, and we are excited to be able to turn these into open data, free for anyone to use.\nWhat makes this new tax data particularly useful is that they have annual frequency and come at fine geographies. This can help fill in gaps between the censuses. And most importantly provide more recent data than the last census. T1FF Taxfiler data has very good coverage of the overall population, the 2017 tax data covers about 95.6% of the Canadian population, with other tax years having similarly high coverage.\nThe data is now available on CensusMapper for anyone to map, and is also available via the CensusMapper API for anyone to download. This makes it automatically available through the cancensus R package for convenient use for data analysis.\nIn this post we want to give a brief introduction how to work with the new data in R. The basics are identical how we work with census data. To simplify accessing the data across years we have ensured that variable names are uniform, so it is easy to pull timelines. The taxfiler data for a given year comes on census geographies based on the previous census (with the exception of 2011 tax data that is based on the 2006 geography), so we will have to TongFen the data to create longer timelines. There were some census tract level taxfiler custom tabulations in circulation previous to this release, but not as open data.\nOne important caveat about the census tract level geographies is that the tax data was approximated to that geography via postal codes. This can introduces a mismatch when directly comparing with census data. However that mismatch should be relative stable over time, making direct comparisons of tax data through time quite robust.\nThis data should be useful for researchers, but also for municipalities and other local government bodies, planners, non-profits, and others that are interested in more granular and more timely income and basic demographic data than what’s generally available."
  },
  {
    "objectID": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#example-use",
    "href": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#example-use",
    "title": "Census tract level T1FF tax data",
    "section": "Example use",
    "text": "Example use\nTax data is great for understanding how income changed over time. But it also has basic demographic data on (economic) families and age groups. To make it easier to follow along how to use the data, as well as the tongfen functionality, we will leave the code blocks visible in this post.\nTo start us off we are taking the number of economic families in low income between 2001 and 2017. We use the convenience get_tongfen_census_ct method from the tongfen package to grab the share of people in low income in Vancouver, Musqueam 2 and the UBC peninsula on a common geography.\nvsb_regions &lt;- list(CSD=c(\"5915022\",\"5915803\"),\n                    CT=c(\"9330069.01\",\"9330069.02\",\"9330069.00\"))\n\nyears=seq(2005,2017)\nlico_vectors &lt;- setNames(paste0(\"v_TX\",years,\"_551\"),paste0(\"lico_\",years))\n\nvancouver_lico &lt;- get_tongfen_census_ct(regions=vsb_regions, vectors=lico_vectors, \n                                        geo_format = \"sf\",na.rm=FALSE)\nUnder the hood the TongFen package does the magic of standardizing the different census geographies and aggregating up the data, in this case taking a weighted (by population) average of the share of people in low income. Now we can plot the share of low income people in each region over the years. Data on the share of low income people is only available starting 2005 and not for our full timeline.\nvancouver_lico %&gt;%\n  pivot_longer(cols=starts_with(\"lico\"), names_pattern = \"lico_(\\\\d+)\",\n               names_to = \"Year\", values_to = \"Share\") %&gt;%\n  mutate(Share=Share/100) %&gt;%\n  st_sf() %&gt;%\n  ggplot(aes(fill=Share)) +\n  geom_sf(size=0.1) +\n  scale_fill_viridis_c(option = \"magma\",labels=scales::percent) +\n  facet_wrap(\"Year\") +\n  tax_theme +\n  coord_sf(datum=NA) +\n  labs(title=\"Share of people in low income\")\n\nWe can see some changes over time if we squint hard, simply plotting a series of maps is probably not the best way to show changes over time. Simple timeline graphs do a better job at that, focusing on tracts with high share of low-income people. This is where having the data on a uniform geometry becomes really important, the maps above we could have also drawn based on a changing geography.\nvancouver_lico %&gt;%\n  pivot_longer(cols=starts_with(\"lico\"), names_pattern = \"lico_(\\\\d+)\",\n               names_to = \"Year\", values_to = \"Share\") %&gt;%\n  mutate(Year=factor(Year,levels=years),\n         Share=Share/100) %&gt;%\n  mutate(highlight=GeoUID %in% filter(.,Share&gt;=0.4)$GeoUID) %&gt;%\n  ggplot(aes(x=Year,y=Share,group=GeoUID)) +\n  geom_line(data=~filter(.,!highlight),color=\"grey\") +\n  geom_line(data=~filter(.,highlight),aes(color=GeoUID)) +\n  scale_y_continuous(labels=scales::percent) +\n  tax_theme +\n  labs(title=\"Share of people in low income by census tract\",\n       x=\"Tax year\", y= \"Share of people in low income\")\n\nBut this looses the intuitive geographic context that maps provide. Mapping the percentage point change between 2005 and 2017 in each region is a good way to round this up.\nvancouver_lico %&gt;%\n  mutate(Change=(lico_2017-lico_2005)/100) %&gt;%\n  st_sf %&gt;%\n  ggplot(aes(fill=Change)) +\n  geom_sf(size=0.1) +\n  scale_fill_gradient2(labels=scales::percent) +\n  tax_theme +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in share of people in low income 2005-2017\",\n       fill=\"Percentage\\npoint change\")"
  },
  {
    "objectID": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#other-variables",
    "href": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#other-variables",
    "title": "Census tract level T1FF tax data",
    "section": "Other variables",
    "text": "Other variables\nTax data has a wealth of variables reaching beyond just incomes. Beyond a variety of income metrics it provides information on economic family structure, children in economic families, taxfilers and dependents by detailed age group, share of taxfilers that are married, or that live in apartments, sources of income, child benefits and low income status.\nelementary_age_vectors &lt;-  lapply(seq(2001,2017),function(year){\n    paste0(\"v_TX\",year,\"_\",seq(253,259))\n})  %&gt;% \n  unlist\n\nvsb_children_data &lt;- get_tongfen_census_ct(vsb_regions, na.rm=FALSE,\n                                           vectors = elementary_age_vectors,geo_format = 'sf')\nextract_year_count &lt;- function(d) {\n  d %&gt;% \n    select(GeoUID,starts_with(\"v_\")) %&gt;%\n    set_names(names(.) %&gt;% gsub(\":.+$\",\"\",.)) %&gt;%\n    group_by(GeoUID) %&gt;%\n    pivot_longer(cols=starts_with(\"v_\"),\n                 names_pattern = \"^v_TX(\\\\d{4})_(\\\\d+)$\", \n                 names_to=c(\"Year\",\"Age\"),\n                 values_to = \"count\") %&gt;%\n    mutate(Age=as.integer(Age)-253+5) %&gt;%\n    group_by(GeoUID,Year) %&gt;%\n    summarize(count=sum(count))\n}\n\nleft_join(vsb_children_data %&gt;% select(GeoUID),\n          vsb_children_data %&gt;% \n            st_set_geometry(NULL) %&gt;%\n            extract_year_count(),\n          by=\"GeoUID\") %&gt;%\n  ggplot() +\n  geom_sf(aes(fill=count),size=0.1) +\n  facet_wrap(\"Year\") +\n  scale_fill_viridis_c(labels=scales::comma) +\n  tax_theme +\n  coord_sf(datum=NA) +\n  labs(title=\"Children aged 5-11\", fill=\"# 5-11yo\")\n\nFrom this we again compute total change of number of children aged 5 to 11 in each region across the entire time frame.\nleft_join(vsb_children_data %&gt;% select(GeoUID),\n          vsb_children_data %&gt;% \n            st_set_geometry(NULL) %&gt;%\n            extract_year_count() %&gt;%\n            pivot_wider(id_cols = GeoUID, names_from = Year, values_from = count) %&gt;%\n            mutate(absChange=`2017`-`2001`,relChange=`2017`/`2001`-1),\n          by=\"GeoUID\") %&gt;%\n  ggplot() +\n  geom_sf(aes(fill=absChange),size=0.5) +\n  scale_fill_gradient2(labels=scales::comma) +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in children aged 5-11 between 2001-2017\", fill=\"Change in\\n# 5-11yo\",\n       caption=\"MountainMath, StatCan T1 taxfilers, via CMHC\")\n\nWe note that some of the single year age data for some of the regions has been suppressed. This hints at some of the limits when using this data. To work around that we can repeat the analysis using the 5-year age group of 5 to 9 year olds, which has more robust data.\nget_tongfen_census_ct(vsb_regions, na.rm=FALSE,geo_format = 'sf',\n                      vectors = c(\"2001\"=\"v_TX2001_743\",\"2017\"=\"v_TX2017_743\")) %&gt;%\n  mutate(absChange=`2017`-`2001`) %&gt;%\n  ggplot() +\n  geom_sf(aes(fill=absChange),size=0.5) +\n  scale_fill_gradient2(labels=scales::comma) +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in children aged 5-9 between 2001-2017\", fill=\"Change in\\n# 5-9yo\",\n       caption=\"MountainMath, StatCan T1 taxfilers, via CMHC\")\n\nThe result is of course similar to what census data has been telling us, and it provides the backdrop of the struggles VSB has had to match children to school capacity."
  },
  {
    "objectID": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#median-couple-family-incomes",
    "href": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#median-couple-family-incomes",
    "title": "Census tract level T1FF tax data",
    "section": "Median couple family incomes",
    "text": "Median couple family incomes\nFor better or worse, median incomes are everyone’s favourite income metric. We give a quick example to show how median incomes of couple economic families evolved over time.\nHowever, there is a slight wrinkle when using medians. We need to tongfen to make data comparable over time, and strictly speaking medians can’t be aggregated up from sub-geographies as required during tongfen. By default, tongfen will aggregate medians as if they were averages and emit a warning (which we suppress in the output here). But this is only an approximation.\nyvr_inflation &lt;- get_cansim_vector(\"v41695228\",start_time=\"2001-01-01\",end_time=\"2017-01-01\") %&gt;%\n  mutate(Year=substr(REF_DATE,1,4),Index=VALUE/last(VALUE,order_by = Year)) %&gt;%\n  select(Year,Index)\n  \nmedian_income_vectors &lt;- paste0(\"v_TX\",seq(2001,2017),\"_612\")\nincome_data &lt;- get_tongfen_census_ct(vsb_regions, vectors = median_income_vectors,\n                      na.rm=FALSE,geo_format = 'sf') \n\nincome_data %&gt;% \n  pivot_longer(cols=starts_with(\"v_\"),names_pattern = \"v_TX(\\\\d{4})_612\",\n               names_to=\"Year\",values_to = \"income\") %&gt;%\n  st_sf() %&gt;%\n  left_join(yvr_inflation,by=\"Year\") %&gt;%\n  mutate(adjusted_income=income/Index) %&gt;%\n  ggplot() +\n  geom_sf(aes(fill=adjusted_income),size=0.1) +\n  facet_wrap(\"Year\") +\n  scale_fill_viridis_c(labels=scales::dollar) +\n  tax_theme +\n  coord_sf(datum=NA) +\n  labs(title=\"Median couple family income\", fill=\"2017 dollars\")\n\nThis paints the well-known east-west gradient, with Arbutus Ridge standing out as a particularly high-income area when using median couple family incomes as the metric. Here we used Metro Vancouver CPI to adjust for inflation.\nincome_data %&gt;%\n  mutate(change=v_TX2017_612/v_TX2001_612*filter(yvr_inflation,Year==\"2001\")$Index-1) %&gt;%\n  ggplot() +\n  geom_sf(aes(fill=change),size=0.5) +\n  scale_fill_viridis_c(labels=scales::percent,option=\"inferno\") +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in median couple income 2001-2017\", fill=\"Relative change\\n(inflation adjusted)\",\n       caption=\"MountainMath, StatCan T1 taxfilers, via CMHC\")\n\nLooking at change between the start and end year of this time series highlights where median couple family incomes have been growing the fastest. And where they have been stagnant or even slightly declining."
  },
  {
    "objectID": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#different-regions",
    "href": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#different-regions",
    "title": "Census tract level T1FF tax data",
    "section": "Different regions",
    "text": "Different regions\nThe CT level tax data covers all CMAs in Canada. It’s trivial to replicate any analysis for a different region by simply changing the region parameter. As an example, we can look at the City of Toronto.\ntoronto_city_region &lt;- list(CSD=c(\"3520005\"))\ntoronto_lico &lt;- get_tongfen_census_ct(regions=toronto_city_region, \n                                      vectors=lico_vectors, geo_format = \"sf\")\nCopy and pasting the code for the percentage point change in the share low income population between 2005 and 2017 gives the graph for Toronto.\ntoronto_lico %&gt;%\n  mutate(Change=(lico_2017-lico_2005)/100) %&gt;%\n  st_sf %&gt;%\n  ggplot(aes(fill=Change)) +\n  geom_sf(size=0.1) +\n  scale_fill_gradient2(labels=scales::percent) +\n  tax_theme +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in share of people in low income 2005-2017\",\n       fill=\"Percentage\\npoint change\")"
  },
  {
    "objectID": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#upshot",
    "href": "posts/2020-04-23-census-tract-level-t1ff-tax-data/index.html#upshot",
    "title": "Census tract level T1FF tax data",
    "section": "Upshot",
    "text": "Upshot\nT1FF taxfiler data is extremely useful for research purposes, having this available as open data will hopefully prove useful to others. We left the code visible in this blog post, but the whole document is also available on GitHub for convenience."
  },
  {
    "objectID": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html",
    "href": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html",
    "title": "Toward universal TongFen: Change in polling district voting patterns",
    "section": "",
    "text": "Geographic data often comes on different geographic breakdowns. A prime example is census data, where the underlying census geographies can change from census year to census year. This makes it difficult to compare census data across censuses. But comparing census data across censuses at fine geographies is important for many applications.\nThere are two main ways how people deal with this problem. 1. Estimate data for one of the two geographies by (usually at some point) relying on area-weighted interpolation. 2. Aggregate up areas in both geographic datasets until one arrives at a common tiling.\nThe first method has the advantage that it always works, but the disadvantage that it is only an estimate and inevitably produces incorrect estimates. One example of an effort along this lines is this dataset giving weights to pairs of Canadian census tracts across the 1971 through 2016 censuses, details can be found in this article. Analysis based on this method needs to include an extra level of sensitivity checks to quantify how sensitive the analysis is to the assumptions (implicitly) used in the estimates. I am not aware of a good tool to aid such a sensitivity analysis, which significantly diminished the usefulness of this approach for academic research.\nThe second method has the advantage that it’s (fairly) exact. The Statisitcs Canada Correspondence files are a prime example of this approach, they encode instructions on how to link geographies in each census in order to achieve a common tiling. While only correspondence files for adjacent censuses are available, and the only geographies covered are Dissemination Blocks and Dissemination Ares, this can be trivially extended to span multiple censuses and other census geographies via joining multiple tables."
  },
  {
    "objectID": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#tongfen",
    "href": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#tongfen",
    "title": "Toward universal TongFen: Change in polling district voting patterns",
    "section": "TongFen",
    "text": "TongFen\nBoth of these methods have value, and we have incorporated them into our TongFen, which has become an integral part of our data analysis pipeline.\nThe tongfen_estimate function provides the ability to estimate data given on one geography on a different geography. It is essentially a downsampling method that distributes data from a larger geography into smaller geographies. In its simplest form this reduces to area weighted interpolation, and can be extended to dasymetric interpolation via the proportional_reaggregate function if other contextual data, for example Dissemination Block level counts in the census context, or secondary data like land use patterns that can help reduce downsampling errors, are available. It handles arbitrary geographies and is not constrained to any particular type like e.g. census tracts.\nThis is the method we use when geographies aren’t congruent in the sense that they don’t support a common tiling (other than aggregating the data into a single unit), or when we have fixed geographic target areas that we don’t want to aggregate up further. A rundown of how to use this method, including refining via dasymetric estimates, as well as an analysis in the errors this introduces, can be found in our example estimtes of census data for Toronto Wards, where a custom tabulation with exact counts is available as an authoritative check against the estimated values.\nBecause of the implicit and hard to estimate errors when estimating data for a fixed target geography we try to resort to aggregating areas up to a common tiling whenever possible. This process, making data comparable by finding a (least) common geography, is what we call TongFen, named after the Mandarin word for bringing two fractions onto the same (lowest) common denominator.\nWe have demonstrated this using multi-year census data for Vancouver and for Toronto and more recently using Census Tract level annual CRA tax data (https://doodles.mountainmath.ca/blog/2020/04/23/census-tract-level-t1ff-tax-data/), and this is also what’s baked into CensusMapper for 2011/2016 comparative interactive maps. The main drawback is that StatCan correspondence files are only available for census geographies, and that they only allow linking of census data back to 2001. On the plus side, the TongFen package leverages CensusMapper metadata to also automatically aggregate up the census variables, which can be a bit of a headache when dealing with variables that aren’t straight-up counts like averages or percentages. But this breaks down for medians, medians can’t be aggregated up this way and TongFen resorts to estimation instead (while emitting a warning message)."
  },
  {
    "objectID": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#universal-tongfen",
    "href": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#universal-tongfen",
    "title": "Toward universal TongFen: Change in polling district voting patterns",
    "section": "Universal TongFen",
    "text": "Universal TongFen\nThere are two obvious ways to extend the existing methods. One is to add tools for automated sensitivity analysis to accompany tongfen_estimate, which is still work in progress. The other is to extend TongFen methods that aggregate geographies to a common tiling for arbitrary geographies, not just the ones where we already have correspondence files like the 2001 through 2016 censuses. That’s what we mean by universal TongFen. At the base of this the new estimate_tongfen_correspondence function that generates a correspondence between two arbitrary geographic datasets. We will first demonstrate how this works using census data, and then apply this to federal election polling district level data."
  },
  {
    "objectID": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#estimating-correspondence-files",
    "href": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#estimating-correspondence-files",
    "title": "Toward universal TongFen: Change in polling district voting patterns",
    "section": "Estimating correspondence files",
    "text": "Estimating correspondence files\nTo start out we need to understand why this necessarily is a matter of estimation and not computation. Geographic data tends to be messy and the spatial accuracy of geographic dataset varies. Because of the nature of how inaccuracies enter into spacial data, these accuracy issues tend to be distance based.\nWhen finding a common tiling of two separate geographic datasets we have to answer the question when we consider geographic regions form each of the datasets to be “the same”. The estimate_tongfen_correspondence function addresses this by allowing the specification of a tolerance parameter, that determines by how much we allow regions to differ before we consider them to be different regions.\nTo see how this works let’s take a look at the dissemination areas in the City of Vancouver, including Musqueam 2 and the unincorporated area around UBC to the west.\n\nMost of the areas appear unchanged between the censuses. We notice some larger changes highlighted in the red square, let’s take a closer look.\n\nZooming in, and using the fixed road network (taken from Open Street Map) for reference, we notice that next to the larger boundary changes where polygons got re-assembled, many of the other polygon boundaries changed slightly between 2006 and 2016, mostly to better align with the road network.\nThis gives us some indication of what kind of tolerance we want to choose when trying to match up the areas. A 10 metre tolerance will probably trip up on some of the small boundary adjustments to better align with the road network, but if we choose the tolerance too large we might miss meaningful boundary changes. Given that a typical lot in Vancouver is about 110 feet deep, a 30 or 40 metre tolerance will be probably coarse enough to allow for minor alignment adjustments, but fine enough to pick up boundary changes that move one lot to a different geographic area.\nWe can put that theory to the test by plotting the results of estimate_tongfen_correspondence with 10, 20, and 30 metre tolerance, while also plotting the original geographies and the results of using get_tongfen_census_da that utilizes the official StatCan correspondence file for matching up regions.\n\nIndeed, using 10 metre tolerance gets tripped up on a lot of the minor boundary adjustments and amalgamates the geographies into large regions. A 20 metre tolerance gets a much finer view, but still get tripped up by the boundary adjustments along Broadway along the bottom of our area of interest. The 30 metre tolerance seems to strike the right balance, at least of the geographies in this map view.\nWe notice a difference between the amalgamation using the StatCan correspondence files and the 30 metre tolerance estimate in the top right quadrant, where the StatCan correspondence files tell us to dissolve an east-west boundary that our estimate_tongfen_correspondence leaves untouched. This is likely a change introduced by StatCan because of issues of comparability for the data across those censuses, possibly due to geocoding errors across the boundaries. In theory it could also be due to boundary changes in the 2011 census, as the StatCan correspondence files between 2006 and 2016 are bridged by the 2011 geographies, but closer inspection reveals this is not the culprit in this case.\nFor completeness here is a comparison of the 2006, 2016, TongFen based on the StatCan correspondence files and TongFen based on estimating the correspondence via estimate_tongfen_correspondence. Next to our 30m tolerance, we’ll throw in estimates based on 50m and 100m tolerance.\n\nThe TongFen estimates with 30 metres tolerance still loose a fair bit of detail in several areas, especially where census boundaries are curvy and 2006 tracing of them is proving rather rough.\nThe 50m tolerance recaptures a lot of the lost detail, but also fails to amalgamate some ares where the StatCan correspondence files dictate a boundary change should have happened, for example at UBC. However, the StatCan correspondence based amalgamation at UBC may well be driven by geocoding issues rather than significant boundary changes. Aided by having the comparison with the StatCan correspondence files, it appears that the 50m tolerance gives very good overall results for our area of interest.\nThe 100m tolerance estimates captures some areas correctly that the 50m estimate missed, for example in the north-east corner of Vancouver or at the south end of Imperial Drive at Pacific Spirit Park. At the same time, it introduces quite a few more issues where it fails to amalgamate data. Given that a 100m tolerance will miss some block-level changes, this should not come as a surprise.\nIn summary, in our example the estimate_tongfen_correspondence is quite effective at reproducing the tiling obtained via the StatCan correspondence files, and there is nothing in the physical geography that can explain the coarser tiling from the StatCan correspondence file for the cases where both tilings differ. One has to take care how to pick the tolerance, understanding context, like the fact that census boundaries generally follow city blocks and knowledge about the shape of blocks can help to pick an useful tolerance that walks the line between retaining as fine a geography as possible without running the risk of missing meaningful boundary changes."
  },
  {
    "objectID": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#federal-election-polling-districts",
    "href": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#federal-election-polling-districts",
    "title": "Toward universal TongFen: Change in polling district voting patterns",
    "section": "Federal election polling districts",
    "text": "Federal election polling districts\nSteve Tornes has been comparing polling district level election results between the 2015 and 2019 federal elections.\nThe Federal Election Maps for both British Columbia and Metro Vancouver for 2015 and 2019#r #elections #bcpoli #vanpoli pic.twitter.com/W6o4IywiY1— Steve Tornes (@Steve_Tornes) May 5, 2020\n\n\n\nThe maps are quite pretty, and the obvious question is how to get results for both of those election onto the same map to compare the data polling district by polling district. This of course is a perfect application case for TongFen. And Steve asking me about this was part of the motivation for me to get my act together and add this functionality to the tongfen package.\nBefore we jump into mapping we need to understand some of the details around voting. For our purpose, it’s important to distinguish three types of voting:\n\nVoting at a polling station\nAdvance voting\nVoting by mail\n\nPeople voting by the first method will do so at their designated polling stations, which is determined by their residential address. There are great for mapping purposes. Some last minute changes mean that some voting districts get pooled in this end, that adds a minor inconvenience.\nPeople voting by the second method go to their advance voting station. Several polling districts typically share the same advance polling station. We can still map this data, but at a coarser geography than polling districts.\nThe last method, voting by mail, has no geography associated with it other than the riding, so they can only be mapped at the riding level.\nWe will look at the ridings that cover the City of Vancouver, Musqueam 2 and the unincorporated area around UBC. These ridings are Vancouver Centre (59034), Vancouver East (59035), Vancouver Granville (59036), Vancouver Kingsway (59038), Vancouver Quadra (59039), and Vancouver South (59040).\n\nThere are few people voting by letter in each riding, but the number of people voting in advance polls is quite substantial. If we are mapping polling district level data, we are only counting the votes of the people that voted in person at the polls. If people living in a polling district voting in advance polls vote substantially different from people voting on election day, then polling district level map won’t accurately reflect the voting pattern living in the polling district.\nThat’s a problem if we want to do serious analysis with the data, but for today we are mostly interested in how to apply TongFen to this data. For now we will just take a quick peak at differences in shares of votes by voting method, this gives some indication how the different campaigns have structures their get-out-the-vote initiatives.\n\nWith that done, it’s time to dive into TongFen. The main step is to generate the correspondence estimates. Visual inspection shows that polling districts generally also follow block outlines and are in size not dissimilar from dissemination ares. So we will use the same tolerance of 30m for this.\n\nThe east side looks fairly intact after TongFen. Areas on the west side have changes considerably, which translates into large agglomerations of poll areas through TongFen. This is emphasized by polling areas on the west side being larger, due to lower population density but also large green areas like the Pacific Spirit Park. If one was interested in a more detailed analysis it might pay of to first cut out large unpopulated areas and thus ignoring boundary changes that do not result in people getting moved to a different polling district.\nTo better understand the mechanics we focus in on the red square in Kitsilano.\n\nWe observe many boundary changes in the centre of the area, as well as the left side, with smaller changes on the right side of the cutout. Visual inspection confirms that the TongFen works as intended in finding an optimal common tiling.\nTime to do some mapping. Now that we have a common tiling based on the 2015 and 2019 polling districts, we can compare the voting data for those elections. At least for the portion of the population that voted at the polls on election day. Here is the percentage point change in votes for each of the larger parties in each area.\n\nVancouver Granville stands out, that’s where Jody Wilson-Raybould ran as a Liberal in 2015 and as Independent in 2019. On the east side, where we have retained a fairly granular geographic breakdown, we notice interesting patterns of diverging vote movement.\nWe can repeat a similar process for the people that voted in advance polls by first aggregating polling districts that had a common advance poll station, and then using TongFen to bring these advance polling districts onto a common geography.\n\nFor Vancouver Granville the advance poll data confirms observations from the election day poll seen above that show that the vote swing away from Liberal was in line with many of the adjacent areas, and NDP as well as conservatives also significantly contributed to Jody Wilson-Raybould’s vote total. But that’s something we can already read off from the riding level summaries at the top of this section and does not require TongFen.\nBottom line, while there is some geographic variation visible, most of the interesting story about voter migration happens at the riding level. One interesting sub-riding level story might be in Vancouver East, where advance polling data shows an increase in NDP votes on the western side and a decrease on the east, while election day polling district data shows the reverse. A mirrored pattern can be observed in the Liberal vote change, partially masked by an overall decline in their vote share."
  },
  {
    "objectID": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#upshot",
    "href": "posts/2020-05-20-toward-universal-tongfen-change-in-polling-district-voting-patterns/index.html#upshot",
    "title": "Toward universal TongFen: Change in polling district voting patterns",
    "section": "Upshot",
    "text": "Upshot\nOur new estimate_tongfen_correspondence opens the door to universal TongFen, enabling quick and easy estimation of correspondence data for different geographic breakdowns. It find the least common denominator geography, given a specified tolerance, and enables us to make the data geographically comparable.\nThis is only useful if the different geographic breakdowns are sufficiently congruent, if not we will have to resort to the traditional tongfen_estimate and make due with estimates instead of exact counts. A logical next step would be to round up the functionality of tongfen_estimate by adding an easy way to do sensitivity analysis. But that will require some thinking and will have to wait for another day.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own TongFen project.\n\n\nReproducibility receipt\n\n## [1] \"2020-05-20 22:00:00 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [6983afd] 2020-05-14: refine text and add health region breakdown graphs\n## R version 4.0.0 (2020-04-24)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Catalina 10.15.4\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.1 sf_0.9-3                 \n##  [3] tongfen_0.1.1             cancensus_0.2.2          \n##  [5] forcats_0.5.0             stringr_1.4.0            \n##  [7] dplyr_0.8.99.9003         purrr_0.3.4              \n##  [9] readr_1.3.1               tidyr_1.0.2              \n## [11] tibble_3.0.1              ggplot2_3.3.0            \n## [13] tidyverse_1.3.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] nlme_3.1-147       fs_1.4.1           lubridate_1.7.8    httr_1.4.1        \n##  [5] rmapzen_0.4.2      tools_4.0.0        backports_1.1.6    R6_2.4.1          \n##  [9] KernSmooth_2.23-16 rgeos_0.5-2        DBI_1.1.0          lazyeval_0.2.2    \n## [13] colorspace_1.4-1   withr_2.2.0        sp_1.4-1           tidyselect_1.1.0  \n## [17] git2r_0.26.1       curl_4.3           compiler_4.0.0     cli_2.0.2         \n## [21] rvest_0.3.5        geojsonsf_1.3.3    xml2_1.3.2         labeling_0.3      \n## [25] bookdown_0.18      scales_1.1.1       rmapshaper_0.4.4   classInt_0.4-3    \n## [29] digest_0.6.25      foreign_0.8-78     rmarkdown_2.1      pkgconfig_2.0.3   \n## [33] htmltools_0.4.0    dbplyr_1.4.3       jsonvalidate_1.1.0 rlang_0.4.6       \n## [37] readxl_1.3.1       rstudioapi_0.11    httpcode_0.3.0     generics_0.0.2    \n## [41] farver_2.0.3       jsonlite_1.6.1     magrittr_1.5       Rcpp_1.0.4.6      \n## [45] munsell_0.5.0      fansi_0.4.1        lifecycle_0.2.0    stringi_1.4.6     \n## [49] yaml_2.2.1         jqr_1.1.0          grid_4.0.0         maptools_0.9-9    \n## [53] crayon_1.3.4       geojsonio_0.9.2    lattice_0.20-41    haven_2.2.0       \n## [57] geojson_0.3.2      hms_0.5.3          knitr_1.28         pillar_1.4.4      \n## [61] geojsonlint_0.4.0  codetools_0.2-16   crul_0.9.0         reprex_0.3.0      \n## [65] glue_1.4.1         evaluate_0.14      blogdown_0.18      V8_3.0.2          \n## [69] modelr_0.1.6       vctrs_0.3.0        cellranger_1.1.0   gtable_0.3.0      \n## [73] assertthat_0.2.1   xfun_0.13          broom_0.5.6        e1071_1.7-3       \n## [77] class_7.3-16       ggspatial_1.1.1    units_0.6-6        ellipsis_0.3.1"
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html",
    "title": "On mixing covid-19 and census data",
    "section": "",
    "text": "Mixing census data with COVID-19 case and mortality data seems like an obvious thing to do when trying to understand how COVID-19 affects different groups. But it’s only of very limited use. COVID-19 data is only (openly) available on coarse geographies and can only be matched at the ecological level. Deriving individual level relationships from this is extremely ambitious. At best, it can inform decisions on what individual level data should be collected moving forward.\nA good example is when the City of Toronto chief medical officer looked into the possibility that black and low income communities in Canada are disproportionately affected by COVID-19. To understand this they mixed census data with finer geography COVID-19 case data. The TL;DR:\nIn other words, they looked at ecological level correlations between case load and socio-demographic variables and found results concerning enough to start collection individual level data. In short, they fully understand that ecological level correlations are difficult to interpret and can at times give wrong results. They just ran it as a first screening to inform data collection that can give them accurate answers to their question.\nThat pretty much sums up the usefulness, as well as the potential misuse, of mixing census data with covid case and mortality data."
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#yet-another-preprint",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#yet-another-preprint",
    "title": "On mixing covid-19 and census data",
    "section": "Yet another preprint",
    "text": "Yet another preprint\nStill, it seems like others are trying to follow into the footsteps and try to add their own analysis. For example, this article on the Tyee describes a preprint that is trying to do pretty much the same thing, but for all of Canada and at coarser geographies. Unfortunately, the preprint is much less careful in the conclusions they draw. That’s something peer review will weed out.\nWhile I doubt that this particular preprint is useful or the results are informative given the methodological difficulties already outlined above, we are starting to get COVID-19 data on finer geographies and people will continue to look into how socio-economic data can help us understand the spread. So maybe this preprint is a good occasion for a quick outline on the do’s and don’ts when mixing in census data.\nUsing our CanCovidData R Package this is quite simple to do, so we will keep the code visible in this post to lower the barrier in case others want to go down the road of mixing COVID-19 and census data."
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#geographic-units",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#geographic-units",
    "title": "On mixing covid-19 and census data",
    "section": "Geographic units",
    "text": "Geographic units\nBe clear about the geographic units you are using. In this case this depends on the geographic granularity of the available COVID-19 data. The preprint takes the data collected by the excellent COVID-19 Open Data Working Group. The geography is based on Health Regions, but it’s important to note that in many cases the geography is much coarser. While Ontario does publish Health Region level data, others don’t. For example British Columbia only publishes much coarser Health Authority level data. Researchers should make sure that they are aware and accurately state what geographies they are using.\nThe preprint also discards some geographic areas. Excluding Yukon, Northwest Territories and Nunavut makes some sense, their case counts are very low. The preprint also excludes the Southwestern Health Region in Ontario, but this strikes me as a mistake grounded in missing that Southwestern Health Region amalgamated with the Port Huron Health Unit and COVID-19 data consequently only gets reported for the combined region. So we should follow suit and amalgamate the other data sources too, even if Stats Canada’s most recent Health Region geography still hasn’t incorporated that change.\nGrabbing the COVID-19 data for health regions is easy, the preprint used aggregate counts up to May 5th. Importing the case data via our CanCovidData R Package which also adds the Health Region geographic identifiers to the data table.\ncovid_data &lt;- get_canada_covid_working_group_health_region_data() %&gt;%\n  filter(!is.na(HR_UID)) %&gt;%\n  filter(Date&lt;=as.Date(\"2020-05-05\")) %&gt;%\n  group_by(GeoUID=HR_UID) %&gt;%\n  summarise(Confirmed=last(Confirmed,order_by = Date),\n            Deaths=last(Deaths,order_by = Date))"
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#census-data",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#census-data",
    "title": "On mixing covid-19 and census data",
    "section": "Census data",
    "text": "Census data\nThe preprint gets a bit funky here, they seem to be using CSD level data to then aggregate up to health regions. They also talk about StatCan suppressing variables for CSDs with fewer than 5,000 residents, which is incorrect.\nAlso, this seems like an overly complicated way to get match census data with our Health Region based geography, we will depart a little from the paper’s eccentric methods and instead just take the census profile data for the health region geographies that StatCan conveniently provides. We don’t have the data in CensusMapper, so it takes a bit of poking around to find the variables we are interested in (see the full source code for details if interested in the poking around part).\nThe paper was looking at the black population, population 65 and older, median income number of foreign born people and population density, so we collect those variables. Things get funky again though with the median income, medians can’t be aggregated up when joining geographies, unfortunately the paper does not mention how this was dealt with. We will approximate the median on a joint geography by a weighted sum just like one would do for an average income.\nBut it’s probably better to just use one of the low income measures from the census data, the preprint uses very loose language on how they are using the income data that seems to suggest that’s what they wanted to use in the first place. While we are at it, we can also throw in some occupation data. There are lots of other variables one could test, but again, the usefulness of this kind of analysis is quite limited anyway.\nCensus data also needs to get aggregated up to the coarser geography that the COVID-19 data is available at, but that’s easy thanks to the replace_all_health_region_geocodes function from our CanCovidData R Package. Except for the median income estimate, which, quite frankly, is a terrible proxy for low-income populations and probably should never have been used for this in the first place.\nmembers &lt;- c(pop=1,\n            age_65p=24,\n            lico_at=862,\n            immigrants=1142,\n            num_income_at=664,\n            med_income_at=665,\n            npr=1150,\n            pop_priv=1323,\n            black=1327)\nmembers2 &lt;- set_names(names(members),as.integer(members))\n\ncensus_data &lt;- get_health_region_census_2016_data() %&gt;%\n  mutate(GeoUID=`GEO_CODE (POR)`, Name=GEO_NAME) %&gt;%\n  replace_all_health_region_geocodes() %&gt;%\n  filter(GEO_LEVEL == 2) %&gt;%\n  filter(`Member ID: Profile of Health Regions (2247)` %in% as.integer(members)) %&gt;%\n  mutate(label=recode(`Member ID: Profile of Health Regions (2247)`,!!!members2)) %&gt;%\n  select(HR_UID=`GEO_CODE (POR)`,GeoUID,Name,label,\n         Total=`Dim: Sex (3): Member ID: [1]: Total - Sex`) %&gt;%\n  mutate(Total=as.numeric(Total)) %&gt;%\n  pivot_wider(names_from = label,values_from = Total) %&gt;%\n  mutate(income_at=med_income_at*num_income_at) %&gt;%\n  select(-HR_UID) %&gt;%\n  group_by(GeoUID,Name) %&gt;%\n  summarize_all(sum) %&gt;%\n  mutate(med_income_at=income_at/num_income_at) %&gt;%\n  mutate(log_med_income_at=log(med_income_at)) %&gt;%\n  select(-income_at) %&gt;%\n  mutate(share_65p=age_65p/pop,\n         share_black=black/pop_priv,\n         share_foreign_born=(immigrants+npr)/pop_priv,\n         share_lico=lico_at/pop_priv)\nThat might seem a bit lengthy, but looking closer at it it’s just a number of quite simple transformation steps of the data."
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#geographic-data",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#geographic-data",
    "title": "On mixing covid-19 and census data",
    "section": "Geographic data",
    "text": "Geographic data\nThat’s simple too, we just take the newest (2018) representation of Canadian Health Regions from Stats Canada. We will have to consolidate the ones that got amalgamated between 2018 and now, and then we cut it down using a nicer outline of Canada to make the boundaries look prettier for mapping.\nhealth_regions_raw &lt;- get_health_region_geographies_2018()\nhealth_regions &lt;- health_regions_raw %&gt;%\n  replace_all_health_region_geocodes() %&gt;%\n  st_make_valid() %&gt;%\n  group_by(GeoUID) %&gt;%\n  summarise() %&gt;%\n  rmapshaper::ms_simplify() %&gt;%\n  st_intersection(get_census(\"CA16\",regions=list(C=\"01\"),geo_forma=\"sf\") %&gt;% \n                    select() %&gt;% \n                    st_transform(st_crs(health_regions_raw)))"
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#mixing-the-data",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#mixing-the-data",
    "title": "On mixing covid-19 and census data",
    "section": "Mixing the data",
    "text": "Mixing the data\nAll that’s left to do is join up the data, and we also compute the density (in terms of people per hectare, we think that’s what the preprint was doing) and filter out the northern health regions.\nall_data &lt;- health_regions %&gt;%\n  left_join(inner_join(covid_data,census_data, by=\"GeoUID\"),\n            by=\"GeoUID\") %&gt;%\n  mutate(density=pop/as.numeric(st_area(.))*10000) %&gt;%\n  filter(!grepl(\"^6\",GeoUID))"
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#inspecting-the-data",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#inspecting-the-data",
    "title": "On mixing covid-19 and census data",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nThis is probably a good time to inspect the data to make sure nothing went awfully wrong. A quick and dirty map of some of the variables looks like things worked out fine.\nall_data %&gt;%\n  pivot_longer(c(\"share_foreign_born\",\"share_lico\",\"share_65p\",\"share_black\")) %&gt;%\n  st_sf() %&gt;%\n  ggplot() + \n  geom_sf(aes(fill=value),size=0.1) +\n  scale_fill_viridis_c(labels=scales::percent) +\n  facet_wrap(\"name\") +\n  coord_sf(datum = NA)\n\nSimilarly, the summary stats show that our data broadly agrees with the on in Table A1 in the preprint. Some smaller differences should be expected given the difference on how the data was assembled.\nall_data %&gt;% \n  st_set_geometry(NULL) %&gt;% \n  select_at(vars(matches(\"share_|density|med_income_at\"))) %&gt;%\n  pivot_longer(names(.),names_to = \"variable\") %&gt;%\n  group_by(variable) %&gt;%\n  summarize(mean=mean(value),sd=sd(value),min=min(value),max=max(value)) %&gt;%\n  mutate_at(vars(-one_of(\"variable\")),~round(as.numeric(.),3)) %&gt;%\n  knitr::kable()\n\n\n\nvariable\nmean\nsd\nmin\nmax\n\n\n\n\ndensity\n1.808\n6.279\n0.000\n41.266\n\n\nlog_med_income_at\n10.319\n0.111\n9.894\n10.595\n\n\nmed_income_at\n30491.473\n3361.438\n19802.107\n39934.000\n\n\nshare_65p\n0.182\n0.044\n0.038\n0.264\n\n\nshare_black\n0.017\n0.021\n0.002\n0.095\n\n\nshare_foreign_born\n0.119\n0.119\n0.000\n0.532\n\n\nshare_health\n0.072\n0.013\n0.040\n0.133\n\n\nshare_lico\n0.065\n0.029\n0.000\n0.179\n\n\nshare_sales_service\n0.230\n0.018\n0.179\n0.292\n\n\n\nFor completeness, here is a quick plot of the number of COVID-19 confirmed cases (mislabelled as infections) in the preprint.\nall_data %&gt;%\n  ggplot() + \n  geom_sf(aes(fill=Confirmed),size=0.1) +\n  scale_fill_viridis_c(labels=scales::comma, trans=\"log\") +\n  coord_sf(datum = NA) +\n  labs(title=\"Confirmed COVID-19 cases up to May 5th\")"
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#analysis",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#analysis",
    "title": "On mixing covid-19 and census data",
    "section": "Analysis",
    "text": "Analysis\nNow we get to the tricky part. We really should be running an ecological inference model, regular regression models are known to regularly over-estimate effects and sometimes even reverse the sign on correlation coefficients.\nBut let’s first follow the method the preprint chose and fit a simple negative binomial model, with the only difference that we are taking the logarithm of the income variable.\nmodel_cases &lt;- MASS::glm.nb(Confirmed ~ share_black + share_foreign_born + log_med_income_at + share_65p + density, data = all_data)\njtools::summ(model_cases,exp = T)\n\n\n\n\nObservations\n\n\n90\n\n\n\n\nDependent variable\n\n\nConfirmed\n\n\n\n\nType\n\n\nGeneralized linear model\n\n\n\n\nFamily\n\n\nNegative Binomial(0.7037)\n\n\n\n\nLink\n\n\nlog\n\n\n\n\n\n\n\n\n𝛘²()\n\n\n0.57\n\n\n0.06\n\n\n1218.73\n\n\n1236.23\n\n\n\n\nPseudo-R² (Cragg-Uhler)\n\n\n0.57\n\n\n0.06\n\n\n1218.73\n\n\n1236.23\n\n\n\n\nPseudo-R² (McFadden)\n\n\n0.57\n\n\n0.06\n\n\n1218.73\n\n\n1236.23\n\n\n\n\nAIC\n\n\n0.57\n\n\n0.06\n\n\n1218.73\n\n\n1236.23\n\n\n\n\nBIC\n\n\n0.57\n\n\n0.06\n\n\n1218.73\n\n\n1236.23\n\n\n\n\n\n\n\n\n\n\nexp(Est.)\n\n\n2.5%\n\n\n97.5%\n\n\nz val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n374.09\n\n\n0.00\n\n\n135420317310750.00\n\n\n0.44\n\n\n0.66\n\n\n\n\nshare_black\n\n\n249711609031186994012618752.00\n\n\n129949461868607296.00\n\n\n479847217435902986079547148984123392.00\n\n\n5.57\n\n\n0.00\n\n\n\n\nshare_foreign_born\n\n\n12.53\n\n\n0.52\n\n\n299.84\n\n\n1.56\n\n\n0.12\n\n\n\n\nlog_med_income_at\n\n\n0.83\n\n\n0.06\n\n\n10.72\n\n\n-0.15\n\n\n0.88\n\n\n\n\nshare_65p\n\n\n18.18\n\n\n0.04\n\n\n8327.82\n\n\n0.93\n\n\n0.35\n\n\n\n\ndensity\n\n\n0.95\n\n\n0.89\n\n\n1.01\n\n\n-1.63\n\n\n0.10\n\n\n\n\n\n\n Standard errors: MLE\n\n\n\n\nThis corresponds to Table 1 in the preprint, except that our results are grotesquely out of scale, calling this method into question. This also goes to show how sensitive this kind of analysis is to even small changes in assumptions."
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#ecological-inference",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#ecological-inference",
    "title": "On mixing covid-19 and census data",
    "section": "Ecological inference",
    "text": "Ecological inference\nAt this point it might be useful to specify an ecological inference model. The particular implementation in the ecoreg R package seems particularly well-suited for this application.\nlibrary(ecoreg)\nmodel.eco &lt;- eco(cbind(Confirmed,pop) ~ log_med_income_at+density,\n                 binary = ~ share_black + share_65p+share_foreign_born,\n                 data = all_data)\n\nmodel.eco\n## Call:\n## eco(formula = cbind(Confirmed, pop) ~ log_med_income_at + density, \n##     binary = ~share_black + share_65p + share_foreign_born, data = all_data)\n## \n## Aggregate-level odds ratios:\n##                          OR\n## (Intercept)       0.2944706\n## log_med_income_at 0.6130313\n## density           1.0469473\n## \n## Individual-level odds ratios:\n##                              OR\n## share_black        0.0001248824\n## share_65p          0.0358417248\n## share_foreign_born 0.0072685290\n## \n## -2 x log-likelihood:  37230.61\nThis model comes out with a markedly different result, showing a fairly low added risk for black populations. The median income variable seems to suck up a lot of the effect. As already mentioned, it’s not a very useful variable to start with, plus it correlates with the share black population (and anti-correlates with share of seniors) which might throw a wrench in our estimates.\nMaybe it’s best to just strip down the model and throw out the income variable, as well as the foreign born population that doesn’t seem to be doing much.\nlibrary(ecoreg)\nmodel.eco &lt;- eco(cbind(Confirmed,pop) ~ density,\n                 binary = ~ share_black + share_65p,\n                 data = all_data)\n\nmodel.eco\n## Call:\n## eco(formula = cbind(Confirmed, pop) ~ density, binary = ~share_black + \n##     share_65p, data = all_data)\n## \n## Aggregate-level odds ratios:\n##                       OR          l95          u95\n## (Intercept) 0.0001351304 0.0001171961 0.0001558092\n## density     1.0098295335 1.0091606600 1.0104988502\n## \n## Individual-level odds ratios:\n##                  OR      l95      u95\n## share_black 69.9372 65.57338 74.59143\n## share_65p   18.6860 15.91682 21.93697\n## \n## -2 x log-likelihood:  32329.84\nThat gives a much cleaner model, with significantly heightened odds ratios for blacks and also for seniors. But quite frankly, we are just randomly adding or dropping variables at this point.\nWe will leave it at this for today, if anything sensible is going to come out of this it requires a much more concerted effort of building the model. Stratifying by province would be an obvious (and probably useful) next step."
  },
  {
    "objectID": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#upshot",
    "href": "posts/2020-05-27-on-mixing-covid-19-and-census-data/index.html#upshot",
    "title": "On mixing covid-19 and census data",
    "section": "Upshot",
    "text": "Upshot\nThe usefulness of this kind of analysis is greatly limited by how finely the data is aggregated. Health Region (or coarser) geography does not really lend itself to doing this.\nSome regions, like Alberta or Toronto publish finer geographic data, where this kind of analysis probably makes more sense.\nSome people are doing that, and are exploring relationships of COVID-19 cases with census variables at the neighbourhood level. That’s a lot more useful than looking at health region data, ultimately this would benefit from running this as ecological inference models, and also deal with spatial autocorrelation and other issues that regularly pop up during this kind of analysis.\nAs usual, the code for this post is available on GitHub in case others find it useful.\n\n\nReproducibility receipt\n\n## datetime\nSys.time()\n## [1] \"2020-07-06 13:18:16 PDT\"\n## repository\ngit2r::repository()\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [e01bdc0] 2020-05-28: census-covid post\n## Session info\nsessionInfo()\n## R version 4.0.0 (2020-04-24)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Catalina 10.15.5\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] ecoreg_0.2.3       sf_0.9-4           cancensus_0.3.2    CanCovidData_0.1.2\n##  [5] forcats_0.5.0      stringr_1.4.0      dplyr_1.0.0        purrr_0.3.4       \n##  [9] readr_1.3.1        tidyr_1.1.0        tibble_3.0.1       ggplot2_3.3.1     \n## [13] tidyverse_1.3.0   \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.4.6       lubridate_1.7.9    lattice_0.20-41    class_7.3-17      \n##  [5] assertthat_0.2.1   digest_0.6.25      R6_2.4.1           cellranger_1.1.0  \n##  [9] backports_1.1.8    reprex_0.3.0       evaluate_0.14      e1071_1.7-3       \n## [13] httr_1.4.1         blogdown_0.19      pillar_1.4.4       rlang_0.4.6       \n## [17] readxl_1.3.1       rstudioapi_0.11    blob_1.2.1         rmarkdown_2.2     \n## [21] munsell_0.5.0      broom_0.5.6        compiler_4.0.0     modelr_0.1.8      \n## [25] xfun_0.15          pkgconfig_2.0.3    htmltools_0.4.0    tidyselect_1.1.0  \n## [29] bookdown_0.19      codetools_0.2-16   fansi_0.4.1        crayon_1.3.4      \n## [33] dbplyr_1.4.4       withr_2.2.0        grid_4.0.0         nlme_3.1-148      \n## [37] jsonlite_1.6.1     cansim_0.3.5       gtable_0.3.0       lifecycle_0.2.0   \n## [41] DBI_1.1.0          git2r_0.27.1       magrittr_1.5       units_0.6-7       \n## [45] scales_1.1.1       KernSmooth_2.23-17 cli_2.0.2          stringi_1.4.6     \n## [49] fs_1.4.1           xml2_1.3.2         ellipsis_0.3.1     generics_0.0.2    \n## [53] vctrs_0.3.1        tools_4.0.0        glue_1.4.1         hms_0.5.3         \n## [57] yaml_2.2.1         colorspace_1.4-1   classInt_0.4-3     rvest_0.3.5       \n## [61] knitr_1.28         haven_2.3.1"
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html",
    "title": "Covid Series 3 Survey",
    "section": "",
    "text": "I was browsing the Canadian Perspectives Survey Series 3 on Resuming Economic and Social Activities During COVID-19 and thought that some of the results were interesting. I don’t have time to do a detailed post on this, but thought that others might enjoy a quick series of graphs highlighting some of the result. The survey is only released as microdata, and I have not seen much uptake or reporting other than the high-level results put out by StatCan in The Daily.\nStatCan PUMF microdata can be a bit of a pain to work with, StatCan does not include general purpose metadata. It comes as a flat fixed-width file and the metadata needs to get scraped out of language-specific command files. This shows that StatCan still has a long way to go to actually open up their data. The secondary purpose of this post is to share code to parse the SPSS command files to scrape out the metadata needed to read and interpret the PUMF data. The code for the post can be found on GitHub."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#tracing-app",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#tracing-app",
    "title": "Covid Series 3 Survey",
    "section": "Tracing app",
    "text": "Tracing app\nPart of my motivation to look at the data was to understand people’s attitudes toward the contact tracing app. To start off we can look at how likely people say they will use the app. This is important, as the usefulness of the contact tracing app depends to a large degree on how many people will install it. Research says that that the app will start to become useful when about 15% of the population has the app installed, and a much-cited preprint estimates that starting at an adoption rate of 60% the app could suppress the epdimeic on it’s own without other major interventions. This last stat has often been misrepresented to mean that the app won’t be useful below 60% adoption rate, which is false.\nThe other part about the Canadian contact tracing app I am interested in is privacy concerns. Generally Canadian provinces have botched this badly. Alberta started out with rolling out a contact tracing app with centralized design, that is quite a bit more privacy invasive than our national app (and also was next to useless on iOS). BCCDC pitched the possibility of a location based app to BC residents in their covid survey, which serves to poison the well for much more privacy focused (and at the same time more effective) solutions like Canada’s national contact tracing app.\nGiven that, I was somewhat dismayed that a StatCan survey gave people the option to give I don’t want the government to have access to my location data as a possible answer to why they don’t want to use a contact tracing app. But as others poited out to me, this might still be good information to have (although I am not convinced it needs to be an official government survey that asks this question), it’s a fine line between collecting information and amplifying misinformation.\nBut what’s done is done, so we might as well look at the data.\n\nTurns out 55% of respondents are at least somewhat likely to install the app. Not enough to reach the 60% install base, especially when accounting for the fact that only about 80% of Canadians have a smartphone and not all smartphones can run the app. But it’s certainly enough for the app to be useful and help reduce the spread alongside other measures.\nLet’s check in more detail how comfortable the people that said they were at least somewhat likely to install the app are with the data sharing. Of course this is a weird question, it’s essentially equivalent to asking how comfortable people are walking around with wifi or bluetooth turned on and thus their MAC address being advertised. But most won’t read the question this way and again, I am not sure how I feel about misleading questions being asked on a government survey that could result in lower adoption. But here we go.\n\nSimilarly we can look at the reasons people were somewhat unlikely or very unlikely to use the app.\n\nAnd what do you know, people flocked to the invasion of privacy and don’t want government to have my location data answers. But also very interesting that many answered that they won’t install it because they believe not enough people will install it. Some mixture between misinformation about what kind of install base makes this app useful and just circular logic.\nAs expected, not having a smartphone or data plan also shows up as a reason."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#vaccines",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#vaccines",
    "title": "Covid Series 3 Survey",
    "section": "Vaccines",
    "text": "Vaccines\nAnother interesting question is that of vaccines and attitudes toward them.\n\nThe good news is that the vast majority of Canadians would get a vaccine it if was available. Let’s look at the reasons the others give.\n\nSome of those unlikely to take the vaccine simply want to wait until they are convinced it’s safe, but more aren’t confident that the vaccine, whenever we get one and whichever we get, will be safe, or have general concerns about risks and side effects. The percentages of people with these concerns seem a bit higher than the overall anti-vaxxer attitudes in Canada, maybe because of talk about ‘fast-tracking’ of a covid vaccine.\nThere are also people that don’t think a vaccine is necessary.\nAnd then there are people with pre-existing conditions. And interesting 295,308 people (respondents weighted and scaled to the Canadian population 15 years old or older) that say they think they have already had Covid, which is not out of line with the 104,204 confirmed covid-19 cases in Canada by June 30, the release date of the survey, indicating under reporting cases of up to a factor 3, depending on assumptions about how many may have mis-diagnosed themselves."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#vaccines-vs-tracing-apps",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#vaccines-vs-tracing-apps",
    "title": "Covid Series 3 Survey",
    "section": "Vaccines vs tracing apps",
    "text": "Vaccines vs tracing apps\nOne interesting question is around attitudes toward vaccines vs attitudes toward tracing apps.\n\nPeople that say they are very likely to get vaccinates also said that they were very or somewhat likely to install the contact tracing app. Which is not surprising, given that effective contact tracing and vaccines work similarly to reduce the spread of Covid-19, with the main difference that vaccines protect the vaccinated person too whereas the benefit of a contact tracing app incurs only to the contacts of the person that installed it but not the person themselves. Selfishness might help explain the higher willingness of people to take a vaccine than to install a contact tracing app.\nThere are interesting outliers, some people are very likely to get vaccinated but very unlikely to use the contact tracing app. And of course there are people that are very unlikely to either get vaccinated or use the app."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#commute-to-work",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#commute-to-work",
    "title": "Covid Series 3 Survey",
    "section": "Commute to work",
    "text": "Commute to work\nAnother interesting question was about changes in commute to work pre and during covid-19.\n\nThe diagonal dominates in all rows and columns except the telework during covid 19 and public transit before covid 19."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#precautions",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#precautions",
    "title": "Covid Series 3 Survey",
    "section": "Precautions",
    "text": "Precautions\nAnother interesting question is on what kind of precautions people are taking.\n\nTurns out Canada is really good at washing hands but has failed to evolve with the science to take the same level of precautions against transmission through aerosols."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#health-risk-concerns",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#health-risk-concerns",
    "title": "Covid Series 3 Survey",
    "section": "Health risk concerns",
    "text": "Health risk concerns\nPeople are concerned about health risks when resuming various activities as safety measures are relaxed."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#fear-of-being-targeted",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#fear-of-being-targeted",
    "title": "Covid Series 3 Survey",
    "section": "Fear of being targeted",
    "text": "Fear of being targeted\nAn interesting set of questions revolve around people being afraid that they are being the target of unwanted behaviours because they may be judged for putting others at risk.\n\nAlmost 20% answered Yes to this. Let’s look at what their concerns are.\n\nThe main concern people have is that they will be targeted for not wearing a mask. Health conditions that may be mistaken for Covid-19 also rank prominently."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#preparing-for-potential-future-waves",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#preparing-for-potential-future-waves",
    "title": "Covid Series 3 Survey",
    "section": "Preparing for potential future waves",
    "text": "Preparing for potential future waves\nThe way people are preparing for potential future waves is interesting.\n\nCancelling travel plans seems to be on top of the list for Canadians, followed by stockpiling and home repairs. Upgrading internet speeds is generally a low concern."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#economic-recovery",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#economic-recovery",
    "title": "Covid Series 3 Survey",
    "section": "Economic recovery",
    "text": "Economic recovery\nAnother question was about what people would spend more or less money on as stores and businesses reopen relative to pre-covid spending.\n\nIt’s interesting that in most categories people don’t think they will change their spending habits, and generally there are not vastly different numbers estimating they will spend less as the ones estimating they will spend more in that category."
  },
  {
    "objectID": "posts/2020-08-25-covid-series-3-survey/index.html#next-steps",
    "href": "posts/2020-08-25-covid-series-3-survey/index.html#next-steps",
    "title": "Covid Series 3 Survey",
    "section": "Next steps",
    "text": "Next steps\nThere are more variables in the data that we haven’t looked at yet, and there are interesting ways to cross-tab the data. As usual, the code for this post can be found on GitHub for anyone to reproduce and adapt for their own purposes."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html",
    "href": "posts/2020-09-08-covid-school-modelling/index.html",
    "title": "Covid school modelling",
    "section": "",
    "text": "BC schools are about to restart, and there is a high level of anxiety among parents and teachers. In this post we will"
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#cases-in-schools",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#cases-in-schools",
    "title": "Covid school modelling",
    "section": "Cases in schools",
    "text": "Cases in schools\nThere will be cases at schools. Quebec already has over 180 schools with reported cases, Ontario had over 7 on their first day of school. Even in Metro Vancouver we already had an exposure at a school that started a week early.\nBut this is not in of itself alarming. We need to distinguish between two types of cases,\n\nexternal introductions, and\ntransmissions at school.\n\nThe former is inevitable, given our current levels of community spread. External introductions will lead to school disruptions because it forces some children and teachers into precautionary quarantine. But external introductions are not the main reason why we should worry about sending kids back to school.\nThe latter type, transmissions at school, is more concerning as it means that schools are contributing to an accelerated spread as children are carrying the infection back into their homes and out of the school community. The possibility of transmissions at schools is at the core of everyone’s concerns.\nThere are plenty of examples of transmissions and even outbreaks at schools, including in Canada. And more recently there have already been two outbreaks at Quebec schools after reopening, including one where four teachers tested positive. The question is if the BC back to school protocol is enough to prevent in-school transmissions.\nLet’s review some of the main concerns people have about BC’s school reopenings."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#concerns",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#concerns",
    "title": "Covid school modelling",
    "section": "Concerns",
    "text": "Concerns\nProvincial guidelines cap learning group sizes at 60 students for elementary and middle schools and 120 for high schools, where the decision to allow larger cohort sizes for older students seems to be oriented more on current school structures than epidemiological considerations.\nMuch of the implementation details have been offloaded to local school boards, while the province is not making critical information like community spread rates available.\nThe province hasn’t released modelling that informs the school reopening, and implementation details of how the province plans to detect and manage outbreaks are fuzzy. This makes it difficult to independently assess the reopening plan.\n\nCommunity spread\nFrom an epidemiological perspective, the level of community spread is the most important factor in setting guidelines for school reopening. Unlike other provinces BC is not making community spread statistics available at finer geography. The closest we have is daily case numbers at the Health Authority geography, as well as two-week averages of new daily cases at the Health Region geography that (I am guessing) will get published every 2 weeks in image form. In BC we also have provincial level data on new cases by symptom onset date and type of exposure published occasionally in the surveillance reports in image form. This publicly released data is insufficient to make informed decisions at the school district level in our environment of elevated (and rising) case numbers, let alone make adjustments for individual schools within a school district that may face a higher risk. There are broader Covid data issues in this province, but that’s beyond the scope of this post.\nWeekly provincial case number have been rising, and new confirmed cases are now higher than they were in spring when schools initially closed down and later reopened under stricter rules than now. That does not necessarily mean that the number of new infections is higher, our testing protocol has since changed, and we were likely missing more cases back then than we are now. But our current elevated level of community spread is a concern that warrants close monitoring.\n\nApart from elevated case numbers, we are seeing higher shares of cases being school-age children. Children are still under-represented in case counts, but this may partially due to children being shielded by staying home during earlier stages, and partially due to children having a higher share of asymptomatic cases and even symptomatic cases generally experience much milder symptoms and might not seek testing.\n\n\n\nImplementation details\nModelling school reopening is hard, a detailed understanding of individual level virus transmissions in the school setting is still missing. What we can do is\n\nrun relative modelling scenarios to understand what kind of interventions, ranging from cohort sizes to mask policies, and to test, trace and isolate (TTI) protocols, compare to one another in likely reduction of spread, and\nlook at other regions that already opened schools and learn from their mistakes.\n\nBC seems to be betting on the second option (at least we haven’t seen any official models looking at different school protocol implementations). In particular, provincial health officials have been pointing at Germany as one of the places they are monitoring and learning from. And indeed, the general school guidelines in BC are in line with what we see in Germany, and so far there have been only a few transmission clusters at German schools, and they got under control fast.\nFor example, in one school a teacher tested positive after developing symptoms which triggered broad testing of 120 students, turning up 10 infected students across three cohorts. All of the infected students were taught by the infected teacher and none of them showed symptoms at the time of the test. The 10 students with positive test results triggered further testes and contact tracing outside of the school network without any new cases showing up. The school moved to online learning for a week and all students are set to get re-tested before returning to in-person learning.\nImplementation details matter, and and from the example we see that philosophically Germany is taking a very different approach to TTI than BC has done. To highlight the difference, it’s worthwhile to look at how BC has been approaching outbreaks at Long Term Care facilities. BC has moved early to prevent caregivers from working at multiple Long Term Care centres, which has cut the spread between centres and is probably the most important intervention to reduce the number of outbreaks. But in managing outbreaks once they occur provincial health officials have repeatedly insisted that they don’t want to test non-symptomatic staff and residents. The strategy has been to increase monitoring for symptoms, and then isolate and test symptomatic people only. As a reason officials have been pointing to a false negative rate of 30% of the PCR tests (which seems a tad high but that’s besides the point), so PCR does not give conclusive answers on who is infected and who isn’t. Without going into details, claiming that one should pass up on the option to identify 70% of infected cases early, especially in a high vulnerability environment, defies logic. It’s a question of fairly straight-forward mathematics to verify that even test with low sensitivity can help suppress outbreaks, and simulation studies confirm this.\nNotably, the provincial Back to School Plan makes no mention of testing in case of an exposure at a school. This does not bode well for chances of a robust provincial TTI strategy in the much lower vulnerability school setting, but hopefully the province will change their mind on this and also follow this aspect of Germany’s model.\n\n\nCohort sizes\nThis is where the size of the community spread and the TTI strategy come together. The size of the community spread determines how many cases will likely get introduced into the schools, and the TTI strategy negotiates the balance between quarantining a large number of students and risking in-school transmissions. The way this has played out in Germany is that once a case is detected, the whole class is immediately isolated. Comprehensive testing will then determine if and how classes can resume. Depending on situations and test outcomes, this has led to entire classes and even cohorts or schools getting quarantined, as well as cases where everyone except closest contacts were able to return to in-person classes after testing. And everything in between, for example a case where the entire class was quarantined and the rest of the cohort was allowed back while wearing masks full-time.\nIf community spread is low then implementation details don’t matter that much. But at moderate or higher community spread, like parts of BC is facing right now, implementation details become important as we will see in the modelling below. If we don’t broadly test in an outbreak we are forced to choose to either quarantine a large number of students or risk spread in the schools.\n\n\nThe A-word\nAt the beginning of the pandemic we have been focusing on two of the three main modes of respiratory viral transmission: formites and droplets. But increasingly we have evidence that the third one, aerosols, plays an important role too, in particular during outbreaks. Aerosols don’t play much of a role outdoors where they get dispersed fast, while droplets and formites behave fairly similarly indoors. But aerosols become increasingly important the less ventilated a place is. We know by now that viral dose matters for infections as well as severity of Covid-19, and aerosols generally carry a lower dose than droplets. But if a room is not well ventilated, or even worse, air is internally recirculated, aerosols can accumulate enough to become effective infection sources that can trigger larger outbreaks.\nIn Germany the schools are currently keeping windows and doors open as much as possible, and there is an ongoing discussion on adding or upgrading HVAC systems so that good ventilation can be maintained into winter. Ontario has similarly been dedicating funds to improve ventilation in schools, although the total amount seems insufficient to guarantee good ventilation in all schools. In BC there hasn’t been much discussion about measures to improve school ventilation from provincial health officials that go beyond recommendations to open windows, although there is a side remark in the Back to School Plan only mentions improvements in ventilation as a possible use of federal funds.\nSimple measures like opening windows and doing everything possible to increase ventilation is curiously absent from the BC Back to School Plan. In the short term ventilation is likely not a major concern as the weather is still warm and we can keep windows open to maintain a steady exchange with outside air. (Classrooms without windows are likely already serviced by an HVAC system and schools should make sure their throughput is maximized and filters be installed on any recirculating components.) But this strategy is bound to run into problems as soon as it gets colder, with many BC school currently having neither an HVAC nor any other in-class filters installed."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#the-model",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#the-model",
    "title": "Covid school modelling",
    "section": "The model",
    "text": "The model\nTo understand the tradeoffs in school spread we are adapting the University of Washington network-based model and tweaking it for our purposes. The model is a standard SERIS model with the usual adaption of including infectious pre-symptomatic and asymptomatic compartments to fit what we know about Covid-19 spread, as well as having separate compartments for quarantined states that help us model the TTI cycle. The model used in this post differs somewhat from the one the province has been showing off in their press conference in that the model used by the province is a mean-field model where all individual in a compartment are assumed to behave in identical ways (with some added compartments to allow for some non-compliance with social distancing). These models have a hard time modelling outbreaks, and the are generally fitted to data that excludes cases from outbreaks. (There are a variety of other models that have also been used in BC, although less prominently.)\nThe model we are using is a network-based model that allows variability at the individual person level. Network-based models are easy to adapt to the school setting and also allows for enough variability to model outbreaks and potential super-spreader events. At the same time the model’s number of parameters is huge, getting reliable predictive estimates from such a model requires imposing structure informed by what we know about the spread of the virus. And we don’t really have a good enough understanding to fix these all in place.\nWhile we can roughly calibrate the parameters to fit with what we have seen in other places, model can’t predict exactly what will happen as we reopen schools. It allows to test how different settings impact the spread of the virus in schools. What happens when community spread goes up? How do cohort sizes impact transmissions at schools? How do different TTI strategies impact school operations.\nWe will only highlight some of the more important parameters we are setting in this post, for more detail we refer the interested reader to the code.\nTo set up the model we build a school network that is roughly based on the BC provincial guidelines. Elementary cohorts have about 60 students, secondary cohorts have 120. Each classroom is strongly connected, and there are some connections between classrooms, in the secondary schools more than in elementary schools. Cohorts also feature some inter-connections, which we attribute to siblings and playdates.\nWe divide each classroom into yet again smaller groups of around 4 students, which we think of mini-networks with stronger connections. Other within-classroom connections are weighted down by 30%.\nWe omit teachers in this model out of laziness, so we won’t be able to use this current version to answer questions about the effect of interrupting teacher-to-teacher connections, different masking procedures or the effect of resource teachers that move between classrooms.\nTeachers do play an important role in this because they are much more effective as spreaders as they talk (and thus produce aerosols) at higher rates, because they likely have stronger connections to students than students on average to each other, and because they are more vulnerable to negative outcomes in case they contract the virus. This is something that should be dealt with in subsequent modelling.\nTo account for different classroom conditions, in particular different levels of crowding and ventilation, we scale down the transmission rates for each classroom based on a random factor uniformly distributed between 0.2 and 0.7. Ventilation levels is a key assumption, in subsequent modelling we might want to make this time dependent to account for weather getting successively colder which will reduce ventilation in classrooms without HVAC systems.\nIn our model we are considering a school district with a total of 42,000 students, roughly modelled on the Vancouver School Board."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#elementary-school-learning-group-network",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#elementary-school-learning-group-network",
    "title": "Covid school modelling",
    "section": "Elementary school learning group network",
    "text": "Elementary school learning group network\nHere is a graph of a prototypical elementary school network in our model. We see three classrooms, and smaller sub-network within each classroom. There are also some connections between classrooms within the cohort."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#secondary-school-learning-group-network",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#secondary-school-learning-group-network",
    "title": "Covid school modelling",
    "section": "Secondary school learning group network",
    "text": "Secondary school learning group network\nSimilarly, for secondary schools we have 6 classrooms in the cohort, again with smaller mini-clusters inside each classroom and connections between classrooms. One could probably improve on this model, but it will do for our purposes."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#general-covid-parameters",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#general-covid-parameters",
    "title": "Covid school modelling",
    "section": "General Covid-parameters",
    "text": "General Covid-parameters\nWe need to set some basic covid-related parameters that govern the spread. These are fairly standard parameters that have been used in modelling, we include graphs of their distribution for convenience.\n## Individual R0:  mean = 2.50, std = 0.50, 95% CI = (1.62, 3.58)\n\nThe distribution of \\(R_0\\) sets the general infectiousness of the virus, and we will draw from this distribution to assign each person how infectious they are in our model.\n## latent period:  mean = 2.20, std = 1.11, 95% CI = (0.60, 4.86)\n## \n## pre-symptomatic period:  mean = 2.99, std = 1.78, 95% CI = (0.56, 7.40)\n## \n## (a)symptomatic period:  mean = 3.99, std = 1.60, 95% CI = (1.50, 7.67)\n\nSimilarly, we assign a distribution for the latent, pre-symptomatic and symptomatic (or asymptomatic in case the person does not develop symptoms) periods.\nHospitalization (and death) is not a critical parameter for our model as children generally have comparatively low rates, but we include these in the model anyway.\nLastly we need to tell the model how to go from a general \\(R_0\\) to individual level pairwise transmission probabilities. We depart from the standard implementation of the model and replace the normalization by node degree by 6, essentially assuming that on average people have 6 close contacts a day outside of their home. This is a bit of a fudge factor, and the main reason that the estimates should not be used as predictions, but that we should look at relative usefulness of interventions."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#testing-and-quarantine-protocol",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#testing-and-quarantine-protocol",
    "title": "Covid school modelling",
    "section": "Testing and quarantine protocol",
    "text": "Testing and quarantine protocol\nFinally, we need to settle on the TTI protocol. Unfortunately we don’t have details on how BC will do this. If what happened at Long Term Care centres can be a guide we won’t be testing non-symptomatic contacts or classmates, and will be conservative in quarantining students. In the recent Metro Vancouver school exposure some students and staff were asked to self-quarantine for two weeks. The school was not told if the index case was a student or staff, or if students were exposed to an outside index case. This makes it hard to gain insight in how exactly TTI is handled in BC, but it does appear that BC is not employing broad testing like what has been done in Germany. But again, it is hard to draw too many conclusions from this since the details of the recent school exposure aren’t known.\nGiven what we do know, we assume that that positive cases in schools will self-isolate with 100% probability. Symptomatic cases will self-isolate with 90% probability, some might ignore or misinterpret their symptoms and the symptoms will go undetected at the school. We assume that the entire closer within-classroom networks will quarantine if it has a positive case, and some of the other contacts within and outside of the classroom will also self-isolate.\nWe assume a time-varying false positive rate around 20%, depending on the state of the infection."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#exogenous-introductions-and-other-assumptions",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#exogenous-introductions-and-other-assumptions",
    "title": "Covid school modelling",
    "section": "Exogenous introductions and other assumptions",
    "text": "Exogenous introductions and other assumptions\nWe assume that there are 2.8 new cases per 100,000 population in the school district we are looking at, which matches the rate VSB was seeing about two weeks ago. These numbers are outdated and likely higher now, but that’s the last numbers that were made public. We furthermore assume that the rate among children is similar to the overall rate, which roughly aligns with provincial data by age after accounting for higher asymptomatic cases that will likely fall through the provincial testing protocol. Our model assumes that 40% of cases are asymptomatic.\nThis sets up the model with 5 exposed children on day 1 and an exogenous introduction of around 1.2 introductions per day or 70.6 introductions over our 60-day modelling period.\nWe assume that transmission rates between children are at 75% of that of adults, and that pre and asymptomatic cases are on average only 75% as infectious as symptomatic cases. We also allow for a small percentage of random close interactions independent of the established contact network."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#simulated-scenarios",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#simulated-scenarios",
    "title": "Covid school modelling",
    "section": "Simulated scenarios",
    "text": "Simulated scenarios\nThe model is probabilistic in nature, each model run differs slightly from the previous. For robust estimates we should average over several runs, but for our purposes a single model run should to do show the general effect of modifying the interventions.\nRunning the model for 60 days we get 428 total infections. We can visualize the infected and quarantined population over time.\n\nThe graph shows how many students are in the exposed or infectious stages, as well as their quarantined counterparts. The green susceptible population in quarantine are non-infected students that were forced into quarantine."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#isolating-the-entire-class",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#isolating-the-entire-class",
    "title": "Covid school modelling",
    "section": "Isolating the entire class",
    "text": "Isolating the entire class\n\nThat brings it down to 339 total infections over the 60-day period, at the cost of quarantining more students."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#testing-all-contacts",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#testing-all-contacts",
    "title": "Covid school modelling",
    "section": "Testing all contacts",
    "text": "Testing all contacts\nHere we consider testing all contacts, including the entire classrooms, but only immediately isolating the closer in-classroom networks of a positive case.\n\nThat brings it down to 315 total infections over the 60 day period, at the cost of quarantining more students."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#random-testing",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#random-testing",
    "title": "Covid school modelling",
    "section": "Random testing",
    "text": "Random testing\nAnother way to reduce the number of infections is to add random testing. In schools we expect a fair share of asymptomatic cases, which we will never catch before they infect others unless we introduce random testing. We run a model that’s the same as the previous with additionally a quarter of the students are randomly tested each day.\n\nThat brings it down to 153 total infections over the 60 day period, at the cost of quarantining more students.\nFor better comparison, here are the results for all four model runs in one graph.\n\nThis makes it easy to see the tradeoffs between the different strategies and how different quarantine and testing protocols impact the spread in the school (yellow and red colours) as well as the precautionary quarantine of non-infected people (green). The model (naively) assumes perfect quarantine, quarantining the entire classroom instead of smaller contact networks within classrooms predictably leads to smaller outbreak sizes but a larger number of healthy students in quarantine. A middle ground between these two is only quarantining smaller contact networks within classrooms by default but test and trace the entire contact network. This leads to a similar sized outbreak with much fewer healthy children being quarantined.\nIn school environments the higher share of expected asymptomatic cases present a challenge. That’s where random testing can be helpful as it can detect index cases early and avoid or reduce outbreaks. The random testing scenario assumes an aggressive testing regimen where a random sample of a quarter of the students gets tested daily. We don’t have the testing capacity to do this right now, but this is something to consider, especially with cheaper fast-turnaround saliva tests starting to become available. These antigen tests have lower sensitivity and specificity, but those disadvantages are outweigh by their fast turnaround time and ease of use. And that it is in principle much easier to produce these kind of tests at volume and actually use them broadly in schools. This would likely still be complemented by PCR tests to weed out false positives."
  },
  {
    "objectID": "posts/2020-09-08-covid-school-modelling/index.html#upshot",
    "href": "posts/2020-09-08-covid-school-modelling/index.html#upshot",
    "title": "Covid school modelling",
    "section": "Upshot",
    "text": "Upshot\nWhat can we learn from this? The number of infections in school aren’t the main focus. As explained before, the model is hard to calibrate and not that useful to make accurate predictions. The cases the model produces are at the upper end of what one might expect to see. What it does show is how implementation details on the test, trace and isolate (TTI) response matter to moderate the outbreak size.\nThe specific implementation of our TTI protocol matters in controlling school outbreaks, and it can be the difference between schools being infection accelerators and places that experience occasional exposures but don’t add significant transmissions.\nThere are many interesting questions unanswered. The role of classroom and cohort sizes is one of them, the role of exogenous background spread that can seed school cases is another, the role of teachers is completely ignored in these above model runs, and the feedback between school cases and background spread was also ignored in this post.\nIdeally, we first get a better idea what our TTI protocol will look like, and then run an updated post where we fix the TTI and look at how other factors vary, otherwise there are too many moving parts for one post.\nLastly, given the implementation differences especially with respect to TTI and to ventilation, we caution against using Germany as a guide to what level of transmissions we might expect at BC schools. Both of these differences likely have a large impact on how things play out on the ground.\nAs usual, the code for the post is available on GitHub for anyone to reproduce or adapt for their own purposes."
  },
  {
    "objectID": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html",
    "href": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html",
    "title": "First-time buyer Lorenz curves",
    "section": "",
    "text": "At the end of this odd COVID-19 summer we launched a reading group to bring together people interested in diving into papers and books looking at housing issues.\nGeoffrey Meen and Christine Whitehead’s recently released book Understanding Affordability: The Economics of Housing Markets has been the group’s first read. We highly recommend the book, it’s a good read for anyone looking for a practical understanding of how housing markets work and ways to think about supply and demand and what they mean for housing affordability.\nPart of the idea of the reading group was to take some of the methods and insights and use Vancouver or at least Canadian data to reproduce them. Both of us felt that a wider audience might benefit from this, so we decided to turn one aspect into a quick blog post: First-time buyer Lorenz curves.\nAs we’ve argued on this blog time and time again our housing crisis is ultimately a crisis for renters. Meen and Whitehead take this view as well and argue the best metrics for housing affordability should be based on renters, specifically the affordability of renting and the ability for renters to access home ownership."
  },
  {
    "objectID": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#issues-with-price-to-income",
    "href": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#issues-with-price-to-income",
    "title": "First-time buyer Lorenz curves",
    "section": "Issues with price-to-income",
    "text": "Issues with price-to-income\nThe most widely reported income metric on affordability is price-to-income ratio usually reported as the mean or median multiple of market prices of all housing units to (typically) household income of all households regardless of tenure.\nPrice-to-income is an attractive metric because it’s easy to compute, data to produce it is readily available, and it’s easy to understand. But it suffers from a slew of problems, some of which were discussed here before. Meen and Whitehead conclude:\n\nWe are highly critical of the simplest – the house price to earnings or income ratio – despite the fact that it is the most widely used and is built into land use planning policies. The ratio provides no information on the distribution of outcomes across household types and income levels, it can be misleading as an indicator of changes in affordability over time even at the aggregate level, and it is worrying that it is still widely used.\n\nBut composition of housing and households matters. For instance proportions of household-types accoss different areas are far from uniform.\nAlso, as Meen and Whitehead point out, high price-to-income ratios are not considered “bad” by many owner-households and in fact represent owner-households’ generally high net worth in high price-to-income regions. This is important to keep in mind, because for most geographies we’re interested in Canada the majority of households own their homes.\nOne point in particular that Meen and Whitehead stress throughout is that access to capital, or mortgage market constraints, have a large impact on affordability and generally serve to deepen inequality between existing and would-be owners. This applies especially in our low interest and low property tax rate environment (that additionally has seen significant home price appreciation).\nFundamentally, the ability to rent in a city or metro area is at the core of the affordability question. But culturally Canadian place a lot of value on home ownership. And this is reflecting in the preferential tax treatment, most importantly the non-taxation of imputed rent, but also the non-taxation of principle residence capital gains, and even at the provincial level, the homeowner grant which reduces the property tax, as well as extremely low interest loans for property tax deferrals. In light of this, it makes sense that the public is also worried about making the jump from renting to owning, and that policy pays attention to that."
  },
  {
    "objectID": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#first-time-home-buyer-lorenz-curves",
    "href": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#first-time-home-buyer-lorenz-curves",
    "title": "First-time buyer Lorenz curves",
    "section": "First-time home buyer Lorenz curves",
    "text": "First-time home buyer Lorenz curves\nTo better understand the the issues (potential) first-time buyers face Meen and Whitehead employ Lorenz curves. This is trying to understand the question what proportion of (potential) first-time buyers can afford what proportion of the housing stock. For the purpose of the metric we define potential first time buyers in a CMA as non-student renter households not in subsidized housing with income greater than zero between the ages of 20 to 49. Income data for that population is available from the census. This is only a rough approximation but it should do for our purposes. For simplicity we estimate home prices also from the census as estimated by owner-occupiers, in aggregate the differences between home price distributions from different sources does not matter much for our purposes.\nArmed with the income distribution of potential first-time buyers, and the distribution of home prices, we can check what portion of potential buyers can (at least) afford a given proportion of homes in the metro area, giving what Meen and Whitehead refer to as “Lorenz curves” of potential first time buyers in (rough) analogy to the well-known inequality metric.\nThere is still one missing step, we need to determine how much housing a given income level can afford. Here we are focusing strictly on the ability to service a mortgage and assume 5% percent down payment. This also triggers mortgage insurance which brings the effective loan value to 99% of the home value. Starting at home values above $500k stricter down payment requirements kick in, escalating to a minimum 20% down payment for homes over $1M. Here we will assume an effective loan value of 99% for the entire spectrum of homes rather than adjust the down payment value, which implicitly helps account for longer time required to save up for a higher down payment. We use an interest rate of 2.25% and an amortization period of 25 years. Finally, we use mortgage cost as 30% of income as the affordability threshold. 30% is the most widely used affordability threshold, and while financial institutions will lend to borrowers loans with payments above 30% of income those calculations include property taxes, utilities, and stress-testing the borrower(s) for potentially higher future interest rates.\nLet’s take a look what such a Lorenz curve for potential first-time buyers (essentially renters aged 20 through 49) looks like.\n\nIn Calgary, potential first-time buyers at the bottom end of the renter income can’t afford any of the homes. Someone at the 20th income percentile of potential first-time buyers can only afford 5% of Calgary homes. But after that things pick up fast and someone at the 60th percentile can afford 57%, at which point things even out and the Lorenz curve hugs the diagonal.\nThis says nothing about what kind of home someone can afford, or where in the metro area an affordable home may be, or how well e.g. the number of bedrooms in the home matches the requirements of the potential first-time buyers. It also skirts the discussion about “starter homes” or more generally the question if there is a mismatch between homes first-time buyers are buying and the overall distribution of homes in the metro area in terms of quality or size. Or if there should be such a distinction. What this does is it gives a rough matchup between incomes of (potential) first-time buyers and home values.\nLooking at the Calgary data we are led to ask if the Calgary Lorenz curve is typical for Canadian metropolitan areas. Looking at the largest six metro areas, the answer is mostly Yes, except for Toronto and Vancouver.\nNote: The Census Public Use Micro File (PUMF) combines smaller metropolitan areas in the same province together (e.g. Regina-Saskatoon).\n\nLooking more broadly, Victoria and Hamilton have patterns broadly similar to Toronto and Vancouver, with Oshawa also exhibiting a lot of similarities, except in the top income decile.\n\nThis begs several questions: * Is there a good way to condense these graphs into a simple to use metric? * How does such a metric compare to median multiples? * What are ways that we can refine the Lorenz curves to account for needed space and not just affordability?"
  },
  {
    "objectID": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#the-first-times-buyer-gini-index",
    "href": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#the-first-times-buyer-gini-index",
    "title": "First-time buyer Lorenz curves",
    "section": "The first times buyer GINI index",
    "text": "The first times buyer GINI index\nThe term “Lorenz curve” that we have adapted from Meen and Whitehead is a bit of a misnomer, Lorenz curves are used to compare a distribution with it’s cdf, we are comparing the cdfs of two different distributions and compare them to a hypothetical case where both match up. One important difference is that our “Lorenz curves” can take values above the diagonal.\nThere are many ways to condense the comparison between two distributions into an index, we have recently looked in detail at several such indices. In this case we can stick with our Lorenz curve analogy and compute a First-Time Buyer GINI Index, defined as (double) the (signed) area between the first-time buyer Lorenz curve and the diagonal in analogy to the definition of the regular GINI index The regular GINI index takes values between 0 and 1, our first-time buyer GINI index takes values between -1 and 1. A value of -1 would mean that all potential first-time buyers could afford all homes, a value of 1 corresponds to no buyers being able to afford any of the homes. A value of 0 corresponds to the situation where, on average, a buyer at income percentile \\(x\\) can buy all homes below the home value percentile \\(x\\).\n\nTaking the first-time buyer Lorenz curve to be piecewise linear, the calculation of the corresponding GINI coefficient is straight-forward. We contrast the first-time buyer GINI index with the familiar median multiple metric, computed over incomes of all households vs the values of owner-occupied homes.\n\nThere is a broad correspondence between the two metrics, but it’s interesting to explore the differences. Vancouver as expected stands out on the median multiples chart, but it trades places with Toronto when looking at the first-time buyer GINI index."
  },
  {
    "objectID": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#refining-the-lorenz-curves",
    "href": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#refining-the-lorenz-curves",
    "title": "First-time buyer Lorenz curves",
    "section": "Refining the Lorenz curves",
    "text": "Refining the Lorenz curves\nOne way to refine this is to take a closer look at home size requirements. A one-person household has very different requirements from a 3 or 4 person family. Moreover, household income is probably not the best metric here. Household income is useful when looking at ability to pay shelter costs at the current home, but it is less useful for looking at affordability of buying a new home. Households generally don’t buy homes, family units and unattached individuals do. Economic family income (or maybe census family income) and income of unattached individuals is a better metric to use. And we should match up the family size with the number of bedrooms in a home. After all, we are not that concerned about whether an individual is or is not able to purchase a 4 bedroom home, but we are much more concerned about the ability to purchase a studio or 1-bedroom home. Similarly, it’s not that relevant if a family of 3 can purchase a 1-bedroom unit, but it is very relevant if they can purchase a 2 or 3 bedroom unit.\nSo we need to make a decision of housing that’s adequate for an economic family, but also does not compare they to homes where they would be “overhoused”. That’s a tricky thing to do, we are going with the following rough categories:\n\n\n\nFamily size\nNumber of bedrooms\n\n\n\n\nSingle\n0 or 1\n\n\n2 persons\n1 or 2\n\n\n3 persons\n2 or 3\n\n\n4+ persons\n3 or more\n\n\n\n\n\nLooking at the distributions we notice that 2 person families fair quite well in terms of this metric.\n\nVancouver and Montreal stand out with a fairly high share of (owner-occupied) 1 and 2 bedroom homes. Montreal also has a noticeably lower share of 4+ bedroom homes, with Ottawa-Gatineau also showing markedly lower levels."
  },
  {
    "objectID": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#adjusted-first-time-buyer-gini-index",
    "href": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#adjusted-first-time-buyer-gini-index",
    "title": "First-time buyer Lorenz curves",
    "section": "Adjusted first-time buyer GINI index",
    "text": "Adjusted first-time buyer GINI index\nDealing with a slew of different family sizes and bedroom counts gets complicated fast, to simplify things we can try and condense the home value into an “adjusted home value”. For our purposes the question is how home values scale with the number of bedrooms. Experimenting with the functional form it appears that dividing by the square root of the number of bedrooms gives a decent fit to normalize the home values. This dove-tails well with the established concept of adjusted family income, that scales family income by the square root of the family size. As an aside, we note that while the adjusted family income is likely a useful metric to measure housing affordability from the perspective of a family, banks and government regulation generally don’t take family size into consideration in loan underwriting requirements.\n\nWe removed bachelor units from this consideration because there are only very few owner-occupied bachelor units in each CMA. The relationship is nothing to write home about, but good enough for this post. It would be worthwhile to investigate this in more detail. But this likely requires better data, in particular on location as well as better data on dwelling type, which we are missing when working with CMA-level data. Assessment data should work reasonably well for this, but comparable data is not available on a national scale. Ideally we would build our own model, like our (highly experimental and gpu-intensive) interactive fine-geography home valuation model.\nFor now we will go with the simplistic adjusted home value that divides by the square root of the number of bedrooms, and pair that up with adjusted family income. A choice has to be made for how we should match up people and with homes. We can stick with family units, and homes, or take it down to people and bedrooms. Conceptually it might be easier to work with family units and homes, as this is the level at which purchase decisions are made. The difference when taking it down to people and bedrooms is that families with a larger number of members get weighted more, which seems like a reasonable adjustment. For this post we will stick with matching family units to homes.\nNote that this is a little more generous as our previous matchup of bedrooms to families as it implicitly affords each family member their own bedroom, instead of drawing from the distribution of homes with the same or one fewer bedrooms as family members.\n\nThese curves look broadly similar to the ones we had initially by just naively using household income, but taking a closer look we also notice differences, especially at the upper end. We can again condense the data into an adjusted first-time buyer GINI index.\n\nThis has the net effect of making housing more affordable for first-time buyers (And shuffles the order a little.) This suggests that one could make substantial affordability gains for first-time buyers by allowing housing to be used, and sold, more flexibly. In Vancouver we already have an important example of that with a comparatively high proliferation of secondary suites, sometimes officially sanctioned but mostly as informal housing. While this helps housing to be used more flexibly, it does little to help with first-time home buyer affordability as this is lacking flexibility on how portions of the house can be sold. Easing restrictions on flexible use of housing, in particular removing the legally dubious municipal definitions of “family” that regulate how a dwelling unit can be shared, as well as adding flexibility in subdividing homes and bringing informal dwelling units into the official housing stock and allowing these to be sold separately, is likely to enhance first-time home buyer affordability. Splitting or combining strata units remains a formidable undertaking in Canada and is rare, this may be an area where Canada can learn from other countries where this is more common.\nWe should emphasize that having a negative GINI coefficient does not mean that everyone can afford a home matching their position in the income distribution. This is only true on average, as comparing with the Lorenz curve of e.g. Edmonton shows. Condensing the data into an index loses information, and we might want to also retain more complex representations like the Lorenz curve for better context."
  },
  {
    "objectID": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#upshot",
    "href": "posts/2020-09-28-first-time-buyer-lorenz-curves/index.html#upshot",
    "title": "First-time buyer Lorenz curves",
    "section": "Upshot",
    "text": "Upshot\nIn summary, Lorenz curves and the derived GINI index provide an interesting view into affordability for first-time buyers, a group whose position has been substantially weakened by growing wealth inequality combined with low interest and property tax rates. We think this provides a useful metric to watch.\nRefinements, like the adjusted first-time buyer Lorenz curves and GINI index can help remove distributional effects while still being easy to work with. It would be worthwhile to explore this further, and find a better way to add location information. This may also help firm up the best functional form of the adjustment on home values by bedrooms.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes."
  },
  {
    "objectID": "posts/2020-10-22-covid-19-data-in-bc/index.html",
    "href": "posts/2020-10-22-covid-19-data-in-bc/index.html",
    "title": "COVID-19 Data in BC",
    "section": "",
    "text": "We are growing increasingly concerned with the COVID-19 situation in BC. In particular the way there seems to be no strategy or goal to stop rising case numbers, and the relativism that excuses this by pointing to other provinces and countries that are doing worse. At upward of 150 cases a day we are looking at an average of one death a day and unknown numbers, likely in the mid to high two digits, with long lasting morbidity due to a COVID-19 infection."
  },
  {
    "objectID": "posts/2020-10-22-covid-19-data-in-bc/index.html#whats-our-goal",
    "href": "posts/2020-10-22-covid-19-data-in-bc/index.html#whats-our-goal",
    "title": "COVID-19 Data in BC",
    "section": "What’s our goal?",
    "text": "What’s our goal?\nThe basic fact is that at some point we will have to act to at least stop the current growth, if not reduce current daily case counts. So what’s the right level of daily cases at which we should stop the growth? There is no reason to believe that stopping the growth at 10 cases a day, like we had in late spring, is any harder than stopping the spread at 150, or 200, or 500 cases a day. And given that TTI (test-trace-isolate) resources are limited, there is good reason to believe that stopping the growth gets harder the longer we wait. Which begs the question why we wait. And also, why we don’t aim for zero.\nFor some context, let’s consider our daily case counts, cleaned up a bit by removing the strong weekly seasonality in the daily counts and smoothing over nearby variations. For better context we added significant events where BC imposed restrictions or loosened restrictions to react to the behaviour of the spread.\n\nThe case counts at the beginning of the period aren’t directly comparable to the ones toward the middle and end as we were not testing in the same way, and not testing as much. (Testing data in BC has issues too and is largely uninterpretable at this point, we will leave this for another post.)\nWe want to pay particular attention to the period in late spring and early summer, where we dipped into single digit daily new cases, only to let all that progress go to waste and allow the virus to grow again.\n\nBecause of the inherent lag between infections and reported cases we expect about a week or two for changes in COVID-19 restrictions to become visible in the data. Going to Phase 2 seems to have resulted in only a modest increase in cases. In contrast, going to Phase 3 has started us on an upward trajectory undoing all the gains our hard work in Phase 1 has gotten us.\nLooking only at overall provincial numbers looses some important aspects of the story. In the short term, COVID-19 spread is a local story. But local spread can quickly jump and take hold in other areas.\n\nInitially Vancouver Coastal may have acted as a seed, the “Kelowna cluster” visible in yellow around July 18 may have served as a catalyst to trigger the subsequent increase in Fraser and Vancouver Coastal. And the rapid increase in Fraser, reporting over 200 cases in a single day yesterday, may well spill back over into the neighbouring health authorities. Unfortunately we don’t have more detailed epidemiological data available to verify these hunches to better understand how the virus spreads to avoid future outbreaks."
  },
  {
    "objectID": "posts/2020-10-22-covid-19-data-in-bc/index.html#problems",
    "href": "posts/2020-10-22-covid-19-data-in-bc/index.html#problems",
    "title": "COVID-19 Data in BC",
    "section": "Problems",
    "text": "Problems\nHand in hand with formulating a goal that’s different from our current “muddling along without blowing through our hospital capacity until we maybe have a vaccine that’s hopefully effective”, we need to understand some of the problems with our response in BC. Those problems aren’t unique to BC, but I strongly believe that they need fixing in order to have an effective response.\nBC’s problems can be roughly divided into two parts:\n\ncatching up with the science\ncatching up with the data\n\nThe second one is squarely in our wheelhouse, so we will talk mostly about that. That does not mean that the first one is less important, it centres around the issue that Canada and BCCDC have been slow in recognizing the role of aerosol spread and non-symptomatic transmissions. This has translated into problematic policy decisions and is also reflected in the inability to grapple with the over-dispersed nature of the virus that (at this point) manifests itself through the combination of aerosol and non-symptomatic spread. While this does deserve a lot more attention, we will save that for a different post."
  },
  {
    "objectID": "posts/2020-10-22-covid-19-data-in-bc/index.html#catching-up-with-data",
    "href": "posts/2020-10-22-covid-19-data-in-bc/index.html#catching-up-with-data",
    "title": "COVID-19 Data in BC",
    "section": "Catching up with data",
    "text": "Catching up with data\nWe have written about the sorry state of COVID-19 data in Canada before, and things haven’t improved much since. Some provinces and health regions, for example Toronto, have improved the spatial resolution of their data. For example, Toronto is now publishing epidemiologically relevant data, including onset of symptoms date and transmission types, at the local neighbourhood levels. But data gaps abound.\nIn BC the data situation is pretty much as bad as it has been from day 1. It’s frustrating to watch that the PHO is still presenting grossly outdated data in the press briefings and misrepresents the associated outdated models and summary stats as current, when the conclusions often simply don’t hold any more in our fast-moving pandemic environment.\n\nOut of date data\nIn a pandemic it’s incredibly important to have timely data to react fast to a changing landscape. We have a lag between infection and onset of symptoms, which often serves as a trigger for a test, that we cannot control. But we have control over everything that happens after. If we can get people to get a test right when they experience symptoms, and get the test result fast, TTI can stay ahead of the virus. Unfortunately, in BC this process is too slow with still around 5 days between onset of symptoms and test result, which means close contacts of the index case may have already lead to secondary infections before TTI even starts.\nBut this is far from being the only issue, we frequently make decisions based on outdated data. Take the example of the last modelling update on October 5, where the PHO was showing off modelling that put BC on a strong downward trend. However, this modelling was old, and already outdated at the time of the press conference. The dip in cases that the model was reflecting had already been undone in the meantime, and BC was on a clear upward trajectory.\n\nThis type of misinformation, spread by the chief medial officer no less, is dangerous during a pandemic. Claiming that we are “doing well” while current data does not support that can lead the public to let their guard down at a time when the opposite is required. And this isn’t just a problem of using a different model, had the PHO included the data up to the previous day, Oct 4th in their dynamic compartmental model run, the PHO would have also noted that BC was on an upward trajectory.\n\nUsing outdated data to claim that “our growth rate is decreasing” while current data shows the opposite, that’s obviously very problematic. (And not just because it is obvious to anyone watching the video that the PHO mixes up the function with it’s first derivative.) Saying that number of new infections is decreasing (or that the growth rate is negative), when in fact the opposite is true, is sending counter-productive signals to the public.\nOne might hope that BCCDC has up-to-date internal modelling, and only the modelling presented at the press conference lags for some reason. But it’s preposterous to assume the PHO would have made those statements based on outdated modelling if she knew that up-to-date modelling told a different story."
  },
  {
    "objectID": "posts/2020-10-22-covid-19-data-in-bc/index.html#lack-of-spatial-resolution",
    "href": "posts/2020-10-22-covid-19-data-in-bc/index.html#lack-of-spatial-resolution",
    "title": "COVID-19 Data in BC",
    "section": "Lack of spatial resolution",
    "text": "Lack of spatial resolution\nAs we have seen when looking at health region geography data, finger geographies matter in understanding short term trends In BC, much of the implementation details on school opening has been downloaded to the school districts. But school boards don’t have timely information on the COVID-19 spread in their district, if at all.\nIn BC we only have daily data at the Health Authority geography, with weekly updates at the Health Region level. Health region level is only available in image form and needs to be manually transcribed to be useful. Moreover, historical data is not made available and it requires people to scrape the data to establish timelines.\nAs an example we scraped today’s and last week’s data to compute the 7-day incidence, that is the (cumulative) number of cases over seven days per 100k population, at the health region geography.\n\n\nTimely finer geography data does not just enable the population to understand the exposure risk in their community and adjust their behaviour accordingly, it also enables a more tailored government response. For example, in regions in Germany where the 7-day incidence exceeds 50, higher level restrictions are triggered. Masks become mandatory in all public spaces and children have to wear masks full-time in school."
  },
  {
    "objectID": "posts/2020-10-22-covid-19-data-in-bc/index.html#upshot",
    "href": "posts/2020-10-22-covid-19-data-in-bc/index.html#upshot",
    "title": "COVID-19 Data in BC",
    "section": "Upshot",
    "text": "Upshot\nData sits at the base of our pandemic response. Without timely and accurate data, we can’t generate timely and accurate information. Without timely and accurate information we can’t mount an effective response. Our data problems are still massive, and they are preventing us from getting timely and accurate information and hinder our response.\nData on its own is not enough, we also need to have a discussion on what our goals should be. We celebrated days with single digit new cases in late spring. We did not worry when case numbers doubled to 20 a day. We did not act when case numbers doubled yet again to 40. When it doubled to 80 a day we strengthened enforcement a little, at 120 a day we imposed restrictions on bars and banquet halls. But when daily cases doubled again to 160 a day we did nothing. There are no clear goals, we are slowly sliding up the ladder of growing cases. Without a clear goal there won’t be a clear strategy.\nAs usual, the code for this post is available on GitHub in case others find it useful.\n\n\nReproducibility receipt\n\n## [1] \"2020-10-22 20:06:25 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [65bd024] 2020-10-05: higher precision to the percentage of people that have experienced homelessness\n## R version 4.0.2 (2020-06-22)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Catalina 10.15.7\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] ggtext_0.1.0              mountainmathHelpers_0.1.2\n##  [3] CanCovidData_0.1.4        covidseir_0.0.0.9010     \n##  [5] forcats_0.5.0             stringr_1.4.0            \n##  [7] dplyr_1.0.2               purrr_0.3.4              \n##  [9] readr_1.3.1               tidyr_1.1.2              \n## [11] tibble_3.0.3              ggplot2_3.3.2            \n## [13] tidyverse_1.3.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] matrixStats_0.57.0   fs_1.4.1             sf_0.9-6            \n##  [4] cansim_0.3.5         lubridate_1.7.9      httr_1.4.2          \n##  [7] rstan_2.21.2         tools_4.0.2          backports_1.1.10    \n## [10] R6_2.4.1             KernSmooth_2.23-17   DBI_1.1.0           \n## [13] colorspace_1.4-1     withr_2.3.0          tidyselect_1.1.0    \n## [16] gridExtra_2.3        prettyunits_1.1.1    processx_3.4.4      \n## [19] curl_4.3             compiler_4.0.2       git2r_0.27.1        \n## [22] cli_2.0.2            rvest_0.3.6          xml2_1.3.2          \n## [25] bookdown_0.19        scales_1.1.1         classInt_0.4-3      \n## [28] callr_3.4.4          digest_0.6.26        StanHeaders_2.21.0-6\n## [31] rmarkdown_2.3        pkgconfig_2.0.3      htmltools_0.5.0     \n## [34] dbplyr_1.4.4         rlang_0.4.8          readxl_1.3.1        \n## [37] rstudioapi_0.11      generics_0.0.2       jsonlite_1.7.1      \n## [40] inline_0.3.16        magrittr_1.5         loo_2.3.1           \n## [43] Rcpp_1.0.5           munsell_0.5.0        fansi_0.4.1         \n## [46] lifecycle_0.2.0      stringi_1.5.3        yaml_2.2.1          \n## [49] pkgbuild_1.1.0       grid_4.0.2           blob_1.2.1          \n## [52] parallel_4.0.2       crayon_1.3.4         haven_2.3.1         \n## [55] gridtext_0.1.1       hms_0.5.3            knitr_1.30          \n## [58] ps_1.4.0             pillar_1.4.6         codetools_0.2-16    \n## [61] stats4_4.0.2         reprex_0.3.0         glue_1.4.2          \n## [64] evaluate_0.14        blogdown_0.19        V8_3.2.0            \n## [67] RcppParallel_5.0.2   modelr_0.1.8         vctrs_0.3.4         \n## [70] cellranger_1.1.0     gtable_0.3.0         assertthat_0.2.1    \n## [73] xfun_0.18            broom_0.7.0          e1071_1.7-3         \n## [76] class_7.3-17         units_0.6-7          ellipsis_0.3.1"
  },
  {
    "objectID": "posts/2020-12-07-what-to-expect-from-an-empty-homes-tax/index.html",
    "href": "posts/2020-12-07-what-to-expect-from-an-empty-homes-tax/index.html",
    "title": "What to Expect from an Empty Homes Tax",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\n\nEmpty Homes Taxes are back in the news!\nIn a very short time period, we’ve got Vancouver raising its Empty Homes Tax rate from 1% to 3%, based in part on a report from CMHC about a sharp rise in condos on the rental market, we’ve got Toronto eyeing its own Empty Homes Tax, and now reports suggest that even Ottawa is considering getting in on the game.\nWe’ve long argued that Empty Homes Taxes are a pretty good tax. Consider it as equivalent to a bump up to property taxes (which cities like Vancouver could really use!) paired with a principal residency exemption, kind of like BC home owner’s grant, but also applicable to property owners who rent out their properties on a long-term basis, hence providing incentive to keep housing occupied.\nThe incentive is real. But we have questions about whether Empty Homes Taxes are being oversold as solutions to the broader housing crises facing Metro Vancouver, Toronto, and Ottawa. To start with, as we’ve demonstrated previously, none of these metro areas rank particularly high in North America in terms of vacant housing stock on census day. Indeed, all Canadian cities appear to be on the low end, implying relatively few of the abandoned homes and vacation pied-a-terres that seem to push up vacancies in many US cities.\n\nVancouver and Ottawa appear high for Canada, but somewhere between low and middle-of-the-road for North America as a whole. Toronto is definitely on the low end. Of note, a scan of the data for the US, which includes reason for vacancy, suggests that regular housing processes (dwellings up for sale or rent, awaiting new residents; dwellings caught in temporary legal limbo after the death of an owner, etc.) account for a substantial portion of vacant homes overall. For metros at the high end of vacancies, these numbers are boosted by abandoned homes and/or pied-a-terre vacation homes. This suggests that abandoned homes and pied-a-terres just aren’t that common in Canada.\nWith some caveats, we can test this by looking at Vancouver’s Empty Homes Tax and BC’s Speculation & Vacancy Tax data. Most homes that appear as if they might be empty qualify for exemptions from these taxes, reflecting regular housing processes. After exemptions, there just don’t seem to be very many empty dwellings left. In the most recent Vancouver EHT data, declared vacancies range by neighbourhood from 0.08% (in Sunset & Grandview Woodlands) to 1.26% in the West End, roughly matching the City of Vancouver’s 0.7% of properties non-exempt from the tax in the provincial SVT data (excepting out “Satellite Families”, which would bump the figure to 1%).\nOf course, taxes may be bringing dwellings back into the rental market that weren’t there in 2016, meaning our EHT and SVT data might be reflecting big declines in empty units. What about that CMHC study showing a bump of condos being rented out after the Empty Homes Tax was imposed? Well, funny story… first it’s important to know that the study is based on condo managers reporting from their Form K, which are meant to be filed when condo units are rented out, but in the past have been largely inconsequential. Indeed, in previous work we have highlighted that the CMHC estimate of rented condos in Metro Vancouver differs significantly with census estimates. \nHere it’s notable that the first year of the EHT’s existence did not see a great many condos added to the rental market. But after the Speculation and Vacancy Tax came into place, the number of condos being rented out seemed to grow quite a bit. Was this a real change, perhaps because the added taxes became higher? Or did this represent a reporting change? Due to a variety of policy changes (including SVT), suddenly failure to file Form K has more teeth. As a result, it’s likely the reporting compliance for From K has gone up significantly. In other words, we’re not actually certain that a slew of condo units recently came onto the rental market. It may be, instead, that a slew of condo units already on the rental market were suddenly reported correctly. Overall, it is hard to get robust estimates of how many units have entered the market in response to the tax, but there’s no doubt some have. Looking at City of Vancouver data on homes that are either exempt or pay the tax, and cross-referencing this with the Ecotagious study estimating vacancy by electricity usage, we can arrive at a very rough estimate of the number of homes returned to the market being roughly double the number of homes that end up paying the tax. Which is a sizable achievement.\nSo what should Toronto and Ottawa expect from an empty homes tax? We have previously used City of Vancouver data to give fairly accurate projections for the Speculation and Vacancy Tax, and we can apply the same method to Toronto and Ottawa at the city level. The estimate is quite crude, it simply scales the units “unoccupied” on census day to match the City of Vancouver Empty Homes Tax numbers. So let’s take a quick look at what kind of dwelling registered as “unoccupied” in the Census.\n\nWhile there is some variation across the regions, the duplex category, which generally captures houses with basement suites, comes out universally with the highest share of unoccupied homes. We have written about this at length before and it should not be surprising given the flexible nature of secondary suites that they are used flexibly, which frequently means that they aren’t rented out. Of course, these suites also aren’t taxed as empty, since they’re considered part of one residential property and can so easily be reabsorbed into the main dwelling. The high prevalence of basement suites in Vancouver is a big part of what drives up its vacancy rate in the census.\nTaking account differences in housing stock we can apply a crude formula from the City of Vancouver Empty Homes Tax experience, assuming exemptions are structured similarly. Accordingly we can project that an Empty Homes Tax would capture around 2,000 units in Ottawa and 6,000 in Toronto. Roughly twice that number might be induced to re-enter the rental market in each city.\nSo should Toronto consider an Empty Homes Tax of its own? Relative to the size of Toronto’s housing market, we probably shouldn’t expect an Empty Homes Tax to a) find very many empty homes, or b) create much new revenue. We’re likely looking at shifting over no more than a single percentage point of units into the market. But adding any new units to the market is good. And we like Empty Homes Taxes overall. Just ensure expectations are set accordingly!\nWhat about Ottawa? Similar wisdom pertains. Set expectations accordingly! At the same time, Ottawa is instructive to consider insofar as it’s the centre of government for Canada. We actually kind of expect a certain number of properties will be empty a substantial portion of the year. Why? Well, Members of Parliament and Senators are both expected to represent other parts of the country in Ottawa. In other words, they’re expected to split their time between Ottawa and elsewhere. Indeed, Senators are still required to own at least $4,000 worth of real property in the province they represent, though there’s currently a bill to repeal that requirement (property requirements for MPs were abolished with the 1920 Dominion Elections Act). Again, not to say an Empty Homes Tax is a bad idea for Ottawa, and why not tax politicians a bit more? But Ottawa is also uniquely well positioned to demonstrate why some people, including - but not limited to - MPs and Senators, maintain some form of residence in multiple places. And Empty Homes Taxes necessarily tend to hit hardest for anyone who finds it difficult to choose just one.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{lauster2020,\n  author = {Lauster, Nathan and von Bergmann, Jens},\n  title = {What to {Expect} from an {Empty} {Homes} {Tax}},\n  date = {2020-12-07},\n  url = {https://doodles.mountainmath.ca/posts/2020-12-07-what-to-expect-from-an-empty-homes-tax},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLauster, Nathan, and Jens von Bergmann. 2020. “What to Expect from\nan Empty Homes Tax.” MountanDoodles (blog). December 7,\n2020. https://doodles.mountainmath.ca/posts/2020-12-07-what-to-expect-from-an-empty-homes-tax."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html",
    "title": "BC back to (COVID) school",
    "section": "",
    "text": "(Joint with HsingChi von Bergmann, health science education researcher, professor at UBC Faculty of Dentistry, previously associate professor and science teacher educator at UofC)\nSchools across BC are set to resume this coming Monday, January 4th. Meanwhile British Columbia universities have delayed the start of the semester by a week to gauge the fallout from Christmas and New Year celebrations, and other Canadian provinces with similar case rates are delaying their school starts, Ontario by at least a week, Manitoba will keep grades 7 to 12 in remote learning for two weeks while younger children can choose between remote and in-class learning, and Regina is keeping students in remote learning for the first week after the break. The discovery of the new UK variant, which is thought to have higher transmission rates and may have higher infectiousness among children adds to these concerns.\nResuming schools immediately after winter break is in line with BC’s libertarian approach to COVID-19 that relies on recommendations instead of robust public health measures, leaving vulnerable populations to protect themselves. The failure to protect our most vulnerable reaches across all strata, but it can most easily be seen when looking at the case rates among our seniors, in particular the 90+ year olds.\nThis failure to protect our seniors is amplified by the high death rate of cases in that age group, and by the fact that many live in Long Term Care homes and are unable to protect themselves and instead have to rely on public health guidance and protocols to protect them. And we failed them miserably.\nFor families with immune-compromised or otherwise vulnerable household members this means parents have to decide whether to send children back to school on January 4th, without the benefit of better gauging the effect of the holidays. And without even knowing the case counts for the three previous days, as BC is not reporting cases on weekends or holidays and four days of data will only come in once school is out on Monday afternoon.\nThe latest reasoning behind the official BC school guidelines are explained in the December 23 modelling presentation starting on page 13, so let’s take a closer look."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-14",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-14",
    "title": "BC back to (COVID) school",
    "section": "Modelling update slide 14",
    "text": "Modelling update slide 14\n\n\n\npage 14\n\n\nThere are several issues with this slide, it’s hard to know where to start. Probably the most problematic aspect of this slide is that it very misleading. People that have been following the pandemic in BC and are familiar with basic math will easily see through this, but the same can’t be said for some journalists\n This tweet has fortunately been deleted by now, but it serves as a good example to show just why the slide is so misleading to laymen. The slide is doing a string of calculations aimed at minimizing the impact of COVID-19 on schools, leading the journalist to misrepresent the positivity rate as 7 out of 1000 instead of 7 out of 100. Of course the whole pretext in the slide is set up to encourage such misrepresentation, starting with the vacuous headline that “most school age children with symptoms don’t have COVID-19”, the same is of course true for any other age group in BC. After all, the age-specific positivity rates in BC top out at roughly 12% and the statement would remain true even if positivity rates for public symptom-based testing would go up to 49%.\nMoreover, as anyone familiar with elementary school mathematics will notice, there is more going on here. If 1 out of 100 students get tested and 7 out of 100 test positive, then 7 out of 10,000 students have been confirmed to have COVID-19, not 7 out of 1000 as the slide suggests. In other words, at least one of these three statements on the slide is wrong. Which one we can’t say for sure because the BCCDC has so far refused to release data on school age children, but we can try and triangulate things. Instead of school age children, the province does release data on children under 10 and children aged 10-19. While these are not the same, looking at these two groups can give a reasonable expectation of where school age children may land.\nLet’s start with the last metric and look at the incidence of COVID-19 among K-12 children.\n\nOverall case counts were low for the first part of the pandemic, initially because we severely under-tested and then because case counts were low over summer. With the start of our second wave in June we see a continuous increase in the incidence of COVID in children. The provincial modelling slides don’t give information over the timeframe, but between the start of school on September 8th and the time of the modelling update on December 23rd we have 0.4% of children below 10 and 0.8% of children between 10 and 19 being confirmed with COVID-19, so the overall prevalence of “fewer than 7 out of 1000” seems reasonable.\nNext up is the positivity rate of 7% for children. This one is tricky, as the province is still refusing to release data on public testing volumes and public testing positivity rates. But we now have graphs in the weekly Situation Reports that give information on public testing and public testing positivity rates, and Brett Favaro has been racking up karma points by digitizing the provincial graphs on age-specific positivity rates and presenting them in a more convenient form. Strikingly, the positivity rate for 10-14 year old children spiked at 11% for the week ending December 12th, the last date for which we have testing data with the next update due on January 8th.\n(Yes, BC is refusing to provide data on public testing so that people resort to scraping it out of graphs, but even then BC goes without an update on public testing data for several weeks, meanwhile tricking the public and journalists into using the misleading testing data that includes private testing that severely skews testing volumes and positivity rates that’s available via the dashboard and data download. But that’s another post.)\nAnd we only have data back to the last two weeks of October, so we are missing a month and a half of the school semester. But the available time frame does coincide roughly with the time of rapid case growth, so we should not be too far off by just using this time frame. To estimate the overall positivity rate for school age children, which was much lower at the beginning of the time period, we also need to know the overall number of tests performed on children, which we don’t have. As the number of assumption we have to put into this is getting out of hand we might as well just eyeball it off of Brett’s graph and take the week of November 15 through 21 as an estimate for the overall time period. That yields positivity rates of about 5.5 for &lt;10yo, 8% for 10-14yo and 8% for 15-19yo, which is in the same ballpark as on overall rate of 7%.\nWhich leaves us with the last stat that 1 in 100 school age children got tested for COVID-19. While we don’t have data, the Situation Reports has a graph depicting this, and we can scrape out the data, or for our purpose just eyeball it off the graph. It shows an “average weekly testing rate per 100,000” of roughly 1000 for each of the three school age children age groups for Phase 3b, so starting September 13. That’s not quite the same thing as saying 1 in 100 school age children got tested per week as some children got tested more than once, for example taking both, a NPS and gargle test. But accounting for that, plus noting that there were 15 weeks in the school year, we get to roughly 1 in 10 children getting tested and not 1 in 100 as claimed in the provincial modelling report. Mystery solved, although this exercise makes it abundantly clear how hard it is to fix provincial incompetency without access to proper data."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-15",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-15",
    "title": "BC back to (COVID) school",
    "section": "Modelling update slide 15",
    "text": "Modelling update slide 15\n\n\n\npage 15\n\n\nThe next slide is about “COVID-19 exposures in a school setting”, specifying a date range between Nov 1 and Dec 18, claiming “there were 526 schools exposures” in BC schools in that time frame. However, the BC COVID School Tracker volunteer project lists 1,392 exposure notifications in this time frame. This glaring discrepancy is readily resolved by noting that the province, in it’s effort to downplay school exposures, somewhere along the editing process reduced “exposures in 526 schools” to (the grammatically questionable) “526 schools exposures”. And indeed, the school tracker project’s 1,392 exposures occurred in 577 different schools, with the remaining small discrepancy likely due to questions about whether to use the notification or one of the exposure dates as underlying date variable. The next sentence confirms the reading by concluding that “7 in 10 schools have not had a school exposure” while applying ample text highlighting. And the following graph title also states correctly that the 512 refers to the number of schools not the number of exposures, although the graphs are fairly useless as they aren’t normalized by school population, continuing with embarrassingly poor data visualization. (Pro tip, if you give out the raw data and others will likely volunteer to clean it up and make more informative graphs for the benefit of everyone, including the government.)\nThe biggest issue with the slide is that what matters is the number of exposures, not the number of schools. If one school received 24 school exposure notifications (actually two schools received this many in Fraser Health), then that is not something that should get trivialized into saying this is a “school with COVID-19 exposure”. That school had 24 separate exposure notifications, 24 times when parents were considering if they should continue to send their kids to school the next couple of days, 24 times the parent rumour mill is trying to understand how close the case was to their particular kids, 24 times where parents are disparately trying to learn if the index case wore a mask, and 24 times the parents second-guess the Health Authority definition of “close contact” (in a province that still officially ignores aerosol-based spread and still negates that pre-symptomatic transmissions play a significant role). Let alone the worries all teachers and staff have had in these high-exposure schools. No one should have to go to work in such a high risk environment without more stringent health and safety measures being implemented first. Schools have been prevented by the public health office to implement more stringent measures, for example requiring mask use at all times, in the past.\nTrivializing these 24 exposure notifications by equating them to a school that had 1 is an affront to the anxiety and grief that these 24 exposure notifications cost parents, children, teachers and staff."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-16",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-16",
    "title": "BC back to (COVID) school",
    "section": "Modelling update slide 16",
    "text": "Modelling update slide 16\nThe following slides try to fill in some of the missing context in the previous slide by looking at schools in Vancouver Coastal and Fraser.\n\n\n\npage 16\n\n\nPage 16 normalizes number of students and staff exposed in Vancouver Coastal by the overall school population, which is good. In true BCCDC fashion is using a different (and hard to pin down) timeframe from the previous slide, underscoring yet again that the BCCDC is incapable of systematic analysis on things like school exposures but has to rely on a patchwork of incoherent bits and pieces to build their understanding.\nBut let’s go with the slide and read this as exposures up to December 12th. The slide claims that\n\nthere were over 600 cases associated with the fewer than 200 exposures\nschool-based transmissions occurred for fewer than 20 of these exposures\n\nNot much of this information can be independently verified, but the BC COVID School Tracker project listing 313 school exposures in Vancouver Coastal in this time frame. It’s hard to say how to best resolve this, maybe Vancouver Coastal used a different timeframe, or Vancouver Coastal skipped the exposures from the beginning of the school year when VCH was refusing to inform the public on school exposures against the PHO expectations, or some other reason.\nBut taking the information at face value it leaves us to guess about 400 out of 600 infected students or staff that did not result in an exposure notification. Some of these will be the result of in school-transmissions, which we are reassured only occurred in the context of “fewer than 20” exposures. But even allowing for multiple transmissions in a single exposure, this does not add up. And that’s not even accounting for the fact that the timelines of COVID are such that we won’t be able to isolate most close contacts before they become infectious and thus most close contacts should result in another exposure notification in the case that they did become infected. For some cases pubic health may have determined that they weren’t at school during their infectious period, maybe someone developing symptoms and testing positive on a Tuesday after a long weekend and betting that the person likely wasn’t infectious on the Friday before, given that the the density of infections is only about one quarter four days before symptom onset. Some decisions about cutoffs have been made depending on risk tolerance, although the parent’s risk tolerance may differ from public health’s risk tolerance. Either way, all this reasoning is unlikely to explain the 400\nOverall, this slide raises more questions than it answers. And the rotated tiled square visualization does not help."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-17",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-17",
    "title": "BC back to (COVID) school",
    "section": "Modelling update slide 17",
    "text": "Modelling update slide 17\n This slide has more details on the settings in which cases among students and staff got infected. The first bullet states that they were able to link 89% of all school related case to another case. The second is probably the most alarming, pointing out that one quarter of all school related cases were in staff. So staff are over-represented in school cases, which makes sense given our testing protocol that focuses exclusively on symptomatic cases likely misses about half of student cases.\nThird bullet point rules out in-school transmissions as a cause for over 90% of cases, which is a bold statement given the inherent uncertainties that comes with case linking. At the very least this should qualify that it’s only looking at confirmed transmissions between confirmed cases.\nFor most of it there is no way to verify the information provided in the slide, although we can use the snapshot of data on school transmissions that was shared previously, but we will have to focus on students by picking the respective (somewhat broader) age groups and looking at overall BC data.\n\nThis shows that out of the roughly 695 cases of children below 20 that were linked to a probable transmission case and setting, about 116, or 17%, were identified to be in a school setting. Given that this includes cases of non-school aged children and young adults, and this also does not take into consideration of cases where transmissions were determined to possibly have occurred in multiple settings (labelled by BCCDC as “unnamed setting”), for example a case where the transmission may have occurred within a household or within school, the claim that fewer than 10% of transmissions happened in school seems to imply that almost all teachers were determined to have contracted COVID-19 outside of school and on top of that transmission patterns have shifted or are different in Vancouver Coastal compared to overall BC."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-18",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#modelling-update-slide-18",
    "title": "BC back to (COVID) school",
    "section": "Modelling update slide 18",
    "text": "Modelling update slide 18\n\n\n\npage 18\n\n\nThe last slide regarding schools is about Fraser Health, the hardest hit health authority. It continues with the questionable practice of focusing on schools that had exposure events instead of the number of exposures. The BC COVID School Tracker project details 1,289 exposure events across 390 schools, with the discrepancy in the number of schools perfectly explained when cutting the notification data off at the date of the press conference and some coming in after that. It details that 133 public and independent schools in Surrey school district had at least one exposure event. The Ministry of Education counts 110 public and independent schools in Surrey, implying that around 121% of Surrey schools have had at least one exposure event, many of which had multiple exposure events.\nThe slide goes on to detail that 12.8% of schools had one or multiple “potential in-school transmission events”, which is telling in that it allows for uncertainty in case linking when cases pop up in schools after exposure events by labelling those as “potential” in-school transmissions, yet at the same time rules out the possibility of missing cases and transmission events due to e.g. the high rate of asymptomatic cases in children. Of course this selective bias is again perfectly consistent with the overall theme of public health working to minimize in-school transmissions in these slides."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#conclusion",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#conclusion",
    "title": "BC back to (COVID) school",
    "section": "Conclusion",
    "text": "Conclusion\nThis turned out to be a very long post, what’s the takeaway? Public health messaging on schools continues to be frighteningly bent on minimizing in-school exposures and transmissions, going as far as producing slides that are misleading enough to trick journalists into obviously false statements. Basic mathematics continues to elude BCCDC, with slides apparently undergoing zero quality control where inconsistencies resulting in numbers being off by an order of magnitude slipping through unnoticed.\nThe BCCDC analysis on schools continues to be a patchwork of disparate approaches, time intervals and mini-stats that make it impossible to paint a comprehensive picture. It is very hard to believe that the BCCDC does have consistent statistics internally, but for some reason decided to release an incoherent patchwork publicly. The alternative, that the BCCDC has no clear picture themselves, is frightening. But it’s hard to come up with an alternative reasonable explanation of this mess."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#are-schools-safe",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#are-schools-safe",
    "title": "BC back to (COVID) school",
    "section": "Are schools safe?",
    "text": "Are schools safe?\nThe short answer is, we don’t know. The BCCDC doesn’t make relevant data publicly available and the modelling presentation instills little confidence that the BCCDC has paid sufficient attention to this.\nCan schools be made sufficiently safe? We believe the answer is Yes in almost all cases. The basic mechanics are clear. If community transmissions is low there will be few in-school exposure events and school can resume with our minimal distancing protocols, including cohorting and wearing of masks outside the classroom, augmented by ensuring proper ventilation, in order to minimize the chance of in-school transmissions. But if the level of community transmission rises we will need to strengthen in-school protocols accordingly to keep the frequency of in-school transmissions low. The obvious low-hanging fruits are to require mask wearing at all times and pay extra attention to ventilation, but if community transmissions rise to the high levels we have e.g. seen in parts of Surrey, we will need to consider split in-person and remote learning setups and even school closures until schools can again be operated safely. Not doing so means leaving families with vulnerable household members fend for their own health and safety.\nWe could, if the BCCDC would choose to do so, monitor community transmissions at the school district (or finer) geographic level to assist in this. We could use the federal funding to improve ventilation in schools as outlined in BC’s Back to School plan. Providing timely and actionable data, as well as clear justifications of school protocols based on clearly cited research, can help restore confidence in public health policies regarding schools.\nAt the same time we should support remote learning options any family that decides to pull their children out of school because e.g. they have immune-compromised household members.\nAs usual, the code to pull in the data and make the graphs is available on GitHub in case anyone wants to reproduce or adapt them."
  },
  {
    "objectID": "posts/2021-01-02-bc-back-to-covid-school/index.html#addendum-feb-17-2021",
    "href": "posts/2021-01-02-bc-back-to-covid-school/index.html#addendum-feb-17-2021",
    "title": "BC back to (COVID) school",
    "section": "Addendum (Feb 17, 2021)",
    "text": "Addendum (Feb 17, 2021)\nWe were made aware that Réka Gustafson wrote a response to our blog post, embedded below for convenience. We will not go into too much detail, but briefly address two main points.\nGustafason’s response further highlighted how the BCCDC slides weren’t the result of a coherent analysis on the situation in schools, but a mishmash of incoherent points, made with data from constantly shifting time frames and other ad-hoc slices of the data. To illustrate: the response to point 1 slide 14 claimed that the data on slide 14 being correct, but each data point was constructed from a different time window. Why anyone would slice data this way, let alone present that to the public, is byond our understanding.\nSimilarly, the response to point 4 slide 17 reiterated the point that VCH school transmission data are very different from overall BC data, highlighting the piecemeil approach taken.\nWe are noting that Gustafson’s insistence in their confident statements about transmissions in BC schools are hard to justify simply because that a large fraction of children with COVID-19 remain asymptomatic, many of whom will be missed by BC’s testing protocol. Gustafson’s statements are based on confirmed cases and determined likely transmission routes, the latter of which is necessarily biased by public health priors. Statements like “for the vast majority of these […] further transmission does not occur in the school” fail to reflect the large uncertainties inherent in the underlying data.\nAs a takeaway Gustafson writes that &gt; Going forward, presentations will be reviewed to ensure dates and details are added either directly to the slides, or to an appended document, to minimize the likelihood of misinterpretation.\nReviewing slides before presentation and providing more in-depth description of the methods are critical for transparent and effective communication. But the main problem isn’t that slides were missing details, it’s that there was no coherent analysis of COVID-19 in schools underlying this presentation in the first place. If there was a coherent analysis there would be no need for this confusing mix of metrics scattered across the slides.\n\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Mountain Doodles&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Mountain Doodles&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-1\"&gt;Other Canadian data focused blogs:&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:https://homefreesociology.comHomeFreeSociology\"&gt;HomeFreeSociology&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:https://www.dshkol.comDmitry-Shkolnik&#39;s-blog\"&gt;Dmitry Shkolnik’s blog&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:https://www.simoncoulombe.comSimon-Coulombe&#39;s-blog\"&gt;Simon Coulombe’s blog&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:https://financesofthenation.ca/category/commentary/Finances-of-the-Nation\"&gt;Finances of the Nation&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:https://deny.substack.comDeny’s-Substack\"&gt;Deny’s Substack&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:https://higheredstrategy.com/author/alex/Alex-Usher&#39;s-blog\"&gt;Alex Usher’s blog&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Archive\"&gt;Archive&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/archive.html\"&gt;/archive.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://github.com/mountainMath/mountain_doodles\"&gt;https://github.com/mountainMath/mountain_doodles&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://twitter.com/vb_jens\"&gt;https://twitter.com/vb_jens&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://www.linkedin.com/in/vb-jens/\"&gt;https://www.linkedin.com/in/vb-jens/&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:mailto:jens.mountainmath.ca\"&gt;mailto:jens.mountainmath.ca&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.xml\"&gt;/index.xml&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"hidden\" data-render-id=\"sidebar-header\"&gt;\n&lt;div class=\"sidebar-header-item\"&gt;\n&lt;p&gt;&lt;strong&gt;Too much data, too little time&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"hidden\" data-render-id=\"sidebar-footer\"&gt;\n&lt;div class=\"sidebar-footer-item\"&gt;\n&lt;p&gt;Mountain Doodles is an insomnia driven side project, born out of random questions, trying to give partial answers through data. During daytime at &lt;a href=\"https://mountainmath.ca\"&gt;MountainMath&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"hidden\" data-render-id=\"footer-center\"&gt;\n\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Mountain Doodles - BC back to (COVID) school&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Mountain Doodles - BC back to (COVID) school&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Mountain Doodles - BC back to (COVID) school&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Mountain Doodles&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;School is about to re-start after Christmas break, lots of questions remain.&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;School is about to re-start after Christmas break, lots of questions remain.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) =&gt; {\n    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';\n    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';\n    let newTheme = '';\n    if(darkModeDefault) {\n      newTheme = isAlternate ? baseTheme : alternateTheme;\n    } else {\n      newTheme = isAlternate ? alternateTheme : baseTheme;\n    }\n    const changeGiscusTheme = () =&gt; {\n      // From: https://github.com/giscus/giscus/issues/336\n      const sendMessage = (message) =&gt; {\n        const iframe = document.querySelector('iframe.giscus-frame');\n        if (!iframe) return;\n        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');\n      }\n      sendMessage({\n        setConfig: {\n          theme: newTheme\n        }\n      });\n    }\n    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;\n    if (isGiscussLoaded) {\n      changeGiscusTheme();\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  const darkModeDefault = false;\n  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n    toggleGiscusIfUsed(toAlternate, darkModeDefault);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  const viewSource = window.document.getElementById('quarto-view-source') ||\n                     window.document.getElementById('quarto-code-tools-source');\n  if (viewSource) {\n    const sourceUrl = viewSource.getAttribute(\"data-quarto-source-url\");\n    viewSource.addEventListener(\"click\", function(e) {\n      if (sourceUrl) {\n        // rstudio viewer pane\n        if (/\\bcapabilities=\\b/.test(window.location)) {\n          window.open(sourceUrl);\n        } else {\n          window.location.href = sourceUrl;\n        }\n      } else {\n        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));\n        modal.show();\n      }\n      return false;\n    });\n  }\n  function toggleCodeHandler(show) {\n    return function(e) {\n      const detailsSrc = window.document.querySelectorAll(\".cell &gt; details &gt; .sourceCode\");\n      for (let i=0; i&lt;detailsSrc.length; i++) {\n        const details = detailsSrc[i].parentElement;\n        if (show) {\n          details.open = true;\n        } else {\n          details.removeAttribute(\"open\");\n        }\n      }\n      const cellCodeDivs = window.document.querySelectorAll(\".cell &gt; .sourceCode\");\n      const fromCls = show ? \"hidden\" : \"unhidden\";\n      const toCls = show ? \"unhidden\" : \"hidden\";\n      for (let i=0; i&lt;cellCodeDivs.length; i++) {\n        const codeDiv = cellCodeDivs[i];\n        if (codeDiv.classList.contains(fromCls)) {\n          codeDiv.classList.remove(fromCls);\n          codeDiv.classList.add(toCls);\n        } \n      }\n      return false;\n    }\n  }\n  const hideAllCode = window.document.getElementById(\"quarto-hide-all-code\");\n  if (hideAllCode) {\n    hideAllCode.addEventListener(\"click\", toggleCodeHandler(false));\n  }\n  const showAllCode = window.document.getElementById(\"quarto-show-all-code\");\n  if (showAllCode) {\n    showAllCode.addEventListener(\"click\", toggleCodeHandler(true));\n  }\n    var localhostRegex = new RegExp(/^(?:http|https):\\/\\/localhost\\:?[0-9]*\\//);\n    var mailtoRegex = new RegExp(/^mailto:/);\n      var filterRegex = new RegExp(\"https:\\/\\/doodles\\.mountainmath\\.ca\");\n    var isInternal = (href) =&gt; {\n        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);\n    }\n    // Inspect non-navigation links and adorn them if external\n \tvar links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');\n    for (var i=0; i&lt;links.length; i++) {\n      const link = links[i];\n      if (!isInternal(link.href)) {\n        // undo the damage that might have been done by quarto-nav.js in the case of\n        // links that we want to consider external\n        if (link.dataset.originalHref !== undefined) {\n          link.href = link.dataset.originalHref;\n        }\n      }\n    }\n  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {\n    const config = {\n      allowHTML: true,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start',\n    };\n    if (contentFn) {\n      config.content = contentFn;\n    }\n    if (onTriggerFn) {\n      config.onTrigger = onTriggerFn;\n    }\n    if (onUntriggerFn) {\n      config.onUntrigger = onUntriggerFn;\n    }\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      if (note) {\n        return note.innerHTML;\n      } else {\n        return \"\";\n      }\n    });\n  }\n  const xrefs = window.document.querySelectorAll('a.quarto-xref');\n  const processXRef = (id, note) =&gt; {\n    // Strip column container classes\n    const stripColumnClz = (el) =&gt; {\n      el.classList.remove(\"page-full\", \"page-columns\");\n      if (el.children) {\n        for (const child of el.children) {\n          stripColumnClz(child);\n        }\n      }\n    }\n    stripColumnClz(note)\n    if (id === null || id.startsWith('sec-')) {\n      // Special case sections, only their first couple elements\n      const container = document.createElement(\"div\");\n      if (note.children && note.children.length &gt; 2) {\n        container.appendChild(note.children[0].cloneNode(true));\n        for (let i = 1; i &lt; note.children.length; i++) {\n          const child = note.children[i];\n          if (child.tagName === \"P\" && child.innerText === \"\") {\n            continue;\n          } else {\n            container.appendChild(child.cloneNode(true));\n            break;\n          }\n        }\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(container);\n        }\n        return container.innerHTML\n      } else {\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(note);\n        }\n        return note.innerHTML;\n      }\n    } else {\n      // Remove any anchor links if they are present\n      const anchorLink = note.querySelector('a.anchorjs-link');\n      if (anchorLink) {\n        anchorLink.remove();\n      }\n      if (window.Quarto?.typesetMath) {\n        window.Quarto.typesetMath(note);\n      }\n      // TODO in 1.5, we should make sure this works without a callout special case\n      if (note.classList.contains(\"callout\")) {\n        return note.outerHTML;\n      } else {\n        return note.innerHTML;\n      }\n    }\n  }\n  for (var i=0; i&lt;xrefs.length; i++) {\n    const xref = xrefs[i];\n    tippyHover(xref, undefined, function(instance) {\n      instance.disable();\n      let url = xref.getAttribute('href');\n      let hash = undefined; \n      if (url.startsWith('#')) {\n        hash = url;\n      } else {\n        try { hash = new URL(url).hash; } catch {}\n      }\n      if (hash) {\n        const id = hash.replace(/^#\\/?/, \"\");\n        const note = window.document.getElementById(id);\n        if (note !== null) {\n          try {\n            const html = processXRef(id, note.cloneNode(true));\n            instance.setContent(html);\n          } finally {\n            instance.enable();\n            instance.show();\n          }\n        } else {\n          // See if we can fetch this\n          fetch(url.split('#')[0])\n          .then(res =&gt; res.text())\n          .then(html =&gt; {\n            const parser = new DOMParser();\n            const htmlDoc = parser.parseFromString(html, \"text/html\");\n            const note = htmlDoc.getElementById(id);\n            if (note !== null) {\n              const html = processXRef(id, note);\n              instance.setContent(html);\n            } \n          }).finally(() =&gt; {\n            instance.enable();\n            instance.show();\n          });\n        }\n      } else {\n        // See if we can fetch a full url (with no hash to target)\n        // This is a special case and we should probably do some content thinning / targeting\n        fetch(url)\n        .then(res =&gt; res.text())\n        .then(html =&gt; {\n          const parser = new DOMParser();\n          const htmlDoc = parser.parseFromString(html, \"text/html\");\n          const note = htmlDoc.querySelector('main.content');\n          if (note !== null) {\n            // This should only happen for chapter cross references\n            // (since there is no id in the URL)\n            // remove the first header\n            if (note.children.length &gt; 0 && note.children[0].tagName === \"HEADER\") {\n              note.children[0].remove();\n            }\n            const html = processXRef(null, note);\n            instance.setContent(html);\n          } \n        }).finally(() =&gt; {\n          instance.enable();\n          instance.show();\n        });\n      }\n    }, function(instance) {\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            div.style.left = 0;\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n        // Handle positioning of the toggle\n    window.addEventListener(\n      \"resize\",\n      throttle(() =&gt; {\n        elRect = undefined;\n        if (selectedAnnoteEl) {\n          selectCodeLines(selectedAnnoteEl);\n        }\n      }, 10)\n    );\n    function throttle(fn, ms) {\n    let throttle = false;\n    let timer;\n      return (...args) =&gt; {\n        if(!throttle) { // first call gets through\n            fn.apply(this, args);\n            throttle = true;\n        } else { // all the others get throttled\n            if(timer) clearTimeout(timer); // cancel #2\n            timer = setTimeout(() =&gt; {\n              fn.apply(this, args);\n              timer = throttle = false;\n            }, ms);\n        }\n      };\n    }\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"mountainmath/mountain_doodles\"\n        data-repo-id=\"R_kgDOLuoDtA\"\n        data-category=\"Blog posts\"\n        data-category-id=\"DIC_kwDOLuoDtM4Cetfc\"\n        data-mapping=\"pathname\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"bottom\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        data-loading=lazy\n        async&gt;\n&lt;/script&gt;\n&lt;input type=\"hidden\" id=\"giscus-base-theme\" value=\"light\"&gt;\n&lt;input type=\"hidden\" id=\"giscus-alt-theme\" value=\"dark_dimmed\"&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &nbsp;\n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &lt;div class='footer-contents'&gt; &lt;/div&gt;  \n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/2021-01-15-capital-gains-income/index.html",
    "href": "posts/2021-01-15-capital-gains-income/index.html",
    "title": "Capital Gains Income",
    "section": "",
    "text": "We have previously look at T1FF tax data which is an extremely rich annual administrative data source. The cansim tables have a range of variables to inform about incomes of individuals, families (sliced by number of children, including zero children), low income statistics, and just statistics about the number of taxfilers and dependants by age.\nIt’s available on cansim for Canada overall, the provinces and CMAs/CAs. That’s great, but sometimes it’s nice to have finer geographic detail. Especially when trying to understand neighbourhood change, census tract level data on incomes and basic demographics is extremely valuable, and is now available on CensusMapper for all years 2000 through 2018 at the census tract level.\nWhen working with data we always need to pay attention to the definitions, and income data is no difference. Income data generally only contains regular income sources, and some income data, like for example capital gains income, is excluded. But capital gains income can be quite sizable, so it’s a good idea to take a more detailed look at it."
  },
  {
    "objectID": "posts/2021-01-15-capital-gains-income/index.html#geographic-distribution",
    "href": "posts/2021-01-15-capital-gains-income/index.html#geographic-distribution",
    "title": "Capital Gains Income",
    "section": "Geographic distribution",
    "text": "Geographic distribution\nWe have seen above that (taxable) capital gains income is not distributed uniformly across income groups. And we know income groups aren’t distributed uniformly geographically.\nTherefore it is reasonable to expect that (taxable) capital gains income is not distributed uniformly geographically. Unfortunately capital gains is not a regular census variable, and it is not part of our census tract level T1FF data extract. But the 2011 NHS did report on capital gains income. Which allows to take a look at the geographic distribution of capital gains income in 2010, taking Vancouver as an example. While NHS data is generally sub-optimal as a data source, we have seen that pure income data, not taking cross-tabs with other census variables, performs quite well, despite some ill-informed claims to the contrary."
  },
  {
    "objectID": "posts/2021-01-15-capital-gains-income/index.html#upshot",
    "href": "posts/2021-01-15-capital-gains-income/index.html#upshot",
    "title": "Capital Gains Income",
    "section": "Upshot",
    "text": "Upshot\nCapital gains income is a significant source of income that is absent from general income data. And the capital gains data we have still misses non-taxable capital gains income, for example capital gains from selling a principal residence.\n(Taxable) capital gains income is especially prevalent in higher income groups, likely due to the strong (yet imperfect) correlation between income and wealth. And it also shows strong geographic variation, and also varies considerably over time.\nThis suggests that analysis based on income may need to pay more attention to capital gains income, at least as a cross-check or robustness test. To this end it would be useful to also have census tract level capital gains income. Or even better, start adding “income including (taxable) capital gains” as a standard product in income metrics in StatCan tabkles and census data.\nMoreover, I would like to see a longitudinal analysis to identify to what extent taxpayers manage to structure all or most of their income as capital gains, having regular (although possibly volatile) capital gains income (almost) every year. This may well already exist and I would appreciate pointers.\nFrom a policy perspective, capital gains income should get a very close look. Michael Smart recently made a convincing case for a capital gains tax reform.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-07-14 10:54:49 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [f96b9e6] 2021-06-09: upate image text\n## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] patchwork_1.1.1           mountainmathHelpers_0.1.2\n##  [3] tongfen_0.3.3             cansim_0.3.9             \n##  [5] cancensus_0.4.3           forcats_0.5.1            \n##  [7] stringr_1.4.0             dplyr_1.0.7              \n##  [9] purrr_0.3.4               readr_1.4.0              \n## [11] tidyr_1.1.3               tibble_3.1.2             \n## [13] ggplot2_3.3.3             tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.6         lubridate_1.7.10   class_7.3-19       assertthat_0.2.1  \n##  [5] digest_0.6.27      utf8_1.2.1         R6_2.5.0           cellranger_1.1.0  \n##  [9] backports_1.2.1    reprex_2.0.0       evaluate_0.14      e1071_1.7-7       \n## [13] httr_1.4.2         blogdown_1.3       pillar_1.6.1       rlang_0.4.11      \n## [17] readxl_1.3.1       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.8     \n## [21] munsell_0.5.0      proxy_0.4-26       broom_0.7.6        compiler_4.1.0    \n## [25] modelr_0.1.8       xfun_0.23          pkgconfig_2.0.3    htmltools_0.5.1.1 \n## [29] tidyselect_1.1.1   bookdown_0.22      fansi_0.5.0        crayon_1.4.1      \n## [33] dbplyr_2.1.1       withr_2.4.2        sf_1.0-0           grid_4.1.0        \n## [37] jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0    DBI_1.1.1         \n## [41] git2r_0.28.0       magrittr_2.0.1     units_0.7-2        scales_1.1.1      \n## [45] KernSmooth_2.23-20 cli_2.5.0          stringi_1.6.2      fs_1.5.0          \n## [49] xml2_1.3.2         bslib_0.2.5.1      ellipsis_0.3.2     generics_0.1.0    \n## [53] vctrs_0.3.8        tools_4.1.0        glue_1.4.2         hms_1.1.0         \n## [57] yaml_2.2.1         colorspace_2.0-1   classInt_0.4-3     rvest_1.0.0       \n## [61] knitr_1.33         haven_2.4.1        sass_0.4.0"
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html",
    "title": "On COVID Trend lines",
    "section": "",
    "text": "COVID-19 got me thinking about trend lines and the different ways people generate and interpret them. This is a question that’s of course more general than just COVID-19, but let me use this as an example to explain some very basic principles. This post is motivated by discussions I have had with a number of journalists, including Chad Skelton who nerd-sniped me into writing a post on trend lines and a thread discussing trend lines with Roberto Rocha and Tom Cardoso."
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html#why-trend-lines",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html#why-trend-lines",
    "title": "On COVID Trend lines",
    "section": "Why trend lines",
    "text": "Why trend lines\nData is inherently noisy, trend lines are a way to filter out noise and focus on the main movements of a time series. “Are we trending up or down?” is a simple yet important question, but it can be hard to answer by just looking at the raw data. In that sense a trend line is simply a way to de-noise the data.\nBut how do we distinguish noise from the “real” signal in the data? First we need to understand the “data generating process”. In case of COVID-19 timelines that’s viral growth, so we expect the main signal of our time series to be exponential growth with periods of constant growth rates, with noise added on top of that. (This can also be tested directly by running tests on the COVID-19 cases time series.)"
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html#semi-log-plots",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html#semi-log-plots",
    "title": "On COVID Trend lines",
    "section": "Semi-log plots",
    "text": "Semi-log plots\nAt the beginning of the pandemic John Burn-Murdoch at the Financial Times popularized semi-log plots that elegantly incorporate this basic insight into the data generating process, periods of constant (exponential) growth rates show up as straight lines on a semi-log plot, making it easy and intuitive to see if countries were “bending the curve” (i.e. reducing their growth rate). The plots can also serve as forecasting models by people simply mentally extending the (straight) lines.\nHowever, while the semi-log plots helped people to understand doubling times, people were still shocked how fast daily case numbers were piling up. It does not solve the fundamental problem that a lot of people don’t have an intuitive understanding of exponential growth. Plus this kind of visualization became less useful once countries bend the curve and reduced daily case counts to much lower levels, followed again by periods of rising cases.\nAnd semi-log plots still don’t solve the problem of how to remove noise, even if human brains are good at mentally adding a line through a noisy set of points, intuiting a trend line."
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html#trend-lines",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html#trend-lines",
    "title": "On COVID Trend lines",
    "section": "Trend lines",
    "text": "Trend lines\nOne way to de-noise COVID-19 timelines that gets a lot of use is to use moving averages. This approach simply replaces each data point with the average taken in within a window around that point. There are several fundamental problems with this approach.\n\nMoving averages introduce a time lag\nMoving averages disrespect the data generating process\nMoving averages are not very good at smoothing\n\nTo understand this, consider the following synthetic example of a timeline with exponential growth followed by exponential decline, with (multiplicative) random (normally distributed) noise. We generate 100 data points and show three different types of moving averages, plus an STL trend line which we will explain in more detail later.\n\nThe first thing we notice is that moving averages lag. 7-day averages lag by 3 days, 14 day averages lag by 7 days. That’s ok if we aren’t that interested in the most recent trends, but is a problem if recent trends are the focus.\nSometimes people “solve” this problem by shifting the trend line to the right, as shown in the “right-aligned” example above, which makes the problem of the lag less obvious. But of course this introduces a new problem, non-data oriented viewers will misinterpret the shifted trend line as not having a lag, and the shifted trend line representing the actual trend. In short, shifting the moving average is quite misleading. This is particularly problematic if the shifted moving average is overlayed over case count data.\nAnother problem with moving averages is that it’s not a good idea to take averages when the data generating process is multiplicative. This has the effect that moving averages systematically under-estimate the trend line during growth periods and over-estimates it during periods of decline. This can be fixed by first taking logs before averaging (and then exponentiating again).\nLastly, the moving averages do a poor job at removing noise. We still see a lot of wiggles, especially in the 7 day moving averages, that have nothing to do with the underlying trend. Taking wider averaging windows can help with that, but we loose temporal resolution of trends and most importantly increase the lag.\nAll of this can be seen much more clearly on a semi-log plot.\n\nWhich leaves us with the question of what we should be using, if not rolling averages. But before we get there, we need to look at one other source of noise that we see in COVID-19 data: a very strong weekly pattern."
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html#weekly-patterns-and-seasonal-adjustments",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html#weekly-patterns-and-seasonal-adjustments",
    "title": "On COVID Trend lines",
    "section": "Weekly patterns and seasonal adjustments",
    "text": "Weekly patterns and seasonal adjustments\nIt is now common knowledge that COVID-19 timelines show a strong weekly pattern that can distract from spotting underlying patterns. This can be easily verified by testing the time series for seasonality. The choice of window size in rolling averages is informed by that and is thus chosen as (a multiple of) 7 days. Sometimes commenters interested in less laggy metrics will take week over week differences in daily case counts to discern trends. The existence of this weekly trend tells us that we should model the daily case counts as made up of a trend line, a weekly pattern and random noise. And the weekly pattern is relatively large, accounting for this is essential when determining trend lines. To complicate things, the weekly pattern itself is also time-varying, the pattern we saw earlier in the pandemic is different from the pattern we are seeing today. Fortunately this problem is not unique to COVID data, and there is a wealth of well-established methods to deal with time series decompositions into trend, seasonal pattern, and random noise. (Here seasonality is a technical term that refers to the main mode of the repeating pattern in the de-trended time series, in this case 7-days.) Accounting for seasonality sounds like a big thing, but we do this all the time when dealing with time series. For example, Statistics Canada does exactly this for many of their time series and publish a “seasonally adjusted” version. Time to try out some methods using real COVID-19 data.\nTo start out we use a decomposition based on moving averages. That is we first determine the trend line using moving 7-day averages, and then compute the seasonal pattern by averaging over the residuals for each day of the week, and interpret the remaining residual as random noise. This decomposition will generate the exact same trend line as our moving average above and suffer from the same time lag problem. What this will do is help us better understand the problems with moving averages.\n\nWe notice that the random component varies strongly with the case numbers, when case numbers are relatively now the random component is low, when case numbers are high the random component is large. This tells us right away that we should not choose an additive model.\nTo verify this we can choose a multiplicative decomposition that models seasonal and random components as multiplicative (rather than additive) components of cases.\n\nThe trend line is identical to the previous version, but the random component now looks much more regular. It is larger early in the pandemic, which we should expect as our testing was not very consistent at that time.\nWhat this allows us to do is also show a seasonally-adjusted timeline, taking out the expected weekly variation.\n\nThe seasonal decomposition gives us a better way to think about by how much individual day case counts are above or below the moving average under consideration of the weekly variation in case counts. In particular we notice how the raw case counts (circles) are more noisy than the seasonally adjusted (black) line that remove the weekly pattern from the raw data. Which adds some insight, but still leaves several problems. One is that we still have a 3-day time lag, which further delays our response if we use moving averages as a guide. The other is that the seasonal pattern changes over time. To see this, we divide our time series up into several intervals, using the “phases” identified by the BCCDC as a guide, and compute the decomposition separately for each.\n\nThis way of slicing the data into different phases is very naive, but we see clear differences in the seasonal pattern the decomposition picked up in the different phases. And more importantly, it managed to reduce the size of the “random” component, or in other words, the seasonal pattern and trend line better describe the case data when we fit the seasonal pattern to sub-intervals of our timeline. The trend line will now show 6 day gaps where we broke up the time intervals, missing three days on each end. And the trend line still shows features that look more like noise than signal. We are on the right track, but we need some slightly more sophisticated tools."
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html#stl-decomposion",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html#stl-decomposion",
    "title": "On COVID Trend lines",
    "section": "STL decomposion",
    "text": "STL decomposion\nLots of ink has been spilled on time series decomposition, for this post we will focus on STL decomposition which in our view is a good compromise between simplicity and effectiveness when dealing with COVID-19 timelines.\n\nAs a result we have\n\na cleaned trend that lacks obvious signs of random noise\na trend line that has no lag and also covers the most recent days\na varying seasonal pattern\na remainder that has not suffered much by increasing smoothing in the trend\n\n\nThe difference between these two methods is not large, but it’s noticeable. The STL decomposition manages to remove more noise, and most importantly, give an estimate of the trend for the most recent three days too. Accounting for the weekly seasonality is essential for this.\nI won’t go into details how the sausage is made, in broad strokes STL is an iterative process of fitting a Loess trend and estimating the seasonal component.\nOne disadvantage of STL trend lines is that new data can wiggle the last data points. Smoothing and the estimation of the seasonal component happens in both forward and backward time, so new data can impact the tail of the curve. The effect will generally be small, but it is something to watch out for."
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html#right-censoring",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html#right-censoring",
    "title": "On COVID Trend lines",
    "section": "Right censoring",
    "text": "Right censoring\nUnfortunately, no discussion on covid data can be complete without talking about right-censoring of data. The issue is that COVID-19 data systems often lag. The common understanding is that case data get reported each day, and we expand our time series day by day as we add new cases. But unfortunately that’s not how it works. For reasons not clear to me, each new data release wiggles a bunch of data points for previous days, often even for days several months back.\nMore importantly this is especially acute for the most recent day, which typically gets a bunch of extra cases as new data comes in. What this means for trend lines is that the tail is systematically biased down. We have not explored this systematically, but here are some typical examples of differences in case count between two recent consecutive days.\n\nDuring the past few weeks, frequently about 20 cases have been added to the most recent day as new data comes in. Additionally cases get added or shifted around for earlier dates. This is obviously problematic. On January 7 BCCDC announced that they were switching over to a new data system that was getting case data directly from the labs, which would result in more timely and better quality data. Which it did. But data issues still remain, and it’s frustrating that these aren’t openly discussed and documented, but instead left for everyone who uses the data to discover for themselves.\nWe visualize the effect of the right-censoring on the end of the tail of the data series for the dates we used above.\n\nThe effect of the right-censoring is clearly visible in the STL trend lines. It also shows up in the moving averages that we added as dashed lines, but it is less pronounced as the moving average lags by three days. We do not adjust for this effect in our auto-updating STL trend lines for BC. However, doing so would be prudent for modelling applications, in particular ones that put extra emphasis on recent trends to derive their forecasts.\nThe same kind of right-censoring happens in a much more severe way when looking at data by symptom onset, as for example Island Health does. Showing data by symptom onset can be very useful for analysis, but it is a very poor choice for a “dashboard” where people come to look for how things are going. The (weirdly overlapping) bar graph is the only graphical representation of case counts on the Island Health dashboard, and this will likely lead quite a few people to believe that cases have come down recently when this is entirely due to right-censoring and the opposite is the case.\nRight-censoring can be avoided to some extent by adopting the “journalist’s case counts” instead of using dashboard case counts. The “journalist’s counts” are the case numbers announced in the news briefings. There is no digital data on this, the numbers have to be manually assembled into time lines, and each release contains the cumulative count for that day, including the part (if any) that gets re-distributed over previous days. The resulting timeline trades long-run accuracy for avoiding right-censoring in the short-run. I was hoping that the new BCCDC data system would avoid the right-censoring and both timelines would merge, but unfortunately that does not seem to be the case."
  },
  {
    "objectID": "posts/2021-01-30-on-covid-trend-lines/index.html#upshot",
    "href": "posts/2021-01-30-on-covid-trend-lines/index.html#upshot",
    "title": "On COVID Trend lines",
    "section": "Upshot",
    "text": "Upshot\nTrend lines are important for understanding where we are, how we got there and getting an indication of the direction of where things are going. They are quick and easy to do and involve minimal outside choices. As such they are a very useful descriptive scaffolding for understanding time series.\nMoving averages are one possible choice, but they add extra lag and need to be used responsibly. Right-aligned moving averages should be avoided as they hide the data lag which is often not obvious to the audience and can thus be misleading. In the case of COVID-19 data, moving averages don’t respect the data generating process and will lead to systematic under- and overestimates during periods of rising or falling cases, respectively. Moving averages are also not very good at removing all noise. The saving grace for rolling averages is that they are stable when new data comes in, existing values won’t change. That’s the tradeoff for the extra data lag.\nThe simplicity of moving averages is another appeal, most people will understand how it’s derived. This is particularly important when data is to be used to trigger public health interventions. In cases like this it can be very useful to use a rolling sum instead of a rolling average, that avoids the misinterpretation as a trend line and the triggers for public health measures can be encoded in terms of rolling sums. This is exactly how the 7-day incidence metric, the cumulative cases in the past 7 days per 100k population, works in Germany. It’s easy to understand, does not change as new data comes in (assuming data processes are reasonably clean and right-censoring is minimal) and makes anti-COVID measures predictable. For example, German guidelines call for all children to wear masks in schools at all times when the regional 7-day incidence rises above 50, and for older children the cutoff is at 35.\nWe feel that STL decompositions give a good balance between simplicity and effectiveness for trend lines. Choosing a multiplicative decomposition is useful to more closely model the data generating process. STL trend lines have a superior smoothing compared to moving averages, making it easier to spot underlying trends. At the same time, the incorporation of seasonal adjustment allows to remove the weekly signal, find better trend lines, and also give good estimates of the trend line for the most recent days, removing the data lag that plagues moving averages. If data is exceptionally noisy, STL can be tuned to be robust against outliers.\nThere are several ways to improve on STL trend lines. Pure time series models like ARIMA attempt to extract information from the random component, which may hold patterns that we have not used in the STL decomposition. At that point it would probably be better to choose a more structured model and directly model viral spread.\nWhile models of viral spread are extremely useful, they leave the world of descriptive analysis. Models of viral spread involve a myriad of choices and the results will be model dependent. Fitting the models can be computationally intensive, which can add a different kind of lag. Ideally we would use simple descriptive STL decompositions for quick visual checks on trend lines and communication purposes, and fit an ensemble of viral models to gain deeper understanding, as well as for forecasting purposes.\nAs usual, the code for this post available on GitHub in case anyone wants to reproduce or adapt it.\n\n\nReproducibility receipt\n\n## [1] \"2021-01-30 21:51:34 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [73cb8fa] 2021-01-31: covid trend lines\n## R version 4.0.3 (2020-10-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n## [1] forcats_0.5.0   stringr_1.4.0   dplyr_1.0.3     purrr_0.3.4    \n## [5] readr_1.4.0     tidyr_1.1.2     tibble_3.0.4    ggplot2_3.3.3  \n## [9] tidyverse_1.3.0\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.5        git2r_0.27.1      cellranger_1.1.0  pillar_1.4.7     \n##  [5] compiler_4.0.3    dbplyr_1.4.4      tools_4.0.3       digest_0.6.27    \n##  [9] lubridate_1.7.9.2 jsonlite_1.7.2    evaluate_0.14     lifecycle_0.2.0  \n## [13] gtable_0.3.0      pkgconfig_2.0.3   rlang_0.4.9       reprex_0.3.0     \n## [17] cli_2.2.0         rstudioapi_0.13   DBI_1.1.0         yaml_2.2.1       \n## [21] blogdown_0.19     haven_2.3.1       xfun_0.18         withr_2.3.0      \n## [25] xml2_1.3.2        httr_1.4.2        knitr_1.30        fs_1.4.1         \n## [29] hms_0.5.3         generics_0.1.0    vctrs_0.3.5       grid_4.0.3       \n## [33] tidyselect_1.1.0  glue_1.4.2        R6_2.5.0          fansi_0.4.1      \n## [37] readxl_1.3.1      rmarkdown_2.5     bookdown_0.19     modelr_0.1.8     \n## [41] blob_1.2.1        magrittr_2.0.1    backports_1.2.0   scales_1.1.1     \n## [45] ellipsis_0.3.1    htmltools_0.5.0   rvest_0.3.6       assertthat_0.2.1 \n## [49] colorspace_2.0-0  stringi_1.5.3     munsell_0.5.0     broom_0.7.4      \n## [53] crayon_1.3.4"
  },
  {
    "objectID": "posts/2021-02-10-industrial-strength-zombies-vancouver-edition/index.html",
    "href": "posts/2021-02-10-industrial-strength-zombies-vancouver-edition/index.html",
    "title": "Industrial Strength Zombies: Vancouver Edition",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nThe “real estate has swallowed Vancouver’s economy” zombie is back, with wild claims by a City Councillor that\nHere we want to try and put the zombie out of our misery (again!), but also use this moment to ask some interesting questions about Vancouver history and what we can get from the long-form census. Mostly what we get from the census, of course, is what people list as their jobs. We can use this to ask a series of questions, including:\nJust how many people work in the real estate industry in Vancouver? Is it growing?\nWhat about finance? Are we turning into a “Global City”?\nHave these activities truly replaced selling logs (or other extractive industries) as the basis for Vancouver’s economy in terms of jobs?\nHow about manufacturing? Didn’t we used to make things?\nWhat about retail? Or health care and social services? Are we mostly relegated to being a regional commerce and service centre for BC?\nWhat about the “creative class”? Is it growing? And what even is that?\nBefore we get to that we should mention that there is another way to look at industries, instead of using the census to look at people and their jobs, we can look at money (GDP). There is no GDP data for small area units like municipalities within a CMA, but CMA level (and higher geographies) GDP data is available from StatCan and we have written extensively about the size of the Real Estate Industry in terms of GDP before.\nBut here, for jobs, we got some nice longitudinal data to answer these questions, looking at the Industrial classification of our workforce via Census running back to 1971! The biggest trick is making the industrial categories speak to our questions above and to one another across time. There were three major shifts in categories, going from SIC 1970 to SIC 1980 and finally to NAICS (and various refinements of NAICS, which are relatively minor). We can also break out interesting municipalities within Metro Vancouver. Here we’ll explore the City of Vancouver, Surrey (its largest suburb), Maple Ridge (an outlying working class suburb), and West Vancouver (its wealthiest suburb), providing some sense of geographic variation in the structuring of the labour force through time. Some of those city geographies changed through our timeframe, for consistency we will use 2016 census subdivision boundaries throughout.\nLet’s start with an overview of our categories for each of the periods covering the major categories.\nLet’s start to tackle the real estate question by examining two general groups: those engaged in building (construction), and those engaged in sales & leasing (real estate agents, managers, etc.). In 1971-1981, we get categories for “Construction Industries” and “Finance, Insurance, and Real Estate” so we can’t entirely pull out real estate. But this is a start. By 1986-1996, we still get “Construction” separated, but we pull “Real Estate operator and insurance agent” apart from “Finance and insurance.” From 2001-2016, we get consistent categories with “Construction” separated from “Real estate and rental and leasing.”\nSo let’s start with construction work.\nConstruction work looks either stable or cyclical, with low points in 1986 and 2001 rising to high points in 1981, 1991, and 2016. Of note, only in the outlying suburb of Maple Ridge do we see our most recent census year (2016) eclipsing previous high points in terms of construction labour force. This reflects a dearth in building through recent decades across much Metro Vancouver, leaving us with our present housing deficit. We’re only now approaching the levels of construction that were prominent in past cyclical peaks. In general, we can think of construction work as varying cyclically and geographically, but occupying about 5%-10% of the workforce.\nWhat about the rest of the Real Estate Industry? All those realtors and property managers?\nThese folks are not as big a part of the workforce as the construction industry, occupying about 2%-3% of the workforce in most municipalities. This appears to be remarkably stable through the decades. But there’s one big exception, and that’s in West Vancouver. The metro’s ritziest suburb is the only one with more people engaged in the real estate industry than in the construction industry, with the former reaching up to 7% of the workforce.\nWhat about Finance? This is often grouped in with Real Estate, but extends more broadly into banking. As we recall from above, Finance is mixed up with Insurance and Real Estate in 1971-1981, but separated into “Finance and Insurance” from 1986 onward. By some definitions, a rise in Financial occupations and related services helps differentiate the world’s “Global Cities” from the rest. Does Vancouver look like an emerging Global City? Let’s take a look…\nIf we’re a rising Global City, we appear to be getting there very slowly. Indeed, there’s not much change in Finance in the City of Vancouver proper, with a bit more evidence of a rise in the suburbs. Geographically, Finance generally tracks with Real Estate, occupying the most people in West Vancouver. But the peak Finance year there was in 2001, when Real Estate was at its nadir.\nWe can combine Finance back with Real Estate and Construction to get perhaps the most comprehensive look at what’s sometimes termed FIRE (Finance Insurance Real Estate) industries. This allows us to go back to our full time-line, from 1971-2016, though we should still be wary of changing definitions through the era.\nOverall, we get the sense that even this widest possible categorization of the Real Estate related sector generally provides around 15% of our municipal jobs. Fewer in the City of Vancouver and more in West Vancouver. Vancouver and Surrey show a fairly stable share of jobs in these sectors, Maple Ridge and West Vancouver show an increasing trend. The reason for the variation is diverse, Surrey and Maple Ridge have more construction workers, West Vancouver is heavier in Finance.\nJust to send the zombie home, let’s put this on a map. Here’s the full geographic distribution of Real Estate and Construction as a proportion of the labour force in each municipality. We start the map in 1986, where the quote above begins (and where many critics trace Vancouver’s turn toward real estate as arising after Expo 86). So let’s see how is started and how it’s going.\nOverall the picture is… not much change. Definitely not in the City of Vancouver. Maple Ridge got more construction workers and West Vancouver got more high-end realtors. The tiny communities of Belcarra and Anmore traded places in seeing slightly higher proportions in the sector. But nowhere do we see real estate and construction as dominant. For a fully interactive map, head over here.\nHuh. So did the quote above get it backward? Did we actually go from selling real estate to selling logs?\nAs it turns out, logging and forestry have been a very small part of Vancouver’s labour force for a long time. Indeed, in newer years this category is so small it gets lumped in with agriculture. In 1970, back when Maple Ridge remained at its most remote, it still only recorded just over 2% of its work force in the forestry industry.\nBut maybe we’re still extracting! What about mining? Mining makes up a similarly small portion of the labour market, and the consistent categorization makes for an easier way to track this through to the present.\nSomewhat strikingly, the biggest proportion of the population engaged in mining is in West Vancouver, reaching all the way up to 1.5% in 2011. Are these rough-and-ready miners, back from working their tunnels? No. These are mostly mining executives, living in Vancouver’s swankiest suburb.\nWe can combine the above two industries with agriculture to get a fairly consistent picture of the combined categories through time, tracking SIC Divisions A, C, and D and NAICS 11 and 21. Together these speak to the “Staples” of the Canadian economy insofar as the country’s history has been linked to international trade. These industries have always been exceedingly small in Vancouver proper. But Surrey and Maple Ridge have seen marked declines as they’ve gradually shifted from more rural primary sites of timber and agriculture to more integrated positions as metropolitan suburbs. That said, even if the workforce remains small, the Agricultural Land Reserve insures agriculture continues to be a defining feature of the metropolitan landscape.\nSo if most of us are neither selling logs nor selling real estate, then what are we doing? Are we… making things? We’re certainly no Detroit or Hamilton, but the idea doesn’t seem too bizarre. After all, the rise of manufacturing drove the rise of big cities through the Nineteenth and early Twentieth centuries. So let’s take a peek at manufacturing! Fortunately for us, it’s been pretty consistently defined since 1971. How’s it doing?\nWoof! Back in 1971, manufacturing really had a claim in the region, accounting for more than one in five jobs in Surrey. It used to beat the Construction industry! But it’s declined precipitously - by roughly two-thirds - enabling the Construction industry to pull ahead. Hello North American de-industrialization!\nSo we don’t mostly sell real estate, we never mostly sold logs, and we don’t manufacture very much. What do we do? A big answer is Retail. Retail alone is nearly as large as Finance, Real Estate and Construction combined and surpasses Manufacturing. And it’s pretty evenly distributed across municipalities (even if it increasingly pays too little to get a place in West Vancouver).\nWhat else do we do? We take care of people! Let’s have a look at Health Care and Social Services. Here we see a widespread rise over time across the Metro Region. Health and social services are now remarkably evenly distributed across our four exemplar municipalities.\nRetail, health, and services are basic city functions, providing hubs for their surrounds. When it comes to more specialized services (e.g. Women’s and Children’s Hospital) Vancouver helps serve and take care of the entire province.\nFinally, and perhaps trickiest to define, let’s briefly touch on the “Creative Class” as those often considered the drivers of our new, post-industrial economies. Popularized by Richard Florida, they’ve been understood as those “involved in the creation of new knowledge, or use of existing knowledge in new ways” (e.g. Cliffton 2008, p. 68). This is often defined rather loosely (those working in science, and maybe arts, and information and stuff) or via occupation. How could we think about it in terms of industry? Let’s smash together some things and see what happens. In our most recent era, 2001-2016, we can combine “Educational services” with “professional, scientific and technical services” as constitutive of a knowledge core with “Information and cultural industries” and “Arts, entertainment and recreation” as representing more of our aspirationally Bohemian, Hollywood North-type creativity. Unfortunately, back in the 1986-1996 period, we lose most of these categories, “Educational service” is there, but the rest is gone, probably absorbed into “Other services.” In 1971-1981, we don’t even get “Educational Service” broken out. What do we see across the Twenty-First Century so far? Is Vancouver increasingly creative?\nKind of! We can see a definite rise in the City of Vancouver itself, as well as in its largest suburb of Surrey. For Maple Ridge and West Vancouver, the historical patterns are less clear, but we get a real sense of geographic sorting. West Vancouver, in particular, seems to be a place that many of our “creative class” aspire to live. At least the ones that make money.\nWe note that in the City of Vancouver and in West Vancouver the creative class on it’s own clearly outperforms our widest possible categorization of the Real Estate related sector, whereas the situation is reversed in Surrey and Maple Ridge.\nOverall, there is no evidence to support the zombie narrative that Vancouver once sold logs and now we sell real estate. Instead, we get the sense that Vancouver has a relatively diverse economy. It’s solidly backed by the supportive role in retail and services that the metropolis plays for the province as a whole. But its growth is arguably also supported by a rising “creative class” replacing older manufacturing jobs. Our industrial strength diversity leaves the region in a pretty good economic position. But adding a few more construction workers would really help with our housing shortage!\nAs usual, the code for this post is availabe on GitHub for anyone to reproduce and adaped. That data we used for this post is a custom tabulation that we have made use of before on several occasions that only covers the Vancouver and Toronto CMAs. Interested analysts can tweak the code to break out their own municipalities and industries."
  },
  {
    "objectID": "posts/2021-02-10-industrial-strength-zombies-vancouver-edition/index.html#note",
    "href": "posts/2021-02-10-industrial-strength-zombies-vancouver-edition/index.html#note",
    "title": "Industrial Strength Zombies: Vancouver Edition",
    "section": "Note",
    "text": "Note\nAn earlier version of this post had a problem with graphs for multiple categories not stacking properly which has been fixed now. The previous version can be accessed in the GitHub version control.\n\n\nReproducibility receipt\n\n## [1] \"2021-02-22 09:03:23 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [69a0e31] 2021-02-22: exponential covid post\n## R version 4.0.3 (2020-10-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.2 forcats_0.5.0            \n##  [3] stringr_1.4.0             dplyr_1.0.4              \n##  [5] purrr_0.3.4               readr_1.4.0              \n##  [7] tidyr_1.1.2               tibble_3.0.4             \n##  [9] ggplot2_3.3.3             tidyverse_1.3.0          \n## [11] cancensus_0.4.2          \n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.0  xfun_0.18         haven_2.3.1       colorspace_2.0-0 \n##  [5] vctrs_0.3.5       generics_0.1.0    htmltools_0.5.0   yaml_2.2.1       \n##  [9] blob_1.2.1        rlang_0.4.9       pillar_1.4.7      glue_1.4.2       \n## [13] withr_2.3.0       DBI_1.1.0         dbplyr_1.4.4      modelr_0.1.8     \n## [17] readxl_1.3.1      lifecycle_0.2.0   munsell_0.5.0     blogdown_0.19    \n## [21] gtable_0.3.0      cellranger_1.1.0  rvest_0.3.6       codetools_0.2-16 \n## [25] evaluate_0.14     labeling_0.4.2    knitr_1.30        fansi_0.4.1      \n## [29] broom_0.7.4       Rcpp_1.0.5        scales_1.1.1      backports_1.2.0  \n## [33] jsonlite_1.7.2    farver_2.0.3      fs_1.4.1          hms_0.5.3        \n## [37] digest_0.6.27     stringi_1.5.3     bookdown_0.19     grid_4.0.3       \n## [41] cli_2.2.0         tools_4.0.3       magrittr_2.0.1    crayon_1.3.4     \n## [45] pkgconfig_2.0.3   ellipsis_0.3.1    xml2_1.3.2        reprex_0.3.0     \n## [49] lubridate_1.7.9.2 assertthat_0.2.1  rmarkdown_2.5     httr_1.4.2       \n## [53] rstudioapi_0.13   R6_2.5.0          git2r_0.27.1      compiler_4.0.3"
  },
  {
    "objectID": "posts/2021-03-04-data-variants/index.html",
    "href": "posts/2021-03-04-data-variants/index.html",
    "title": "Data variants",
    "section": "",
    "text": "Variants of concern are named such because they are concerning. The ones we worry about are B.1.1.7 (the variant first documented in UK), B.1.351 (the variant first documented in South Africa), and P.1 (the variant first documented in Brazil).\nCurrently, B.1.1.7 is probably the most concerning in BC because we know it is significantly more infectious, with a daily growth rate average of around 10%. This means that in our current BC environment, where we have been seeing a decline by about 0.7% a day since our tougher restrictions enacted in November, B.1.1.7 would grow at about 9.3% a day. As a comparison, we have seen roughly 5% daily growth between July 1 through mid November, with a brief period of lower growth around September.\nThe other two variants are concerning because they show some ability to escape immunity. This will have less of an impact short-term, but could cause problems down the road.\nIf B.1.1.7 becomes established in BC, it will grow at almost twice the rate as regular COVID-19 did this past fall. It will lead to chaos very quickly as several modelling approaches have tried to highlight."
  },
  {
    "objectID": "posts/2021-03-04-data-variants/index.html#data-on-variants",
    "href": "posts/2021-03-04-data-variants/index.html#data-on-variants",
    "title": "Data variants",
    "section": "Data on variants",
    "text": "Data on variants\nSo how are we doing right now? That’s hard to say, we don’t have useful data. Which is quite surprising given how concerning everyone agrees the variants of concern are. By “useful” I mean data with dates and denominators. How many variants were found when, and how many samples were screened/tested for them? Also, it would be good to know which specific variants were found and if they were locally acquired or not.\nUnfortunately BC does not make this data public. But there is some data on VOC that is available. Let’s review what we have and run through the variants of data we can use to make guesses at the missing data on variants.\n\nPoint-prevalence study\nThe point-prevalence study aimed to look at all COVID-positive samples between January 30 and February 5 and determine how many of them were VOC and which ones they were. For the process, the samples were first screened with a second PCR to look for the N501Y mutation common to the variants, and then sequence all those flagged for the mutation to determine which variant they were.\nThe BC PHO announced that the point-prevalence study found three variants, two B.1.1.7 and one B.1.351 out of 3099 cases, which is a very low rate. She explained that 30 cases screened positive for N501Y, but only three were confirmed to be one of the variants by whole genome sequencing. This looks like great data, we have dates and denominators. The problem is that the results, 3/3099 VOC, is implausibly low given other data we have as I have argued elsewhere using schoole exposures and overall reported variants. And only 3 out of 30 N501Y cases being variants of concern is also suspiciously low, screening should be quite good at picking out the variants.\nThe point-prevalence study should have been the comprehensive baseline for data on variants, but as much as it pains me to say this, unfortunately there are too many big questions to take her reported results at face value.\n\n\nSituation reports\nThe weekly situation reports list cumulative variant of concern case counts by variant and acquisition, and they give a broad window of episode weeks in which they fall. We can look at week over week changes in the cumulative counts, but there is likely a lot of backfilling of older data happening and it’s unlikely the difference in cases can be attributed to the last reporting week. Moreover, we don’t know how many cases were screened for variants. We have fuzzy denominators and dates. At best. Let’s take a look.\n\nWe see a clear increase in cumulative variants of concern being reported in the weekly reports, with the newest one covering cases with onset date up to week 7 (Feb 14-20). We don’t have denominators since we don’t really know if the cases added to the cumulative total were from the previous week or from earlier. And we don’t know what fraction of cases got screened and sequenced.\nBut not all variants are created equal. To see this we facet the graph by variants and acquisition, and allow the y-axis to scale independently for each graph so we can better see each individual one.\n\nWhat we see is a neat exponential growth pattern for locally acquired B.1.1.7 cases. Let’s pull that out and fit an exponential to it.\n\nWe case almost doubled every week, that translates to a daily growth rate of 14%, which is higher than what we would expect. Growth rates tend to be higher at the start of variants taking off, just like they are at the start of any outbreak simply because “at the start of taking off” is conditional on the event “taking off”, so it is not implausible. However, understanding how the data got derived the growth rate is likely being pushed up by right-censoring getting better, so the backlog of old cases slowly clearing up, and the fraction of case samples increasing with time. We can’t pin down dates for these cases, and we don’t know the sample size. All this approach yields is some rough estimates.\n\n\nSchool exposures\nSchool exposures notices have dates, and at least in Fraser Health exposure notices get updated when they involve a variant of concern. That means we have dates and denominators, and we can look at the share of school exposures each weak that involved a variant of concern.\nBC does not give out good data on school exposures, the list on the Health Authority websites are incomplete at the beginning period, and the information for exposures get updated when a new letter comes in and a different exposure at the same school is still active, making it hard to count exposure events without going through website scrapes.\nFortunately, the excellent BC School Covid Tracker meticulously collects and verifies exposure data and makes it available on their site. They provide a tremendous service that fills a data hole left by the province.\nWe look at the share of school exposures involving variants of concern for each week, where we split the week between Monday and Tuesday as exposure notifications coming out on Monday usually relate to exposures in the preceding week.\n\nFraser Health Authority is the only one reporting exposures involving variants of concern, either because these are concentrated in Fraser or because other Health Authorities choose not to update exposure notifications when they learn that they involve a variant of concern.\nThe last week for which we have complete data is the one ending March 1st, the next week only has two days of data at this point and exposure notification often get updated only a couple of days later because it takes time to screen for variants.\n\n\nEstimates from data\nWe have observed before that the entrance of the new variants, in particular B.1.1.7, means that we need to treat COVID-19 as two separate pandemics, regular COVID-19 and B.1.1.7. With B.1.1.7 having a daily growth rate of about 10% higher than regular COVID-19. This gives us an opportunity to estimate B.1.1.7 directly from the data, assuming that the only thing that changed recently is the introduction of variants and all other control measures stay about the same. In BC the growth rate has been fairly constant at -0.7% a day since the end of November, and we can fit a double exponential model to the curve with difference in growth rate fixed at 10% a day or 9.5% continuous growth.\nThis is a bit dangerous as such a fit will over-emphasize recent changes in cases. Which may have been caused by other events (e.g. a trivia night). But let’s give it a try. At the same time, we can also run a naive model that assumes three B.1.1.7 cases on February 2nd, which comes out to 21 cases in the point-prevalence week. That roughly jives with what we would expect from using exposure letters or B.1.1.7 case counts for week 5, the point-prevalence week.\n\nWe note that the double exponential model picks up the recent increase in cases and interprets it as being caused by B.1.1.7. And it predicts this rise to accelerate. This is likely not where we are actually headed, it would imply we had 163 B.1.1.7 during the point-prevalence week, which is too high given all the data points we have.\nThe “Manual” line is a more realistic scenario, but it is conditional on the assumption of 21 B.1.1.7 in week 5 and that our attempts to prevent B.1.1.7 from becoming established failed. The starting value of three daily cases on February 2nd is too low to assume simple exponential growth from them on, B.1.1.7 is likely still in it’s stochastic phase and may easily be delayed or accelerated by a week or two. So it’s hard to predict when exactly B.1.1.7 will take off, but once we are over 10 or 20 daily cases the growth will become very predictable."
  },
  {
    "objectID": "posts/2021-03-04-data-variants/index.html#upshot",
    "href": "posts/2021-03-04-data-variants/index.html#upshot",
    "title": "Data variants",
    "section": "Upshot",
    "text": "Upshot\nIf these projections look scary, then that’s because they are. We now have some older people vaccinated, so that will reduce the death toll. But Long COVID will continue, and as cases shoot up contact tracing will break down and case growth will accelerate further.\nThat’s why variants of concern are concerning. And that’s why it’s important to monitor them closely, and that means have proper data. Which we still don’t have for BC. Ontario has now been sharing daily updates with variant screening results, which gives a minimal delay update on the share of variants. Which now make up over 20% of cases in Ontario. Using proxies can get a rough idea and the ballpark, but we need actual data to understand where we are at and make decisions on how to react.\nAs usual, the code is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-03-04 09:56:40 PST\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [882992a] 2021-03-04: data variants\n## R version 4.0.3 (2020-10-10)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] lubridate_1.7.9.2  broom_0.7.4        CanCovidData_0.1.5 forcats_0.5.0     \n##  [5] stringr_1.4.0      dplyr_1.0.4        purrr_0.3.4        readr_1.4.0       \n##  [9] tidyr_1.1.2        tibble_3.0.4       ggplot2_3.3.3      tidyverse_1.3.0   \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.5         lattice_0.20-41    class_7.3-17       assertthat_0.2.1  \n##  [5] digest_0.6.27      R6_2.5.0           cellranger_1.1.0   backports_1.2.0   \n##  [9] reprex_0.3.0       evaluate_0.14      e1071_1.7-4        httr_1.4.2        \n## [13] blogdown_0.19      pillar_1.4.7       rlang_0.4.9        english_1.2-5     \n## [17] readxl_1.3.1       rstudioapi_0.13    blob_1.2.1         Matrix_1.2-18     \n## [21] rmarkdown_2.5      splines_4.0.3      labeling_0.4.2     munsell_0.5.0     \n## [25] compiler_4.0.3     modelr_0.1.8       xfun_0.18          pkgconfig_2.0.3   \n## [29] mgcv_1.8-33        htmltools_0.5.0    tidyselect_1.1.0   bookdown_0.19     \n## [33] codetools_0.2-16   fansi_0.4.1        crayon_1.3.4       dbplyr_1.4.4      \n## [37] withr_2.3.0        sf_0.9-7           grid_4.0.3         nlme_3.1-149      \n## [41] jsonlite_1.7.2     cansim_0.3.5       gtable_0.3.0       lifecycle_0.2.0   \n## [45] DBI_1.1.0          git2r_0.27.1       magrittr_2.0.1     units_0.7-0       \n## [49] scales_1.1.1       KernSmooth_2.23-17 cli_2.2.0          stringi_1.5.3     \n## [53] farver_2.0.3       fs_1.4.1           xml2_1.3.2         ellipsis_0.3.1    \n## [57] generics_0.1.0     vctrs_0.3.5        RColorBrewer_1.1-2 tools_4.0.3       \n## [61] glue_1.4.2         hms_0.5.3          yaml_2.2.1         colorspace_2.0-0  \n## [65] classInt_0.4-3     rvest_0.3.6        knitr_1.30         haven_2.3.1"
  },
  {
    "objectID": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html",
    "href": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html",
    "title": "Forced Out in Canada: New Data from CHS",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#tldr",
    "href": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#tldr",
    "title": "Forced Out in Canada: New Data from CHS",
    "section": "TL;DR",
    "text": "TL;DR\nThe new data release from CHS 2018 enables us to return to looking at mobility, with a special focus on forced moves. We estimate and compare the risk of forced moves for renters across Canada. We also provide some evidence for its sharp decline in BC in 2018, following protections put in place by the NDP. Finally, we compare risk of “forced move” to risk of “choice move” for renters. In BC, “choice moves” are low relative to the rest of Canada, illustrating how the high percent of moves that are forced across BC is in part a product of lack of rental options (given our low vacancy rates) and high rent penalties for moving."
  },
  {
    "objectID": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#new-chs-data",
    "href": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#new-chs-data",
    "title": "Forced Out in Canada: New Data from CHS",
    "section": "New CHS Data",
    "text": "New CHS Data\nThe 2018 Canadian Housing Survey (CHS) public use microdata file (PUMF) is finally out, time to dig deeper into the questions we explored when the first tables came out and the second batch on data on ethnicity and core housing need was released. The first thing we get a nice look at is how recently people moved. Within the Census, this is binned into simple “within the last year” and “within the last five years” categories. Here we get finer grained data extending up to 10 years or more. While we don’t get people’s full moving histories, this is still pretty cool insofar as we can map timing of moves to different years. The survey was fielded from November 2018 through March of 2019, so we can start there and work backward to date moves.\n\nLooking at how recently people moved, the distribution of residential mobility is fairly even across regions, except that rural areas stand out as having lower residential mobility. Part of the reason is that mobility is confounded by tenure. Renters move a lot more often than owners, reflecting a combination of factors including, for instance, demographics (renters tend to be younger on average and more open to moving), transaction costs (selling property is more involved than switching landlords), and power (renters can be forced out more easily than owners). Sure enough, comparing across regions generally less than half of owners have moved in the last ten years, compared to more than three-quarters of renters.\n\nWhen conditioning on tenure of the current dwelling we still see slightly lower residential mobility in rural areas, but it’s reduced. The variation that remains can likely be explained by a variety of factors, like net migration and rental moving penalties. Net migration has mechanical effects on estimates of the timing of mobility for metro areas, even though most moves are local (migrants are added as movers in places where they arrive and subtracted as movers from places where they depart, boosting mobility rates in the former while decreasing them in the latter). But places where rent control maintains much lower rent for long-term tenants than new tenants (e.g. Vancouver & Toronto) tend to see renters staying in place longer, a pattern that only shows up when differentiating mobility by tenure.\nWe can tease this out further by looking backward to graph a set of “survival” functions by tenure, using years since last move to effectively look at what proportion of people remain in a given dwelling by how long they’ve lived in the dwelling. Here we’ll look at some of the larger metro areas across the country, with a special focus on BC.\n\nAs one would expect, owner mobility is much lower than renter mobility. The difference between regions is wider for renter mobility. For better comparison across regions we can flip the way we show the data and graph all regions on the same panel.\n\nRenters tend to stay in place longest in Toronto, followed by Montreal and Vancouver. Renters in Calgary and Edmonton move around more often. As noted above, rental penalties and tenant protections may explain some of this variation. But to investigate further, it helps to know why people are moving. In particular, we want to know if they are moving by choice, or because they’ve been forced to do so."
  },
  {
    "objectID": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#forced-moves",
    "href": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#forced-moves",
    "title": "Forced Out in Canada: New Data from CHS",
    "section": "Forced Moves",
    "text": "Forced Moves\nOne of the (many) great features of the CHS is that it allows us to look at why people moved. Including if their move was voluntary or if people were “forced to move by a landlord, a bank or other financial institution or the government.” This allows us a closer look at power differentials as a factor in moving. In our previous posts we noted that the proportion of moves that were forced looked quite high in BC, a pattern that, with some cautions, can also be examined with reference to reason for move in US datasets. With the PUMF data we can explore in more detail who these people are that were forced to move, and what their circumstances were, as well as getting a finer geographic breakdown within BC.\nTo start out, let’s reproduce what we previously knew about forced moves. Unfortunately the CHS PUMF does not come with bootstrap weights which makes it hard to derive confidence estimates. It has also been altered from the original data to preserve user privacy, so estimates are expected to be a little different from the ones derived from the master file. So it’s a good idea to see how well the PUMF data reproduces the previously released data on forced moves.\n\nComparing to our original graph we notice some slight difference, but overall things look good. We already hypothesized that the tenure of the previous home would have a large impact on the frequency of forced moves, with renters being particularly impacted. The legal process of eviction is likely far more common than foreclosure. With the PUMF data we can check this.\n\nNot surprisingly, we see that forced moves were far more common among those renting their previous home. But there’s interesting variation here. BC, in particular, continues to stand out, with consistently high proportions of those moving from a rental dwelling describing their move as forced. Though Metro Vancouver leads in this regard, the rest of BC looks pretty similar.\nThose owning their last dwellings are far less likely to describe their previous moves as forced, but forced moves still show up. Foreclosures seem the most obvious explanation, but events like government expropriations and condominium (strata) wind-ups may also play a role in dislodging owners. We also get a peek at a new category, those living “rent-free” in their last residence! The extent to which forced moves affect this group seems to vary widely. But it’s not as large as the other groups, so we’ll turn our attention to forced moves for renters, where it matters most. First we’ll simply pull out those who rented their previous dwelling from movers above to examine what proportion described their last move as “forced.”\n\nNow we can think a bit about the limitations of this measure. Above we’re only looking at movers who rented their previous dwelling. We’re also only looking at those who have moved within the previous five years, and looking only at their last move to ascertain what proportion of last moves included “forced move” as a listed reason for move. It’s a funny measure, without a clearly defined risk (it includes all movers within the past five years, but not all moves, and says nothing about those who stayed in place). What we’re probably more interested in is what the risk of being evicted looks like for all renters, in which case the above graph can become confounded by general levels of mobility.\nTo understand this better, let’s first look at overall mobility for renters in the past year, that is the share of renters that moved during the past year.\n\nWe see that one-year renter mobility is indeed very low in Vancouver, roughly on par with Hamilton and Toronto. If not very many people are moving, this could inflate the relative proportion of those being forced to move.\nSo let’s create an estimate of the risk of being forced to move! We’re going to start simple by attempting to estimate the risk of being forced out of a rental dwelling within the past year for everyone who began the year as a renter. So we take the total number of movers from rental housing within the past year who describe their move as being “forced,” and we divide by the total number of movers from rental housing within the past year and the total number of renters who have not moved within the past year. Here’s the resulting one-year risk of being forced from rental housing.\n\nThe one-year risk of being forced from rental housing looks a bit different that the proportion of movers listing “forced move” as a reason for move. First off, the estimated risk of experiencing a forced move for renters drops to somewhere between 0.2% and 2.0%. Second, the variation really shifts. While most of BC remains at the upper end in terms of risk of forced move, the Vancouver CMA drops to the middle of the pack, fitting between Edmonton and Ottawa, and way below Saskatoon.\nOverall, this is a pretty nice, readily interpretable measure of risk of eviction!\nBut maybe we want to see what happens if we go further back in time, giving us more moves overall to work with, and more time at risk of being forced to move. Unfortunately, the further back we go, the trickier it is to establish our denominator of renters and link them to discrete moves. We could be missing some moves (as when people move twice or more in rapid succession, leaving us with only information about the most recent move). And we could be missing some renters (as when young people leave home to become renters, adding to our denominator, but not showing up as a renter prior to their last move). Glossing over people possibly moving multiple times, and possibly being forced to move multiple times, we can extend our above graph to include most recent move in the past 5 years where the last move was forced, taking all renters as denominators.\n\nWith longer time spent at risk of being forced to move, the overall risk to renters increases over a five year period, now ranging from roughly 3% to over 10%. There are also some notable shifts in regional variation; in particular, Metro Vancouver rejoins the rest of BC at the highest risk of forced moves for renters. Looking back historically, this suggests evictions risks may have been higher in BC from 2013-2017 than they were 2017-2018. We’ll come back to exploring this. But first let’s use our longer time-lines to look at sub-populations of interest."
  },
  {
    "objectID": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#demographics-on-forced-moves",
    "href": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#demographics-on-forced-moves",
    "title": "Forced Out in Canada: New Data from CHS",
    "section": "Demographics on forced moves",
    "text": "Demographics on forced moves\nThe CHS has also collected information on visible minority status of household members, which allows us to understand how this affects the risk of being forced to move. The PUMF data only breaks out a simple yes/no status on whether at least one household member belongs to a visible minority group, but give that the sample gets quite thin that’s probably the best that the PUMF can do.\n\nTo avoid small samples we are only breaking this down for broad regions. At this level there seems to be no strong signal that visible minority households overall are more impacted by forced moves, with the exception of the Atlantic provinces. Of note, this does not mean that the same holds for all subgroups of visible minorities. There weren’t enough households with visible minority members in the Territories to show meaningful data, and we chose to not show groups with fewer than 200 renters.\nThis is also a good reminder that people of Indigenous identity aren’t classified as visible minorities in Canada, and we should take a separate look at how they are faring in terms of eviction risks.\n\nWe again suppressed data with fewer than 200 renters, which only leaves the Territories and the Prairies, the latter exhibiting a worryingly heightened risk of being forced to move for households with household members of Indigenous identity."
  },
  {
    "objectID": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#historical-and-comparative-risks-of-forced-moves-and-choice-moves",
    "href": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#historical-and-comparative-risks-of-forced-moves-and-choice-moves",
    "title": "Forced Out in Canada: New Data from CHS",
    "section": "Historical and Comparative Risks of Forced Moves and Choice Moves",
    "text": "Historical and Comparative Risks of Forced Moves and Choice Moves\nLet’s return to comparing one-year risks to five-year risks. To better compare the one year and five year forced move risk, we can compute the 1-year equivalent risk for 5 year movers, assuming that the risk to be forced to move is independent of having been forced to move in the past and has not changed over time. Looking at the differences of the 1-year equivalent risk of forced move on the most recent move over the past five years to the in one year risk to be forced to move gives some way to compare risk from 2017-2018 to risks for the full period from 2013-2018 side-by-side.\n\nIn effect, where the blue bar (one year risk) exceeds the orange bar (one-year equivalent of five year risk), we might be looking at evidence of recent historical change where risks of eviction have risen in the past year. The high risk of forced moves looks quite recent in Saskatoon, for instance. Similarly, in Manitoba, New Brunswick, and rural Nova Scotia the repeated one year risk is substantially higher than the risk of being forced to move on the last move and the last move happening during the past five years prior to 2018.\nOn the other hand, wherever the orange bar vastly exceeds the blue bar, eviction risks within the past year seem to have gone down dramatically. Vancouver looks like potentially the biggest effect in this direction, though it also shows up outside of CMAs in BC, ON, and QC.\nIt’s tempting to describe these as straightforward historical effects, and the pattern in Vancouver, for instance, might suggest that changes to the Rental Tenancy Act providing more protections to renters and made effective in May of 2018 - about six months prior to our survey - might be showing up in the data. But it’s difficult to fully separate out these changes from other things going on in the rental market, or from potentially confounding selection effects (e.g. those evicted may face higher risk of subsequent moves, or may be especially prone to seek out more stable forms of housing in their next move). Still, we can refine this further by looking at the risk for forced moves (on the last move) over increasingly longer time frames, zooming in on BC and a handful of comparison regions and normalizing the risk as a 1-year equivalent, understanding the same caveats as in the previous graph apply.\n\nThis brings out more clearly how the risk of being forced to move dropped off sharply during the last year in Vancouver and the rest of BC, but stayed roughly flat in Toronto and Montreal, and increased in Saskatoon. This lends some further credence to the hypothesis that rental protections enacted in BC had an effect on forced moves. Well done BC NDP!\nFlipping the analysis around, it may also be interesting to zoom in on choice moves, which here we’ll simply consider as all moves not described as forced. Let’s plot choice moves with forced moves.\n\nThe data confirm that the risk of choice mobility looks especially low in Vancouver and Toronto. This fits with the low vacancy rates and likely lack of adequate choices in these markets combined with the workings of rent control, often creating a moving penalty for renters that increases with length of tenure. And this answers the question we asked higher up why residential mobility is lower in Toronto, Montreal and Vancouver. The answer is that it’s mostly due to fewer voluntary moves.\nIt’s the combination of a low risk of Choice moves with a moderate or high risk of Forced moves that creates the effect we’ve demonstrated in past posts, whereby we see a dramatically higher proportion of moves showing up as forced in Vancouver and the rest of BC relative to the rest of Canada. This has important implications, insofar as reducing the proportion of moves that are forced will involve both insuring more adequate protections (which we’re starting to see in BC) and insuring higher vacancy rates so people have better choices available to them about where they might move."
  },
  {
    "objectID": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#takeaways",
    "href": "posts/2021-03-29-forced-out-in-canada-new-data-from-chs/index.html#takeaways",
    "title": "Forced Out in Canada: New Data from CHS",
    "section": "Takeaways",
    "text": "Takeaways\nOverall, we can clearly see expected patterns between tenure and mobility. We can also construct a new and useful estimate of the Risk of Forced Move for all renters. For subpopulations, we see some evidence of heightened Risk of Forced Moves for indigenous tenants in the Prairies. There’s also good evidence that declining Risks of Forced Moves might be the result of a policy shift to strengthen tenant protections in BC in 2018. Combinations of moderate to high Risk of Forced Move, with low Risk of Choice Move produce the troubling patterns we have past documented in terms of Vancouver and BC’s much elevated proportion of all moves described as Forced. This suggests increasing the rental vacancy rate is important to enabling more renters to Choose where they want to live, in addition to continuing to protect tenants’ protections against being displaced.\nAs usual, the code for this post is available on GitHub, although people that want to run it will have to request a copy of the CHS PUMF data from StatCan. While PUMF data is now freely available, it still requires a special request to get the data."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#tldr",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#tldr",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "TLDR",
    "text": "TLDR\nWe estimate the land value lost by lot subdivision restrictions in the RS (single-family) zoned lands of Vancouver. These restrictions, also known as the zoning tax, subsidize hoarding of land for the wealthy at the cost of those who wouldn’t mind sharing. We conservatively estimate the overall cost of preventing splitting of lots at $43 billion, or an average of 37% of existing lot land value. Alternative formulations enabling deeper subdivisions raise our zoning tax estimates to $146 billion. We provide examples of what subdivision could look like, tally up non-conforming lots by zone, and discuss some of the implications.\nThe zoning tax is real, and it is enormous. The exact amount of the zoning tax is hard to pin down because we are so far away from the equilibrium of where people would stop subdividing land or air parcels if they were allowed to do so."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#lots-of-potential",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#lots-of-potential",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Lots of Potential",
    "text": "Lots of Potential\nHow much land do you have to buy in order to buy a house?\nTheoretically this could vary a great deal, running from tiny lots (hosting tiny houses) up to large estates. Historically the latter (often farms) were surveyed; subdivided, and sold off into the smaller lots around most urban areas, setting in place the landscape we see today. Prior to zoning, increments of the 66 foot-long chains of surveyors tended to set lot sizes (one chain by ten chains makes an acre), but these could be further subdivided and sold off by land owners (half a chain by two chains remains a pretty normal lot size in Vancouver). Zoning regulations, and specifically subdivision regulations, imposed firm limits on how small lots were allowed to go, generally setting minimum parcel sizes and frontages. This prevented land from being further subdivided, or split up into smaller lots. It also kept out poorer folks from more exclusive neighbourhoods (in many cases as advertised).\nIn effect zoning forces some people to buy more land than they might otherwise want, making their housing more expensive, but also making the neighbourhood more exclusive. At the same time, and because you can’t really do much with it, zoning depresses the price of land ($ per square foot) on large indivisible lots relative to smaller lots. In other words, we make it difficult for two less rich households to compete with one very rich household for the same plot of land. Accordingly we subsidize the lifestyles and exclusiveness of landed wealth and we pay for it by reducing opportunities for everyone less wealthy.\nJust how much is this subsidy costing us?\nMost estimates of the size of the subsidy come from the field of Economics, where it’s understood slightly differently: as a tax. Following Glaeser & Gyourko (2002), it’s often referred to as a zoning tax. Economists (correctly) tend to see this tax as lowering general welfare in cities. But it’s also a tax placed upon potential transactions between prospective buyers who don’t need all the land on offer and current land owners, who would be able to sell their land for more if it were subdividable. A new NBER working paper by Joseph Gyourko and Jacob Krimmel tries to quantify the zoning tax, that is the loss in affordability due to restrictive zoning, in single-family zoned areas across multiple US metros. Below we attempt to quantify the same figure, which we can also understand as a subsidy for the lifestyles of the very rich, for Vancouver.\nWhy is this called a zoning tax? The basic idea is that as land gets more expensive people tend to use land more intensively. This can happen in several ways, via subdivision of land, subdivision of air parcels (stratification), and sharing of existing land. Zoning restricts all of these. Restrictions on minimum lot size, minimum lot frontage, as well as minimum setbacks renders subdivision of land at some point unfeasible. Restrictions on height, FSR, number of units and stratas restricts subdivision of air parcels or stratification. Definitions of family and restrictions of number of unrelated people per household in zoning bylaws restricts sharing of land and the homes on them.\nIt’s possible, of course, that these restrictions don’t matter much. If they only operate to replicate what people want and can afford to buy anyway (maybe everyone’s a wealthy gardener!), then they may produce little in the way of a tax or subsidy. Put differently, if these restrictions just curb some rare outliers there is not much aggregate economic and social loss associated with such regulation. But when the regulations become hard constraints limiting the trade-offs most people might be happy to make they can incur enormous economic (and social) losses. The zoning tax (a.k.a. landed wealth subsidy) offers one estimate of the selective losses and gains on offer."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#binding-constraints",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#binding-constraints",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Binding constraints",
    "text": "Binding constraints\nHow can we tell if the zoning restrictions in Vancouver are hard constraints limiting what people would do otherwise? There are a number of ways. The size of new houses in Vancouver RS (“single family”) zoning gives one view into this. Here is a graph of the GFA (gross floor area, the total floor space used for living) and the FSR (the ratio of GFA to lot size) for City of Vancouver single family homes by year built.\n\nThis list is somewhat biased in that it does not show the distributions of GFA and FSR for all homes built in a give year, but only for those that are still standing. This biases the floor area of older homes upwards, as smaller homes are more likely to have been torn down and replaced. GFA measures the size of the house, FSR is what is restricted by zoning regulation. Initially the FSR ceiling was set at 0.6 FSR, but this was fairly recently extended to 0.75 as visible in the data. There were also numerous changes of what does or does not count toward FSR.\nInitially the GFA followed a fairly narrow band, we built homes to fit families and needs were similar. This led to a broad span in FSR as lot size varied. We notice how starting in the 1960 the size of homes increased, and by the mid 1970s the range of FSR narrowed considerably, approaching the legal limit. What changed is that the land economics incentivized people to maximize floor space, the FSR range narrowed and the GFA range widened, depending on lot size. The FSR constraints, and with it land sharing constraints, became binding and the zoning became dysfunctional."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#zoning-tax",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#zoning-tax",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Zoning tax",
    "text": "Zoning tax\nIn this post we want to explore the zoning tax as different evidence of a hard constraint, in this case the constraint on sharing land by sub-dividing. Gyourko and Krimmel look at the extensive and intensive margin values of land, which is to say they compare the average land value per square foot of large and small lots near one another. They argue that these two should be the same in an unregulated market. In other words, if the intensive margin is higher than the extensive margin, a landowner could subdivide and sell off some land, or buyers could team up and split the land. But zoning and subdivision bylaws prevent these moves.\nThere are of course other things aside from zoning that may prevent subdivision. If land is already parcelled and built out like Vancouver one can’t always just sell off a few extra square feet so that someone else can build a house on it. But pretty much any Vancouver lot could easily be subdivided into two or more parcels and built on individually. A quick look at Tokyo shows how zero setback single family homes can be built on very small parcels and result in a varied, refreshingly quirky, and dense urban form. Essentially, this is the “zoning tax” we are after. The difference in land value that would result from allowing lots in Vancouver to subdivide into two (or more) parcels."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#how-to-measure-the-zoning-tax",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#how-to-measure-the-zoning-tax",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "How to measure the zoning tax?",
    "text": "How to measure the zoning tax?\nTo determine the zoning tax we have to estimate land values of parcels of different sizes. And use that to estimate the extensive and intensive margins. We can use this to estimate the zoning tax. We can then take the result and validate it by checking in on some non-conforming lots that pre-date or have otherwise been enabled to elide zoning laws.\nTo facilitate estimating land values we will take a shortcut by piggy-backing off of BC Assessment land value estimates. That’s quite problematic as we may well just pick up artifacts in the BC Assessment algorithm, so it requires some validation to make sure that we don’t produce nonsense. But if the zoning tax is large, then we should be able to easily detect it using any half-way decent land value estimation mechanism, including the one employed by BC Assessment.\nTo start off let’s take a look at the subject of our study: RS lots in the City of Vancouver. To clean things up a little we removed parcels that are parks, schools or other non-single family land use. We colour the parcels by their minimum lot size mandated by zoning, lots can’t be subdivided if one of the resulting parts falls below this limit.\n\nApart from minimum lot area, minimum frontage is another way we limit subdivisions, and often this constraint is stronger than the minimum area. In practice it does not matter that much though; most parcels can’t be subdivided because that would violate both of these zoning constraints.\nNext we need to estimate the extensive and intensive margins of land. To do this we take a group of roughly comparable lots with different lot sizes and compare their land values. For our pool of “roughly comparable” lots we will take a string of adjacent lots along one side of a block and remove corner lots (proxied as lots with only one neighbour). To “compare” land values in relation to lot size we regress log land value on log area, so effectively we estimate \\[\nLV = c\\cdot A^\\beta,\n\\] where \\(LV\\) is the land value, \\(A\\) is the lot area, \\(c\\) is a constant depending on the group of parcels, and \\(\\beta\\) is the scaling exponent that describes the relationship. The choice of using log land value and log area is informed by general considerations of how people value things, we will have to check in the data how well it fits these model assumptions.\n\nWe see (randomly coloured) groups of “comparable” lots, with corner lots (in dark grey) removed. Removing corner lots not only removes issues around small general price differences between corner vs interior lots, it also deals with issues when the corner lot is on a busy arterial and the price difference can be substantial.\nThis grouping is not perfect, but it is easy to do and does a good enough job for making our comparisons.\nNext we take a look at how land values and areas relate in some of our comparison groups. We will select groups that have at least 8 parcels after throwing out corner lots, and ask there to be a variation in lot size of at least a factor of 1.25 within the group so that estimates are more robust and not just picking up noise generated by parcels of almost equal size, leaving us with the following 1,534 clusters across the city.\n\nTo understand how the land values relate to lot areas within these groups we take a random sample of 32 such groups and fit our model to each group.\n\nThe fits aren’t perfect, but generally pretty good. Some groups have their lot areas cluster at the extreme values, others have them distributed throughout. The slopes of these lines, the exponent \\(\\beta\\) in our model, are generally quite similar, which is encouraging. This indicates that our model seems to make sense and fits the data reasonably well. The big caveat here is that this may well just be recovering assumptions baked into the BC Assessment model that estimates the land values in the first place, we will have to validate this against actual transaction prices further down in this post.\nWe fit a random intercept model that estimates a fixed slope and we allow to intercept to vary across comparable groups, acknowledging that land values differ broadly across different regions of the city, as well as between arterial and residential lots, or lots abutting parks. This gives us a slope (or exponent in our original model) of 0.544 with confidence interval [0.5393,0.5479].\n\nThe part of the plot that we are most interested in is toward the lower left. The median Vancouver RS lot size is 402m^2, what’s the opportunity cost of preventing this lot from getting subdivided? That’s easy to check. This is asking what value we lose by preventing a lot from getting split into two. It’s most easily expressed as a share of current land value. \\begin{eqnarray} OC(A) &=& \\frac{2\\cdot LV(A/2)-LV(A)}{LV(A)} &=& 2^{1-\\beta}-1 \\end{eqnarray}\nwhich comes out at 37%. In other words, the zoning tax that’s preventing RS lots from getting subdivided in half stands at 37% of current land value. Aggregated over all residential RS lots in the City of Vancouver the zoning tax comes out at $46bn.\n\nHowever, some parcels are already below the current minimum area requirements, so maybe we should exclude those from the zoning tax calculation and assume they are already sufficiently subdivided.\nExcluding those currently non-conforming lots, but keeping the conditionally conforming, the zoning tax estimate gets reduced to $43bn."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#what-would-subdivision-look-like",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#what-would-subdivision-look-like",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "What would subdivision look like?",
    "text": "What would subdivision look like?\nIn Vancouver we have several examples of homes on lots well below the minimum lot size. Examples that show what exactly subdividing a typical Vancouver lot would look like, and what the effects would be.\nTo start off, let’s look at a group of properties in the north-east of the city close to the PNE on the south side of Dundas Street between Slocan and Kaslo.\n\nThe coloured properties are the “comparables” in our cluster, two of the properties are 24 foot wide, half the 48 foot width of all the others. The lots are zoned RS-1(A), so while the 48 foot wide lots are wider than the minimum frontage of 30 foot required by the zoning bylaw, they can’t be subdivided into two 24 foot lots. At 2,928 square feet the narrow lots are also just shy of the 3,000 square foot minimum area limit.\nThe two properties in red sold recently, the larger one in October 2019 for $1.55M, the smaller one in September 2020 for $1.28M. The assessed land values pegged at July 2020 are $1.45M and $1.01M, respectively, with both having building values estimated around $150k. Eyeballing streetview photos this seems reasonable, although an exact attribution is difficult to make without an in-depth assessment. Which highlights a major shortcoming of our methods, but this should not prevent us from getting the ballpark estimate right.\n \nIf we take the land value estimates at face value, the larger lot valued at $1.45M could turn into two lots valued at $1.01M if we allowed it to be subdivided, totalling $2.02M. This amounts to a 39% increase in land value. This is consistent with our overall estimate of a zoning tax share of 37% of existing value for preventing lots from being subdivided.\nAnother interesting example is (former) 3582 McGill Street which got rezoned from RS-1 category C to RS-1 caregory A in 2012, reducing it’s minimum frontage requirements from 50 to 30 feet and minimum lot size from 5,000 to 3,000 square feet. The original property had a frontage of 66 feet and an area of 7973 square feet, each of which provided a binding constraint preventing subdivision before the rezoning, but both allow for subdivision after rezoning to category A.\nThe process of reclassifying from one subdivision schedule to another involves staff time for outreach to neighbours, tallying of their positions (for and against), and detailing of relevant policies and precedents. Then it comes before City Council, who make their final decision with a recommendation report from planning staff. Since no neighbours were opposed (only 3 of 25 nearby neighbours asked to comment responded at all), the McGill reclassification was successful. This allowed the original property to get subdivided into two separate ones of roughly equal size, 3586 and 3592 McGill Street. We can watch this redevelopment in the building values for the site with PID 015-717-577 getting split into properties 028-985-630 and 028-985-648.\n\nWe notice how the assessed values of each subdivision is initially lower than the original combined land value, but the sum of the assessed values of the subdivided lots is larger than the original one. This is what we would expect, if there wasn’t a value gain by splitting the lots, the developer would not have split them. Here we should remember that we are not actually observing land values directly, but using BC Assessment estimates."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#non-conforming-lots",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#non-conforming-lots",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Non-conforming lots",
    "text": "Non-conforming lots\nJust how many lots in Vancouver are already non-conforming? That’s a tricky question, especially when it comes to frontage, as the city dataset on that is messy and it’s somewhat ambiguous how to count frontage for corner lots. To make it even trickier, Vancouver has two kinds of conforming. There is outright conforming with subdivision classification in zoning. There there is conditional conforming, dependent upon the approval of the director of planning (or appointed officer) on a case-by-case basis. As it turns out, four combinations of zoning district and subdivision category have area and frontage limits that can be lowered somewhat, conditional on approval from the planning department.\nOverall we estimate there are 4,062 lots, or 6% of all lots that don’t conform to either area (816) or frontage (2,695) or both (551) requirements, even granting each lot conditional approval of lowered limits if applicable.\nAdditionally, there are 1,405 properties that don’t conform with the outright requirements but satisfy the conditional ones. We can break this down by zoning and subdivision category.\n\nThings are particularly awkward in RS-7, RS-1A and RS-2, where the outright minimum frontage requirements are 40 foot and the minimum area requirement is 4,800 square feet, but the director of planning may conditionally lower them down to 30 foot frontage and 3,000 square feet. The result is that the majority of properties in each of these zones only complies with the conditional requirements, begging the question why the City of Vancouver insists on this kind of case-by-case zoning in these areas.\nNon-conforming lots can be found all over the City of Vancouver, but conditionally-conforming lots cluster in the four zoning and subdivision areas that have conditional subdivision zoning."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#repeated-subdivisions",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#repeated-subdivisions",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Repeated subdivisions",
    "text": "Repeated subdivisions\nSo far we have looked at allowing each lot to subdivide once, but we could go further and ask what would happen if we allowed our standard 33 foot by 122 foot lot to be split into four 16 by 60 foot lots, halving the frontage and upgrading the alleyways to allow independent homes. This would require us to rethink how we deal with some aspects of how we organize services like power cables, sewer lines, size of our fire trucks and garbage pickup. But these are hardly insurmountable challenges and many parts of the world have figured out how to deal with them. To deal with edge cases in this scenario we will set the minimum frontage at 15’ and the minimum depth at 55’, or a minimum lot size of 825 square feet.\nComparing our current zoning to one that allows for this “quadruple subdivision” on standard lots yields a zoning tax of $146bn, substantially more than our initial estimate. Despite this massive overall lift in land values, individual lots are estimated to be priced between $519k and $1.26M at the 5th and 95th percentile.\n\nThis kind of subdivision has the potential, if fully realized, to have a sizable number of lots around $500k, while currently the entry price for a buildable lot in the City of Vancouver sits around $1M."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#broad-and-narrow-rezoning",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#broad-and-narrow-rezoning",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Broad and Narrow Rezoning",
    "text": "Broad and Narrow Rezoning\nOur zoning tax estimates are based on comparing the land value of relatively scarce small lots (generally not allowed) with larger lots (generally mandated). If restrictions on subdivision were lifted, these two prices would move toward one another (per square foot) as small lots became more common and larger lots more scarce, eventually erasing the differential. As such, estimates of the zoning tax would eventually move toward zero if constraints on subdivision were lifted all across the City. On the other hand, if constraints were selectively lifted in one place, then another (as occurs with spot zoning), we’d expect to see price differentials remain.\nPut slightly differently, the zoning tax estimates are based on the current situation and are based on the premise of making small changes. Yet lifting restrictions on subdivisions across the city is everything but a small change. It has the potential to dramatically alter the availability of small lots, which in turn could lower prices for small lots and change the zoning tax calculus. The effect on the prices of larger lots is more complex, but this would lower the overall zoning tax estimate. At the same time it would increase affordability making the lifting of restrictions a win-win situation, with a trade-off between the total zoning tax unlocked and affordability.\nIn practice, subdivision would be a gradual process even if allowed all across the city. Depending on the value of the structure on a given lot, subdivision will not be economically feasible as it generally requires demolition (or equally expensive, relocation) of the existing structure. And while Vancouver has a lot of single family structures that are at the end of their economic life span, turnover is usually conditional on a property transaction. In summary, one would expect the transition to smaller lots to be gradual, even if the ability to subdivide is likely to accelerate trends in sales and redevelopment."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#subdivision-versus-stratification",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#subdivision-versus-stratification",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Subdivision versus Stratification",
    "text": "Subdivision versus Stratification\nSubdivision of parcels is not the only way to unlock the zoning tax. Subdivision of air parcels, a.k.a. stratification (condos), is an alternative to subdivision of land that is similarly suited to unlock the zoning tax. We did not explicitly model air parcel subdivision mostly to keep things simple, but one should expect similar effects. On top of this, stratification makes it easier to subdivide even further, when lot sizes become so small that fee-simple structures become less feasible. One could even go further and consider assemblies in order to achieve buildings with more efficient (and accessible) layouts.\nIn practice we might want to allow all of these, subdivision of land as well as subdivision of air space of single lots as well as assemblies. Tokyo shows how larger buildings can nicely co-exist with zero-setback single detached homes and result in a delightfully varied streetscape.\nAt the same time, we likely want to change our rules on setbacks and FSR. The examples of 16 foot wide lots that we have show that current rules can still result in viable homes on half-sized regular lots, but at the latest when we subdivide further to also allow front/back subdivisions on top of that we need to increase FSR and make dramatic changes to front and back yard in order to achieve functional homes. The same is true if we allow stratified 4-plexes on regular lots with all family-sized units. If this happens, land values may increase further, rendering our zoning tax figure of $146bn an under-estimate."
  },
  {
    "objectID": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#upshot",
    "href": "posts/2021-07-25-lots-of-opportunity-estimating-the-zoning-tax-in-vancouver/index.html#upshot",
    "title": "Lots of Opportunity: Estimating the Zoning Tax in Vancouver",
    "section": "Upshot",
    "text": "Upshot\nThe exact amount of the zoning tax is hard to pin down because we are so far away from the equilibrium of where people would stop subdividing land or air parcels if they were allowed to do so. But it is clear that the zoning tax is real, and it is enormous. At the lot level the zoning tax materializes as a subsidy for hoarding of land paid for by lost opportunities for those willing to share land via smaller lots or stratification. Collectively the zoning tax quantifies (a portion of) the problems with City of Vancouver land use policies, pushing up housing prices and forcing people into long commutes.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2021-07-25 13:30:03 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [77951fa] 2021-07-25: fix og image url\n## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.3 mapdeck_0.3.4            \n##  [3] sf_1.0-0                  VancouvR_0.1.6           \n##  [5] forcats_0.5.1             stringr_1.4.0            \n##  [7] dplyr_1.0.7               purrr_0.3.4              \n##  [9] readr_1.4.0               tidyr_1.1.3              \n## [11] tibble_3.1.2              ggplot2_3.3.3            \n## [13] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.7         lubridate_1.7.10   class_7.3-19       assertthat_0.2.1  \n##  [5] digest_0.6.27      utf8_1.2.1         R6_2.5.0           cellranger_1.1.0  \n##  [9] backports_1.2.1    reprex_2.0.0       evaluate_0.14      e1071_1.7-7       \n## [13] httr_1.4.2         blogdown_1.4       pillar_1.6.1       rlang_0.4.11      \n## [17] readxl_1.3.1       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.8     \n## [21] urltools_1.7.3     htmlwidgets_1.5.3  triebeard_0.3.0    munsell_0.5.0     \n## [25] proxy_0.4-26       broom_0.7.6        compiler_4.1.0     modelr_0.1.8      \n## [29] xfun_0.24          pkgconfig_2.0.3    htmltools_0.5.1.1  tidyselect_1.1.1  \n## [33] bookdown_0.22      fansi_0.5.0        crayon_1.4.1       dbplyr_2.1.1      \n## [37] withr_2.4.2        grid_4.1.0         jsonlite_1.7.2     gtable_0.3.0      \n## [41] lifecycle_1.0.0    DBI_1.1.1          git2r_0.28.0       magrittr_2.0.1    \n## [45] units_0.7-2        scales_1.1.1       KernSmooth_2.23-20 cli_3.0.1         \n## [49] stringi_1.7.3      fs_1.5.0           xml2_1.3.2         bslib_0.2.5.1     \n## [53] ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.8        tools_4.1.0       \n## [57] glue_1.4.2         hms_1.1.0          yaml_2.2.1         colorspace_2.0-1  \n## [61] classInt_0.4-3     rvest_1.0.0        knitr_1.33         haven_2.4.1       \n## [65] sass_0.4.0"
  },
  {
    "objectID": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html",
    "href": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html",
    "title": "Satellites, Sprawl, and City Six-Packs",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWe’re getting better and more accessible datasets for exploring land use change all the time. We have played with the Global Human Settlement Layer (GHSL) data in the past, where we looked at the population data on a 250m grid to compare how different city’s population distribute spatially, as well as the 1975, 1990, 2000, 2015 time series to see how it changed over time. These GHSL population datasets take a variety of input data to build, one part is census or other population-based datasets, the other is the built-out area derived from satellite data that is used to estimate population data at the fine 250m grid.\nThe emergence of fairly high quality global datasets is quite remarkable, and the ability to go back in time by having these estimates at several points in time back to the mid-70s is amazing.\nThe city density post was just a first excursion into this data, there is a lot more than can be learned from it. The next logical step - inspired in part by a recent Washington Post piece - is to more closely examine the built-out area timelines that went into making the global population density dataset, and give that data a light workout to better assess it’s strengths and limitations. The dataset comes at an extremely fine 30m resolution, where the extent of the built-out area was estimated off of Landsat satellite data at 1975, 1990, 2000, and 2014.\nSo let’s do it! To get started we take a look at the built-out area of Canadian metropolitan areas. Here we look at a circular area around the centres of the regions, 40km radius for the larger areas and 25km for the smaller ones, and colour them depending of by when they were built out.\nAll cities have a light pink centre and have been sprawling outward, but at quite different rates. More recent build-out seems scant in Vancouver, but quite prominent in Calgary and Edmonton. We can directly quantify how much of the current built-up area has been built out across each of the epochs.\nHere we get confirmation that Vancouver has remained the most contained since 1975 (right around the time BC put the Agricultural Land Reserve into place), followed by Toronto (with its more recently established Green Belt). By contrast, Calgary and Edmonton have sprawled outward, more than doubling their built footprint since 1975.\nAn additional glory of the data is its international coverage.\nWe can pull data from just about anywhere! Here get a glimpse of how old Metropoles of the 19th and 20th Centuries, like London and New York, compare to some of the faster growing megacities in the 21st Century.\nMore of Shanghai appears to have been built out between 2000 and 2015 than existed in 1975!"
  },
  {
    "objectID": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html#mystery-city-came",
    "href": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html#mystery-city-came",
    "title": "Satellites, Sprawl, and City Six-Packs",
    "section": "Mystery city came",
    "text": "Mystery city came\nWe can also play fun “Guess the City” games… Here’s the Canadian version.\n\n\nThe Euro version.\n\n\n\nUSA. USA. USA.\n\n\n\nAsia\n\n\n\nAfrica and Western Asia\n\n\n\nOceania, Latin America, & the Caribbean\n\nAside from having some fun, it’s also worth doing some checking to see how well the Landsat data works! We notice that a few of the cities we’ve looked at appear to have some of their built environment misclassified in terms of when it was built out. There are lots of potential sources of error, including, for instance, resolutions and image quality from 1975 satellites and how things like tree coverage and foliage might influence attempts to categorize land as built upon.\nWe’ll zoom in on two places we know relatively well, and have played with before: Vancouver and Minneapolis.\n\nHere we’ve really zoomed in on the City of Vancouver. Overall it looks pretty good, which is to say Landsat estimations of built-out areas mostly fits with our expectations. But there are some weird bits, like wealthy and leafy parts of Shaughnessy, far west Point Grey, and the Southlands, that all look like they were recently developed. This might be explained by tree cover changes associated with redevelopment (and new mansion construction) in these areas. We can also see the recent construction of Champlain Heights in the SE corner of Vancouver, but this fits our priors insofar as the area famously remained undeveloped until the 1970s.\nWhen we turn to Minneapolis what look like much larger misclassifications. Large parts of the central area of the city (e.g. Uptown near Lake Calhoun) are classified as being built up between 1975 and 1990.\n\nBut experience, confirmed by a cross-check at a building age map of Minneapolis shows that almost all homes in these areas were built before 1970. In fact some pockets showing as solidly built up between 1975 and 1990 have homes almost exclusively built before 1910.\nThere are several possibly explanations for this misclassification. There is literature showing how difficult it is to train a classifier to deliver consistent results globally. A possible problem could have been dense tree cover that got thinned over the years, resulting in areas initially getting classified as undeveloped to later be classified as built-up, without much changes to what was on the ground. This could also happen because of better spatial resolution of Landsat satellites used for later years. That said, Minnesota was the worst example we found when browsing through various cities. (It’s addictive, we highly encourage grabbing the code and spending a couple of hours pointing it to different cities around the globe.)"
  },
  {
    "objectID": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html#boomtown",
    "href": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html#boomtown",
    "title": "Satellites, Sprawl, and City Six-Packs",
    "section": "Boomtown",
    "text": "Boomtown\nWe can find sprawl in well-established cities like Calgary and Edmonton, but the phenomenon becomes even more prominent when we look at cities that were quite small in 1975 and have experienced rapid growth since. The smaller boomtowns are where we can still watch how land gets eaten up for building more homes. Squamish provides a good example. Looking down from the top of the Chief it is hard not to notice the new development happening at the fringes, while the city centre seems to consist of more surface parking than buildings."
  },
  {
    "objectID": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html#more-city-six-packs",
    "href": "posts/2021-08-21-satellites-sprawl-and-city-six-packs/index.html#more-city-six-packs",
    "title": "Satellites, Sprawl, and City Six-Packs",
    "section": "More City Six-Packs",
    "text": "More City Six-Packs\nPulling down satellite data to look at patterns of historical urban sprawl is entirely too much fun. But we’re going to keep doing it. And while there are still some bugs to be worked out in terms of ground-truthing the satellite data, it mostly seems to provide a good approximation of the big picture for land use change. We’ll keep thinking through ways of understanding how different cities are growing, and returning to the real trade-offs in terms of reducing the footprint of growth.\nWhile we are at it, here are some more cities. (we can’t stop. send help!) For those that can’t wait to try out even more cities, the code for this post is available on GitHub for anyone to reproduce, adapt, or run their favourite cities.\n\n\n\n\nReproducibility receipt\n\n## [1] \"2021-08-22 18:40:16 PDT\"\n## Local:    master /Users/jens/Google Drive/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [8ab33a5] 2021-08-21: city 6-packs\n## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] sf_1.0-2            patchwork_1.1.1     cityDensities_0.1.0\n##  [4] forcats_0.5.1       stringr_1.4.0       dplyr_1.0.7        \n##  [7] purrr_0.3.4         readr_2.0.1         tidyr_1.1.3        \n## [10] tibble_3.1.3        ggplot2_3.3.3       tidyverse_1.3.1    \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.7         lubridate_1.7.10   class_7.3-19       assertthat_0.2.1  \n##  [5] digest_0.6.27      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  \n##  [9] backports_1.2.1    reprex_2.0.0       evaluate_0.14      e1071_1.7-7       \n## [13] httr_1.4.2         blogdown_1.4       pillar_1.6.2       rlang_0.4.11      \n## [17] readxl_1.3.1       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.8     \n## [21] munsell_0.5.0      proxy_0.4-26       broom_0.7.6        compiler_4.1.0    \n## [25] modelr_0.1.8       xfun_0.24          pkgconfig_2.0.3    htmltools_0.5.1.1 \n## [29] tidyselect_1.1.1   bookdown_0.22      fansi_0.5.0        crayon_1.4.1      \n## [33] tzdb_0.1.2         dbplyr_2.1.1       withr_2.4.2        grid_4.1.0        \n## [37] jsonlite_1.7.2     gtable_0.3.0       lifecycle_1.0.0    DBI_1.1.1         \n## [41] git2r_0.28.0       magrittr_2.0.1     units_0.7-2        scales_1.1.1      \n## [45] KernSmooth_2.23-20 cli_3.0.1          stringi_1.7.3      fs_1.5.0          \n## [49] xml2_1.3.2         bslib_0.2.5.1      ellipsis_0.3.2     generics_0.1.0    \n## [53] vctrs_0.3.8        tools_4.1.0        glue_1.4.2         hms_1.1.0         \n## [57] yaml_2.2.1         colorspace_2.0-1   classInt_0.4-3     rvest_1.0.1       \n## [61] knitr_1.33         haven_2.4.1        sass_0.4.0"
  },
  {
    "objectID": "posts/2021-09-07-transnational-property-ownership-in-canada/index.html",
    "href": "posts/2021-09-07-transnational-property-ownership-in-canada/index.html",
    "title": "Transnational property ownership in Canada",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWe know transnational ownership of properties is real. But how should we define it? And how many properties are owned by who where?\nFirst to definitions. We’re primarily interested in ownership of dwellings, where we can define ownership of properties in terms of titles and – in the relatively rare case of corporate ownership – in terms of beneficial ownership. Given this start, we can define transnational ownership of properties in at least two ways, the key distinction being how we locate property owners. After all, properties tend to stay put. It’s the property owners who move around and sometimes change their national affiliations. So we can define transnational property ownership in terms of nationality of owner (what’s their citizenship status?). Or we can define transnational property ownership in terms of primary residency of owner (often, but not always, associated with tax status).\nNow how many properties are owned by who where? It would be fabulous if we had some kind of world-wide registry. But we don’t, so let’s focus on defining this question in terms of Canada.\nNo one needs convincing that transnational ownership of properties is real in Vancouver. Here, and now in many places across Canada, a kind of Housing Nationalism has taken root, viewing “foreign buyers” and “foreign owners” and even transnational “satellite” families as a problem. Indeed, one need only have a look at the housing platforms of major parties to see Housing Nationalism in action. Strikingly, no credible analysts (including us) expect this to have any large effects on affordability. Why?\nThree reasons: 1) since the pandemic shutdown, we’ve seen very little transnational travel or purchasing, but extraordinarily large price gains. 2) Here in BC we’ve seen several rounds of Housing Nationalism in action (e.g. the Foreign Buyer Tax), and while their effects remain debatable, they’ve now definitely been swamped by other forces. 3) We’ve now undertaken multi-year collection of data on ownership of properties by those whose primary residence is outside of Canada. The Canadian Housing Statistics Program (CHSP) enumerates homes owned by “non-resident owners” in BC, ON, and NS in great detail, allowing us to quantify how many homes are at least partially owned by someone living outside of Canada, how many are majority owned by people living outside of Canada and how many are exclusively owned by people living outside of Canada, giving the opportunity to carefully select metrics relevant to a question at hand. And the difference between the share of homes where all owners are living outside of Canada vs those where at least one of the owners lives outside of Canada is quite sizable. Defined either way, transnational ownership is certainly real! But it’s not very big. And as we previously discovered, about half of those owning property within Canada but primarily living in another country are actually Canadian citizens or permanent residents. So if we’re using citizenship as our definition of transnational ownership of properties in Canada, the numbers of transnational properties are quite small indeed.\nOverall non-resident involvement in property ownership in Canada comes in somewhere between 2% to 6% depending upon where and how it’s measured. It’s also proportionally largest in Nova Scotia, the province engaged in the least Housing Nationalism.1 Just a reminder that most properties are occupied regardless of ownership and there are very few problematic empty homes, especially in BC where multiple rounds of Speculation & Vacancy Tax Data confirm these patterns.\nBut non-resident ownership of Canadian properties is only half the story. What about properties in other countries owned by Canadians?\nThe latest Survey of Financial Security in Canada provides data on whether properties owned by respondents are within or outside of Canada. So let’s have a look at properties outside of Canada owned by Canadian residents. This is based on survey data and we have to be a little more careful about the confidence intervals around this data.\nThe data comes split into how many family units (families or unattached individuals) own one property outside of Canada and how many own two or more properties. Combining them together, we can see that a small, but very real share of Canadian families own properties abroad. How many properties are we talking about? We can collapse this into a single value by assuming that those that own two or more properties own exactly two, which will be an under-estimate, but likely not a large under-estimate.\nLet’s plot the CHSP and SFS data as raw numbers of properties both on the same graph for the provinces for which we have data for both. That makes it easier to compare transnational ownership patterns for Canada. Remember that we’re (slightly) underestimating properties owned by Canadian families abroad insofar as we’re capping them at two!\nEven our lowball estimate shows that residents in BC and Ontario own significantly more properties outside of Canada than there are homes in those provinces that are (partially) owned by people living outside of Canada. In fact, BC and Ontario residents own about twice as many properties outside of Canada as people living outside of Canada own in the respective provinces. For Nova Scotia the situation is reversed. We should point out that the two categories aren’t mutually exclusive, some family units may be in both categories, where one member of the family resides in Canada and one abroad, and the family owning properties both inside and outside of Canada."
  },
  {
    "objectID": "posts/2021-09-07-transnational-property-ownership-in-canada/index.html#upshot",
    "href": "posts/2021-09-07-transnational-property-ownership-in-canada/index.html#upshot",
    "title": "Transnational property ownership in Canada",
    "section": "Upshot",
    "text": "Upshot\nWhere we have data to compare, we can see that Canadian residents own far more properties abroad than residents elsewhere own in Canada. This is especially the case in BC and ON (where we’ve seen the most Housing Nationalism in rhetoric and policy), and likely reflects at least in part the transnational ties of large immigrant communities. We see it less at work in NS. Unfortunately we don’t have the data to compare for all of Canada, but there’s good reason to expect that prominent “snowbird” provinces (e.g. in the Prairies) might own disproportionately even more properties abroad, as reflected in the SFS data. That the imbalance in transnational property ownership tends to favour Canadian residents owning property abroad suggests yet another reason to be skeptical of Housing Nationalism focusing on the “foreignness” of owners as a problem for Canada.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes."
  },
  {
    "objectID": "posts/2021-09-07-transnational-property-ownership-in-canada/index.html#footnotes",
    "href": "posts/2021-09-07-transnational-property-ownership-in-canada/index.html#footnotes",
    "title": "Transnational property ownership in Canada",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough Nova Scotia does give preferential property tax treatment to properties where at least 50% of longtime property owners are a resident of the province via their CAP program which is similar to California’s Prop 13.↩︎"
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html",
    "href": "posts/2021-10-03-fixing-parking/index.html",
    "title": "Fixing parking",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nNew parking proposal just dropped! As Vancouver City Council once again discusses parking it seems like a good time to give a brief overview of the trade-offs involved, with special focus on the progressivity of parking permit fees. Vancouver proposed to introduce a city-wide parking permit program, requiring residents to buy a $45/year parking permit to park their vehicles on city streets (reduced to $5 for people with low incomes), or pay a $3 overnight visitor parking fee. Additionally the City proposes higher annual fees of $500 to $1000 for new gasoline-powered cars built 2023 or later.\nThe motion reads:\nTo emphasize the key point the program has two components:\nIn this post we will mostly concern ourselves with the second component, this gets at the fundamental problem of privileging land use for vehicle storage above other uses and lays the groundwork for removing minimum parking requirements as explained in the motion:"
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html#parking-101",
    "href": "posts/2021-10-03-fixing-parking/index.html#parking-101",
    "title": "Fixing parking",
    "section": "Parking 101",
    "text": "Parking 101\nTo understand why this is being proposed let’s take a step back and take a careful look at parking. Don Shoup explains in his book The High Cost of Free Parking\n\nEvery transport system has three elements: vehicles, rights of way, and terminal capacity. Rail transport, for example, has trains, tracks and stations. Sea transport has ships, oceans and seaports, Air transport has planes, the sky and airports. Automobile transport has cars, roads and parking spaces. Two aspects of its terminal capacity set automobile transport apart from all other transport systems. First, automobile transport requires enormous terminal capacity - it is land-hungry - because there are so many cars and several parking spaces for each one. Second, motorists park free for 99 percent of their trips because off-street parking requirements remove the cost of automobile terminal capacity from the transport sector and shifts it everywhere else in the economy.\n\nThis hints at while it is so expensive to find land for people to live in Vancouver, it is free to find land for cars to park. This is how we designed it. We have strong restrictions on building space for people to live, and at the same time strong requirements to provide space for cars to park. This also links into the growing phenomenon of people living in their vehicles in Vancouver.\nIn the words of Don Shoup:\n\nWe have free parking for cars and expensive housing for people. We’ve got our priorities the wrong way around!\n\nFor example, the City of Vancouver requires one off-street parking space for every 28 gross square metres of space used as a detoxification centre, but one for every 9.3 square metres of space used as a bingo hall, only counting the area of the actual hall. A curling rink needs to supply three off-street parking spaces for every ice sheet, two spaces are needed for every racquet court. A one-family dwelling with suite and laneway house needs at least one off-street parking space, but a side-by-side duplex with one secondary suite needs three off-street parking spaces even though it has the same number of dwelling units.\nIf this sounds ludicrous and like pseudo-science, then that’s because it is. But why do we have minimum parking requirements like this in the first place? Does a curling rink in the West End really need to provide the same number of parking spaces as (a hypothetical) one in Southlands? What’s wrong with letting people decide how much off-street parking space they need? If a family wants to go car-free, that should be encouraged. But in Vancouver a family can’t buy a condo without a parking space. We are forcing car-free families to pay extra for something they don’t need. How much extra? It costs about $200 to $250 per month to finance and maintain an underground parking stall. That’s a lot of extra cost for something a family does not need.\nWait, an off-street parking space costs $200 per month? Who pays for that you might ask. The answer is: Everyone. That’s because the cost of off-street parking is bundled with other services."
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html#parking-is-bundled",
    "href": "posts/2021-10-03-fixing-parking/index.html#parking-is-bundled",
    "title": "Fixing parking",
    "section": "Parking is bundled",
    "text": "Parking is bundled\nBecause we require so much parking, parking is over-supplied. And we can’t charge the true price for the cost of parking. But someone has to pay for it, and we do this by bundling parking with other services.\nWhen you buy a condo, it comes with a parking space and the price of parking becomes part of the price for the home. Maybe you have the option of buying an extra space, or not buying any parking, but the cost or savings for that rarely amount to the ~$50,000 that it costs to build, finance and maintain that space. Part of that space is cross-subsidized with the cost of your living space.\nSimilarly, when you are renting there may be a an extra charge for a parking space. But that charge will typically run around $60 and not the $200 to $250 it actually costs. Who pays the rest? You pay for that through your rent. And if you choose not to rent a parking space, and your neighbour gets a second car and buys that extra space for $60, you are still cross-subsidizing your neighbour’s second car parking spot to the tune of ~$150/month through your rent.\nWhen you go shopping you often don’t pay for parking. But the store is (hopefully) not losing money, they pay for it through higher prices on the groceries you are buying. You walked to the store to get groceries? You still pay for someone else’s parking spot.\nWe make driving artificially cheap and cross-subsidize that by making housing and shopping more expensive. You think that’s messed up? Yup, that’s messed up. But that’s how we roll. As Don Shoup famously noted in his influential article “The High Cost of Free Parking”:\n\nMinimum parking requirements act like a fertility drug for cars. Why do urban planners prescribe this drug?\n\nThis is crazy, and it needs to change. We need to get rid of off-street parking requirements and let people decide if they want parking with their housing or if they don’t. But there is a catch. As long as on-street parking is free or cheap, people will simply store their vehicles on the street if we stop requiring off-street parking. In fact, the whole reason we have these off-street parking requirements in the first place is to keep street parking free.\nHistorically, as cars arrived in cities, they started to clog up space when parked. There were essentially two ways people solved this problem of streets clogged by parked cars. Some places, like North America, solved this by requiring ample off-street parking, so on-street parking would not be scarce. Others, like Tokyo, solved this by requiring people wanting to register their car to prove that they had an off-street parking space. In North America everyone pays for the cost of parking, whether they use that parking or not. In Tokyo, only drivers pay for their off-street parking spot."
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html#the-need-to-manage-on-street-parking",
    "href": "posts/2021-10-03-fixing-parking/index.html#the-need-to-manage-on-street-parking",
    "title": "Fixing parking",
    "section": "The need to manage on-street parking",
    "text": "The need to manage on-street parking\nThis brings us back to the city proposal. If we want to rectify the ridiculous situation where we force people to pay upward of $200 a month for parking they don’t use, we need to tackle the problem of congested on-street parking. And there is an effective solution to this tragedy of the commons: Charge the lowest price so that about 20% of spaces are available for people to park. This means that the resource of street parking is well-utilized while anyone who wants to park can find parking. Vancouver already does exactly this along commercial areas, where parking fees are performance-based. Parking utilization gets monitored, and when utilization goes down the city drops the prices. If utilization goes up, the city raises them.\nThe new city proposal expands this to residential parking. It establishes a (nominal) baseline fee of $45 per year for a parking permit, with the possibility of raising these fees if parking becomes scarce. We have already seen something like this in the West End, where the city has introduced significantly higher parking fees for new vehicle registrations while grandfathering in old registrations at a lower rate that also applies to low-income households.\nA system to manage on-street parking is a necessary step in order to drop or significantly lower off-street parking requirements."
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html#existing-parking-permit-districts",
    "href": "posts/2021-10-03-fixing-parking/index.html#existing-parking-permit-districts",
    "title": "Fixing parking",
    "section": "Existing parking permit districts",
    "text": "Existing parking permit districts\nSome areas of the city are already covered by permit parking zones, so they won’t be affected by the rule change. Other areas, like Yaletown or Coal Harbour, are essentially covered by metered parking, or don’t allow parking at all.\n\nWhile the parking meter data we have gives only approximate coverage of the metered areas and does not contain information on parking restricted portions, it is worthwhile to estimate characteristics of the population already covered by paid permit parking. In those areas 20% of the population is in low income (LIM-AT) compared to 19% in the City overall, and 70% of households are renting compared to 53%. In total 33% of City of Vancouver renter households live in existing paid parking permit areas.\nAdditionally the city has residential only parking districts that don’t require permits and don’t require paying a fee. They operate on a snitch basis, where residents can park on their block and enforcement is only on a complaint basis, where residents call in when they observe non-residents parking on their block.\n\nThese are zones where residents have managed to secure exclusive access to public street parking without having to pay for it. Unsurprisingly, low income people at 16% of the population and renters at 43% are under-represented in these areas.\nThe new proposal would expand paid parking for overnight parking from the current permit parking zones to the rest of the city, including the no-permit snitch-enforced residential parking only areas."
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html#equity",
    "href": "posts/2021-10-03-fixing-parking/index.html#equity",
    "title": "Fixing parking",
    "section": "Equity",
    "text": "Equity\nDon Shoup observed in his book that\n\n[parking requirements] increase traffic congestion and air pollution, distort urban form, degrade urban design, increase housing costs, limit homeownership, damage the urban economy, harm the central business district, and penalize poor families.\n\nYet just like clockwork, whenever a policy change gets proposed that might inconvenience upper middle class people, like increasing property taxes - or in this case charging for parking - the ample evidence concerning how the existing policies hurt poor people gets ignored and wealthy people explain: “But this will hurt low-income people!”\nWhile many planners have changed their view on parking and are pushing for change, it should not be surprising that some of the most outspoken critics of change are also planners, who after all created and upheld the perverse minimum parking requirements that privilege cars and car ownership and disproportionately hurt low-income people. And the proposed $45 per year ($5 for low income people) pales in comparison to the ~$200/month planners have imposed on car-free low income people in this city.\nFor example one local commentator noted correctly that “18 per cent of workers who are living in Vancouver have starting or ending commute times that might not fit into the transit schedule because they start between midnight and 6 a.m.” where “the start times don’t match transit schedules”, but conveniently failed to mention that those living in the City of Vancouver and commuting to work between midnight and 6am are more likely to to take transit and less likely to drive to work than people commuting to work at other times.\n\nOthers make vague arguments about which neighbourhoods get impacted by parking fees, citing neighbourhood level aggregate stats without ever attempting to understand how this relates to individuals. Claims like half of commuters driving or being driven to work […] in areas with rapid-transit stations (with the exception of Downtown and Strathcona) are also not helpful. Examining mode share to work from the 2016 Census in the 10 minute walk sheds around skytrain stations show that this is selling several stations short.\n\nThere are however stations with catchment areas still showing a high share of car commuters, which incidentally coincide with the stations where low-density land use and elevated share of owner households remain.\n\nAdditionally, much of the catchment areas of stations around regions with higher density land use are already covered by existing permit parking zones.\nWe can also explore fine-area ecological correlations between mode share, incomes and tenure using our CensusMapper map from 2017. While ecological correlations are known to be misleading at times, at fine levels of analysis they can provide insight into geographic variation. And they reflect the same trends. Lower income commuters are less likely to drive to work.\n\nWhat all this tells us is what we pretty much knew already. Higher income people drive more than lower income people, owners drive more than renters, and our current setup of bundling parking disproportionately hits lower income people and renters. That’s the real equity issue here. Worth remembering as well that the big fees being charged apply only to new gas-guzzling vehicles which lower income people are least likely able to afford!\nThat is not to say that all equity concerns are unfounded. There are some real equity concerns that come with almost any policy change, and parking is no different. There are winners and losers, and some of the losers are lower income people. Moreover, charging for parking only works if people have alternatives to fall back on. The City of Vancouver is in a fortunate position of having almost universal frequent transit coverage, and it is relatively easy to expand this to the few missing places. Still, the level of service varies considerably throughout the frequent transit network.\nBut for some people this may not be a viable alternative even when they live close to transit. Disabled people come to mind, and switching from driving to other modes of transport might be unduly burdensome for them. That’s why the City of Vancouver proposal exempts vehicles equipped to transport wheelchairs. More broadly, the city has amended their initial proposal to charge everyone else $45/month to park overnight to lower this to $5 for people in low income, similar to analogous exemptions already in place for the West End parking plan. There is definitely work to be done around the edges of changing the rules for parking, and the City appears to have done just that."
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html#the-bigger-equity-picture",
    "href": "posts/2021-10-03-fixing-parking/index.html#the-bigger-equity-picture",
    "title": "Fixing parking",
    "section": "The bigger equity picture",
    "text": "The bigger equity picture\nBut is it really just work that needs to be done around the edges, or is something like charging for parking regressive on average? In that case, tinkering at the edges won’t be enough. Who benefits from the status quo and who doesn’t? Does Don Shoup’s statement that parking requirements “penalize poor families” hold in today’s Vancouver? Let’s take a look at Vancouver data.\nTo understand equity issues we are interested in the relationship of driving and car reliance to income and other socio-economic factors at the individual level, understanding this question needs cross tabulations at the individual person or household/family level.\nCensus data can inform on this, and while we don’t have a city level cross tabulation on income and commute mode to work available we can check the Metro Vancouver level relationships using PUMF data.\n\nThe results are of course not surprising, drivers have the lowest share of low-income commuters. After all, owning and operating a car is expensive! This means the status quo hits low-income commuters the hardest as they disproportionately pay for car parking they don’t tend to use as much, be at their home, shopping, or in many cases also at work where parking continues to be subsidized.\nBut are these Metro level relationships still valid at the City level? And what about trips other than going to work, or more generally, just owning a car?\nThe 2019 City of Vancouver Transportation Panel Survey has information on incomes and car ownership and usage. We can confidently note that higher income people are more likely to have access to private motor vehicles.\n Similarly, higher income people are more likely to use private motor vehicles for trips (not just counting trips to work).\n\nThis shows quite clearly that low income people have been particularly hurt by our massive planning failure of making parking cheap and socializing the cost of parking into housing and the price of goods and services. And to add insult to injury, low income people are also most impacted by the induced car dependence as well as downstream effects on climate.\nBut these are average effects, and even if we undo minimum parking requirements today it will take time to unlock the benefits of reversing almost a century of planning malpractice. This tells us we should pay more attention to the individual level short-term changes, and ask how to structure the transition in a way that minimizes stress on low-income populations."
  },
  {
    "objectID": "posts/2021-10-03-fixing-parking/index.html#upshot",
    "href": "posts/2021-10-03-fixing-parking/index.html#upshot",
    "title": "Fixing parking",
    "section": "Upshot",
    "text": "Upshot\nWhat have we learned from all of this? The City of Vancouver is no outlier in how existing parking regulations adversely impact low-income populations. This proposal, putting in place a mechanism to charge for on-street parking city-wide, is a necessary step in reversing one of the most profound planning failures: minimum parking requirements.\nYet change is hard and affects different people differently, and it will take time until citizens can reap the benefits of removing minimum parking requirements. To ease the transition the proposal significantly lowers the fee for low-income people, from $45 to $5. Down the road prices for street parking permits may increase depending on demand, just like we have already seen in the West End. Grandparenting and discounts for low-income populations - as in the West End - can ease that transition.\nIf planners had paid as much attention to affordable housing as we have to affordable parking, Vancouver would be a very different city. It is good to see that city planners are realizing this and are taking steps to adjust their priorities. The benefits of this won’t materialize overnight, but this city-wide permit parking proposal is an important and necessary step along the way.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt."
  },
  {
    "objectID": "posts/2021-11-20-acting-locally-on-housing/index.html",
    "href": "posts/2021-11-20-acting-locally-on-housing/index.html",
    "title": "Acting locally on housing",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nIt may be a measure of the issue’s importance that in the midst of a major climate disaster (not to mention COVID, breakdowns in reconciliation, and other ongoing crises), the leader of the BC Green Party, Sonia Furstenau, dropped a new op-ed on housing. Timing aside, we see this as promising. As we’ve noted in comparing platforms going into the last election, the BC Greens could use some better planning on housing policy.\nFurstenau’s op-ed does not yet offer a plan. Instead it mostly questions the NDP about their plans. Which, for now, is fair… after all, the NDP are in charge, having ditched their partnership with the Greens after the last election. But despite not offering a plan, Furstenau’s op-ed provides some insight into Green Party thinking, and it’s worth a read. This passage, in particular, stuck out:\nOur first take here is that it’s great to see the Green Party leader clearly state that “we need more supply in both market and non-market housing options.” We’re going to take that as a win! Bonus: we see a very green focus on healthy housing close to transportation and social infrastructure (reduced greenhouse gasses from driving!) and accessibility for the diverse needs of individuals (equity!) These are very positive signs that stand in stark contrast to some of the language and direction we see in other manifestations of the Green Party. As some of the experts disagreeing about factors, we thought we’d offer our take on the Green Party leader’s factors, and also point toward a bit of a corrective on the GDP language."
  },
  {
    "objectID": "posts/2021-11-20-acting-locally-on-housing/index.html#think-globally-act-locally",
    "href": "posts/2021-11-20-acting-locally-on-housing/index.html#think-globally-act-locally",
    "title": "Acting locally on housing",
    "section": "Think Globally, Act Locally",
    "text": "Think Globally, Act Locally\nFurstenau’s factors could use some organization. To put to work a very Green-ish slogan, let’s divide them up into global and local factors. This is actually pretty easy. “Global flows of capital” already comes with the label! The “financialization of real estate” is interconnected with these flows and also global in reach (as well as longstanding in various forms), and we could say the same for “speculation” as a force directing flows and finance. So what’s left as local? Pretty much “population growth” and “limited supply.” Together these set the supply and demand dynamics mostly determining local prices.\nFor illustrative purposes, we can draw a circle around local forces, but then embed them within and link them up to how they’re acted upon by broader global forces. Does it look like an avocado? Purely coincidental, we assure you.\n\nBC (and Canada) is embedded within a set of international trade networks, banking relations, treaties and governance structures that provide for global flows of capital and the financialization of real estate, with the future-oriented logic of speculation driving many flows and investments. These factors relate to the local conditions of population growth and limited supply, but do so in somewhat complicated ways. We’ve simplified them here to consider first how global flows of capital accompany flows of people into the population growth of BC. No matter where the growing number of people looking for housing in BC come from, they bring their globally mobile wealth with them. People with a lot of wealth looking for housing can crowd out people with less wealth from competing within the market for a limited supply. We can consider, second, how the financialization of housing is embedded within how developers supply housing. Market developers, in particular, speculate on where they can make a reasonable return on constructing new housing. Their construction efforts work to add supply. But if they don’t add enough supply to meet the housing needs we’re mostly expressing through population growth at the local level, then prices will rise. As prices rise, we move from the local level back to the global level, where rising prices can influence speculation about the future of BC’s housing markets.\nNote that global speculation can have positive effects on local supply to the extent it brings more flows of capital in to finance the development of new supply. But it’s also possible for global speculation to have negative effects, at least in the short term, by encouraging the treatment of housing solely as an appreciating asset, leaving it empty and removing existing local supply from meeting the needs of population growth. We’ve drawn this as a red dotted arrow moving from the global level to the local in our avocado figure above. But we’ve also drawn this as a small, and not especially consequential arrow, reflecting three years of Speculation and Vacancy Tax data demonstrating very few empty dwellings (less than half a percent) subject to taxation as reflecting speculation. In short: we tax speculation leading to empty homes, and there isn’t very much of it.\nWhere does this leave us? From here it’s great to think globally, but figure out ways of acting locally. Pragmatically, this principle follows from how BC has a lot power over local factors, but very limited powers to influence its global environment. In acting locally, we also want to act ethically. In a lesson that environmentalists (and demographers) have increasingly taken to heart, that means we need to be very careful about attempting to mess with population growth. Anyone who thinks there’s an ethical way to limit population growth needs to detail who it is they plan on excluding from BC. And be prepared: we’re probably going to think your plans for who to exclude are bad.\nBy process of elimination, that leaves a focus on limited supply. We can do a lot to lift limitations on supply. We’ve got a whole project detailing how municipal zoning acts as a limit on supply across Metro Vancouver, and zoning is within the control of provincial jurisdiction. We’ve also got ready models of reform; see New Zealand! Aside from that, we can also encourage direct and indirect government construction of housing, supplementing the construction of market housing by financially backed developers. Government investment in social housing is a good way to prevent markets from excluding people in the composition of our population growth. There’s a lot to work with here in terms of constructing a Green Party Platform insuring the new housing we need gets built and also that it meets our climate goals and ethical obligations.\nBut this brings us to a final caution regarding Sonia Furstenau’s thinking on housing so far. How do we carry out all the construction we need to do while diminishing the Real Estate Sector’s contribution to our GDP? The answer, we suggest, is that first we need to break out the Real Estate Sector’s contribution to GDP into its component parts. As we’ve noted before, only a small portion of the Real Estate Sector involves Residential Building Construction. This is the portion we need to grow! Ideally by a lot. We update this finding below. As we can see, a much larger portion includes the rent paid for housing within the Real Estate Sector. Crucially, this includes the rent tenants pay (“lessors of real estate” below) as well as the rent homeowners are modelled by analysts as paying to themselves (“owner-occupied dwellings” below). This is the portion of the Real Estate Sector we need to shrink, but the only way to get there is to make housing plentiful enough to that rents go down. And tying a lower portion of our GDP up in housing consumption leaves a larger portion to spend on other parts of our economy, so this doesn’t just help lower income people afford their housing but doubles as an economic stimulus program.\n\nOverall, Sonia Furstenau has taken some positive steps toward identifying the housing problem in BC and thinking through factors that might be involved. Here we suggest she can go even further toward forming a solid Green Housing Platform by organizing these factors to match what we see in local and global terms. By Thinking Globally, but Acting Locally, Furstenau could offer up the kind of Green New Deal building just what we need: a lot more housing.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt."
  },
  {
    "objectID": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html",
    "href": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html",
    "title": "First Peek at Population and Household Data During COVID & Caveats",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nIn this post we look at the most recent population (and household) estimates to see if we can detect any signals concerning how the COVID-19 pandemic may have impacted how (and where) we live. This is inherently tricky; lots of things changed during COVID times, including how well our normal methods of estimation work. That makes time series less reliable, even as we’re especially concerned with how conditions have changed. So in this post we attempt to pay special attention to what we can and can’t glean from the signals we’re receiving so far."
  },
  {
    "objectID": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#population-estimates",
    "href": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#population-estimates",
    "title": "First Peek at Population and Household Data During COVID & Caveats",
    "section": "Population estimates",
    "text": "Population estimates\nPopulation is the most basic unit of measurement for demographic data. But how do we know how many people live in a region at a given point in time? The short answer is: we don’t know exactly how many, unless we are talking about a highly regulated environment like a prison. Population is floating, constantly in motion, moving, getting born, or dying. But we generally don’t need exact counts insofar as good estimates will work just fine for most purposes.\nSo how do we estimate the population in a given region at a specific point in time? The simplest way is we count, and we do that every 5 years in the census. We’ll be getting estimates from 2021 soon! But even the census misses people (who are mostly asked to count themselves). And it occasionally counts people twice. Ultimately, it’s only an estimate. Which leads us to the question: how good of an estimate is it?\n\nCensus population coverage error\nBy how much does the census under or over-count people? We’ve got ways to estimate that. It starts with the Reverse Record Check. This is a post-census survey that aims to estimate over and undercoverage errors. Next the Record Check gets combined with the results from the Census Overcoverage Study that uses census responses to find possible instances where people have been counted multiple times. Administrative data from CRA tax returns aid in this process. The result is census under and overcoverage estimates. So far undercoverage always exceeds overcoverage, and so combining the two we can get a net undercoverage estimate.\n\nStatistics Canada works hard to get better at its job, and there have been changes in methods over time, continuously refining how to estimate these errors. Statistics Canada also breaks out reasons for overcoverage, the largest of which is that some kids live in and get reported as members of more than one household (hello dual custody!) But there are all kinds of reasons that it’s tricky to keep track of counting everyone once and only once as they move around.\n\n\n\nAnnual population estimates\nWhat do we do in years when we don’t have a census? This was a major problem making it difficult to judge the effects of the 1918 pandemic; it occurred between censuses, along with a major Depression and World War. Sure would’ve been nice to have annual population estimates! Now we do. Basically, we use tax records to extrapolate forward from the census. When people file their taxes they give their address, and the T1 taxfiler data allows the count of taxfilers per census geography. Sort of… the details of assigning taxfiler address to census geographies are messy. Tax data also has information on family structure. For example, people with children may claim child benefits. This gets compiled into a richer dataset that contains information on “taxfilers and dependants”, called the T1FF taxfiler data.\n\n\nT1 and T1FF data\nIn Canada everyone aged 15 and over is supposed to file a tax return. But not everyone does, and legal obligation is relaxed for those without taxes owing. The T1 and T1FF coverage gets estimated by comparing with demographic estimates (that again use T1FF tax data) anchored around the census. We can tell that over time the rate of people (of all ages) filing a tax return has increased.\n\nDespite the rise in tax-filing, it’s worth noting that there are still a lot of people missed (a problem, for instance, in redesigning the CRA system to deliver benefits to those in need). Once the T1 data is processed to identify children and other dependants that did not file a return, we arrive at the T1FF file.\n\nNow we notice a slight overcoverage of children, likely reflecting multiple people filing the same children as dependants (“That’s what we get when we use this denominator”). We also notice a persistent under-coverage for other age groups, especially young adults around university age. We can also assess provincial and territorial variation in T1FF coverage, where Yukon appears the farthest off the grid administratively speaking.\n\n\n\nAnnual population estimates (for real now)\nWe have seen that T1 and T1FF data give us a pretty good idea how many people live in an area, but it’s not really good enough to use that as population estimates, with the possible exception of well-claimed children. But how do we know that? What are the coverage estimates based on? This is where things get slightly tricky and might seem circular at first sight.\nFor census years, we know how to measure the T1 and T1FF coverage by using census counts plus the census coverage estimates. To get population estimates for the in-between years we might be tempted to use the T1FF data, but instead of using the counts directly StatCan looks at the change of population in T1FF data, and estimates the population the year after the census by multiplying it with the growth rate of the T1FF population. This assumes that coverage is roughly constant in each region. Then there are a whole bunch of quality control steps that look at births, deaths, immigration and other records to round things off, using standard demographic accounting formulae (subtract deaths, add births and net immigration), but these typically happen at census district or higher level geography.\nThe basic assumption that makes things work is that population changes gradually, and the composition of population changes slowly too. So any kind of errors in the T1FF data change slowly and divide out when looking at rates of change of T1FF data year over year. But as we get further and further out from the last census our estimates get worse. Fortunately we have a census every 5 years in Canada to anchor things again, and the StatCan population estimate time series gets updated retroactively to reflect that. We can see the echo of this in the estimates of the components of population change that contains the “Residual deviation” component, which distributes the difference of the original population estimates and the post-censual population estimates over the last 5 years. For example, by the time the 2016 census rolled around the population estimate for Metro Vancouver was 50,296 people short (an error of about 2%), so these extra people were retroactively distributed as population growth of unknown source over the five year over year growth periods between 2011 and 2016."
  },
  {
    "objectID": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#show-me-the-data",
    "href": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#show-me-the-data",
    "title": "First Peek at Population and Household Data During COVID & Caveats",
    "section": "Show me the data",
    "text": "Show me the data\nLet’s take a look at the population estimates. As we have seen, they aren’t perfect. And the smaller the areas we choose the more error-prone they become. The finest geography that StatCan publishes population estimates on is census subdivisions, i.e. municipalities. This data informs municipal planning. There may be some quality issues at times, but hopefully not too bad.\nTo add some fun, StatCan just released provisional population estimates for BC municipalities for 2021 to aid with disaster response. T1FF taxfiler data for the 2020 tax year, which are used as a population growth proxy for their filing date in spring 2021, are not yet available, so this is based off of projecting forward the growth rate of 2019 tax year T1 data vs the preliminary 2020 T1 data. In effect, it’s a bit more funky than usual, but let’s throw that into the mix.\nTo better see how the population changes over the years we compute the year over year growth rate, essentially reverse-engineering the process used to derive the population estimates in the first place (i.e. looking at change in tax-filers).\n\nWe see that for the most part population does indeed change slowly, we don’t see huge jumps year over year. However, the smaller municipalities do see rapid changes in population growth at various times. For example, Port Moody was growing fast until 2010, when the growth dropped and stayed relatively flat from 2015 on. This can be explained nicely with construction activity; completions dropped dramatically after 2009. Population growth goes where housing allows it to go.\nThe real outliers are the preliminary 2021 estimates, likely due to two reasons. First, the methods changed. Now we’re using preliminary T1 data instead of T1FF data to estimate the population change. And the other and possibly more important factor is the COVID-19 pandemic, the effects of which have not really been visible in the 2020 data. And the 2021 data is quite remarkable! City of Vancouver shows a population drop relative to 2020, while West Vancouver shows a strong increase!\nAt this point it is good to remember that the estimates are pegged to July 1st, but derived from tax data filed (mostly) earlier in the year (generally April 30th) based upon income and household situations for the previous year (ending December 31st). Extrapolating to July 1st works reasonably well in normal years that follow predictable patterns. But that assumption is questionable during COVID times, where lots of things changed. Universities being closed and students moving back in with parents, remote work giving people more freedoms where to reside, and other factors. Even people’s propensity to file their taxes may have changed! The 2020 T1FF data probably mostly captured what happened either pre-pandemic or during the initial more static phase of the pandemic, with the full effect hitting with the 2021 T1-derived estimates. But by July 2021 many things had already changed again. Universities were preparing for in-person classes, employers were calling people back to in-person work and immigration was ramping up again. All of that will be missed in the 2021 T1 data.\nIn short, preliminary estimates based on T1 filers suggest we’re seeing some pandemic effects, but it’s still hard to separate out the precise timing of when they occurred and how much they represent real effects versus methodological artifacts.\nIs there other data we can look at?"
  },
  {
    "objectID": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#household-estimates",
    "href": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#household-estimates",
    "title": "First Peek at Population and Household Data During COVID & Caveats",
    "section": "Household estimates",
    "text": "Household estimates\nOn this blog we are often interested in the intersection of people and housing. What links those to together is households. Can we also estimate households? Yes, but not as well. The household economic accounts gives us rough estimates at the national and the provincial level derived from an interesting and complicated mix of reporting. Let’s take a look. In particular, let’s see if we can glean if anything has changed in our most recent data. In this case, our most recent data is for 2020 rather than 2021. But the 2020 household accounts data likely reflects the entirety of 2020, rather than a particular moment early in the year (as seems to be the case for the population estimates). As before, one general caveat with data in COVID times is that lots of things have changed. In particular we should expect survey response bias that is different from normal years, so everything should be taken with a grain of salt.\nBefore we get into this we need to take a closer look at what a household is. The answer of course is: It depends! In the census a household is simply defined as an occupied dwelling unit, which can further be divided into household types and give rise to an interesting variety of household types. For the household economic accounts a household is defined to conform with OECD standards as\n\neither an individual person or a group of persons who live together under the same housing arrangement and who combine to provide themselves with food and possibly other essentials of living.\n\nThis is a refinement of the census household category, for example census roommate households aren’t OECD households unless they share food and possibly other essentials of living. Thus we expect the household estimate from the household economic accounts to be higher than census households. At the same time we expect the estimate of OECD households to be lower than the number of economic families and unattached individuals as e.g. derived from tax data. Those would count all individuals in roommate households as separate units, and also separate out other unrelated people living in the same dwelling unit.\nWith the basic definition cleared up, let’s look at the data. Estimates come mostly at the national level, with provincial breakdowns for household estimates. Before jumping into details let’s see how it fairs against census household counts and economic families and unattached individuals from T1FF data as a cross-check.\n\nAs expected, the households economic accounts estimate of OECD households falls between the census household estimates and the number of economic families and unattached individuals. Eyeballing it the growth rates are quite similar, which allows us to use the growth rate of one of these concepts as a proxy for the other. Let’s check how close the growth rates are when aggregated over the inter-census periods.\n\nThe correspondence is relatively good overall, with Alberta showing the largest divergence. We have at least some confidence that the concepts and estimations are fairly interchangeable when looking at growth rates. Knowing that, let’s take a more detailed look at the year over year growth rates of households form the household economic accounts.\n\nThere is quite a bit of variation in the data, probably at least partially due to noise in the survey. But generally the growth rates of households does align with census estimates, even if the level can be off by a bit. We added a trend line to smooth out some of that, and deliberately removed the last data point for 2020.\nThe 2020 data point is dramatically lower for all provinces, and almost certainly due to COVID. Part of it may well be a change in survey response behaviour, but at least a good portion is likely real, due to net outflow of non-permanent resident workers and students, as well as students moving back in with parents or other mechanisms resulting in lower households, like singles coupling up and moving in together. This matches with observations including increased slack in the rental market, stats about lower NPRs, as well as anecdotal evidence of students moving back in with parents.\nSome of these effects will likely be temporary, and the rental market has tightened up again as immigration and in-migration of non-permanent residents has returned and Universities are back to in-person teaching. But some effects may prove sticky. It will be interesting to watch how the timeline continues for the next couple of years.\nWe can also take a look at how, across Canada, households have changed by generation status of major income earner (as defined by Statistics Canada, with Gen X born between 1965 and 1980, and millennials thereafter).\n\nThe 2020 data points looks roughly in line for all generations except the Millennial generation. Here the Millennial generation also contains a small number of younger people that aren’t broken out separately. To judge if the 2020 data point is an anomaly or just a sign of the millennial generation peaking, just like Generation X did, we can fold in population data and convert this to household maintainer rates.\n\nThis suggests that household maintainer rates for Millennials (or younger) actually dropped a bit in 2020, which is consistent with the narrative of millennials moving back in with parents, but also an outflow of the non-permanent resident population which skews younger.\nWe can also check on changes in household size by separating out single and multiple person households.\n\nMulti-person households have continued to grow, whereas the number of single-person actually dropped significantly, below 2018 levels, again consistent with interpretations above.\nLastly we can look at households by tenure.\n\nRenter households have roughly flatlined over the past year, whereas owner households have grown. But we do see a shift from owner households with mortgage to owner households without a mortgage. However, the data looks quite volatile and we should be careful not to over-interpret these one-year shifts.\n\nThe Rental Market\nLet’s circle back to the rental market. Did it provide a foreshadowing of what we’re now observing with population data? We first got our fist systematic look at the pandemic rental market back in January of 2021, based on the results of CMHC’s annual Rental Market Survey in the field in October of 2020. By October we already saw a dramatic rise in rental vacancy rates within major Metro Areas. And the rise was most pronounced within central cities.\n Here we see the vacancy rate rises corresponding to population loss within central cities and young adults moving back home. In effect, it certainly looks like our rental market data was the first data in suggesting the population and household data that would arrive much later in 2021. The next round of rental market data should be coming out early in 2022, and should give us a foretaste of where we’re heading population-wise. At the same time, vacancy rates can rise due to population loss and household consolidation (as we saw in 2020) or due to increases in rental stock, which we’re hoping to see in years to come as new construction comes on-line.\n\n\nTakeaway\nAgain, the takeaway here is suggestive of notable pandemic-related changes in population and population distribution, both geographically and within households. A consistent theme emerging is that many young adults likely moved back home during the early days of the pandemic, as seemingly reflected in preliminary population data for municipalities in 2021; and household data and rental market data for 2020. That these, our most recent data points in each case, seem to converge is useful and interesting. But that the convergence comes attached to slightly different chronologies is also notable. The question remains how much has already changed since our most recent estimates. And there we just don’t know, and probably won’t know for a little while longer. But we’ll be paying close attention to rental market data when it comes out. As we’ve noted before, those vacancy rates are a really useful metric!"
  },
  {
    "objectID": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#appendix",
    "href": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#appendix",
    "title": "First Peek at Population and Household Data During COVID & Caveats",
    "section": "Appendix",
    "text": "Appendix\nWe make the effort to make our analysis transparent, reproducible and highly adaptable by publishing the code. For example, someone interested in repeating parts of the analysis for a different region can simple download the code and change a line or two to point it to their region of interest. However, there are still barriers to actually doing this, from having an R installation or the rudimentary familiarity with R required to make the necessary tweaks. So we have resolved to start adding some graphs in appendices that offer a view into other relevant regions, if appropriate.\nFor this post, looking at Metro Victoria population growth estimates and preliminary 2021 population growth offers another useful view. (StatCan only released preliminary population estimates for BC, but not for other provinces.)\n\nHere we see and interesting pattern for the central city, Victoria, that looks different from Vancouver. Victoria shows a decline in population in 2020, followed by an increase in 2021, whereas the situation in Vancouver was reversed. This speaks further to the diversity of changes during COVID times, but likely also to the difficulty of measuring this. Victoria’s population skews older than the population in the City of Vancouver, have a higher share of non-permanent residents, and a higher share of recent immigrants, all factors that we would expect to shape the impact of COVID on population growth."
  },
  {
    "objectID": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#update-2021-11-29",
    "href": "posts/2021-11-28-first-peek-at-population-and-household-data-during-covid-caveats/index.html#update-2021-11-29",
    "title": "First Peek at Population and Household Data During COVID & Caveats",
    "section": "Update (2021-11-29)",
    "text": "Update (2021-11-29)\nThe day after we published this post StatCan revised their 2021 preliminary population estimates for BC, one more indication that these stats need to be treated carefully. This data release was produced to aid emergency response efforts of the BC flooding, and produced with very short lead time. It is good to see StatCan rising to the challenge and push their analysis methods to meet needs like this,\nWe have replaced data underlying the graphs above with the updated estimates, for comparison and increased transparency we include the original (and now outdated) graphs based on the first round of released data below.\n\n\n\n\nReproducibility receipt\n\n## [1] \"2021-11-29 09:24:04 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [687c9d7] 2021-11-29: fix legend in T1FF graph\n## R version 4.1.2 (2021-11-01)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Big Sur 10.16\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cmhc_0.1.0      httr_1.4.2      digest_0.6.28   cancensus_0.4.8\n##  [5] cansim_0.3.10   forcats_0.5.1   stringr_1.4.0   dplyr_1.0.7    \n##  [9] purrr_0.3.4     readr_2.1.0     tidyr_1.1.4     tibble_3.1.6   \n## [13] ggplot2_3.3.5   tidyverse_1.3.1\n## \n## loaded via a namespace (and not attached):\n##  [1] sass_0.4.0       bit64_4.0.5      vroom_1.5.6      jsonlite_1.7.2  \n##  [5] here_1.0.1       modelr_0.1.8     bslib_0.3.1      assertthat_0.2.1\n##  [9] highr_0.9        blob_1.2.2       cellranger_1.1.0 yaml_2.2.1      \n## [13] pillar_1.6.4     RSQLite_2.2.8    backports_1.3.0  glue_1.5.0      \n## [17] rvest_1.0.2      colorspace_2.0-2 htmltools_0.5.2  pkgconfig_2.0.3 \n## [21] broom_0.7.10     haven_2.4.3      bookdown_0.24    scales_1.1.1    \n## [25] tzdb_0.2.0       git2r_0.29.0     farver_2.1.0     generics_0.1.1  \n## [29] ellipsis_0.3.2   cachem_1.0.6     withr_2.4.2      cli_3.1.0       \n## [33] magrittr_2.0.1   crayon_1.4.2     readxl_1.3.1     memoise_2.0.0   \n## [37] evaluate_0.14    fs_1.5.0         fansi_0.5.0      xml2_1.3.2      \n## [41] blogdown_1.6     tools_4.1.2      hms_1.1.1        lifecycle_1.0.1 \n## [45] munsell_0.5.0    reprex_2.0.1     compiler_4.1.2   jquerylib_0.1.4 \n## [49] rlang_0.4.12     grid_4.1.2       rstudioapi_0.13  labeling_0.4.2  \n## [53] rmarkdown_2.11   gtable_0.3.0     codetools_0.2-18 DBI_1.1.1       \n## [57] curl_4.3.2       R6_2.5.1         lubridate_1.8.0  knitr_1.36      \n## [61] fastmap_1.1.0    bit_4.0.4        utf8_1.2.2       rprojroot_2.0.2 \n## [65] stringi_1.7.5    parallel_4.1.2   Rcpp_1.0.7       vctrs_0.3.8     \n## [69] dbplyr_2.1.1     tidyselect_1.1.1 xfun_0.28"
  },
  {
    "objectID": "posts/2022-02-09-canada-s-2021-census-part-1/index.html",
    "href": "posts/2022-02-09-canada-s-2021-census-part-1/index.html",
    "title": "Canada’s 2021 census, part 1",
    "section": "",
    "text": "Today we are getting the first release of the 2021 census data. For now, it’s just population, household, and dwelling counts. As well as Census Metropolitan Area and Census Tract boundaries. Census data is a snapshot in time, the reference day for the 2021 census is May 11, 2021. The rest of the data will follow over the coming year."
  },
  {
    "objectID": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#censusmapper",
    "href": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#censusmapper",
    "title": "Canada’s 2021 census, part 1",
    "section": "CensusMapper",
    "text": "CensusMapper\nThe data is now available on CensusMapper for anyone to view, download, and map themselves. As of the writing of this post, the CMA and CT level geographies are still processing, and will be uploaded as that gets done. But the rest of the geographies are available for browsing, down to dissemination area level.\nIn this round we opted to process the StatCan geographies to remove larger rivers and water features, depending on zoom level. This aids orientation on the maps by providing context."
  },
  {
    "objectID": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#what-do-those-numbers-mean",
    "href": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#what-do-those-numbers-mean",
    "title": "Canada’s 2021 census, part 1",
    "section": "What do those numbers mean?",
    "text": "What do those numbers mean?\nNow is a good time as any to review the concepts of today’s data release. The census dictionary is the go-to guide for definitions, we will try to explain the three variables that are being released today.\n\nPopulation: The population counts in a region refers to the people having their usual residence there. The census counts every person only once, at their usual residence. Some people might be away from their usual residence for a while and live elsewhere on census day. This may include people living elsewhere for work for a couple of weeks or months, or students attending university but returning to their usual residence (e.g. their parent’s home) at the end of the semester. These people will not be counted where they reside on census day, but only at their usual place of residence.\nDwellings: The dwelling counts released today are only for private dwellings, which excludes things like nursing homes or prisons or student dorms. These are called collective dwellings, and while these aren’t part of the dwellings release today, people living in collective dwellings will be counted if the collective dwelling is their usual residence. This can lead to interesting effects like regions with no private dwellings but non-zero population.\nHouseholds: The census defines a household simply as an occupied dwelling unit where at least one of the occupants has their usual residence there. The household counts in this release only count private households, so households in private dwellings."
  },
  {
    "objectID": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#derived-metrics",
    "href": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#derived-metrics",
    "title": "Canada’s 2021 census, part 1",
    "section": "Derived metrics",
    "text": "Derived metrics\nWith those three variables there is not all that much we can do. Adding in area we can divide to obtain densities, like population density. But that’s an imperfect metric as it depends to a large extent on the choice of census geographies that are used to aggregate the data at. For example, one highrise on geographic area narrowly cut around it will result in a very high population density, but if we also enclose a nearby large park into the area the population density will drop considerably.\nAnother way to combine the data is to subtract households from dwellings. That gives us the dwellings not occupied by usual residents, which is often taken as a proxy for unoccupied dwellings. That’s problematic in two ways. One is that some of these dwellings are occupied, just not by usual residents. Students need housing during the semester. Workers on short term housing need temporary places to live too. The other problem is that homes that are vacant on census day aren’t a good proxy for how many problematic vacancies there are. The majority of these are likely moving vacancies, units for rent or already rented that aren’t moved in yet, and the same on the ownership side. And of course recent completions, where buildings completed close to census day but have not filled in yet, which hapless users of census data trip over with every census data release. The other big part in this is secondary suites, which the census considers as separate dwelling units but owners may well consider as part of their house that they are using for their own needs. Anecdotally owners have increasingly absorbed suites back into their main unit during COVID times to use them as home offices and in general keep more distance from others. Some vacancies are also problematic longer-term vacancies, although we know from administrative data that these are quite low. We have written at length about this, but misconceptions around this concept don’t seem to die."
  },
  {
    "objectID": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#censusmapper-api-and-cancensus",
    "href": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#censusmapper-api-and-cancensus",
    "title": "Canada’s 2021 census, part 1",
    "section": "CensusMapper API and {cancensus}",
    "text": "CensusMapper API and {cancensus}\nThe data on CensusMapper is also available via the CensusMapper API and the {cancensus} R package. Cancensus has been updated to now include better control of caching and version information. This makes it easier to recall data, which was necessary during the 2016 release when StatCan recalled and updated some of the previously released data. The CensusMapper API sever now sends version information for each data request, and starting with {cancensus} version 0.5.0 this version information is stored in the local cache. This enables the selective deletion/update of data in the local cache, if this will become necessary during this census release cycle."
  },
  {
    "objectID": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#change-over-time",
    "href": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#change-over-time",
    "title": "Canada’s 2021 census, part 1",
    "section": "Change over time",
    "text": "Change over time\nThe other interesting question is how these numbers have changed over time. In which neighbourhoods has population grown, where has it shrunk? The current data comes with 2016 population, households and dwelling counts down to the ADA/CT level, which makes it easy to compare change over time. For DA level data things get more complicated.\nThe fine granularity of census data can offer great insight in how our cities have changed. One slight complication in this is that census geographies change over time, making small-area comparisons a little less straight forward than one might think. The best way to solve this issue is to get a custom tabulation to aggregate several censuses on the same geography. But those won’t be available for a while and for many applications that’s overkill. Sometimes we just want a quick way to compare fine-geography census data across several censuses.\nAt the same time we will likely continue to refine the geographies over the next couple of days, especially the CT and CMA level that only got released today. Geographic data is also tagged with a version number when downloading via {cancensus} and can thus be updated if needed.\nFortunately we have solved this problem via tongfen, which takes fine geography census data across several census years and automatically re-aggregates it to a “least common denominator geography”. This is the same as pulling a custom tabulation on the least common denominator geography, so it represents real census counts as opposed to area-weighted (dasymetric) interpolation that people sometimes employ and that generally should be avoided whenever possible."
  },
  {
    "objectID": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#upshot",
    "href": "posts/2022-02-09-canada-s-2021-census-part-1/index.html#upshot",
    "title": "Canada’s 2021 census, part 1",
    "section": "Upshot",
    "text": "Upshot\nIt’s exciting that the 2021 data is finally making it’s way into the hands of users. With CensusMapper, {cancensus}, and {tongfen} it’s becoming increasingly easy to put this data to use.\nAs usual, the code for this post, including the code to scrape the data out of the PDFs, is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-02-10 16:54:48 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [862b66b] 2022-02-09: census 2021 post\n## R version 4.1.2 (2021-11-01)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.3 tongfen_0.3.4            \n##  [3] forcats_0.5.1             stringr_1.4.0            \n##  [5] dplyr_1.0.7               purrr_0.3.4              \n##  [7] readr_2.1.1               tidyr_1.1.4              \n##  [9] tibble_3.1.6              ggplot2_3.3.5            \n## [11] tidyverse_1.3.1           cancensus_0.5.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] fs_1.5.1           sf_1.0-4           lubridate_1.8.0    bit64_4.0.5       \n##  [5] RColorBrewer_1.1-2 httr_1.4.2         rmapzen_0.4.3      tools_4.1.2       \n##  [9] backports_1.4.0    bslib_0.3.1        utf8_1.2.2         R6_2.5.1          \n## [13] KernSmooth_2.23-20 lazyeval_0.2.2     rgeos_0.5-8        DBI_1.1.2         \n## [17] colorspace_2.0-2   withr_2.4.3        sp_1.4-6           tidyselect_1.1.1  \n## [21] git2r_0.29.0       bit_4.0.4          curl_4.3.2         compiler_4.1.2    \n## [25] cli_3.1.0          rvest_1.0.2        geojsonsf_2.0.1    xml2_1.3.3        \n## [29] bookdown_0.24      sass_0.4.0         scales_1.1.1       classInt_0.4-3    \n## [33] proxy_0.4-26       digest_0.6.29      foreign_0.8-81     rmarkdown_2.11    \n## [37] pkgconfig_2.0.3    htmltools_0.5.2    highr_0.9          dbplyr_2.1.1      \n## [41] fastmap_1.1.0      rlang_0.4.12       readxl_1.3.1       rstudioapi_0.13   \n## [45] httpcode_0.3.0     farver_2.1.0       jquerylib_0.1.4    generics_0.1.1    \n## [49] jsonlite_1.7.3     vroom_1.5.7        magrittr_2.0.1     s2_1.0.7          \n## [53] Rcpp_1.0.8         munsell_0.5.0      fansi_1.0.2        lifecycle_1.0.1   \n## [57] stringi_1.7.6      yaml_2.2.1         jqr_1.2.2          maptools_1.1-2    \n## [61] grid_4.1.2         parallel_4.1.2     crayon_1.4.2       geojsonio_0.9.4   \n## [65] lattice_0.20-45    haven_2.4.3        geojson_0.3.4      hms_1.1.1         \n## [69] knitr_1.36         pillar_1.6.4       crul_1.2.0         wk_0.5.0          \n## [73] reprex_2.0.1       glue_1.6.1         evaluate_0.14      blogdown_1.6      \n## [77] V8_3.6.0           modelr_0.1.8       vctrs_0.3.8        tzdb_0.2.0        \n## [81] cellranger_1.1.0   gtable_0.3.0       assertthat_0.2.1   xfun_0.28         \n## [85] broom_0.7.12       e1071_1.7-9        class_7.3-19       units_0.7-2       \n## [89] ellipsis_0.3.2"
  },
  {
    "objectID": "posts/2022-02-14-unoccupied-canada/index.html",
    "href": "posts/2022-02-14-unoccupied-canada/index.html",
    "title": "Unoccupied Canada",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)"
  },
  {
    "objectID": "posts/2022-02-14-unoccupied-canada/index.html#tldr",
    "href": "posts/2022-02-14-unoccupied-canada/index.html#tldr",
    "title": "Unoccupied Canada",
    "section": "TLDR",
    "text": "TLDR\nCanadian Census data on “Dwellings Unoccupied by Usual Residents” are frequently misunderstood. Now that data from 2021 are out, we provide a timely explainer and draw upon a variety of resources, including comparisons with US data, Empty Homes Tax data, and zooming in on census geographies, to help people interpret what we can see."
  },
  {
    "objectID": "posts/2022-02-14-unoccupied-canada/index.html#canada-unoccupied",
    "href": "posts/2022-02-14-unoccupied-canada/index.html#canada-unoccupied",
    "title": "Unoccupied Canada",
    "section": "Canada Unoccupied",
    "text": "Canada Unoccupied\nGiven that ongoing occupations are so much in the news, let’s turn the channel to talk about those parts of Canada that are unoccupied!\nWe have new census data with population, dwelling and household counts. This gives us a first view on how neighbourhoods in Canada have changed. For better or worse, one metric that can be derived from these three census variables is “dwellings not occupied by usual residents,” otherwise known as the difference between dwelling units and households.\nThis metric is frequently misunderstood and we have devoted a lot of digital ink attempting to clear things up. With new data out we are already seeing bad takes based on this metric in the news. And it only gets worse when looking at social media commentary."
  },
  {
    "objectID": "posts/2022-02-14-unoccupied-canada/index.html#explainer",
    "href": "posts/2022-02-14-unoccupied-canada/index.html#explainer",
    "title": "Unoccupied Canada",
    "section": "Explainer",
    "text": "Explainer\nWhat does “not occupied by usual residents mean”? In Canada this metric can be decomposed into two components: 1) dwellings that were unoccupied on census day, and 2) dwellings that were occupied, but by people who have their usual residence elsewhere. This second category includes, for instance, people working and living temporarily in a different city and students that return to their parent’s place at the end of the semester. Out of these two components, actually unoccupied dwellings make up the vast majority of unoccupied by usual resident dwellings, but in some regions, for example college towns, temporarily occupied dwellings can make up half of such dwellings. By way of illustration, for the censuses 2001 through 2016 we have a cross-tabulation on CensusMapper that shows just the unoccupied dwellings, which can additionally be sliced by structural type of dwelling.\nSo what’s going on with unoccupied dwellings? Social media and many (poorly done) news reports would have us believe these are indicative of some form of “toxic demand” or empty “safety deposit boxes in the sky.” But is this the case?\nThere are multiple sources of data that can help shed light on the nature of dwellings counted as unoccupied in the census. First, we can put Canadian rates of units not occupied by usual residents in context with US cities, where the ACS collects information on the reason why a dwelling unit may be unoccupied. Second, we can look at a cross tabulation of document type (detailing if the dwelling is occupied by usual residents, temporarily present persons, or unoccupied) and structural type. Third, we can look at relevant administrative data, including the City of Vancouver’s Empty Homes Tax and BC’s Speculation & Vacancy Tax. Finally, we can zoom in and look in more detail at the data newly released from the 2021 Census results.\n\nComparison to US ACS data\n\nIn this infographic we match data from the last Census (2016) to ACS data from the same year. The closest to “toxic demand” or “empty safe deposit boxes in the sky” in the ACS data would appear to be dwellings showing up in green for “seasonal, recreational or occasional use.” These are the sort generally welcomed by vacation destinations like Miami. By contrast, other big cities, like Detroit, see what looks more like a “toxic lack of demand” in “other,” likely abandoned dwellings (showing up in brown), though “other” is a broad term encompassing many possible reasons for vacant dwellings (e.g. recent death of owner). The rest of the vacancies showing up in US data tend to be temporary and transactional (related to sales and rentals). We don’t get this same breakdown in the Canadian data, but we expect the underlying patterns to be similar, suggesting a baseline of vacancies associated with normal transactions; not unoccupied long term, but merely on Census day. Larger variation in occasional and other uses which may reflect longer-term vacancies is layered on top.\n\n\nDocument type by structural type\nRefined data from the 2016 Census (not yet available in 2021) also enables us to split out unoccupied units by structural type of dwelling. Let’s take a look at what this looks like for three Canadian cities: the City of Vancouver, City of Toronto, and City of Ottawa.\n\nWe note that in all three cities, the dwelling type most likely to register as unoccupied is an apartment in a “duplex.” Duplex is the designation the Census gives to buildings when a basement apartment is added to a single-family detached house, turning both the upper and lower unit created into “apartments in a duplex.” The census puts in an effort to find these slippery and often informal kinds of housing. But it’s tricky to do well and keep up with these flexible units. When a homeowner reabsorbs the suite into the main unit or chooses not to rent it out for whatever reason, the suite registers as unoccupied in the census. Accordingly, the distribution of housing stock matters for occupancy rates. Cities with lots of secondary suites mixed into their dwelling stock tend to register with higher rates of unoccupied homes."
  },
  {
    "objectID": "posts/2022-02-14-unoccupied-canada/index.html#administrative-data",
    "href": "posts/2022-02-14-unoccupied-canada/index.html#administrative-data",
    "title": "Unoccupied Canada",
    "section": "Administrative Data",
    "text": "Administrative Data\nLet’s turn to administrative data from the City of Vancouver’s Empty Homes Tax (EHT) and BC’s Speculation and Vacancy Tax (SVT). While the most recent data from these taxes extend only to 2020, they provide a bridge, of sorts, between the 2016 Census and the 2021 Census, and also establish estimates of the prevalence of problematic long-term vacancies, as defined by the taxes for these years. Of note, the taxes were put in place in 2017 (EHT) and 2018 (SVT) amid the perception that problematic long-term vacancies were widespread, in part extending from misinterpretations of Census data.\nBelow we plot the percentage of Private Dwellings Unoccupied by Usual Residents from the 2016 and 2021 Census against the percentage of Taxable Properties Containing Dwellings taxed as “Empty” in the publicly reported EHT and SVT data. For EHT, we also note properties declared exempt from taxation but not occupied by an owner or renter. These properties represent possible long-term empty dwellings (as distinct from shorter term empty dwellings that would also be caught by the Census).\n The percentage of dwellings showing up as plausibly vacant in the Empty Homes Tax data is far below the percentage showing up as Unoccupied by Usual Residents in the Census data. This likely reflects a combination of factors, including the Census referring specifically to occupation on Census day, and hence picking up dwellings that were just temporarily vacant. Indeed, most of the dwellings picked up in the EHT data as plausibly vacant are still exempt from taxation. The largest reason for exemption is that the properties changed hands during the tax year, emphasizing the role of regular transactions in driving short-term vacancies.\nProperties actually taxed as problematic vacancies are quite rare in the City of Vancouver, declining from just over 1% to just under 1% between 2017 and 2020. Between the Census years spanning that period, the percent of dwellings declared Unoccupied by Usual Residents declined from just over 8% to 7%. Can we suggest that around 1-in-8 dwellings determined to be unoccupied by usual residents on Census day reflect problematic long-term empty dwellings (a.k.a. “toxic demand”)? Tempting, but not quite. For one, our denominators for percentage figures from Census (private dwellings) and tax data (taxable properties) aren’t quite the same. Mostly this reflects the prevalence of single properties containing multiple dwellings, including our “duplexes” discussed above, but also most purpose-built rental apartment buildings. To re-examine the issue, let’s just get rid of denominators and look at raw numbers of dwellings deemed unoccupied by usual residents or deemed empty and taxed as such.\n\nLooking just at raw numbers, it looks like instead of 1-in-8 unoccupied hit with a tax, we get closer to 1-in-12 when the Empty Homes Tax first when into effect in 2017 (matched to 2016 Census), dropping to more like 1-in-15 by 2020 (matched to 2021 Census). For the Speculation and Vacancy Tax, the figures are even lower, declining to one property taxed in 2020 for every twenty-eight dwellings showing up as unoccupied by usual resident in 2021. (It appears to us that the difference between SVT taxed and EHT taxed is largely explained by different reporting standards emphasizing only properties declared as empty for SVT data vs. properties declared, determined, or deemed to be vacant in the EHT data.)\nIt’s difficult to say what the rate of Dwellings Unoccupied by Usual Residents would look like in the City of Vancouver absent the EHT and SVT. It’s a counterfactual, requiring some thought and many assumptions to work out properly. For instance, we should keep in mind that the 2021 census happened during COVID times, with possible short as well as long term effect of COVID further muddying the waters. But we can say that at this point it appears very little of what shows up as Dwellings Unoccupied by Usual Residents in the Census translates into problematically empty dwellings as defined by the Empty Homes and Speculation & Vacancy Taxes.\nDoes this mean the Empty Homes Tax is useless? Probably not. And it’s notable that Census data and EHT data agree on a decline in unoccupied by usual resident and vacant dwellings. Returning to the ACS data, it’s apparent that inter-city variation is largest in uses like temporary residences that drive EHT or SVT taxed empties. Once taxes are put in place on long-term vacancies, it likely reduces the proportion of properties used for temporary residence. This, in turn, likely reduces the ratio of properties taxed as problematically empty relative to properties showing up as Unoccupied by Usual Residents in the Census. But as in the Vancouver data, we can see that the overall effect is probably limited. This, quite simply, is because in most cities the vast majority of dwellings showing up as Unoccupied by Usual Residents in the Census are not actually long-term or problematically vacant."
  },
  {
    "objectID": "posts/2022-02-14-unoccupied-canada/index.html#zooming-in-on-vancouver",
    "href": "posts/2022-02-14-unoccupied-canada/index.html#zooming-in-on-vancouver",
    "title": "Unoccupied Canada",
    "section": "Zooming in on Vancouver",
    "text": "Zooming in on Vancouver\nFinally, let’s turn to the geographic distribution of Dwellings Unoccupied by Usual Residents, plotting the new 2021 data for the City of Vancouver at the Census Tract level.\n\nWe see the rate of Dwellings Not Occupied by Usual Residents varies throughout the city, but some areas jump out. And just like when the 2016 numbers come out some hapless commentator will inevitably zoom in on the bright areas and complain about empty homes.\nOne census tract, Census Tract 9330049.05 south of 2nd Ave between Cambie and Main, stands out with 27% of dwellings not occupied by usual residents. Let’s take a closer look and try and get ahead of silly arguments like commentators claiming that “speculation is one of our prime suspects” or running headlines decrying this as a case of empty or underused housing.\n\nEnhance\nTract level household and dwelling counts are summed up from dissemination areas, so let’s switch our map resolution to that finer-grained level census geography.\n\nThat only added moderate detail, splitting the census tract into two dissemination areas, the larger of the two now sporting a share of 35% dwellings not occupied by usual residents, adding up to 253 “unoccupied” dwellings. Let’s zoom in even more to locate where the census found those “empty” homes.\n\n\nEnhance more\nDissemination blocks are the finest level of geography for which the census releases this data. So let’s utilize blocks to take an even closer look at our tract of interest and add block counts of dwellings unoccupied by usual residents.\n\nBlocks enable us to pinpoint how the higher level geography numbers were derived and where problematic buildings may be located. Three of the areas show up as 100% unoccupied, but two of them only have a single dwelling unit. These may be live-work spaces in this mostly industrial area. But one block shows up as having 38 dwelling units. And while the block at the corner of Main and 2nd has a lower share of units not occupied by usual residents, the total count comes out at 178. Let’s take a closer look at those two blocks.\n\n\nMain and 2nd\nThe building at the southwest corner of Main and 2nd was newly constructed around census day. The census counted 233 dwelling units on this block with no other apparent housing on it, with 178 dwelling units not occupied by usual residents. It’s likely the case that residents simply hadn’t yet moved in to this building that completed quite near to Census day. The effect here is similar to what we witnessed for newly constructed areas in 2016 where an outcry was raised over vacancies picked up by the census data release.\n\nWhile we know the building is new, we can also perform a quick check with CMHC completions data to see how many units got completed close to census day in 2021. One complication is that CMHC data for 2021 was still reported on 2016 boundaries, and the census tract in question came from 2016 census tract 9330049.01 which was split into four separate tracts in 2021. Doing a quick check at the census tract level from January through June 2016 on census tract 9330049.01 reveals that indeed 263 apartment units completed in April 2021, most of which are likely the ones in the building in question.\n\n\nYukon and 5th\nThere are 38 dwelling units with nobody living in them at the block with the Yukon Shelter and no other apparent housing units. The shelter states that it is for people in “emergency and transitional situations”, which is why people living there would not be classified as usual residents. This is an example where all these units are very likely lived in as shelter or transitional housing units, just not by people classified as usual residents."
  },
  {
    "objectID": "posts/2022-02-14-unoccupied-canada/index.html#the-bigger-picture",
    "href": "posts/2022-02-14-unoccupied-canada/index.html#the-bigger-picture",
    "title": "Unoccupied Canada",
    "section": "The bigger picture",
    "text": "The bigger picture\nWith these examples under our belt we can start to appreciate some of the complexities of this metric. When the Census counts Dwellings Unoccupied by Usual Residents, it’s a by-product of their primary aim, which is simply to find out where everyone in Canada is living on Census Day. Dwellings are tabulated to provide a frame for finding people. As such, the Census doesn’t care that much about overshooting its count of dwellings (as we see with secondary suites in duplexes). What it cares about is finding people and linking them back to a single residence (which explains how we get dwellings occupied by people who aren’t usual residents). We suggest that Dwellings Unoccupied by Usual Residents can still be an interesting metric, but only when treated with appropriate caution.\nSo our final takeaway is that when you see commentators throwing out figures on Dwellings Unoccupied by Usual Residents without appropriate cautions, or as a straightforward indicator of Empty Homes, keep in mind that it’s an indicator of something else entirely. It’s an indicator they don’t know much about housing.\nAs usual, the code for this post, including the code to scrape the data out of the PDFs, is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-03-29 18:09:01 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [b35bffa] 2022-02-19: rent change vs vacanct rates post\n## R version 4.1.2 (2021-11-01)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.3 cmhc_0.1.0               \n##  [3] tongfen_0.3.5             cancensus_0.5.0          \n##  [5] sf_1.0-7                  forcats_0.5.1            \n##  [7] stringr_1.4.0             dplyr_1.0.8              \n##  [9] purrr_0.3.4               readr_2.1.2              \n## [11] tidyr_1.2.0               tibble_3.1.6             \n## [13] ggplot2_3.3.5             tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8.3       lubridate_1.8.0    class_7.3-20       assertthat_0.2.1  \n##  [5] digest_0.6.29      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  \n##  [9] backports_1.4.1    reprex_2.0.1       evaluate_0.15      e1071_1.7-9       \n## [13] httr_1.4.2         blogdown_1.8       pillar_1.7.0       rlang_1.0.2       \n## [17] readxl_1.3.1       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.13    \n## [21] munsell_0.5.0      proxy_0.4-26       broom_0.7.12       compiler_4.1.2    \n## [25] modelr_0.1.8       xfun_0.30          pkgconfig_2.0.3    htmltools_0.5.2   \n## [29] tidyselect_1.1.2   bookdown_0.25      fansi_1.0.3        crayon_1.5.1      \n## [33] tzdb_0.2.0         dbplyr_2.1.1       withr_2.5.0        grid_4.1.2        \n## [37] jsonlite_1.8.0     gtable_0.3.0       lifecycle_1.0.1    DBI_1.1.2         \n## [41] git2r_0.30.1       magrittr_2.0.2     units_0.8-0        scales_1.1.1      \n## [45] KernSmooth_2.23-20 cli_3.2.0          stringi_1.7.6      fs_1.5.2          \n## [49] xml2_1.3.3         bslib_0.3.1        ellipsis_0.3.2     generics_0.1.2    \n## [53] vctrs_0.3.8        tools_4.1.2        glue_1.6.2         hms_1.1.1         \n## [57] fastmap_1.1.0      yaml_2.3.5         colorspace_2.0-3   classInt_0.4-3    \n## [61] rvest_1.0.2        knitr_1.38         haven_2.4.3        sass_0.4.1"
  },
  {
    "objectID": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html",
    "href": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html",
    "title": "Census quirks; using UBC area as an example",
    "section": "",
    "text": "Census data serves as the baseline for a lot of downstream data products, we like to think of it as a solid and authoritative data source. And the data from the Canadian census is indeed amazing. But counting people is hard, and the closer one looks the more one realizes little problems.\nAll it takes to shake your faith in census data is spending 30 minutes browsing dissemination block or dissemination area data in a neighbourhood you know well.\nSome areas are easier to enumerate than others. One of the more challenging areas, and one the census has been struggling with for many years now, is the Point Grey Peninsula west of the City of Vancouver consisting of the UBC/UNA/UEL communities. There are lots of reasons why this is a difficult area to enumerate, the student population is hard to reach and their usual place of residence is often ambiguous. The area is rapidly changing, and it involves lots of judgement calls on whether to classify housing units as private or collective dwellings.\nThese problems plague census data throughout Canada, but they are particularly strong in the Point Grey Peninsula. This makes it a good example case to walk through in detail to help understand the little (and sometimes large) quirks in census data."
  },
  {
    "objectID": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#the-big-picture",
    "href": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#the-big-picture",
    "title": "Census quirks; using UBC area as an example",
    "section": "The big picture",
    "text": "The big picture\nIssues often jump out when comparing census data cross two censuses. For the 2021 census all we have right now is population, household and dwelling counts. These are the backbone of all census data, the base denominators for everything else.\n\nWe can see the transformation that happened across the past six censuses, the regions went from a single census tract in the first four years to two census tracts in 2016 splitting further into 3 in 2021. Of note, census tract 9330069.02 covering South Campus remained mostly intact, but had some important boundary changes while keeping the same name. This means that if we wanted to TongFen the census tracts the resulting least common denominator regions would be the whole Point Grey Peninsula, just like during the first four census years in this period.\nHow large is that difference between the 2016 and 2021 boundaries for tract 9330069.02? GeoSuite comes with the 2016 population data on the new 2021 boundaries, and it quantifies it at 3,926, quite a bit lower than the population of 5,080 it had in the original boundaries. Even small boundary changes like this do matter.\n\nA big cautionary tale in this that geographic identifiers may not change from census to census, even if the underlying geography does. This makes comparing census data across time tricky, we can’t just rely on geographic identifier but need to ensure the associated geographies match. The move to Dissemination Geography Unique Identifiers (DGUIDs) mitigates these issues at the expense of extra complexity.\nBut why would StatCan change the boundaries of this census tract? To understand why this boundary change happened we need to understand that the administrative landscape of the Point Grey Peninsula. It is part of Electoral Area A, an unincorporated area in BC. There is no municipal government, but there are three quasi-administrative entities operating within this Point Grey Peninsula. There is UBC and campus housing serving (mostly) students. Then there is the UNA, a quasi-municipal body with an elected board and limited power that oversee some of the affairs of residents on campus lands, including faculty and staff housing but also general public market rental and condo housing on land leased directly or indirectly from the university. And then there is the UEL, an unincorporated area administered by the province with limited opportunity for community input, the smallest (in population) of the three groups yet people often invoke its name to refer to the whole Point Grey Peninsula.\nThis most recent boundary change (roughly) aligns census tract 9330069.04 with the UEL and leaves the other two for UBC/UNA, which is useful for the different administrative bodies. Unfortunately the boundary change left out leləm, a Musqueam development in the Point Grey Peninsula that will be part of the UEL administrative region but is still part of 9330069.02 in the 2021 census.\nCompare this to the breakdown of the various administrative regions from the rather dated Bike Walk UBC project\n\nWe see leləm and also the Musqueam lands that have been leased to the golf course coloured in orange, and the Pacific Spirit Park that has been leased to the Metro Vancouver in green."
  },
  {
    "objectID": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#looking-in-more-detail",
    "href": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#looking-in-more-detail",
    "title": "Census quirks; using UBC area as an example",
    "section": "Looking in more detail",
    "text": "Looking in more detail\nWe want to focus in what happened between those last three censuses, for that we will go down to census block level and TongFen the geographies and values. We will focus on the most recent three censuses.\n\nThis gives a quick overview of the geographies resulting from TongFen for three consecutive censuses, as well as the changes happening in each region. The strong growth on South Campus is clearly visible. But the regions showing loss of dwelling units, as well as regions with strong population loss seem curious.\nThis warrants a closer look. Let’s graph the change in each individual block for those two periods, for ease of throwing out the blocks where none of our metrics, population, dwellings or households, changed by less than 50.\n\nFirst looking at dwellings we notice a pattern where dwellings got add 2011-2016, and subtracted again in 2016-2021. That’s the little triangle that changes from blue to red in the map, it’s (mostly) undergraduate family student housing. It got neither built nor torn down during any of these periods, it just got reclassified as private housing in the 2016 census and changed back to collective housing in 2021. There was no population change associated with this because the population got counted the same way, mostly as this being not their usual residence. In 2016 the census recorded 785 dwellings with 136 people living there as their usual residence. In 2021 that flipped to 45 dwellings with 74 people living there. A big change in the data, but virtually no change on the ground.\nWhen it comes to population declines 2016-2021, we can identify several red-ish areas on the map higher up that correspond to student housing. UBC was operating fully remote during the semester near the 2021 census, and the on-campus student population was only a fraction of it’s normal size. While a higher portion of the students that did stay on campus would have had their usual residence on campus (as opposed to their parent’s place), there were much fewer of them and the net result was negative.\nLet’s take a look at the development of the absolute number of those metrics across the three censuses. To make it a little easier to understand what’s going on we will roughly classify each metric in each block depending on if the count is falling, rising, flat, or exhibits a sawtooth pattern.\n\nThe areas exhibiting a sawtooth pattern likely saw reclassification of some sort, although a rise and then fall in population may also happen when student housing got added 2011-2016 and then students stayed away from campus during COVID in 2021. Areas showing a fall likely underwent some kind of reclassification too, areas showing a consistent rise have likely seen dwellings and then also population and households added."
  },
  {
    "objectID": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#other-quirks",
    "href": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#other-quirks",
    "title": "Census quirks; using UBC area as an example",
    "section": "Other quirks",
    "text": "Other quirks\nThe UBC area highlight the issues around enumerating private vs collective dwellings, usual and temporary residence, and the implications it has for the people living in these arrangements and how they are represented in the upcoming release of the census profile data.\nTake for example dissemination block 59150946020 hosting the Thunderbird student residences, where the 2021 census lists 1 dwelling unit with one household, and a total population of 635 people. Or dissemmination block 59154100003 encompassing Cecile Green Residences as well as the residence of the President of UBC. The 2021 census lists two private dwelling units, one private household and 104 residents living there.\nAt first sight one might think these blocks are host to giant communes, but what’s likely going on with block 59154100003 is that the two dwelling units are within the presidents residence, with the president’s household and family members living there. But the vast majority of the population is likely the subset of residents of Cecile Green that claim Cecile Green as their usual residence. Similarly for block 59150946020, where a residence might be an in-house caretaker. None of this counts residents of Cecile Green or Thunderbird that are returning back to their parent’s place over the summer, these are counted at their parent’s residence.\nMoreover, the people living at Cecile Green or Thunderbird, while counted in this area, aren’t considered as living in private households and therefore will be excluded from the other demographic data that will be released later this year, with the exception of the age group breakdown. We won’t learn about their ethnicity, immigrant status, languages, income or other census variables from the census profiles.\nThis can make quite a difference, in the 2016 census reported 15,890 people living in the Point Grey Peninsula as their usual residence, 13,360 of which were living in private households and the remaining 2,530 are not included in any of the census profile data except the age group breakdown. And the people with usual residence elsewhere, but who very much shape life in the area for much of the year, aren’t included at all. This adds an extra layer of complexity when interpreting census data for UBC or other areas with similarly complex enumeration challenges.\nWe can get even more extreme with block 59154083011, listing zero dwelling units but 324 people living there. What’s interesting is that there is actually no housing at all in this block, although there is student housing on neighbouring blocks.\nWhich leads us to another problem: geocoding."
  },
  {
    "objectID": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#geocoding",
    "href": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#geocoding",
    "title": "Census quirks; using UBC area as an example",
    "section": "Geocoding",
    "text": "Geocoding\nOther issues persist on UBC campus that may or may not be representative of broader issues with the census. block 59154083011 is just one example, but geocoding issues exist throughout. Be it block 59150863012 listing 15 people living in 9 households out of a total of 15 dwelling units on the beach at the north end of English Bay. Or more personally my condo building on campus, which since the 2016 census has it’s own census block. However, the census lists less than half the number of dwelling units in my building. And this is despite sending an enumerator to the building in 2021.\nThese kind of quirks exist throughout, for example the 679 people that the census had living in the median of the Lougheed Highway in Mitt Meadows in 2016, and moved them out by the 2021 census, presumably because their geocoding improved. While it is impossible to eliminate all problems like this it is somewhat disappointing that the census is still quite sloppy when it comes to geocoding. In BC we have the excellent BC Address Geocoder which, together with BC Assessment roll data, could greatly enhance the accuracy of census data in BC. And this is already in the hands of StatCan via the Canadian Housing Statistics Program."
  },
  {
    "objectID": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#upshot",
    "href": "posts/2022-02-17-census-quirks-using-ubc-area-as-an-example/index.html#upshot",
    "title": "Census quirks; using UBC area as an example",
    "section": "Upshot",
    "text": "Upshot\nCensus data is great, but census data also has lots of quirks and things that can trip people up. When things look funny in a particular area it’s always a good idea to consider who is counted and who isn’t. Using CensusMapper makes it east to zoom into block level and read off the population, dwelling and household counts to better understand where the higher geography totals come from. Satellite images and street view can provide more context, especially where coverage is good and we have to ability to step backward through time and compare with previous censuses to understand how areas have changed. Deeper on-the-ground knowledge is very helpful to add additional context, and sometimes is essential for resolving issues.\nAs usual, the code for this post, including the code to scrape the data out of the PDFs, is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-02-18 16:47:56 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [d9602a5] 2022-02-18: census quirks\n## R version 4.1.2 (2021-11-01)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.2.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.3 sf_1.0-6                 \n##  [3] cancensus_0.5.0           tongfen_0.3.5            \n##  [5] forcats_0.5.1             stringr_1.4.0            \n##  [7] dplyr_1.0.8               purrr_0.3.4              \n##  [9] readr_2.1.1               tidyr_1.2.0              \n## [11] tibble_3.1.6              ggplot2_3.3.5            \n## [13] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8         lubridate_1.8.0    class_7.3-19       assertthat_0.2.1  \n##  [5] digest_0.6.29      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  \n##  [9] backports_1.4.0    reprex_2.0.1       evaluate_0.14      e1071_1.7-9       \n## [13] httr_1.4.2         blogdown_1.6       pillar_1.6.4       rlang_1.0.1       \n## [17] readxl_1.3.1       rstudioapi_0.13    jquerylib_0.1.4    rmarkdown_2.11    \n## [21] munsell_0.5.0      proxy_0.4-26       broom_0.7.12       compiler_4.1.2    \n## [25] modelr_0.1.8       xfun_0.28          pkgconfig_2.0.3    htmltools_0.5.2   \n## [29] tidyselect_1.1.1   bookdown_0.24      fansi_1.0.2        crayon_1.4.2      \n## [33] tzdb_0.2.0         dbplyr_2.1.1       withr_2.4.3        grid_4.1.2        \n## [37] jsonlite_1.7.3     gtable_0.3.0       lifecycle_1.0.1    DBI_1.1.2         \n## [41] git2r_0.29.0       magrittr_2.0.1     units_0.7-2        scales_1.1.1      \n## [45] KernSmooth_2.23-20 cli_3.1.0          stringi_1.7.6      fs_1.5.1          \n## [49] xml2_1.3.3         bslib_0.3.1        ellipsis_0.3.2     generics_0.1.1    \n## [53] vctrs_0.3.8        tools_4.1.2        glue_1.6.1         hms_1.1.1         \n## [57] fastmap_1.1.0      yaml_2.2.1         colorspace_2.0-2   classInt_0.4-3    \n## [61] rvest_1.0.2        knitr_1.36         haven_2.4.3        sass_0.4.0"
  },
  {
    "objectID": "posts/2022-03-29-ubcm-shenanigans/index.html",
    "href": "posts/2022-03-29-ubcm-shenanigans/index.html",
    "title": "UBCM Shenanigans",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nThe Federal, Provincial, and Municipal governance structure in Canada creates a fun pattern whereby all governments are happy to take credit for good things that happen, but when bad things happen, each tends to point the finger at the others. So we get the spider-Man meme.\nWhen it comes to Canada’s housing crisis, pointing Spider-Men are all too common. But sometimes one level of government really is to blame. Recently the Province of BC and Federal government of Canada issued a joint Expert Panel report on the links between housing supply and affordability that placed a fair amount of blame directly on the door step of municipalities and their exclusionary land use controls. As housing researchers engaged in studying municipal zoning powers, we agreed with the expert panel, and provided them with our research pointing in the same direction (Summarized in Expert Report appendices 1, 2, & 3). After repeated difficulties moving provincially supported affordable housing projects through municipal approval processes, BC’s Minister of Housing (and Attorney General) David Eby has also indicated a willingness to go beyond pointing a finger at municipalities. He’s strongly suggested he’ll be offering legislation to reform and limit their powers to exclude housing, potentially following recent examples from New Zealand and California.\nWhat did the Union of BC Municipalities offer by way of a response? Well, first they seemed to suggest there wasn’t a problem with supply at all. If there was a problem, it wasn’t their fault. Then they pretty much fell back on finger-pointing at every plausible villain for Canada’s housing crisis that wasn’t a municipality. In this, the recent UBCM Report titled Building BC: Housing Completions & Population Growth 2016-2021 demonstrates that BC municipalities are a) failing to recognize the role of supply in BC’s housing crisis, and b) failing to recognize their own responsibility for the crisis."
  },
  {
    "objectID": "posts/2022-03-29-ubcm-shenanigans/index.html#failing-to-recognize-the-crisis",
    "href": "posts/2022-03-29-ubcm-shenanigans/index.html#failing-to-recognize-the-crisis",
    "title": "UBCM Shenanigans",
    "section": "Failing to Recognize the Crisis",
    "text": "Failing to Recognize the Crisis\nLet’s start with UBCM’s failure to recognize the problem. UBCM suggests that “housing supply […] has kept pace with population growth in British Columbia”, and erroneously concludes that this implies that housing is “not in short supply”. This posits the wrong relationship (ignoring how housing effectively sets a cap on population growth) at the wrong level of analysis (provincial as opposed to municipal/metro area) while ignoring the massive amount of counter-evidence (enormous price jumps, climbing rents, low inventories and rental vacancy rates, and administrative data documenting very few empty homes, homeless counts) as well as the basic but pretty robust theory of how supply and demand work together within a market context.\n\nWrong Relationship\nIn short, overly simplified form: population growth does not equal the sum of demand. Population growth effectively represents only the portion of demand met by supply at current prices. And even there, population growth remains imperfect as a proxy, combining demographic processes with household formation set to mostly match housing.\nIn other words, housing effectively sets a cap on household growth. Household growth in turn sets a rough cap on population growth (Why? Because relatively few families double-up to share a single dwelling these days, in part because municipalities make it illegal to do so). We’ve covered this ground before many times and also in our appendix two to the expert report. All of this is before we even get to the well-known problems of how the Census a) undercounts people without any housing at all despite their best efforts, which include b) overcounting dwellings to reach out to anywhere people might possibly be staying.\nUltimately demand is a function of price. And if you think prices are too high, you can’t claim that supply meets demand. Just ask those people who have gotten priced out of the region; or those still here but doubling up, stuck in roommate situations; or living with parents for longer than they would like if they feel that their demand for housing has been met.\n\n\nWrong Level of Analysis\nThe UBCM Report mostly provides analysis and figures at the provincial level, which is the wrong level of analysis when attempting to assess how housing markets are working. They provide one figure on “BC Major Census Metropolitan Areas.” CMAs more closely approximate housing markets, but weirdly Squamish and Prince George have also been added, despite neither being Major CMAs - or even CMAs at all. They’re too small to qualify as Metropolitan, so the Census categorizes them as Census Agglomerations. We’re going to return to this weirdness in a different post because the Squamish case in particular is pretty interesting and does not actually seem to support the narrative UBCM is pushing here. But for now we’ll just note that the inclusion of these Census Agglomerations in the figure is indicative of the lush Cherry Blossom season into which this report has been released. Why? Because this is cherry-picking.\n\n\nIgnoring Counter-Evidence\nWe have enormous counter-evidence of a housing shortage linked to the failure of supply to keep up with demand. This comes in the form of low inventories for sale and low rental vacancy rates which are robustly linked to price and rent increases. Here’s just the most recent data on the robust relationship between vacancy rate and rent change.\n We also know that adding new units leads to vacancy chains that quickly open up other, older housing units across far corners of the housing market. See, for instance, these recent studies in Finland and the USA. Furthermore, we know that there are very few empty dwellings in high demand parts of BC based on results from BC’s very own Speculation and Vacancy Tax. The UBCM report doesn’t even attempt to take seriously all the counter-evidence to its attempts to claim that supply isn’t a problem in BC. Instead it quickly shuffles on to the next failure."
  },
  {
    "objectID": "posts/2022-03-29-ubcm-shenanigans/index.html#failing-to-recognize-their-own-role",
    "href": "posts/2022-03-29-ubcm-shenanigans/index.html#failing-to-recognize-their-own-role",
    "title": "UBCM Shenanigans",
    "section": "Failing to Recognize Their Own Role",
    "text": "Failing to Recognize Their Own Role\nIn addition to failing to recognize the crisis, the UBCM also fails to recognize their own role in making it. In particular, they argue that lots of new housing has been approved and constructed in recent years under municipal watch. The report looks at housing completions in British Columbia for the years 2000-2021, and notices that “more housing has been built in British Columbia over the past three years than in any three years in the past 20.” It is not clear what we are supposed to learn from this, there is no effort being made to relate this to the demand for housing nor to justify the time period selected.\nLet’s choose one of many possible links to demand, normalizing new housing completions by population. After all, most new housing is occupied by local residents, who might be understood as steadily generating demand in accordance with population size. Let’s also extend our time scale back as far as accessible data will allow. How do recent completions per capita stack up?\n\nViewed this way the recent completions are barely above the long term mean. We can also see why 2000 might’ve been chosen by UBCM as a starting reference point. It’s when per capita completions hit at an all-time low across the province. Taking the longer view it looks like BC has been consistently under-building since the early 1990s, leaving behind quite a large hole in our housing supply.\nAside from cherry-picking in presentation and time period, there is also no effort in analysis to account for demolitions or CMHC methods changes (as occurred in 2013). Also once again, we’re looking at the province instead of municipalities. Housing just doesn’t work that way. If you are struggling to find housing in Metro Vancouver because that’s where jobs and opportunities are or maybe where you have strong connections with friends and family, it is entirely unreasonable to point to province wide housing stats and ask that you relocate to wherever housing may be available.\nMore directly to the point, as we noted in our appendix Three to the Expert Report, there is clear evidence that municipal zoning powers are both slowing and outright preventing the addition of new housing to municipalities. Municipalities play an enormous role in restraining housing. Exclusionary zoning is largely to blame for the hole in our municipal housing supply."
  },
  {
    "objectID": "posts/2022-03-29-ubcm-shenanigans/index.html#finger-pointing",
    "href": "posts/2022-03-29-ubcm-shenanigans/index.html#finger-pointing",
    "title": "UBCM Shenanigans",
    "section": "Finger-pointing",
    "text": "Finger-pointing\nAfter downplaying the lack of housing as a cause of BC’s housing crisis and failing to recognize their own role in creating it, the UBCM continues, somewhat remarkably, to blame it on everyone else. If there is a lack of housing, it’s the fault of provincial regulations and the development industry itself. If there’s unaffordability in the province, it’s the fault of an even wider cast of villains, ranging from the federal government to investors big and small to pre-sale flipping to short-term rentals to the financialization of the economy as a whole. We attempted to address many of the alternative explanations for BC’s affordability crisis we expected to be identified in our appendix one to the Expert Report. But we must admit, UBCM came up with a few new ones.\nWe also have to admit that not all of the villains are implausible. For instance, it really is the case that the Federal and Provincial governments have dropped the ball on Social Housing investments. But when they’ve picked it up again in recent years, they’ve often found resistance to siting social housing coming from municipalities. Similarly, if municipalities want to blame big financial actors like REITs for affordability issues, they also need to recognize that such actors are very clear in where they invest: places “with constrained supply/demand attributes.”\nOverall, it is certainly fair to note that housing is complex. Solving problems in one sector can create problems for others. At the same time, moving from housing scarcity to housing abundance is the first step to solving a lot of problems, and right now municipalities are all too often standing in the way. The UBCM report is evidence they probably won’t get out of the way in BC without a bit of stern nudging. That said, we should note as well that many municipal politicians also disagreed with the UBCM report. There are plenty of municipal politicians in BC who get it, even if the collective body purporting to speak for them does not."
  },
  {
    "objectID": "posts/2022-03-29-ubcm-shenanigans/index.html#upshot",
    "href": "posts/2022-03-29-ubcm-shenanigans/index.html#upshot",
    "title": "UBCM Shenanigans",
    "section": "Upshot",
    "text": "Upshot\nThe above provides a summary review of the UBCM report. It’s bad. But maybe it’s bad enough to do some good in furthering the case for provincial restraint on municipal powers to exclude much needed housing.\nThis post barely starts to look at specific points and arguments brought up by UBCM within the report, many of which are misleading at best and display a shockingly amateurish level of analysis. Take the claim about “exponential” (do words still have meanings?) increases in pre-sale assignments, where the UBCM exploits the left-censored nature of their data source, and rising condo construction activity in Langley, and misinterpret this as a “remarkable growth in flipping of pre-sale condos.”\nAnother example is the shallow comparison of population to dwelling growth in Squamish. We have looked into this kind of argument in detail before when it was made about the City of Vancouver supposedly building more than enough housing, and how one particular ingredient of these claims, the number of housing units not occupied by usual residents, can easily be misinterpreted and has frequently been misinterpreted in the past in a number of different ways. For good measure it might be worthwhile to repeat this for Squamish, which showed the largest discrepancy between population and dwelling growth in the UBCM report, and we delegate that to a separate blog post.\nIn the meantime, the amateurish level of analysis on display by UBCM jut underscores the problem. BC municipalities have been given enormous power over land use in the province. At the same time, they’re often under-resourced relative to the power they wield, so much so that they don’t recognize the damage they’re doing with their powers. This, of course, brings us back to Spider-Man: “With great power there must also come great responsibility.”\n\nAs usual, the code for this post is available on GitHub for anyone to replicate or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2022-04-05 16:31:26 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [cf74cf8] 2022-03-30: link to nathan's post\n## R version 4.1.2 (2021-11-01)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cansim_0.3.10   forcats_0.5.1   stringr_1.4.0   dplyr_1.0.8    \n##  [5] purrr_0.3.4     readr_2.1.2     tidyr_1.2.0     tibble_3.1.6   \n##  [9] ggplot2_3.3.5   tidyverse_1.3.1\n## \n## loaded via a namespace (and not attached):\n##  [1] tidyselect_1.1.2 xfun_0.30        bslib_0.3.1      haven_2.4.3     \n##  [5] colorspace_2.0-3 vctrs_0.3.8      generics_0.1.2   htmltools_0.5.2 \n##  [9] yaml_2.3.5       utf8_1.2.2       rlang_1.0.2      jquerylib_0.1.4 \n## [13] pillar_1.7.0     withr_2.5.0      glue_1.6.2       DBI_1.1.2       \n## [17] dbplyr_2.1.1     readxl_1.3.1     modelr_0.1.8     lifecycle_1.0.1 \n## [21] cellranger_1.1.0 munsell_0.5.0    blogdown_1.8     gtable_0.3.0    \n## [25] rvest_1.0.2      evaluate_0.15    knitr_1.38       tzdb_0.2.0      \n## [29] fastmap_1.1.0    fansi_1.0.3      Rcpp_1.0.8.3     broom_0.7.12    \n## [33] backports_1.4.1  scales_1.1.1     jsonlite_1.8.0   fs_1.5.2        \n## [37] hms_1.1.1        digest_0.6.29    stringi_1.7.6    bookdown_0.25   \n## [41] grid_4.1.2       cli_3.2.0        tools_4.1.2      magrittr_2.0.2  \n## [45] sass_0.4.1       crayon_1.5.1     pkgconfig_2.0.3  ellipsis_0.3.2  \n## [49] xml2_1.3.3       reprex_2.0.1     lubridate_1.8.0  assertthat_0.2.1\n## [53] rmarkdown_2.13   httr_1.4.2       rstudioapi_0.13  R6_2.5.1        \n## [57] git2r_0.30.1     compiler_4.1.2"
  },
  {
    "objectID": "posts/2022-04-26-planning-for-scarcity/index.html",
    "href": "posts/2022-04-26-planning-for-scarcity/index.html",
    "title": "Planning for scarcity",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nThis is the first in a series of posts where we will explore what’s gone wrong with planning for growth, how misguided planning and policy-making has exacerbated our housing shortage, and ways to start fixing things.\nThe second post in this series tries to estimate suppressed household formation."
  },
  {
    "objectID": "posts/2022-04-26-planning-for-scarcity/index.html#planning-vs-controlling",
    "href": "posts/2022-04-26-planning-for-scarcity/index.html#planning-vs-controlling",
    "title": "Planning for scarcity",
    "section": "Planning vs controlling",
    "text": "Planning vs controlling\nGrowth mostly happens along the intersection between markets and regulation. We are all for ramping up non-market housing, which is badly needed, but most housing creation and exchange in Canada occurs within market contexts. Yet housing markets are heavily structured by regulation. Municipal zoning and code, provincial land and tenancy, and federal mortgage, interest, and taxation policies all create the framework within which housing markets operate. We’re especially interested in local level planning and its relationship to growth. At the local level, we want to highlight two (idealized) regimes in which the nexus between markets and regulation can operate.\n\nThe planning regime, where zoning creates a broad framework for where and how builders can respond to market pressures. Zoning generally does not restrict how much growth happens, but guides it. A prominent example might be Tokyo.\nThe controlling regime, where zoning is tight and essentially all new growth has to be individually greenlit by planners or politicians, tightly controlling which development project will go forward and which won’t. A prominent example might be the City of Vancouver.\n\nWhile most cities fall somewhere in between these two poles, it is instructional to understand the implications of the two idealized regime types on how cities grow and the role of population projections.\nIn the planning regime population projections are important in planning for infrastructure and amenities, but not for shorter term (on the order of 5 years) residential development. The planning regime ensures that there is ample capacity to grow so that builders can react to shorter term fluctuations in demand (producing high supply elasticity).\nIn the controlling regime population projections become essential to guide short term residential development. There is little capacity for builders to react to demand (as expressed in low supply elasticity) without approval from planners and politicians, who in turn might outright deny a development proposal or impose onerous conditions on a project by project basis, impacting economic feasibility. As a result, new residential developments become justified based on “needs” as expressed through seemingly neutral population projections. The control exerted remains asymmetric, planners and politicians can only approve housing, but developers operating within market contexts decide whether it’s economically advantageous to build under a given set of conditions. To put the asymmetry slightly differently, the regime has many strings it can pull to stop development, but pushing on these strings remains ineffectual in producing development.\nThis asymmetry translates to an asymmetry of the impact of population projections. If population projections are too high and planners and politicians approve more housing than justified by demand, developers will likely elect simply to not build some of the approved housing. If population projections are too low and underestimate demand, planners and politicians will likely approve too little housing and the shortage will drive up prices and rents.\nIn conjunction this functions like a ratchet where ever-increasing scarcity drives up prices and rents. And this ratchet is further accelerated by naive and faulty population projection models employed by planners. This first post will take a closer look at what goes wrong with the population and housing projections broadly employed by planners."
  },
  {
    "objectID": "posts/2022-04-26-planning-for-scarcity/index.html#population-and-housing-projections",
    "href": "posts/2022-04-26-planning-for-scarcity/index.html#population-and-housing-projections",
    "title": "Planning for scarcity",
    "section": "Population and housing projections",
    "text": "Population and housing projections\nWhen in the controlling regime, population and housing projections tend to serve as self-fulfilling prophecies. In this section we will explore how planners generally do population and housing projections, and how this process tends to amplify housing scarcity.\nAs an example we take the Metro Vancouver Population and Housing projections, which implicitly builds on the following model.\n\nIt starts from projections of births, deaths and migration, which are simply derived from past trends applied to current age structures and adjusted for changes to federal immigration targets, which then directly translate to population projections. These then are used to derive jobs projections and, after being processing through an age-specific household maintainer rates model (also informed by past trends), translate into household estimates mislabeled as dwelling estimates in the Metro Vancouver projections.\nAt first sight this looks scientific and value-free, but the assumptions of the model, in particular the migration and household maintainer assumptions, encode the value that past trends were fine and should and will continue. It’s worth challenging this encoding of values for places like Vancouver, where housing crisis has been tossed around for decades. In particular, we might consider housing outcomes over the past 10 or 20 years, witnessing persistently low vacancy rates and prices ratcheting dramatically upward, as evidence of a problem. Feeding these trends back into projections bakes in our existing problems and amplifies them into the future. In other words, planners are making things worse through the choice of model they are using.\nTo better understand how exactly this works we need to move away from the framework of projections and flow diagrams that show how we compute the estimates and toward a causal framework that tries to capture how the different components in the model interact. This is quite ambitious, the causal processes at work are complex and hard to measure. Here we will focus on what we see as the most important mechanisms.\n\nWe added a bunch of factors and causal pathways. Most importantly, dwelling growth impacts both (net) migration and household maintainer rates. Demographers understand this when they talk about the “two-sided relationship” between population and dwelling growth. Similarly, this is what economists mean when they say population and dwelling growth are endogenous.\nOther factors are important too. Jobs don’t just grow in response to population growth (when e.g. the need for hairdressers or retail workers scales with population), jobs can also drive population growth (for example a large tech firm opening a jobs cluster or a multi-national company opening a side office). Similarly, boosts to income and wealth have implications for both household maintainer rates and dwelling allocation. In short, rich people consume more housing. Household maintainer rates are also tied to a variety of family processes. Here we add an important feedback to childbearing (reflecting that most childbearing takes place after adults have left their parents’ households, but also after they’ve partnered). We added in amenities as a largely exogenous factor, with Vancouver’s weather and geographic beauty acting as a net pull on migration. Together with other dynamics, this makes Vancouver an attractive place for companies to locate.\nThere are other pathways, but these are the ones we think are most realistic and important to consider in this context. Of note, they also make the values assumption explicit. Projecting past migration trends forward means we think past trends that might have pushed people out of the region, or prevented them from moving here because of a lack of available homes, were fine and these processes should be allowed to continue. Similarly, projecting past trends in household maintainer rates forward means that we think any suppressed household formation, e.g. people doubling up or forced to remain in their parents’ basements in response to the unavailability of housing, is a-ok. Carry on!\nWhile we feel our model offers a more realistic guide to how housing relates to population, the causal pathways we’re left with are complex and circular. How can we resolve this and move forward with a model that can help us guide our expectations about the future?\nOne key observation is that there’s only one prominent policy lever at the local level. Municipalities can’t directly impact migration, or other important aspects of the model. They can tinker with amenities and attempt to draw in new business investment, but these efforts probably aren’t too important. But there’s one set of powers municipalities have been granted in abundance, and that’s control over the addition of new dwellings.\n\nWhich brings us to the final version of our graph, where we add in municipal approvals. And with it we gain a replacement for the Metro Vancouver population projections. We need to do scenario-based modelling, starting from a range of scenarios of municipal housing approvals and their economic feasibility. Of note, adding in estimates of feasibility is just what New Zealand has recently attempted in its reforms to move away from its current control regime and back toward a planning regime. In effect, we can vary the amount, type (market, non-market, rental, ownership, etc.), and location of housing approvals, and then work through the model to estimate the impacts on population, migration, prices and rents, household maintainer rates, jobs and the other factors to paint a picture of the resulting scenarios of a future Vancouver. These estimates will come with uncertainties, but they will roughly show how the demographics change, who will get pushed out and who gets to stay in Vancouver under the given scenarios.\nThe question which scenario should be adopted is a political question, ultimately reflecting our values. As such, demographic and economic modelling should be telling us what the likely outcomes of various housing approvals might look like rather than feeding us a stale number reflecting a dispassionate assessment of need. Which is the future Vancouver we want? How inclusive will it be? How do household maintainer rates and migration patterns differ between a scenario that emphasizes the preservation of low-density neighbourhoods with some towers going up in various pockets vs a scenario that opens up our low density areas to mutliplexes and lowrise rental apartments? Who gets pushed out? Who gets to stay? And which of these scenarios aligns best with values for what the Vancouver of the future should look like?\nThis is the first in a series of posts, the next one will work through the household maintainer model and try to understand how Vancouver’s past choices have impacted Vancouver’s current household maintainer rates."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html",
    "href": "posts/2022-05-11-children-are-good-actually/index.html",
    "title": "Children are good, actually",
    "section": "",
    "text": "There are many useful metrics to understand neighbourhood change, change in the income distribution, change in the share of population in low income and change in dwelling units, change in households who rent, or just overall population change and how that relates to zoning. All these tell us something about how neighbourhoods change, the metric we want to focus on in this post is the number of children under 15.\nChildren under 15 years of age tell us something about now demographic renewal works in neighbourhoods, about demographic sustainability. Some neighbourhoods have more children than others for a variety of reasons, ranging from preference to built form of housing. If a neighbourhood is dominated by 1-bedroom dwelling units there will be few children living there. What we are interested in in this post is how this changes over time, which neighbourhoods are gaining children and which ones are losing children? And why?\nThere are various processes that impact how the number of children in an area changes over time. Children get born to (or adopted by) families living in an area, and families with children move in and out of neighbourhoods. We will look at these two processes separately in another post, in this one we focus on the “renewal” aspect and the change in the number of children under 15 in each region, which combines these two effects. But before that we remind ourselves just how mobile residents of cities are."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html#residential-mobility",
    "href": "posts/2022-05-11-children-are-good-actually/index.html#residential-mobility",
    "title": "Children are good, actually",
    "section": "Residential mobility",
    "text": "Residential mobility\nA little under half of the residents (5 years or older) of the City of Vancouver lived at a different address five year prior, making residential mobility is a significant driver of neighbourhood change. It’s instructional to take a quick look what residential mobility looks like across the City of Vancouver.\n\nThere is significant variation in overall residential mobility, with the overall share of people that moved in the past five years ranging from just under 30% to over 70%, depending on the neighbourhood.\nThe difference in mobility has many reasons. Some are demographic, for example young people move more frequently than older people, so areas that attract a younger demographic will see higher rates of movers. There are also physical factors at play, for example when larger apartment buildings get built everyone in the new building will have lived in a different address five years prior.\nBut even the people that stay fixed at the same address between census periods change, they get older. These two processes, residential mobility and aging, and how they interplay with housing, determine how the age profile of a neighbourhood changes between census periods."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html#children-as-an-indicator-of-demographic-renewal-and-sustainability",
    "href": "posts/2022-05-11-children-are-good-actually/index.html#children-as-an-indicator-of-demographic-renewal-and-sustainability",
    "title": "Children are good, actually",
    "section": "Children as an indicator of demographic renewal and sustainability",
    "text": "Children as an indicator of demographic renewal and sustainability\nOverall population change shows us which areas are growing or declining in overall population, focusing in just on children separates out growth that is driven by other age groups, for example by growth in older populations. As people are living longer and the key demographic cohort of baby boomers is entering retirement age, children often get squeezed out. This process happens over the course of several census periods, we will look at the 20 year timeframe from 2001 to 2021.\n\nFor technical reasons we need to include Musqueam 2 into this because they did not have a separate census tract in 2001 and got lumped in with the neighbouring City of Vancouver census tract, and for good measure we add in the Point Grey Peninsula to the west that’s home to UBC, UEL and the UNA.\nThis shows that our city has been quite divided, with some areas gaining a lot of children, and others losing children.\nWe can split this up in several ways, first taking a look at the inter-census periods separately.\n\nThat helps us see which areas show chronic census over census declines in the number of children, which ones saw sustained increases, and which were wavering.\nAnother way to slice this is to look at each 5-year age group separately.\n\nWhile there is a lot of similarity in growth across these age groups, it’s noticeable how growth in 10-14 year olds is more geographically constrained."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html#children-are-good-actually",
    "href": "posts/2022-05-11-children-are-good-actually/index.html#children-are-good-actually",
    "title": "Children are good, actually",
    "section": "Children are good, actually!",
    "text": "Children are good, actually!\nWhen it comes to neighbourhood change we often ask if change is good or bad. And more often than not that’s ambiguous. A changing physical housing stock is perceived as bad by people attached to the existing physical form and as good by people looking for housing options that the existing stock does not offer. Overall population (and housing) increase is sometimes viewed as good, sometimes as bad, and at other times conditioned on who moves into new housing, with some people questioning who new housing is for, signalling a desire to control who moves into new housing. Increase in overall income in a neighbourhood is sometimes seen as a positive sign of growth, sometimes negatively as a sign of gentrification.\nBut children take a somewhat unique position in that they are almost universally seen as good, and a decline in children is seen as bad."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html#what-causes-change-in-children",
    "href": "posts/2022-05-11-children-are-good-actually/index.html#what-causes-change-in-children",
    "title": "Children are good, actually",
    "section": "What causes change in children?",
    "text": "What causes change in children?\nThis leads to the question what kind of policies are impacting whether children row or decline in a neighbourhood. Looking at the images above an immediate candidate comes to mind. Housing. A key observation is that there are several processes that, assuming a the housing stock remains fixed, combine to work at reducing the number of children\n\nAs Canada’s population ages and the age distribution shifts a lower share of homes will be available for young families. We get more one or two person households in single family homes now that we used to, and people may choose not rent out secondary suites they may have if they don’t need the income.\nVancouver makes up the central part of the metro region, and central areas generally attract a lot of young (childless) professionals aged 25 to 34s. And with the outer parts of the region growing faster than the City of Vancouver (by design, that’s what the Metro Vancouver Growth Strategy is calling for) the City is becoming increasingly more “central” and not just the number but also the share of these young professionals has been growing. Which again leaves less space for young families in their late 30s and 40s, which is amplified by overall demographic shifts in the region and Canada overall.\nBC has seen declining fertility rates of this time period and now has the lowest fertility rate in Canada, which is likely not independent of the other processes noted here.\nAt the dame time, as people get richer they tend to consume more housing. Richer people are less likely to rent out a secondary suite (assuming they have one), and they are less likely to down-size or right-size their housing (having extra bedrooms is nice).\n\nThe first two demographic processes are demographic and we van visualize their effect by comparing the 2021 age pyramid of the City of Vancouver against their 2001 shadow, contrasted by Metro Vancouver, British Columbia and all of Canada.\n\nHere we can observe several demographic trends combining. At the overall country level we can observe that Canada now has fewer people in their 40s than it did 20 years ago, this is part of the Gen-X demographic dip of people who were in their 20s in 2001, where can can identify the same dip (wedged between the Baby and Echo-Boomers). That Gen-X cohort did get boosted by immigration, but not enough to make up for the large number of boomers that have aged out of that age group between 2001 and 2021. We see very similar effects play out in British Columbia.\nAt the Metro Vancouver level in-migration was strong enough so that all age groups have gained, but the Metro region was growing faster (by design) than the City of Vancouver where that Gen-X cohort (and their children) got squeezed out. At the same time we see that the 25 to 34 year old age group has gained in prominence in the City of Vancouver, speaking to the big in-migration draw of the city for young professionals.\nOn net, for the City of Vancouver we have strong increases for population around 30, as well as the population 50 and above, squeezing out people in their 40s and children.\nThe third demographic process does not just touch Vancouver, but also shows up in province-wide numbers.\n\nIn the early 90s BC’s fertility rate was close to the Canadian average. After an initial dip the Canadian fertility rate recovered again around 2008, but at that point BC’s rate had already dropped significantly and showed a much smaller recovery. After 2008 the rates further declined, a trend that is driven by BC’s large CMAs with Victoria and Vancouver having a total fertility rate of 0.95 and 1.09, respectively, which is due to a range of demographic factors, but likely also at least partially impacted by the strong evidence of delayed household formation due to unavailability of housing in Vancouver.\nThese demographic processes combine with, and are partially caused by, the economic process to squeeze out families. But there is a way out: Add housing. If we add housing, we can make up for the demographic and economic shifts and provide space for families, and their children.\nAt least in theory. To investigate this in more detail we can look at the change in the number of children at a finer geographic resolution.\n\nThis shows us at a more granular level where children have grown and where they haven’t, and how that might relate to housing. Olympic Village and Yaletown light up, both areas where we have added housing. Similarly the River District, properties along the Cambie Corridor, Joyce Collingwood. And the little triangle at Arbutus and 33rd that got rezoned around 2003 pops out, as well as some other spot developments across the city. We also see some change along arterials where we have added housing above commercial spaces. And of course the UBC area, where the UNA saw an explosion in housing over this period."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html#housing",
    "href": "posts/2022-05-11-children-are-good-actually/index.html#housing",
    "title": "Children are good, actually",
    "section": "Housing",
    "text": "Housing\nWhich brings us to the question of where we added housing in this timeframe. We can approach this from two angles, one is looking for housing built after 2001, and the other is to look at zoning changes since 2001. We will go both routes, each has it’s own advantages and disadvantages. Looking at housing being built since 2001 we will catch a lot of 1:1 replacements of single-detached houses. Looking at zoning we just capture where housing has been allowed to go, not where it actually went. Except that in Vancouver zoning is tight, essentially no multi-family housing (the only way we add housing in appreciable ways) gets built without a rezoning or at least conditional approval by the director of planning. And the areas that conditionally allow adding multi-family housing are also quite limited.\nFor this part of the analysis we will focus on the City of Vancouver and disregard Musqueam 2 and the UBC area as we don’t have historical zoning or (openly available) building age data for those regions.\n\nZoning\nLet’s start with zoning. We have already looked at how zoning relates to overall population growth in Vancouver, but now we will refine this looking more specifically at change in zoning and how that relates to children. The idea is that it’s not so much the current zoning that determines how cities change, but if the zoning allows adding housing. Which in Vancouver mostly boils down to looking at where zoning has changed.\nDrawing on our Metro Vancouver Zoning Project, in particular the historic zoning of the City of Vancouver we show all areas that have been rezoned to allow more housing since 2001. We also scraped the CD district schedule to include CD districts that have been amended since 2001, as for example has been the case with the “triangle” at Arbutus and 33rd to enable denser housing.\n\nIf this map looks rather scarce, then that’s because it is. The only actual rezoning that is not a spot-zoning in this time period is the RM-7 at Renfrew-Collingwood, but looking at the change in children map it does not seem to have resulted in a net increase in children. We will revisit this when looking at buildings by age."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html#building-age",
    "href": "posts/2022-05-11-children-are-good-actually/index.html#building-age",
    "title": "Children are good, actually",
    "section": "Building age",
    "text": "Building age\nAnother way to look at this is by building age, and our interactive map of Vancouver properties allows us to specify dynamic age cutoffs to investigate this. However, most new buildings are unproductive 1:1 replacements of single family homes, so it’s more useful to look what happens when we filter them out. Our land use data on these online maps are a little bit outdated, so we will make some fresh maps here.\nHowever, even the newer Metro Vancouver Land Use data is already out of date and might miss some recent rezonings, so we also fold in the zoning data to make sure we can capture recent rezonings. This process would be a lot easier if we could just use the more detailed BC Assessment Roll Data, but unfortunately our province still believes that provincial housing data should not be openly available but be sold for revenue generation or available under strict NDAs, which is not worth the trouble for a blog post like this.\n\nThis gives a much more pinpointed view as to which property parcels saw new multi-family development. They don’t match up perfectly with rezonings, but the correspondence is quite good. The addition of new multi-family housing in C-2 zones, as well as Renfrew-Collingwood’s RM-7, is quite sporadic though, as one would expect when comparing the effects of rezonings to the conditional zoning under C-2 and RM-7.\nVisual inspection suggests that the change in children map lines up quite well with the change in zoning or the new multi-family building maps. Time to quantify this a little more. As a first step we can take the zoning map and new multi-family buildings map to label each census area in the change of children map by whether parts of it saw a zoning change or new multi-family developments. This process labels the dissemination areas in Vancouver as follows.\n\nComparing to the previous to maps we note that the areas don’t resemble the zoning changes or new developments in great detail, they are much larger. But those are the regions we are dealt by the census, and they can’t be changed without going through the work of pulling custom tabulations. But this should be good enough to give us a rough idea, we can use these labels to group the regions and count up the change in children in each group.\n\nThis suggests that, using our categories, Vancouver adds children primarily via rezonings, and while conditional C-2 zoning may generate new multi-family housing, it does not seem to add children. This is worth investigating a little further, there is the same data as boxplot normalized by area of each dissemination area.\n\nHere C-2 comes out much more ambiguous, with some area containing C-2 zoning gaining children and some dropping children. Typically C-2 areas will only make up small parts of dissemination blocks, with the surrounding areas adding on their own loss or gain of children, which is probably closer to what we see in C-2 areas that have not seen new multi-family development, which on average amounted to a loss in children.\nThe census also has information on the number of households in each region, so we can compare the change in households to the change in children, coloured by the presence of zoning change (or new multi-family housing) in each region.\n\nThis again shows that children generally scale with the number of households. Fitting a linear model that also includes the starting number of children in each region we get that by 2021 each region roughly lost 21% of their 2001 children, offset by gaining 0.42 per net new household gained 2001-2021, explaining about 57% of the variation."
  },
  {
    "objectID": "posts/2022-05-11-children-are-good-actually/index.html#upshot",
    "href": "posts/2022-05-11-children-are-good-actually/index.html#upshot",
    "title": "Children are good, actually",
    "section": "Upshot",
    "text": "Upshot\nCities are changing. No matter if their physical form changes or not. City councillors and city planners direct how change in the city happens. There are many competing priorities of how we want our city to change, in this post we are singularly focused on understanding change in terms of the population of children under 15 years old.\nTaking this as a metric, council and planners have steered most of the city into decline and funnelled all growth in children into small areas where they allowed new multi-family development that adds enough housing to counter-act the decline elsewhere in the city.\nCity-wide the decline has been small with children under 15 dropping 99%, but the geographic disparities had ripple effects throughout the city with school enrolment dropping in declining neighbourhoods contrasted by overfull schools and pressure on amenities in the growing parts. This trend and the mechanisms behind it have been clear for a long time now, yet council and planners have actively designed Vancouver to grow slower than the overall region, making the city increasingly exclusive, with the predictable consequence of squeezing out families with children.\nOver the years councils have introduced several measures aimed to counter-act these trends. The legalization of secondary suites, the introduction of laneway houses and legalization of duplex fall into this category. But these efforts were too timid to halt the decline of children in the low-density neighbourhoods, with council making compromises like forcing laneway homes to be unnecessarily small, and reducing the overall floor space a duplex can achieve in order to limit uptake to appease voices that were more concerned about “neighbourhood character” than about declining children.\nOther measures include mandated minimum shares of two and three bedroom units in multi-family developments, which led to an increase in children in areas where new multi-family housing was allowed. And at the same time produced the geographic disparities that created challenges for many families in the city.\nJust to be clear, there is nothing special about the the development being multi-family vs single family in order to add children, other than multi-family developments are the only way Vancouver has added significant amounts of housing. Adding single-family housing would also work, except we haven’t, on net, done that in significant ways. Focusing on adding multi-family housing concentrated in a few select areas of the city, as Vancouver is done, is of course a choice. A different option would be to reduce minimum lot and frontage requirements and allow higher FSR to densify single-family areas with denser freehold housing, or with six-plexes as has been proposed by the mayor, maybe sprinkled with low-rise apartment buildings when the opportunity arises. But this would have to be done in a way that allows for faster change than what we have seen with suites, laneways and duplexes. And it remains to be seen if Vancouver is courageous enough to embrace families over their fear of change in low-density areas.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-05-11 18:09:59 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [29c1196] 2022-05-12: children are good\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cansim_0.3.11             patchwork_1.1.1          \n##  [3] sf_1.0-7                  cancensus_0.5.1          \n##  [5] tongfen_0.3.5             mountainmathHelpers_0.1.4\n##  [7] forcats_0.5.1             stringr_1.4.0            \n##  [9] dplyr_1.0.8               purrr_0.3.4              \n## [11] readr_2.1.2               tidyr_1.2.0              \n## [13] tibble_3.1.7              ggplot2_3.3.6            \n## [15] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] httr_1.4.2         sass_0.4.1         bit64_4.0.5        vroom_1.5.7       \n##  [5] jsonlite_1.8.0     modelr_0.1.8       bslib_0.3.1        assertthat_0.2.1  \n##  [9] cellranger_1.1.0   yaml_2.3.5         pillar_1.7.0       backports_1.4.1   \n## [13] glue_1.6.2         digest_0.6.29      rvest_1.0.2        colorspace_2.0-3  \n## [17] htmltools_0.5.2    pkgconfig_2.0.3    broom_0.8.0        haven_2.5.0       \n## [21] bookdown_0.26      scales_1.2.0       tzdb_0.3.0         git2r_0.30.1      \n## [25] proxy_0.4-26       generics_0.1.2     ellipsis_0.3.2     withr_2.5.0       \n## [29] cli_3.3.0          magrittr_2.0.3     crayon_1.5.1       readxl_1.4.0      \n## [33] evaluate_0.15      fs_1.5.2           fansi_1.0.3        xml2_1.3.3        \n## [37] class_7.3-20       geojsonsf_2.0.2    blogdown_1.9       tools_4.2.0       \n## [41] hms_1.1.1          lifecycle_1.0.1    munsell_0.5.0      reprex_2.0.1      \n## [45] compiler_4.2.0     jquerylib_0.1.4    e1071_1.7-9        rlang_1.0.2       \n## [49] classInt_0.4-3     units_0.8-0        grid_4.2.0         rstudioapi_0.13   \n## [53] rmarkdown_2.13     gtable_0.3.0       DBI_1.1.2          curl_4.3.2        \n## [57] R6_2.5.1           lubridate_1.8.0    knitr_1.38         fastmap_1.1.0     \n## [61] bit_4.0.4          utf8_1.2.2         KernSmooth_2.23-20 stringi_1.7.6     \n## [65] parallel_4.2.0     Rcpp_1.0.8.3       vctrs_0.4.1        dbplyr_2.1.1      \n## [69] tidyselect_1.1.2   xfun_0.30"
  },
  {
    "objectID": "posts/2022-05-21-nanaimo-station/index.html",
    "href": "posts/2022-05-21-nanaimo-station/index.html",
    "title": "Nanaimo Station",
    "section": "",
    "text": "With a new redevelopment proposal around Vancouver’s Nanaimo Skytrain station hitting the news, and a local journalist feigning ignorance about zoning around skytrain stations, maybe it’s time for a quick post on zoning and population growth around the Nanaimo Station.\nTo start out, let’s take a look at the zoning around Nanaimo Station.\nWe marked the Nanaimo Station at the centre and the 29th Avenue station to the south-east just outside of the 800m radius circle. The area is dominated by single family and duplex zoning, with a couple of half-blocks of low rise and some mixed-use low rise zoning along Kingsway and sprinkled across a couple of of other lots, as well as some Comprehensive Development parcels to round things off. The Skytrain corridor is visible as a the diagonal low-density (RS-zoned) line.\nThe Skytrain has been operational for almost 40 years now, let’s take a look how zoning has changed over time, utilizing the data from the Metro Vancouver Zoning Project where we have also assembled a timeline of zoning in the City of Vancouver, taking snapshots at several years. For this we will start with 1976, well before the Skytrain arrived, and continue with 1990, 5 years after the station opened, and several snapshots in the following years until now.\nWe see that not much has changed over the years. The industrial parcel right by the station got lost between 1976 and 1990 and some CD-1 parcels came in. Then things were mostly stable until a little RM-7 and RM-9 low-rise zoning, plus some more CD-1, appeared between 2001 and 2020. But almost all of the low-density yellow area remained untouched, despite the rapid transit line."
  },
  {
    "objectID": "posts/2022-05-21-nanaimo-station/index.html#population",
    "href": "posts/2022-05-21-nanaimo-station/index.html#population",
    "title": "Nanaimo Station",
    "section": "Population",
    "text": "Population\nTime to take a look at the impact on population and dwellings in the area. For this we turn to the census, but census regions don’t perfectly line up with our 800m station catchment, and census regions change over time. To get stable and robust estimates over time we will first harmonize our census regions to construct a common geography based on 2001 though 2021 Dissemination Blocks, which we have automated with our TongFen package, and then select the ones with majority overlap with our station buffer.\nHere is what this looks like, including the harmonized dissemination blocks with partial overlap.\n\nWe coloured the regions by their overlap percentage. To keep things simple we select the ones with majority (50% or more) overlap. Which gets us to the following “census catchment” region that we will use to track change in population and dwellings over time.\n\nArmed with that we can now check in on how the population and dwellings have changed in this census catchment area of the Nanaimo Station. For context we also add the corresponding changes for the city overall and the metro region.\n\nHere we observe familiar patterns.Metro Vancouver has been growing faster than the city, making the city more exclusive relative to the region, as dictated by regional planning. The 2001-2006 dwelling growth captures the change in census methods capturing the re-classification of suited detached homes into two “duplex” units, which is especially prominent in the Nanaimo Station region that is dominated by single family homes, and which contrasts the population growth. Regular readers of this blog will recognize this change in census methods as the issue still getting exploited by anti-housers who misrepresent this as dwelling growth exceeding population growth and evidence of the absence of a shortage of housing.\nGenerally we see that the population around the Nanaimo Station has been growing comparatively slowly, except for the 2016 to 2021 period. To better understand what has been going on there we will take a look at population and dwelling growth at the block level.\n\nHere we see the initial boost in dwellings in single family areas, as well as several separate blocks with dwelling boosts, one between 2006 and 2011 on the north side of Kingsway west of Nanaimo where a low-rise mixed use building got built, and another one across the street where a large new development (with the T&T) came between 2016 and 2021.\nThe new development proposal asks to turn 18 single family lots into 861 new homes for a net increase in roughly 835 additional homes to the area (depending on how many have suites), a 16% increase in the total number of homes in the area. Which would single-handedly be the larges increase in dwellings between any of the four inter-census periods of the past 20 years, enabling lots more people to make use of this transit investment."
  },
  {
    "objectID": "posts/2022-05-21-nanaimo-station/index.html#longer-timelines",
    "href": "posts/2022-05-21-nanaimo-station/index.html#longer-timelines",
    "title": "Nanaimo Station",
    "section": "Longer timelines",
    "text": "Longer timelines\nThe 2001 to 2021 timeline above is too short to cover the entire time since the Nanaimo station opened. Before 2001 StatCan did not release data at the dissemination block level, which is why we stopped there. Even dissemination areas did not exist before 2001, the finest geography was the discontinued enumeration areas, which don’t lend themselves well to TongFen with later data. But for Vancouver we do have a custom cross tabulation that codes data from all the censuses all the way back to 1971 to 2016 dissemination area geographies. This allows us to push this back to 1971, at the expense of basing this on dissemination area geographies (and using TongFen to push it forward to 2021). Let’s take a look at how good our geographic match with the Nanaimo Station catchment area is when using the coarser dissemination area geography.\n\nWe see how the areas we are matching now are larger, using the same criterion as before and only including areas with at least 50% overlap we arrive at the following geography that we have consistent data on.\n\nIt’s not as good a match as before, but now we have population data all the way back to 1971 and can track the population growth across this timeline.\n\nHere we converted to annual rate of change because we have periods of different lengths, the first period spans 10 years instead of 5. The Nanaimo Station area did not follow the overall drop in population that Vancouver experienced in the 70s, but since 1981 it grew slower than the city average, with the exception of the 1996-2001 period. We note that even in the final 2016 to 2021 period the growth was slower than that for the overall city, which is different from the result we got higher up where we used dissemination block based estimates, with the main difference being that our dissemination area based estimates exclude the new development on the south side of Kingsway (with the T&T).\nThe only growth period 1996-2001 is interesting, it is connected to the opening of the station 11 to 16 years prior, but given Vancouver’s pattern of extremely slow and unresponsive housing delivery this is certainly plausible. A closer look at the geographic breakdown of the inter-census population change might shed a light on this.\n\nLooking in particular at the 1996 to 2001 period there does indeed seem to be some growth near the station, but the largest population growth is further away from the station to the north and north-west.\nThe net effect over the entire time period remains disappointing, with the City of Vancouver population growth running significantly higher than the growth in the Nanaimo Station area."
  },
  {
    "objectID": "posts/2022-05-21-nanaimo-station/index.html#upshot",
    "href": "posts/2022-05-21-nanaimo-station/index.html#upshot",
    "title": "Nanaimo Station",
    "section": "Upshot",
    "text": "Upshot\nVancouver planners have been neglecting past transit investments, with the new Broadway Line coming in the province is not leaving things to chance and have directly intervened to ensure that Vancouver does not waste this massive infrastructure investment by keeping exclusionary zoning in place that keeps people away from billion dollar transit investments.\nHere is a more comprehensive look at Vancouver’s existing transit stations, ordered roughly by how wasteful the landuse is in the 800m around them. Nanaimo Station is unsurprisingly the worst offender, but there are quite a few others that are not far behind.\n\nI would not be surprised at all if we are going to see provincial intervention to force higher land use around these stations. There is a planing process underway for the Rupert and Renfrew Station Area, which is still deep in the engagement process. The Vancouver Plan might in theory be a vehicle for the city to act decisively first and remove the necessity for provincial intervention, but realistically speaking we are sill several years off before the Vancouver Plan and the subsequent area plans are passed and zoning is changed. The Broadway Plan will serve as a test to see if Vancouver is able to achieve higher land use around stations on their own. The plan has been in the works for four years now, the coming weeks will tell if council is serious about this, but even then, the Broadway Plan may well fall short of achieving its goals.\nAllowing more people to live near the Nanaimo Station, if council allows this proposal to go forward, will be a good start. But we really have to wonder why council after council have not touched the sea of yellow low-density areas around such a key infrastructure investment. And even this current proposal only touches a mere 18 single family lots.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-05-23 15:30:17 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [25f46e1] 2022-05-23: update with da level timeline back to 1971\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.3.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 cancensus_0.5.1          \n##  [3] tongfen_0.3.5             sf_1.0-7                 \n##  [5] forcats_0.5.1             stringr_1.4.0            \n##  [7] dplyr_1.0.8               purrr_0.3.4              \n##  [9] readr_2.1.2               tidyr_1.2.0              \n## [11] tibble_3.1.7              ggplot2_3.3.6            \n## [13] tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.8.3       lubridate_1.8.0    class_7.3-20       assertthat_0.2.1  \n##  [5] digest_0.6.29      utf8_1.2.2         R6_2.5.1           cellranger_1.1.0  \n##  [9] backports_1.4.1    reprex_2.0.1       evaluate_0.15      e1071_1.7-9       \n## [13] highr_0.9          httr_1.4.2         blogdown_1.9       pillar_1.7.0      \n## [17] rlang_1.0.2        readxl_1.4.0       rstudioapi_0.13    jquerylib_0.1.4   \n## [21] rmarkdown_2.13     munsell_0.5.0      proxy_0.4-26       broom_0.8.0       \n## [25] compiler_4.2.0     modelr_0.1.8       xfun_0.30          pkgconfig_2.0.3   \n## [29] htmltools_0.5.2    tidyselect_1.1.2   bookdown_0.26      codetools_0.2-18  \n## [33] fansi_1.0.3        crayon_1.5.1       tzdb_0.3.0         dbplyr_2.1.1      \n## [37] withr_2.5.0        grid_4.2.0         jsonlite_1.8.0     gtable_0.3.0      \n## [41] lifecycle_1.0.1    DBI_1.1.2          git2r_0.30.1       magrittr_2.0.3    \n## [45] units_0.8-0        scales_1.2.0       KernSmooth_2.23-20 cli_3.3.0         \n## [49] stringi_1.7.6      farver_2.1.0       fs_1.5.2           xml2_1.3.3        \n## [53] bslib_0.3.1        ellipsis_0.3.2     generics_0.1.2     vctrs_0.4.1       \n## [57] tools_4.2.0        glue_1.6.2         hms_1.1.1          fastmap_1.1.0     \n## [61] yaml_2.3.5         colorspace_2.0-3   classInt_0.4-3     rvest_1.0.2       \n## [65] knitr_1.38         haven_2.5.0        sass_0.4.1"
  },
  {
    "objectID": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html",
    "href": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html",
    "title": "Ins and outs of CMHC data",
    "section": "",
    "text": "This is a data-focused post, it’s targeted at people consuming or otherwise working with CMHC data. And also at the future me that will stumble again over problems outlined in this post and will have to remember the ins and outs of CMHC data."
  },
  {
    "objectID": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-data",
    "href": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-data",
    "title": "Ins and outs of CMHC data",
    "section": "The data",
    "text": "The data\nCMHC creates and maintains lots of important data on housing. We start with an overview over the more important data sources, and the relevant definitions.\nThe Starts and Completions Survey (Scss) tracks housing starts, completions, and related data. Data is available on a monthly frequency down to census tract geography and can be split by intended market. Part of this is the Market Absorption Survey that tries to track how many units remain unsold after completion, and at what prices completed units sell.\nThe definitions of the structural types differ from StatCan definitions in important ways, for example a single family home with a secondary suite will be categorized as two “duplex” units by StatCan, but as one “single” unit plus one “rental apartment” unit in the Scss.\nThe Rental market survey (Rms) aims to track privately initiated market rental apartments with at least 3 units by bedroom type, age of structure and number of units in structure. It also collects information about rents and vacancy rate. The data is available on an annual frequency (twice annually before 2016) down to census tract level geography.\nRms data often gets misrepresented as covering “rental apartments”, but it excludes a broad range of rental housing like non-profit or other non-market or public housing, seniors housing, student housing and secondary rentals.\nThe Secondary Market Rental Survey (Srms) aims to capture information on the secondary rental market, in particular condominium units but also other secondary rentals. Data quality is substantially lower than the primary rental market survey (Rms), but it adds important context and serves as a cross-check to make sure the conditions found via the Rms broadly carry over to the overall rental market. Data is available at an annual frequency down to the CMA level.\nThe Seniors’ Housing Survey targets senior’s residences."
  },
  {
    "objectID": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-cmhc-r-package",
    "href": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-cmhc-r-package",
    "title": "Ins and outs of CMHC data",
    "section": "The cmhc R package",
    "text": "The cmhc R package\n\nAll of this is important data that is relevant to many housing related questions, and we have been using this data extensively. CMHC does not have a data API, but our cmhc R package develops a pseudo API to facilitate easy programmatic access to the data and enables reproducible and adaptable analysis. Version 0.2.0 contains many breaking changes to the previous version that we have used in past work and blog posts, we have archived version 0.1.0 to maintain reproducibility, installing this package, now called “cmhc2” and loading this instead of the cmhc package will enable older work to continue to run through unchanged. These breaking changes hopefully won’t cause too many issues, but the old version was not very user friendly, and there were (likely) only a handful of people using it anyway. But starting with version 0.2.0 it should be a lot more broadly accessible.\nThere are also various other less organized data sources strewn over a selection of excel sheets and PDFs, including demolitions with older demoltions in PDFs, rents of vacant apartments and others."
  },
  {
    "objectID": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-gaps",
    "href": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-gaps",
    "title": "Ins and outs of CMHC data",
    "section": "The gaps",
    "text": "The gaps\nThere are important data gaps and categorization issues that need to get fixed. As housing becomes more complex, our traditional ways of accounting for, measuring, and tabulating housing need to evolve. Things have changed a bit within CMHC data reporting, but not enough to keep the data robust and maximize it’s usefulness.\n\nNon-market housing\nThere is no good data on non-market housing. The Scss does not distinguish market or non-market rentals, although it does break out co-op housing. The Rms does not cover non-market housing at all, some non-market housing is captured in the Seniors survey but it isn’t broken out.\nThere is the fairly new Social and Affordable Housing Survey that now has data for 2019 and 2021, although it only gets released as Excel sheets, only has provincial and CMA breakdowns, and is generally of low quality.\nNon-market housing is inherently complex and difficult to enumerate. There is a broad spectrum of types of non-market units, how far below market are rents set, what the eligibility criteria for renting are, and how wait lists are administered to name a few important questions.\nBut the lack of comprehensive decent-quality data on non-market housing makes it really hard to understand how regions are doing in meeting housing needs. There are some initiatives underway to catalogue non-market housing, but it is not clear to me that this will turn into a regularly updated and reliable data source.\n\n\nStudent housing\nStudent housing is not enumerated as part of the Rental Market Survey. In theory it should be part of the Scss if it has self-contained units, but in practice coverage is spotty. Here in BC we have seen a resurgence of student housing getting built, which helps take pressure of the overall rental market. But we don’t have good and accessible data sources on this.\n\n\nClassification problems\nStarting sometimes in 2013 CMHC realized that secondary suites are part of the housing spectrum, and CMHC Scss data started to count secondary suites. But how is a start of a house with suite classified in Scss data? It’s a start of a “Single” homeowner unit and a start of an “Apartment” rental unit. Similarly, laneway homes are classified as “Single” single-detached rental unit. We can pinpoint the change in how to account for suites by looking at the data. The idea is to look for apartment completions with just 1 unit in them. We don’t have individual structure data, that’s internal to CMHC. But if we don’t count suites we expect that there only very rarely be a census tract with just one rental apartment unit completions, rental apartments will have more units and only the occasional small stacked townhouse would slip through, maybe having staggered completion times. But if we do count suites, then there should be census tracts with just one suite completing in a given months.\n\nAnd indeed, the number of census tracts that see just one rental apartment units completing jumped in January 2013, this should be the date when CMHC data started counting suites in the Scss. Some census tracts may well have more than one suite completing in a given month, but here we are just focused on determining the timing of when the Scss methods changed. But this does pose an obstacle if we wanted to estimate how many rental completions are suites. CMHC has the ability to break out secondary suites in the Scss data, and they have done so in their recent Housing Supply Report. It would be nice if they could also break it out in their HMIP data.\nAlso, non-market rental housing, as well as senior housing, are classified as rental apartments in the Scss, but they aren’t part of the Rms universe, just like suites and laneways aren’t.\nLaneways can be accounted for fairly easily as “Single” starts or completions with intended market “rental”. But the not being able to break out suites, non-market, and senior housing creates mismatches with the Rms data.\nWe take a look how this works out during the past three years in Metro Vancouver municipalities. For this we are interested in the number of new rental units, so rather than looking at net change in the total number of units in the Rms rental universe we only look at the change 2018-2021 in the units built 2000 or later, assuming that none of these will have gotten torn down. We compare that to Apartment and Row completions with intended market “rental” from the Scss that completed July 2018 through June 2021, the time frame matching the target of the Rms. This removes laneway homes from the completions, but leaves secondary suites, non-market housing and senior housing.\n\nWe can see that the difference between the two is rather large. Vancouver’s Rms universe has about 3k new units, but the Scss records about 6k rental Row and Apartment completions. That’s a factor 2 difference, and it’s not out of the ordinary looking at other municipalities, in some the mismatch is larger, in some it is smaller.\nThe difficult question is what does it tell us? Is the difference mostly secondary suites, or non-market or senior housing? Or is this due to misclassification in mixed-tenure buildings, which CMHC has a hard time properly capturing in their database. Mixed-tenure buildings, where part of a building is operated as strata, and another part as a purpose-built rental, or part is market rental, part is non-market rental, have become increasingly prevalent over the recent years.\nThe answer to this question matters for policy makers. Secondary suites are a great way to better utilize existing housing, but secondary suites are a double-edges sword, they don’t offer the same legal protections to renters as purpose-built rental and are inherently insecure. Of all types of housing, suites are the most frequently classified as “unoccupied” in the census, and there is no regulation that could compel owners to rent them out like we have for e.g. condos in form of the Empty Homes Tax or Speculation and Vacancy Tax. Colloquially they are often referred to as “mortgage helpers”, which captures the power dynamic and who these units mainly serve. For forward looking housing policy, secondary suites are probably not the best form of housing to incentivize.\nOn the other hand, non-market or senior housing is very much something that policy does want to incentivize. We have a desperate need for more housing of this type. If the purpose-built stock shrinks because a non-profit provider bought out a rental building and converted it to non-market housing, then the decrease of the purpose-built (market) rental stock will have a very different effect on overall housing outcomes than when a building just gets torn down. Right now it is difficult to distinguish those two scenarios from the data we have.\nAt the metro level we can use the Seniors survey to pull out net new senior housing spaces, between 2018 and 2021 Metro Vancouver gained 1,084 of these. But the gap between the rental universe and rental Apartment and Row completions was 60,686 units, leaving (roughly, this does not account for senior housing demolitions) 59,602, or 87% of rental completions unaccounted for in Metro Vancouver over those three years.\nOr in other words, about half of Metro Vancouver’s rental completions were secondary suites or non-market housing. Probably. Or misclassifications. This kind of accounting by differencing can be error-prone. But it highlights the problem we have with understanding what kind of housing gets built.\nTake the following graph, versions of which have been making the rounds. It’s the change in purpose-built market rental units in Metro Vancouver municipalities over the past 5 years.\n\nIt shows that some municipalities have gained market rental units, others, most notably Burnaby, have lost market rental. Many municipalities have set adding purpose-built market rentals as a policy goal. But the same is true for non-market units, and Burnaby has now set an explicit goal to replace market rental that is lost with non-market units. This is a fairly recent change, and probably won’t have had much impact on the time frame depicted in this graph, but going forward it would be important to understand this. Just looking at the change in market rental apartments won’t suffice to understand how things are playing out in Burnaby, we need good accounting of non-market rental too.\nWe can get a glimpse of possible demolitions by splitting the data up by year of construction. This is not perfect, some older units might undergo deep renovations that require the building getting vacated, at which point they cycle out of the Rental Market Survey, and then later reappear as “old” (by year of construction) units.\n\nBoth New Westminster and Langley show that they have gained “old” rental over this time frame, hinting at units taken off for deep renovations getting occupied again and cycling back into the survey. Another way to gain old rental is hotel conversions. But without better data it is hard to say what is going on there. How many older rental buildings were demolished? How many were converted to non-market housing? How many are undergoing deep renovations? This data can’t tell us, we would need to painstakingly look through other data sources to answer this question."
  },
  {
    "objectID": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#data-quality",
    "href": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#data-quality",
    "title": "Ins and outs of CMHC data",
    "section": "Data quality",
    "text": "Data quality\nContinuing along the lines of the previous section we can get a closer look is by comparing rental row and apartment completions to changes in the Rms universe at the census tract level.\n\nPlotting the difference between the two turns into an unexpected mess. The light blue regions are somewhat expected, these tend to be single family areas that have plausibly seen additions of suites over these years. The reddish areas are curious, these indicate that the Rms gained more Rms units built “2000 or later” between 2018 and 2021 than there were rental apartment and row completions in the Scss. This should not be possible.\nThe two census tracts at UBC stand out, with both showing large gaps of opposite sign. Looking more closely into the data it appears that some rental units have initially been incorrectly geocoded to the wrong census tract, which was subsequently corrected. But the change has not been applied consistently through time, making it appear in the data that a lot of market rental was demolished in one census tract and replaced with market rental in the other census tract.\nWe see similar issues in the West End, Yale Town, and Joyce-Collingwood.\nThis bring us back to the caveat from earlier, how differencing data can bring out problems in faulty datasets. And the difficulties of using CMHC data for analysis, it generally requires ground truthing and extensive cross-checking. Ideally we would have better quality data, but CMHC data simply isn’t designed to be a robust data source for (rental) housing. This could be fixed of course, but it will require a concerted effort that goes beyond just switching to a more modern database architecture."
  },
  {
    "objectID": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-way-forward",
    "href": "posts/2022-06-12-ins-and-outs-of-cmhc-data/index.html#the-way-forward",
    "title": "Ins and outs of CMHC data",
    "section": "The way forward",
    "text": "The way forward\nCMHC is a great source of housing data. Housing data is important. But CMHC data also problems. It is broadly used to inform housing discussions, but probably lacks the necessary robustness for some of the nuances. In broad strokes we know what the problems driving our housing issues are: We have a housing shortage and need more housing. But we need data to guide our response and monitor our progress and benchmark against. Tracking and understanding our housing stock is one important ingredient in this, and Canada would be well-served by better housing data.\nSome of these gaps are getting addressed, but progress is slow. Administrative data remains locked up by many provinces. Nova Scotia is an outlier here with open data for property characteristics, assessment data and sales data, showing the way forward for other provinces to follow. British Columbia still chooses to keep this data locked up and sell it to private interests (as well as other branches of the Canadian government), fostering an information imbalance between large developers who buy this data and have the analytics capacity to use it on one side, and municipalities and the general public who only get selected subsets of the data at best and don’t have the capacity to create comprehensive analyses using their insular datasets. Ontario is probably in the worst shape, having lost control over how their administrative property data is used after outsourcing and selling it off. Although New Brunswick doesn’t even have a provincial land title registry.\nThe Canadian Housing Statistics Program has been trying to fill some of these gaps by buying and processing this data and linking it to demographic data, the other side that’s needed to address housing questions. StatCan’s refreshed building permit series helps understand how our building stock is changing, but the geographic resolution is unnecessarily coarse. The Canadian Housing Survey fills another important gap by trying to understand why people move and how satisfied they are with their living arrangements, and how this differs across segments of our society.\nSome of CMHC’s methods are woefully antiquated. Starts and completions information is great, many other places don’t have this. The way this works in Canada is that people drive out to addresses where building permits have been issued to monitor construction and collect information on starts and completions. This could be automated by training a model on satellite images. StatCan is rumoured to be working on such a project, but it’s been taking quite a bit longer than one would expect.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2022-06-12 16:29:33 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [bfe3b51] 2022-05-30: link to DM's study.\n## R version 4.2.0 (2022-04-22)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.4\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 cancensus_0.5.1          \n##  [3] cmhc_0.2.0                forcats_0.5.1            \n##  [5] stringr_1.4.0             dplyr_1.0.9              \n##  [7] purrr_0.3.4               readr_2.1.2              \n##  [9] tidyr_1.2.0               tibble_3.1.7             \n## [11] ggplot2_3.3.6             tidyverse_1.3.1          \n## \n## loaded via a namespace (and not attached):\n##  [1] colorspace_2.0-3   ellipsis_0.3.2     class_7.3-20       fs_1.5.2          \n##  [5] rstudioapi_0.13    httpcode_0.3.0     proxy_0.4-26       farver_2.1.0      \n##  [9] ggrepel_0.9.1      bit64_4.0.5        fansi_1.0.3        lubridate_1.8.0   \n## [13] xml2_1.3.3         codetools_0.2-18   knitr_1.39         jsonlite_1.8.0    \n## [17] broom_0.8.0        dbplyr_2.1.1       rgeos_0.5-9        compiler_4.2.0    \n## [21] httr_1.4.2         backports_1.4.1    assertthat_0.2.1   fastmap_1.1.0     \n## [25] lazyeval_0.2.2     cli_3.3.0          s2_1.0.7           htmltools_0.5.2   \n## [29] tools_4.2.0        gtable_0.3.0       glue_1.6.2         geojson_0.3.4     \n## [33] wk_0.6.0           V8_4.1.0           Rcpp_1.0.8.3       cellranger_1.1.0  \n## [37] jquerylib_0.1.4    vctrs_0.4.1        crul_1.2.0         blogdown_1.9      \n## [41] xfun_0.31          rvest_1.0.2        lifecycle_1.0.1    jqr_1.2.3         \n## [45] scales_1.2.0       vroom_1.5.7        hms_1.1.1          parallel_4.2.0    \n## [49] RColorBrewer_1.1-3 yaml_2.3.5         curl_4.3.2         sass_0.4.1        \n## [53] stringi_1.7.6      highr_0.9          maptools_1.1-4     e1071_1.7-9       \n## [57] sanzo_0.1.0        rlang_1.0.2        pkgconfig_2.0.3    evaluate_0.15     \n## [61] lattice_0.20-45    sf_1.0-7           labeling_0.4.2     bit_4.0.4         \n## [65] tidyselect_1.1.2   magrittr_2.0.3     bookdown_0.26      geojsonsf_2.0.2   \n## [69] R6_2.5.1           geojsonio_0.9.4    generics_0.1.2     DBI_1.1.2         \n## [73] pillar_1.7.0       haven_2.5.0        foreign_0.8-82     withr_2.5.0       \n## [77] units_0.8-0        sp_1.4-7           modelr_0.1.8       crayon_1.5.1      \n## [81] MetBrewer_0.2.0    KernSmooth_2.23-20 utf8_1.2.2         tzdb_0.3.0        \n## [85] rmarkdown_2.13     rmapzen_0.4.3      grid_4.2.0         readxl_1.4.0      \n## [89] git2r_0.30.1       reprex_2.0.1       digest_0.6.29      classInt_0.4-3    \n## [93] munsell_0.5.0      bslib_0.3.1"
  },
  {
    "objectID": "posts/2022-07-29-tumbling-turnover/index.html",
    "href": "posts/2022-07-29-tumbling-turnover/index.html",
    "title": "Tumbling turnover",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWe’re increasingly gathering lots of different measures of residential mobility in Canada. Which is great! Especially insofar as we want up-to-date information about demographic response through the pandemic. Here we want to add the CMHC Rental Market Survey (RMS) to the mix, comparing to Census and CHS (Housing Survey) results. Adding it in reveals a general decline in tenant mobility only recently (and partially) reversed. But it also raises a mystery worth solving about divergent CHS results and points toward the value of triangulation."
  },
  {
    "objectID": "posts/2022-07-29-tumbling-turnover/index.html#the-cmhc-rental-market-survey",
    "href": "posts/2022-07-29-tumbling-turnover/index.html#the-cmhc-rental-market-survey",
    "title": "Tumbling turnover",
    "section": "The CMHC Rental Market Survey",
    "text": "The CMHC Rental Market Survey\nIn CMHC’s Rental Market Survey there are several bits of information that tell us about residential mobility, namely the vacancy rate, the availability rate, and the turnover rate.\nThe vacancy rate tells us what share of units are currently not rented and are looking to get rented out. This measures how easy it is for tenants to find new accommodation. A very related metric is the availability rate (now discontinued), which on top of the vacancy rate also considers units where that are still rented out but the current tenant has given notice and it has not yet been rented out to a new tenant.\n\nVacancy rates, in particular, are familiar to most analysts, and we’ve written about them a lot. They’re particularly good at tracking whether we have enough rental apartment stock.\nSpoiler: we do not. And more broadly the primary and secondary rental market are to a large extend fungible as we have noted before when comparing vacancy rates across these markets, even if the primary market generally provides better renter protection and services. Which means that secondary market rentals are also helpful in filling this gap, all housing helps."
  },
  {
    "objectID": "posts/2022-07-29-tumbling-turnover/index.html#turnover-rates",
    "href": "posts/2022-07-29-tumbling-turnover/index.html#turnover-rates",
    "title": "Tumbling turnover",
    "section": "Turnover rates",
    "text": "Turnover rates\nThe turnover rate measures how many new tenants have moved into a unit during the past year. It is expressed as the total number of new tenants moved in during the past year divided by the total number of units. This measures residential mobility in the primary rental market.\nCMHC started measuring this in 2016 and has reported it for every Rental Market Survey (RMS) since then. Unfortunately we aren’t aware of a single clean data source for turnover rates, so we turn to Excel files for the 2019 through 2021 Market Rental Reports and scrape it from the tables in the PDF reports for the years 2016 through 2018. This only yields high-level data for metro areas. It would be good if CMHC could put this data out in more easily accessible form, preferably as part of the Housing Market Information Portal.\nTaking a comprehensive look at the CMA level data, ordered by mean turnover rate over our six years, we see a wide spread in turnover rate across CMAs and across years.\n\nUnsurprisingly, tight rental markets like Toronto and Vancouver come out with very low turnover rates, indicating the lack of housing choices. This is in line with data from the Canadian Housing Survey showing a low propensity of choice moves for overall (primary and secondary market) renters in these regions. This is likely amplified by long term renters benefiting from rent control, which enables choice stability but also discourages choice mobility by accumulating a moving penalty due to market rents increasing significantly faster than in-place rent-controlled rents. Provinces with more stringent rent control - at least on the books - include BC, MB, ON, QC, and PEI (handy abbreviated guide here).\nOf course, 2020 was a special year in this regard, with moratoria on evictions and rent freezes widespread through the early months of the pandemic. We might expect many tenants to have largely hunkered down in place. On the other hand, we might expect other tenants to have moved, following disruptions to workplace and lifestyle. Indeed, we have preliminary evidence that young adults moved back in with their parents during the early months of the pandemic. More recently, pandemic protections for tenants have expired at the same time that rental markets appear to have tightened again.\nLooking more closely at turnover rates allows us to peek at how they’ve evolved over time, both up to and through the pandemic. Eyeballing the graph it looks like turnover rates were higher in the early years and lower later, with 2021 possibly breaking from this trend. To understand this in more detail we zoom in on several CMAs and plot the trend over time, treating 2021 separately.\n\nOverall, trends for Calgary, Vancouver and Montreal seem largely unaffected by the pandemic, but Toronto, Halifax and Edmonton show significant recent increases in mobility. Is this evidence of recent differences in residential sorting processes across metro areas?\nThere are several plausible factors that can impact turnover rates, ranging from physical like unit mix and location, to demographics like age and share of students, non-permanent residents and recent immigrants which tend to have higher mobility rates. Moreover, mobility rates may be impacted by the mixture of primary and secondary rental market housing stock, with people with higher mobility rates sorting into the less stable secondary market and people looking for more stability more willing to pay the premium of renting in the primary sector.\nThat said, none of these factors seem obviously consistent with the data shown above. More work might be needed to sort out what’s going on. We should probably start by checking on how turnover data stacks up to our other measures of residential mobility for tenants."
  },
  {
    "objectID": "posts/2022-07-29-tumbling-turnover/index.html#cross-check-with-other-data-sources",
    "href": "posts/2022-07-29-tumbling-turnover/index.html#cross-check-with-other-data-sources",
    "title": "Tumbling turnover",
    "section": "Cross check with other data sources",
    "text": "Cross check with other data sources\nTo better understand the primary rental apartment mobility rates we can compare them to two other data sources that (roughly) intersect our time frame, the 2016 census and the 2018 Canadian Housing Survey (CHS). We start with the census, which we just recently used to understand overall mobility rates, and how they depend on CMA, age and tenure.\nImportantly, our mobility rates are constructed differently as we switch data sources. In our numerator, we shift from all new tenants moving through primary rental apartment stock within the past year to examining currently resident tenant households of similar structures (see below) who’ve moved in the last year. So the Census and the CHS capture a similar, but slightly different group of tenant households, but record no more than a single move per household. In our denominator, we shift from dividing by total primary rental apartment stock (i.e. including vacant apartments) to dividing by total currently resident tenant households of similar structures (i.e. excluding vacant apartments). In short, these are slightly different measures. But if carried out correctly, they ought to line up pretty well with one another.\n\nCensus data\nTo find similar structures to those recorded as primary rental apartments in the CMHC survey, we will focus on households in non-subsidized apartment rentals that aren’t part of a condominium development. In absence of suitable cross tabulations we default to using PUMF data. Wanting to filter by dwelling type forces us to use the hierarchical PUMF, and even there we don’t have the ability to distinguish people in “duplex” apartments from non-stratified apartments that are closer to what the Rms might capture.\nBefore we do this we want to get an overview how mobility rates depend on different types of housing.\n\nThese are pretty interesting results in their own right. They show a clear hierarchy of tenures in relation to residential mobility. It stands to reason that non-condo renter rates are somewhat higher than what the RMS finds in the purpose-built rental stock because it also includes secondary suite rentals. We can compare the data from the Census, which was held in May 2016, to the RMS data from October 2016, understanding the problem with counting secondary suites in the census data; counting possible multiple turnovers in the RMS data; and dropping tenants recorded as resident elsewhere in the Census. The census will also count renters in newly built units, which should show 100% mobility rates, whereas the RMS turnover rate will exclude those.\n\nThis shows that for Vancouver, Toronto and Montreal the census comes out with higher mobility rates while Edmonton and Calgary show lower rates compared to the RMS (Rental Market Survey). With Calgary and Edmonton showing very high mobility in the census of upward of 30%, it is quite reasonable to assume that the difference may be driven by multiple turnovers within a year, using the naive heuristic that if about one third of the units turned over at least once, then a little under one third of the units that turned over turned over more than once, with total turnover roughly matching the RMS numbers.\nOn the other hand, the higher census mobility rates in Vancouver and Toronto may well be explained by secondary suite rentals slipping into the non-condo apartment mix. In summary, the turnover rates of the two data sources are plausibly consistent.\n\n\nCHS data\nLet’s take a look at the CHS data. The definitions are a little different here than in the census data: apartments don’t include secondary suites in this case and subsidized units are tied to the type of housing and can’t be cash subsidies like in the census. We also get information on whether tenancies are “arm’s length” between tenant and landlord (e.g. not one family member renting to another).\n\nAgain, we notice a strong dependence on the type of unit, but it’s less clear-cut across CMA. Three other things stand out. First it’s remarkable that in Toronto mobility in subsidized units is extraordinarily low. Second, Vancouver stands out with a high share of people moving into non-arms length rentals, which we define here as rentals not rented by “a government” or “a private company or individual”, but instead by “a relative, an employer or other”. Third, and most notable in context, overall mobility rates look quite a bit lower than what we would expect from looking at the census.\n\nThis is a bit baffling, the differences are quite large, the CHS shows implausibly low mobility rates. We can plot all three data sources on the same graph.\n\nAgain, the CHS mobility rates are implausibly low. Even though Census to CHS drops match the trend line from RMS over time, they look too low relative to those trend lines. What’s going on? It’s a mystery!"
  },
  {
    "objectID": "posts/2022-07-29-tumbling-turnover/index.html#cross-checks-with-5-year-mobility-and-overall-mobility-rates",
    "href": "posts/2022-07-29-tumbling-turnover/index.html#cross-checks-with-5-year-mobility-and-overall-mobility-rates",
    "title": "Tumbling turnover",
    "section": "Cross-checks with 5-year mobility and overall mobility rates",
    "text": "Cross-checks with 5-year mobility and overall mobility rates\nLet’s check in on 5-year mobility rates and cross-check with the Census. These should be less sensitive to short-term variation, and also plausibly to undersampling recent movers. Here we can even add in the just-released 2021 CHS comparison (though we can’t yet match it up to Census 2021).\n\nThere’s differences here, but none are especially notable, and they could be explained by sampling and/or historical variation. Census 5-year mobility rates match relatively well both with StatCan tabulated results from the CHS for both 2018 and 2021, and with the Public Use Microdata File (PUMF) results. In other words, we don’t appear to be missing or undersampling movers across the 5-year survey frame.\nLet’s go back to the Census to CHS comparison for 1-year mobility rates. Below we can really see just how stark the differences are between Census results and CHS.\n\nClearly there’s an issue emerging only for recent movers, those moving within the past year in the CHS data. Are they being undersampled? This seems possible… but there’s another intriguing potential explanation.\nData collection for the CHS was carried out between November 2018 and March 2019. This is different from the Census, which was carried out from May through July of 2016. Is it possible that spreading data collection across two years mattered? Let’s compare questions asked.\nThe 2016 Census long-form questionnaire asks about where people lived on a specific date one year or five years prior to when data collection begins (May 10th, 2015), e.g.\nQuestion identifier 22: Where did this person live 1 year ago, that is, on May 10, 2015?\nThe 2018 CHS questionnaire asks about most recent move.\nQuestion identifier: PAC_Q05\nWhen did you move to your current dwelling?\nWould you say:\n01: Less than 1 year ago\n02: 1 year to less than 2 years ago\n03: 2 years to less than 3 years ago\n04: 3 years to less than 4 years ago\n05: 4 years to less than 5 years ago\n06: 5 years to less than 10 years ago\n07: 10 or more years ago\n08: Always lived here\nThere are at least two ways these different questions could result in different estimates of the timing of a recent move for the same person. Given that Census data collection runs from May through July, they may be picking up between 12-14 months of moving risk in their estimates of one-year mover rates. By contrast, the CHS question specifies exactly one year as its cut-off for estimating one-year mover rates. But the CHS data collection also crosses a threshold from one year to the next. This makes it entirely possible that people benchmark their move not from the time of survey, but what year they moved. So those moving in, say, June of 2018, but surveyed in February of 2019, might declare their move as occurring “1 year to less than 2 years ago” - because it occurred last year - rather than “Less than 1 year ago,” which is what we’d want them to choose to get our one-year mover rates. In short, the different questions and procedures of the Census might be systematically overestimating one-year mover rates while the CHS underestimates them.\nIf some respondents benchmarked their responses to calendar year in the CHS instead of estimating backward in months from survey date, it could systematically push the reported timing of their last moves backward. Do we see this kind of movement in the CHS data? Since the CHS breaks down the timing of most recent move in yearly intervals, we can take a look!\nBy way of a primer, we might expect our one-year mobility rates to remain constant for each year prior to the survey, assuming people had the same risk of moving for each year in their past. Or, more accurately given the selectivity of movers (movers tend to move more often), we might expect that mobility rates tend to be highest for the most time period, which catch the most frequent movers, then decline for moves further in the past, where frequent movers are less represented. But if enough people are benchmarking to calendar year for past moves instead of indexing by month, then our estimated mobility rates will systematically shift those people backward, growing higher if we go back further, until the effect of frequent movers takes over.\n\nSure enough, our one-year mobility rates climb the further back we go, mostly peaking somewhere in the middle. In many regions it looks like the 1-2 year mobility picks up the slack left by the 0-1 year mobility shares, providing some possible supporting evidence that there may be issues with how people benchmark their responses to the CHS timing of last move survey question. Definitely worth looking into further.\nAnother way to look at this is to convert the data to 1-year equivalent mobility rates based on aggregate movers up to \\(n\\) years back. This attempts to account for frequent movers and is a cleaner way to view mobility rates.\n\nAgain we see similar patterns, and the “Less than 2 years ago” bracket looks in line with the other brackets, again suggesting that the 0-1 and 1-2 year mobility rates are impacted by how people understand the question. This is of course overplayed with possible changes in overall mobility, where e.g. Halifax looks fairly stable of this timeframe and some other regions showing drops in (estimated) mobility rates. We also observe a drop in the much larger range (up to 10 years ago) bracket, which likely reflects the effect of frequent movers.\nOne takeaway from this is that maybe Canadian Housing Survey results should be “adjusted” for comparability. 1-year mobility rates using 1-year equivalent mobility rates (as per above) based on the cumulative movers in the past two years might be an acceptable proxy for actual 1-year mobility rates. We can try that out by plugging those into our previous graph comparing mobility rates over time.\n\nBetter! There are still noticeable differences between the sources, especially for Edmonton and Calgary which see much lower “adjusted” mobility rates in the CHS than in the RMS, but this moves us in the right direction for enabling comparisons across data sources."
  },
  {
    "objectID": "posts/2022-07-29-tumbling-turnover/index.html#takeaways",
    "href": "posts/2022-07-29-tumbling-turnover/index.html#takeaways",
    "title": "Tumbling turnover",
    "section": "Takeaways",
    "text": "Takeaways\nAre different questions and procedures (along with trends over time) enough to account for the different estimates of one-year mobility rates between Census and CHS? Maybe. We don’t know. But returning to the CMHC’s Rental Market Survey Data we can see that its results more closely line up with the Census. And that’s part of why we’re so glad to have another source of data on mobility!\nOur last takeaway returns us to the trends at hand. Including turnover data, we can clearly see a pattern operating across data sources where tenant mobility in purpose-built apartments seems to be generally on the decline, with evidence of some recent bounceback. As we’ve noted in the past, declines are only good insofar as they reflects declines in forced moves, but they’re mostly bad insofar as they reflect declines in choice moves. Protections against eviction generally increased through the pandemic, just as vacancy rates rose in many big cities. Both of which should lead to more choices. Unfortunately, these trends have since reversed in many places. That leaves our worrisome longer-term evidence of tumbling turnovers, which we’ve also seen linked to negative trends in the USA. Worth keeping an eye on!\nAs usual, the code for this post is available on GitHub.\n\n\nReproducibility receipt\n\n## [1] \"2022-09-12 12:26:27 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [162f6c3] 2022-08-19: fix link to nathan's blog\n## R version 4.2.1 (2022-06-23)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.5.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] canpumf_0.1.6             mountainmathHelpers_0.1.4\n##  [3] ggblend_0.0.0.9000        cancensus_0.5.3          \n##  [5] cmhc_0.2.1                forcats_0.5.1            \n##  [7] stringr_1.4.1             dplyr_1.0.9              \n##  [9] purrr_0.3.4               readr_2.1.2              \n## [11] tidyr_1.2.0               tibble_3.1.8             \n## [13] ggplot2_3.3.6             tidyverse_1.3.2          \n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.8.0     assertthat_0.2.1    digest_0.6.29      \n##  [4] utf8_1.2.2          R6_2.5.1            cellranger_1.1.0   \n##  [7] backports_1.4.1     reprex_2.0.1        evaluate_0.15      \n## [10] httr_1.4.4          blogdown_1.10       pillar_1.8.1       \n## [13] rlang_1.0.4         googlesheets4_1.0.0 readxl_1.4.0       \n## [16] rstudioapi_0.14     jquerylib_0.1.4     rmarkdown_2.14     \n## [19] googledrive_2.0.0   munsell_0.5.0       broom_1.0.0        \n## [22] compiler_4.2.1      modelr_0.1.8        xfun_0.31          \n## [25] pkgconfig_2.0.3     htmltools_0.5.3     tidyselect_1.1.2   \n## [28] bookdown_0.27       fansi_1.0.3         crayon_1.5.1       \n## [31] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n## [34] grid_4.2.1          jsonlite_1.8.0      gtable_0.3.0       \n## [37] lifecycle_1.0.1     DBI_1.1.3           git2r_0.30.1       \n## [40] magrittr_2.0.3      scales_1.2.0        cli_3.3.0          \n## [43] stringi_1.7.8       cachem_1.0.6        fs_1.5.2           \n## [46] xml2_1.3.3          bslib_0.4.0         ellipsis_0.3.2     \n## [49] generics_0.1.3      vctrs_0.4.1         tools_4.2.1        \n## [52] glue_1.6.2          hms_1.1.2           fastmap_1.1.0      \n## [55] yaml_2.3.5          colorspace_2.0-3    gargle_1.2.0       \n## [58] rvest_1.0.3         knitr_1.39          haven_2.5.0        \n## [61] sass_0.4.2"
  },
  {
    "objectID": "posts/2022-09-12-rent-growth-in-gdp/index.html",
    "href": "posts/2022-09-12-rent-growth-in-gdp/index.html",
    "title": "Rent growth in GDP",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nEvery now and then the topic of the GDP share of the “Real Estate Industry” comes up, often linked to the suggestion that an economy has become too dependent upon real estate. But this usually involves a fundamental misreading of the data. As people who pay attention know, the NAICS sector [53] “Real Estate Industry” of the expenditure based GDP produced by StatCan is mostly just rent and imputed rent.\nImputed rent, as a reminder, is simply how much owner-occupiers would be paying themselves to live in their residences if their roles were split into tenant and landlord components. We can estimate this based on how much the dwelling would fetch if rented out at prevailing market rents. There are technical details as to why and how this is done, and what makes them challenging, all of which we’ll save for another post, but they are reasonably accurate enough for our purposes in this post. What they point toward is that the relative size of the “Real Estate Industry” is largely just a measure of rent as share of GDP.\nTo start we take a look at just the rent and imputed rent part of the GDP by province. We can also talk about this rent portion as Housing Consumption.\nThe share of GDP going toward housing consumption has been rising in all provinces since the early to mid 2000s, with BC consistently taking the top spot and the gap to the Canadian mean widening. Nova Scotia takes the second spot throughout, and this shows that there are two things going on at once. A province can have a high share of GDP going toward housing consumption because they spend a lot of money on rent (and imputed rent), or because GDP is low. To disentangle this a little we can look at housing consumption per capita instead of as a percent of total GDP.\nThis shows the extraordinary rise of the dollar value of housing consumption in BC. Nova Scotia, by contrast, closely tracks the Canadian average. So these two provinces had a high share of GDP going toward housing consumption for very different reasons. In BC people spend a lot of money on rent (and imputed rent), boosting the numerator, while in Nova Scotia the GDP per capita is relatively low, minimizing our denominator.\nThis gets us to the question of why people in BC spend a lot of money on rent and imputed rent. Is it because people achieve extraordinary housing outcomes, living in large or very luxurious housing units? Or is it simply because high rent reflects too many people bidding for too little housing? In the first case, we’d expect things like the floor space of housing consumption to be much higher in BC. In the second, we’d expect it to be roughly the same, or smaller (bound, perhaps, primarily by regulatory constraints on minimum dwelling size).\nRobust Canada-wide metrics on housing quality and size are relatively hard to come by, but the Canadian Housing Statistics Program is now publishing average and median living areas in three Canadian Provinces, with hopefully more to come. So that gives us one view into this question.\nAmong these three provinces it does not appear that BC residents have substantially larger homes, with maybe the exception of single detached homes. Indeed, BC is often accused of having apartments that are too small. Overall, while there are some very nice homes in BC, it seems unreasonable that explains more than a small portion of the 45% higher housing consumption per capita BC residents pay over the Canadian average. It’s really hard to conclude that BC’s high housing consumption cost is due to anything but high rents."
  },
  {
    "objectID": "posts/2022-09-12-rent-growth-in-gdp/index.html#takeaway",
    "href": "posts/2022-09-12-rent-growth-in-gdp/index.html#takeaway",
    "title": "Rent growth in GDP",
    "section": "Takeaway",
    "text": "Takeaway\nSo what does all of that mean? Housing consumption makes up a very high share of GDP in BC because rents are so high. People have to spend a lot of money on rent and don’t have that much money left over for other things. For the individual that restricts their quality of life. For the province overall it leads to reduced economic growth with money being spent on rent instead of other consumption goods that can help grow the economy.\nSo what should be done about that? It’s simple really, we know very well how overall housing availability impacts rents. It may seem like a paradox, but to lower the GDP share of the “Real Estate Industry” we really need to build a lot more housing.\nAs usual, the code for this post is available on GitHub.\n\n\nReproducibility receipt\n\n## [1] \"2022-09-12 12:29:49 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [162f6c3] 2022-08-19: fix link to nathan's blog\n## R version 4.2.1 (2022-06-23)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.5.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] cansim_0.3.12   forcats_0.5.1   stringr_1.4.1   dplyr_1.0.9    \n##  [5] purrr_0.3.4     readr_2.1.2     tidyr_1.2.0     tibble_3.1.8   \n##  [9] ggplot2_3.3.6   tidyverse_1.3.2\n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.8.0           assertthat_0.2.1         \n##  [3] digest_0.6.29             utf8_1.2.2               \n##  [5] R6_2.5.1                  cellranger_1.1.0         \n##  [7] backports_1.4.1           reprex_2.0.1             \n##  [9] evaluate_0.15             httr_1.4.4               \n## [11] blogdown_1.10             pillar_1.8.1             \n## [13] rlang_1.0.4               curl_4.3.2               \n## [15] googlesheets4_1.0.0       readxl_1.4.0             \n## [17] rstudioapi_0.14           jquerylib_0.1.4          \n## [19] rmarkdown_2.14            googledrive_2.0.0        \n## [21] munsell_0.5.0             broom_1.0.0              \n## [23] compiler_4.2.1            modelr_0.1.8             \n## [25] xfun_0.31                 pkgconfig_2.0.3          \n## [27] htmltools_0.5.3           tidyselect_1.1.2         \n## [29] bookdown_0.27             fansi_1.0.3              \n## [31] crayon_1.5.1              tzdb_0.3.0               \n## [33] dbplyr_2.2.1              withr_2.5.0              \n## [35] grid_4.2.1                jsonlite_1.8.0           \n## [37] gtable_0.3.0              lifecycle_1.0.1          \n## [39] DBI_1.1.3                 git2r_0.30.1             \n## [41] magrittr_2.0.3            scales_1.2.0             \n## [43] cli_3.3.0                 stringi_1.7.8            \n## [45] cachem_1.0.6              fs_1.5.2                 \n## [47] xml2_1.3.3                bslib_0.4.0              \n## [49] ellipsis_0.3.2            generics_0.1.3           \n## [51] vctrs_0.4.1               tools_4.2.1              \n## [53] mountainmathHelpers_0.1.4 glue_1.6.2               \n## [55] hms_1.1.2                 fastmap_1.1.0            \n## [57] yaml_2.3.5                colorspace_2.0-3         \n## [59] gargle_1.2.0              rvest_1.0.3              \n## [61] knitr_1.39                haven_2.5.0              \n## [63] sass_0.4.2"
  },
  {
    "objectID": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html",
    "href": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html",
    "title": "Still Short: Suppressed Households in 2021",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nIn May we estimated suppressed household formation across Canada using what we called the Montréal Method, finding strong evidence for suppression across many parts of Canada. As a reminder, we designed the Montréal Method to estimate housing shortfalls related to constraints upon current residents who might wish to form independent households but are forced to share by local housing markets. Now that we’ve got 2021 Census data out, it’s time to update our estimates. Given the data available, currently we can only estimate metro area effects of our previous Model 1 (crude household maintainer rates) and Model 2 (age-adjusted household maintainer rates). But that’s a start, and we’re also now enabled to extend the long timelines for Toronto, Montréal and Vancouver from our previous post to include 2021. Overall, current suppression of households alone suggests a shortfall of over 400,000 dwellings in Metro Toronto, and 130,000-200,000 across Metro Vancouver."
  },
  {
    "objectID": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#comparing-2021-to-2016",
    "href": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#comparing-2021-to-2016",
    "title": "Still Short: Suppressed Households in 2021",
    "section": "Comparing 2021 to 2016",
    "text": "Comparing 2021 to 2016\nTo begin, we look at age-adjusted household maintainer rates across the 20 most populous metro areas in 2016 and 2021, ordered by how much they changed.\n\nThe picture we see shows some areas with increases in age-adjusted household maintainer rates, especially in Québec, but most show reductions, especially in in the Greater Toronto Area, implying it’s gotten harder to set up an independent household there.\nAge-adjusted rates can mask how household maintainer rates are shifting across age groups, but looking at Canada overall the picture is quite consistent with reductions in household maintainer rates through almost all age groups.\n\nReductions in maintaining an independent household are particularly pronounced in the 25 to 34 year old age groups, but we also see marked declines in the 70+ age range. Let’s break this out by metro area geography to pick up any differences.\n\nMetro variation is marked! Toronto shows decreasing household maintainer rates for ages below the late 50s, and increases above that. Québec shows increases in household maintainer rates generally across the board. Vancouver shows decreases for ages 35 through 50, whereas Hamilton, Oshawa, Victoria, and Halifax show decreases for almost all but the oldest age group. We also want to note the very different levels of age-specific household maintainer rates, with Québec and Montréal showing higher rates in all age groups than most of their peers, and Toronto having lower rates than most."
  },
  {
    "objectID": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#caveats",
    "href": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#caveats",
    "title": "Still Short: Suppressed Households in 2021",
    "section": "2021 caveats",
    "text": "2021 caveats\nIt’s important to keep in mind that 2021 wasn’t a usual census year. People’s locations in May of 2021, in particular, were heavily impacted by the first year of the COVID-19 epidemic and its whiplash effects on things like local employment, on-campus university enrollments, and the movements of elders into long-term care. How all these trends might impact household maintainer rates still remains quite speculative. In some cases, trends might’ve even cancelled one another out. For example young adults moving back in with their parents during COVID might’ve reduced household maintainer rates even as young adults formerly living as roommates might’ve split up into separate apartments as part of social distancing efforts, boosting household maintainer rates within the same age range.\nDeaths from COVID could also influence maintainer rates directly, particularly for the older adults most vulnerable to the virus. That said, some of the largest impacts were contained within long-term care homes, and thus outside of the population within private households we’re looking at here."
  },
  {
    "objectID": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#timelines",
    "href": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#timelines",
    "title": "Still Short: Suppressed Households in 2021",
    "section": "Timelines",
    "text": "Timelines\nLet’s pull out our three biggest Metro areas for longer timelines on age-specific household maintainer rates.\n\nThe strongest observable patterns, as we noted previously, concern the dramatic historical declines in household maintainership for young adults in Toronto and Vancouver. By contrast young adults saw much greater stability in Montréal, where declines in household maintainership only began to appear quite recently. Adding in the 2021 data indicates that Montréal’s household maintainer rates for young adults have recovered a little from these declines.\nWas Montréal’s recovery in independence for young adults about the return of cheaper rents? To dig into this a little more we look at rent quartiles of purpose-built rental housing in Montreal, adjusted by CPI and a selection of income measures.\n\nWhen adjusted for inflation or income, rents look like they were increasing or stable prior to the pandemic. But rents dropped across all measures by November of 2020, the last Rental Market Survey prior to the 2021 Census. Of note, income-adjusted estimates of rent for the 2020 data point should be viewed with caution due to impacts of COVID and related supports on incomes. Nevertheless, the data broadly supports the idea that lower rents by 2021 may have boosted household maintainer rates for young adults in Montréal.\nTo better directly compare metro areas for each age group, We can flip the age-specific household maintainer rate graph by colouring by metro area and faceting by year.\n\nThis highlights how over time Vancouver and especially Toronto, starting with younger age groups and then moving up the age ranks, have experienced dropping household maintainer rates relative to Montréal.\nWe’re especially interested in Montréal in part for its potential to serve as a counterfactual for what we might expect household formation to look like in an unconstrained housing market in Canada. But as discussed above, Montréal may not be a clean counter-factual for a non-squeezed housing market any more. Despite what looks like a recent recovery in household formation for young adults, household maintainership remains down relative to previous decades, with signs of dropping household formation in the younger age groups in the last several censuses prior to 2021. So here we add Québec City as a second counter-factual, just to give an alternative scenario for a place where households appear to form quite easily.\nWhile Québec City provides us a second counterfactual to consider, there are factors other than age that might impact household formation, and our Model 3 of our previous post tries to get at that, loosely labelling this factor as “culture”. What generally gets captured under “culture” is confounded by other important factors, chiefly income. And there is a real danger of attributing lower household formation rates of certain groups to “culture” when in reality it’s just systemic barriers leading to lower incomes that are partially or chiefly responsible for historic differential household formation rates across “culture”. We can’t run our Model 3 yet given the limited data available for 2021, but we want to point out that Québec City is culturally very homogeneous which may skew the results to the extent that “culture” plays a role in household formation. Montréal is much more culturally diverse and similar to other large Canadian metro areas, and in that sense we view the Montréal scenario as a more robust counter-factual despite recent signs of slowing household formation in younger age groups. So view the Québec scenario with appropriate caution!\nFor our new estimates, We start out by asking by how many households each metro area would have to grow to achieve the same (age-specific) household maintainer rates as Montréal or Québec.\n\nUsing either Montréal or Québec as the standard, we see that our selected metro areas would all have to increase their dwelling stock dramatically to make space for households to be able to form in line with our counter-factual scenarios. The estimated shortage is particularly pronounced if we take Québec City as the counter-factual. Toronto tops the list with a housing shortfall of around 20% of it’s existing dwelling stock.\nImportantly, this shortfall estimate doesn’t account for migration. For example, this shortfall only accounts for the population that already lives in Toronto. If Toronto were to add housing it would likely pull in people that have been pushed out of the metro area to neighbouring areas like Oshawa or Hamilton, easing pressures there but in turn dampening the effect of the added housing in Toronto on boosting household maintainership rates.\nLastly, we can translate this into hard numbers of how many concealed households we have in each metro area. Put slightly differently, we can ask how many more housing units we would need to allow the existing population to form households at the same rates as in our counter-factual metro areas of Montréal or Québec.\n\nThis puts a rough estimate on the number of suppressed households, or alternatively the number of additional dwellings needed to bring household formation rates up to counter-factual levels. The number for Metro Toronto is staggering. Between the 1970s, when Toronto had similar household maintainer rates as Montréal, and 2021, Metro Toronto has accumulated a shortage of over 400k homes, and that’s still not accounting for migration effects or moving vacancies.\nFor Metro Vancouver this pegs the number of suppressed households at somewhere between 130k and 200k, depending on the counter-factual scenario. Again, not accounting for migration or moving vacancies. That corresponds to between 6 and 10 years worth of recent completions, not accounting for demolitions, just to remove the current shortfall, while holding current population constant (narrator voice: it will not be held constant)."
  },
  {
    "objectID": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#comparison-numbers",
    "href": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#comparison-numbers",
    "title": "Still Short: Suppressed Households in 2021",
    "section": "Comparison Numbers",
    "text": "Comparison Numbers\nThe English Housing Survey measures ‘concealed households’ directly, something that would be great to have in Canada too as part of the Canadian Housing Survey. In England the share of ‘concealed households’ came out at 7% in their 2018-2019 survey. Our Model 2 comes out with between 7% and 13% of concealed households for Canada overall, depending on the counter-factual scenario. Having a direct measurement to benchmark these estimates against would be extremely helpful.\nAnother useful comparison is to see how these estimates stack up against CMHC supply shortfall estimates. The CMHC estimates were done very differently from our Montréal Method. CMHC relied on prices to fit a (modified) DiPasquale-Wheaton model to data from Canadian provinces and asked what it would take to get prices to specified levels. The CMHC report did not break out the “current” (2019) shortfall, but only the projected 2030 shortfall under a business-as-usual scenario. But the presentation on this work at this year’s CEA conference did break out the 2019 “current” shortfall, which can provide a useful benchmark for us.\n\nThe CMHC housing shortfall estimates for Ontario are roughly consistent with our more aggressive Québec counter-factual scenario. But for the other provinces they differ significantly; they are lower for Alberta and higher in Quebec and British Columbia than our suppressed household estimates.\nOne reason for this discrepancy is migration, which, to first order, is implicitly included in the CMHC model but is completely missing from our demographics-based estimates. In particular, it is very plausible that in-migration into British Columbia from other parts of Canada would increase significantly if prices were lower. And a good portion of this increase might come from Alberta, easing the need for more housing there. On the other hand, single-region models like the one employed by CMHC might be double-counting demand when people get priced out of a region and buy or rent elsewhere. Some econometric models building on DiPasquale-Wheaton, like the models developed by Geoffry Meen, avoid this by modelling migration effects explicitly.\nApart from not accounting for migration pressures, our household suppressing estimates only look at households and not at how many dwelling units are needed. The number of dwelling units would always have to be larger than the number of households to account for moving vacancies, as well as increase the rental vacancy rate to a healthy level to allow freer household formation that is not constrained by landlord bargaining power."
  },
  {
    "objectID": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#takeaway",
    "href": "posts/2022-10-03-still-short-suppressed-households-in-2021/index.html#takeaway",
    "title": "Still Short: Suppressed Households in 2021",
    "section": "Takeaway",
    "text": "Takeaway\nHousing discussions often get caught up in overly simplistic mental models of how housing works. Here we start to explore just one process involving how housing works for distributing people into households. Thinking through the process enables us to estimate household suppression. This formulates one aspect of the consequences of housing scarcity in terms of demographics, and how it interferes with life trajectories.\nOften calls for building more housing are justified or discredited by pointing to past population growth and population growth projections. But this misses the central endogeneity problem. The number of dwelling units constitute a hard cap on the number of households that can form. When housing is scarce, both population growth and household formation slow down. A housing provision model that ignores suppressed household formation and migration becomes a self-fulfilling prophecy, and systematically enshrines housing scarcity.\nWhile prices and rents are the best indicator of housing scarcity, there are those who doubt that prices and rents are related to scarcity, despite ample evidence to the contrary. This points toward the value of exhibiting the effects of housing scarcity directly on demographics and people’s life trajectories, without making any reference to prices. Moreover, many people seem to have higher sympathy for the welfare of existing residents as opposed to newcomers, and might be more willing to understand the detrimental effects of housing scarcity when phrased in terms of the existing population, rather than more abstractly thought prices and rents, or migration pressures. We have made a similar attempt to highlight the people leaving the region instead of looking at net population flows.\nAt the individual level, the decisions of individuals to form or not form households, or to move into or out of a region, are generally driven by prices and rents. And the ability and willingness to pay determines the outcome. When added up, these individual decisions set the market price and rent of housing. Prices are information aggregators, and the shortcut to all of this is to simply look at prices and rents as the main indicator of housing scarcity, which is what econometric models like the ones in the recent CMHC report on Canada’s Housing Supply Shortage do. So maybe, with this additional demographic context, we can pay more attention to those econometric models and build more housing. We can do this both for the population that’s already here so young people can form households and families, and also to make room for the people that left or that did not come, and for those intending to come in the future, those not wanting to leave in the future, and those wanting and needing to form new households in the future.\nAs usual, the code for this post is available on GitHub.\n\n\nReproducibility receipt\n\n## [1] \"2022-10-03 15:42:31 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [47fc9cc] 2022-10-01: fix author/title mixup\n## R version 4.2.1 (2022-06-23)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Monterey 12.6\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 statcanXtabs_0.1.2       \n##  [3] forcats_0.5.1             stringr_1.4.1            \n##  [5] dplyr_1.0.9               purrr_0.3.4              \n##  [7] readr_2.1.2               tidyr_1.2.0              \n##  [9] tibble_3.1.8              ggplot2_3.3.6            \n## [11] tidyverse_1.3.2           cancensus_0.5.3          \n## [13] cansim_0.3.12            \n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.8.0     assertthat_0.2.1    digest_0.6.29      \n##  [4] utf8_1.2.2          R6_2.5.1            cellranger_1.1.0   \n##  [7] backports_1.4.1     reprex_2.0.1        evaluate_0.15      \n## [10] httr_1.4.4          blogdown_1.10       pillar_1.8.1       \n## [13] rlang_1.0.5         googlesheets4_1.0.0 readxl_1.4.0       \n## [16] rstudioapi_0.14     jquerylib_0.1.4     sanzo_0.1.0        \n## [19] rmarkdown_2.14      googledrive_2.0.0   munsell_0.5.0      \n## [22] broom_1.0.0         compiler_4.2.1      modelr_0.1.8       \n## [25] xfun_0.33           pkgconfig_2.0.3     htmltools_0.5.3    \n## [28] tidyselect_1.1.2    bookdown_0.27       fansi_1.0.3        \n## [31] crayon_1.5.1        tzdb_0.3.0          dbplyr_2.2.1       \n## [34] withr_2.5.0         grid_4.2.1          jsonlite_1.8.0     \n## [37] gtable_0.3.1        lifecycle_1.0.2     DBI_1.1.3          \n## [40] git2r_0.30.1        magrittr_2.0.3      scales_1.2.1       \n## [43] cli_3.4.0           stringi_1.7.8       cachem_1.0.6       \n## [46] fs_1.5.2            xml2_1.3.3          bslib_0.4.0        \n## [49] ellipsis_0.3.2      generics_0.1.3      vctrs_0.4.1        \n## [52] tools_4.2.1         glue_1.6.2          hms_1.1.2          \n## [55] fastmap_1.1.0       yaml_2.3.5          colorspace_2.0-3   \n## [58] gargle_1.2.0        rvest_1.0.3         knitr_1.39         \n## [61] haven_2.5.0         sass_0.4.2"
  },
  {
    "objectID": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html",
    "href": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html",
    "title": "Analyzing Ballot Composition in Vancouver",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nSo we recently had an election in the City of Vancouver. Citizens elected a new mayor, ten council members, park board and school board, giving a majority to the centre-right leaning new ABC (A Better City) Party candidates for each (full results posted by the City). There are a variety of narratives out there about how it all went down. Here we’re interested in examining a couple of them in further detail using the recently released individual ballot data (all ballots remain anonymous, of course). Of note, the mayoral vote is straight-forward, each voter got to vote for one mayoral candidate. The council votes are more interesting. There voters could choose up to 10 candidates. For this post we will focus on council votes, but we’ll return to examining how they relate to mayoral votes.\nIn terms of relevant narratives about the election, two stand out as worth examining with ballot data: the election came down to organizing and/or the election came down to positioning.\nFirst, let’s talk organizing. Here we can start with it’s relative absence in binding together the diverse parties occupying the centre-left. Left-leaning parties, including Forward Vancouver, OneCity, COPE, and the Greens, arguably competed at least as much with each other as they did with the centre-right. No single party has dominated since the collapse of the centre-left Vision party in 2018. That collapse, though largely driven by internal dynamics, was hastened by the Vancouver District Labour Council (VDLC) eschewing Vision in favour of attempting to organize a slate from the other three centre-left parties. In effect the VDLC brokered the number of candidates from each party they would support, and most parties kept to the bargain. The slate was somewhat successful in 2018, electing three Greens, one OneCity, and one COPE councillor together with the VDLC’s chosen independent mayoral candidate. The VDLC attempted the trick again this year, but only OneCity limited themselves to the number of candidates chosen for them, and new parties entered the field altogether running entirely too many candidates.\nMeanwhile, the historical centre-right party, the Non-Partisan Association (NPA), also seemed on the verge of collapse after 2018. And collapse it did, after a takeover by far-right party activists drove out four out of its five sitting councillors. One started a new party (TEAM), largely devoted to opposing development and SkyTrain expansion, and ran as its mayoral candidate. The other three joined up with the NPA’s former mayoral candidate, who very narrowly lost in 2018, to start an entirely new party, staking a strong claim - with a lot of money behind it - to a generally pro-development and socially liberal centre-right. So there’s a rough re-cap. But just how did this give ABC an organizational advantage?\nFirst off, ABC faced no constraints placed upon the size of their slate. So they were able to run with seven candidates: enough to hold a majority on council, without so many that votes were likely to be diluted. Arguably, seven candidates was the perfect size for “[vote plumping](https://www.langleyadvancetimes.com/opinion/editorial-to-plump-or-not-to-plump-your-vote/#” within a council of ten. True party supporters could vote for all of their candidates and no one else’s, avoiding lending support to any other parties and correspondingly boosting the weight of their favoured party. Not coincidentally, this was also the number of candidates Vision generally ran with during the height of its power.\nDid voters really follow this strategy? To start out we take a look at the distribution of the number of votes cast on council ballots.\nAs one would expect, the most popular choice was to vote for 10 candidates, but just over half of the voters voted for fewer than 10 candidates. What’s curious is that the second most popular option was people voting for 7 candidates.\nInteresting! Let’s follow-up by checking how many ballots were straight-party tickets with nobody else on the ballot? So these are ballots with fewer than the maximum of 10 votes where some votes were intentionally wasted, “plumping” up the relative power of the votes that remained.\nWe see that in particular ABC voters made use of this strategy, relating back to the top where we saw a high number of people casting exactly seven votes. The majority of these were pure ABC slate ballots. TEAM voters, looking at a similarly sized slate, also made use of this strategy, there just weren’t nearly enough of them to matter. TEAM’s poor showing emphasizes a lesson worth teaching over and over again: despite what we see at public hearings on rezoning applications, anti-housing politics might be well-organized, but they really aren’t very popular.\nNow let’s shift our analysis slightly. Let’s look at ballots where a full party slate got voted in, but also contains some other votes. These are full party-line ballots, but not plumped by strategic avoidance of voting for anyone outside the party.\nABC still shows strongly with 25% of voters voting for all seven ABC council candidates. Given that the lowest vote share of all elected councillors was lower at 18%, just these party slate ballots were enough to guarantee all seven ABC got elected. But we also see some of the smaller Left of Centre parties emerging. In particular, OneCity partisans really, really liked all four of the OneCity candidates. They just also voted for a bunch of other candidates to fill out their rosters, perhaps inadvertently reducing the overall effect of their support. (Vote Socialist also had a strong showing, but they only had a single candidate running for Council).\nOf note, this graph also counts some ballots twice. For example it is possible to vote the entire Forward and OneCity slates on a single ballot, voting for all six Forward and all four OneCity candidates. This is one way to add up a full slate. Looking at the ballots, 2,269 people did exactly this, demonstrating some affinity between these parties.\nLet’s look at some other ways to demonstrate affinities. Now we’re shifting from sheer organizational advantages as determinants of election outcomes to thinking more about how issues and positioning might’ve tied different parties together or left them competing with one another."
  },
  {
    "objectID": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#correlation-and-voting-blocks",
    "href": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#correlation-and-voting-blocks",
    "title": "Analyzing Ballot Composition in Vancouver",
    "section": "Correlation and voting blocks",
    "text": "Correlation and voting blocks\nA simple way to try and understand voting patterns on council votes is to look at how votes for different candidates on each ballot correlated.\n\nTo keep things manageable we weaned down the list of 59 council candidates to the top 30 vote-getters, ordered by party and number of votes the candidates got. In the correlation plot we can immediately identify major parties, visible as blocks with high cross-correlation around the diagonal. Parties really work as organizers for voters in Vancouver’s at-large council elections (as opposed to the ward systems common across other big Canadian cities).\nThe first block, picking up ABC support, shows the strongest cross-correlation with fairly strong negative correlation with all other candidates except the lone NPA candidate that made the top 30 (and remember that ABC split off from the increasing dysfunction of the NPA). This indicates high party discipline for ABC voters, despite some residual brand loyalty to the NPA, and possibly some bleed over for the noted police affiliations of both parties.\nThe next four party blocks cover the GREENs, OneCity, Forward, and COPE, with weakly positive or neutral correlations to each other. The strongest connections can be found between OneCity and Forward, OneCity and COPE (OneCity first split from COPE in the 2014 election), and GREEN and COPE, suggesting some lingering divisions over the notable urbanism axis in left-wing Vancouver politics.\nThe next block is the single remaining incumbent NPA candidate, and the only one to make it into the top 30, showing weak but positive correlations with ABC.\nLast in our top 30 candidates comes the TEAM block, who attempted to split off from the NPA by gathering all its NIMBY-est voters together with a who’s who of Vancouver’s assorted Neighbourhood Defenders. They exhibit neutral or weak anticorrelation with everyone else."
  },
  {
    "objectID": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#clusters",
    "href": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#clusters",
    "title": "Analyzing Ballot Composition in Vancouver",
    "section": "Clusters",
    "text": "Clusters\nTaking a slightly different tack to tracking how voters related parties to one another, we can run a cluster analysis on the individual ballot data. To keep things manageable we are only showing the results for the top 30 candidates. Given the ballot data, 5 clusters seems like a good choice that captures most of the variation in votes.\n\nThe results give a way to sumamrize voting behaviour. The first cluster, ordered by number of ballots in that cluster, is the ABC cluster. We see very high voting discipline, with some additional votes for Melissa De Genova, suggesting that there was a strong voter movement from the NPA.\nThe second cluster is quite scattered in their voting, demonstrating little in the way of party discipline. Many of these votes might simply reflect incumbency, with the strong showing by Forward Vancouver (a new party) gauging support for the mayor. But while Forward received the highest frequency of votes in this cluster, it’s definitely not JUST or even PRIMARILY a “Forward” cluster. Indeed, other clusters have higher frequency of votes for Forward candidates, but in those Forward appears to be a second or third choice, rather than the first. While Forward arguably ran with the right number of candidates (six), the data demonstrate the difficulty of organizing a party at the last possible minute. We see little evidence of anything approaching Forward party discipline, and the party ultimately failed to get any councillors elected (though they came very close to getting Dulcy Anderson over the line!)\nThe third cluster stands out as the most strongly left-of-centre cluster, and it’s clearly led by the party discipline of OneCity. But with only four candidates, OneCity voters spread their remaining six votes out across a real mix of Forward, COPE, and the GREENs. It wasn’t quite enough to see most OneCity’s new candidates elected, though they returned their incumbent, Christine Boyle, in part on the strength of her support in the next cluser, led by the GREENs.\nThe fourth candidate is the GREEN cluster. What can you say? The GREENs still have a viable party, but their supporters (and to some extent their councillors) remain all over the map. They liked Jean Swanson’s populist antics on council, but it wasn’t enough to return her to chambers, and the same went for the Forward slate, which had relatively broad low-level support from GREEN voters, but not enough. By contrast, GREEN voters also liked Christine Boyle from OneCity well enough for her to comfortably return. And GREEN voters also voted in a striking number of ABC councillors, and even a few TEAMsters. As a result, it’s still not quite clear what GREEN voters want, unless you count the vaguely positive connotations of colour branding.\nThe last cluster is the TEAM cluster, with high voting discipline and a little bit of pull on ABC and GREEN candidates. But TEAM had a very weak showing in the other clusters. As a result, even a strong showing in the cluster with the lowest number of ballots was nowhere near enough to get candidates elected. Running against new housing and SkyTrain expansion is just not popular. Who knew? (Who indeed?)\nLet’s return to the influence of the VDLC. How did the Vancouver District Labout Council election endorsements matter? They endorsed 10 candidates in 2022, three of whom got elected. Only 1,321 voters chose the full VDLC slate on their ballots. But let’s take a broader look at their impact on voters. What share of voters voted for any number of VDLC endorsed candidates?\n\nWhile a little over half of all voters voted for at least one VDLC candidate, the minimum number of votes VDLC candidates got on each ballot drops off fast as we approach the full slate. We can visualize voter discipline by looking just at the correlation plot of VDLC candidates.\n\nAs can be expected this shows that party affiliation had much stronger cohesion than VDLC, but what’s concerning is how little cross-correlation there is between VDLC candidates across party lines. The strongest correlation is between OneCity and Forward candidates, but as we observed before, this correlation was not restricted to VDLC candidates and it is difficult to argue that VDLC strengthened it much. Jean Swanson stands out as having positive correlations to everyone else, with the weakest correlation to Forward candidates."
  },
  {
    "objectID": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#mayoral-pull",
    "href": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#mayoral-pull",
    "title": "Analyzing Ballot Composition in Vancouver",
    "section": "Mayoral pull",
    "text": "Mayoral pull\nOne question we haven’t addressed yet is what kind of pull, if any, mayoral candidates had on the council ticket. First, let’s add up the mayoral votes corresponding to each council candidate’s support.\n\nOnce again we see how ABC really stuck together, with the vast majority of ABC council support accompanied by votes for new ABC mayor, Ken Sim, at the top of the ticket. Interestingly people voting for the NPA candidate Melissa de Genova also mostly voted for Ken Sim instead of the hapless NPA mayoral candidate, Fred Harding.\nA similar but weaker pattern holds with Forward and TEAM candidates. When voters chose council candidates of one party, they also tended to choose the mayoral candidate of that party if one was available. Yet this was only narrowly the case for TEAM, with Colleen Hardwick often less popular than her council candidates. Interestingly, Stewart was the mayoral choice for nearly as many OneCity council voters as he was for those voting for his Forward Vancouver candidates. But GREEN party council voters wandered quite a bit more in terms of their preferred mayoral candidates.\nLet’s reverse this figure and look at the spread of council votes by mayoral ballot.\n\nOnce again, here we see that Ken Sim had a strong pull on the council ballot, with voters for Ken Sim overwhelmingly voting for ABC candidates, and often only ABC candidates. By contrast, Kennedy Stewart support was built up from supporters of a diverse array of left-of-centre parties, but especially Forward, OneCity, and COPE, with the GREENs most likely to spread their love around.\nFinally, let’s take a look at how mayors did at carrying along their slates. While Ken Sim ran with a slate of seven, Kennedy Stewart eventually pulled together a slate of six for Forward Vancouver, and TEAM (Hardwick) and Progress (Marissen) ran at the same size. How many candidates from mayoral party slates got left off the ballot when voters chose a mayor? Let’s take a look.\n\nAgain, we see that almost half of the people voting for Ken Sim for mayor also voted for the full slate of ABC party candidates. In this case, we further evidence of disciplined party voters. Colleen Hardwick’s voters look similarly disciplined, though it’s worth noting once again that she frequently underperformed her council candidates. Kennedy Stewart performed worst by this metric of party discipline, but he also had the most hastily assembled set of council candidates. On the flip side, urbanist Mark Marissen had the highest share of ballots that picked him for mayor but voted for absolutely none of his party candidates."
  },
  {
    "objectID": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#takeaways",
    "href": "posts/2022-12-17-analyzing-ballot-composition-in-vancouver/index.html#takeaways",
    "title": "Analyzing Ballot Composition in Vancouver",
    "section": "Takeaways",
    "text": "Takeaways\nNarratives will continue to spin concerning the outcome of the 2022 election in Vancouver and we can’t put them all to rest. But a few clear takeaways emerge from our examination of the ballots.\n\nABC had the most disciplined voters lined up for the most organized roster of candidates.\nvoters on the Centre-Left remained mostly undisciplined with no clear central organizing roster\nOneCity voters and roster appeared the most disciplined & organized within the centre-left\nGREEN voters still look kind of flaky\nTEAM voters were relatively disciplined, but they’re just not very popular\n\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt."
  },
  {
    "objectID": "posts/2023-02-20-investing-in-definitions-and-framing/index.html",
    "href": "posts/2023-02-20-investing-in-definitions-and-framing/index.html",
    "title": "Investing in definitions and framing",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nWith last week’s CHSP release of data on the investment status of residential properties and the framing of the accompanying article there has been a lot of rather uninformed and misleading news coverage.\nThe misleading reporting, combined with sometimes plainly wrong statements by people quoted in the news coverage, on one hand highlights the poor understanding of housing in the public discourse. On the other hand it highlights the importance of providing careful framing with data releases. In general it is good practice to accompany a data release with a brief analysis to provide framing and context. Analysts close to the raw data will have a much better understanding of what the data is measuring and can properly frame the data. Unfortunately, StatCan fell short of doing so, and the overview analysis provided by StatCan itself contains a number of problems. Given the public attention this has gotten it’s probably worthwhile to take a look at what the data does and does not say, and to correct some of the misinterpretations that have been circulating."
  },
  {
    "objectID": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#the-trouble-with-the-definitions",
    "href": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#the-trouble-with-the-definitions",
    "title": "Investing in definitions and framing",
    "section": "The trouble with the definitions",
    "text": "The trouble with the definitions\nFor any data release it’s important to understand the definitions underlying the categorization in the data. Central to this release is how “investors” are defined.\nIn common language, “investors” are typically defined by motivation and expectation. The New Oxford American Dictionary defines “investor” as: “a person or organization that puts money into financial plans, property, etc. with the expectation of achieving a profit.” Statistics Canada does not have access to motivations or expectations through the CHSP program. This suggests they probably should have considered different language in this study, and at the very least have been prepared for a lot of misinterpretation in media coverage.\nRather than exploring motivation or expectation, we get a definition of which property owners are considered “investors” from Statistics Canada that simply groups by point-in-time category. So investors include:\n\nfor-profit businesses\ngovernments\nindividuals not found resident in the province\nindividuals resident in the province owning more than one property\nindividuals resident in the province owning a property with more than one residence\n\nWe want to pause for a moment here to ponder the choice to include governments as investors. Governments do indeed own a fair amount of property (see, for instance, the City of Vancouver’s vast holdings), but it’s odd to speak of governments as investors. To be sure, they rent out properties and receive incomes on them, but usually holdings have to do with planning objectives and other government purposes.\nStranger yet, we’re told governments are combined with for-profit businesses in reporting, and, “Given the predominance of businesses in this category, they will simply be referred to as “business” in what follows.” This is an odd choice, and surprisingly sloppy coming from StatCan. A StatCan article from November of 2022 initially reporting on business and government ownership, for instance, retained both in the title of the category to avoid misinterpretation in collapsing the two. The collapse of the distinction here seems like a red flag by comparison, especially within a context where “investors” are likely to be interpreted as uniquely profit oriented in media coverage.\nWhat of other groups included? Statistics Canada is implicitly making some very strong assumptions here about who is profit motivated and who isn’t. Purchasing a home for a family member to use? Investor. Resident homeowner basing retirement plans on best time to cash out their equity? Non-investor. Vacation home owner? Investor (but only if their primary residence is also owned, they’re not an investor if their primary residence is rented). Even in these few examples, we can see how temporal considerations might also matter. In practice, property owners regularly shift their motivations and relationships to properties through time, problematizing their neat division into “investors” or “non-investors” assessed at any particular point in time. Similarly, organizations like non-profits, automatically exempt from the definition of investor here, often run some housing as market housing, redirecting profits to support subsidies for other housing.\nOverall, we can cut Statistics Canada some slack here in laying out their categorical definitions insofar as they’re not the only ones making these assumptions. But these assumptions should be treated critically and carefully, ideally making them explicit by discussion of their pros and cons. Here StatCan can do a lot better, and usually does.\nStatistics Canada moves on from defining categories of investors to defining investment properties. This adds a new layer of complication insofar as matching owner categories to property categories is tricky. After all, single owners can hold multiple properties (part of the definition used for “investors”). Moreover single properties can have multiple owners and contain multiple dwellings. Grouping properties by buildings doesn’t help us here, and in fact adds to the complexity insofar as single properties can have multiple buildings, and single buildings can contain multiple properties (legally structured through condominium or strata legislation). Here all the complexity of matching owners to properties to dwellings gets swept up into three new categories: investment properties, owner-occupied investment properties, and non-investment properties. To simplify:\n\nA non-investment property is a property with a single dwelling unit which is either\n\nthe primary residence of at least one of the owners, or\nthe sole property of each of the owners, all of which residing somewhere in the same province\n\nAn owner-occupied investment property is a property with multiple dwelling units that is the primary residence of at least one of the owners.\nAn investment property is everything else.\n\nPart of the confusion in the media stems from the distinction between investment property and non-investment property remaining ambiguous about how the property is actually used, as we have already seen in the example of vacation properties above. Consider how a property with a single dwelling unit that is rented out could be classified as either one. Same for a property that is left empty.\nThe property use that the definition is less ambiguous about is if the property is in some way owner-occupied. In that case it is not solely an investment property, but it could be either a non-investment property or an owner-occupied investment property. Here adding a separate suite, often known as a “granny flat, automatically shifts a property into investment status, even if the owner’s granny moves in, or if nobody moves in and it’s still used as part of the living area of the main owner-occupied unit.\nThis wouldn’t matter a great deal if the stakes for being considered an investor didn’t seem so loaded with negative connotations. Maybe there is some real estate investment we want? In particular, many people have pointed out our housing shortage in Canada, where we’re especially short on rentals. The StatCan release notes that rental properties\n\ncontribute to the rental housing supply—and therefore meet the population’s need for rental housing – but that can also limit the number of properties available to buyers who intend to use it as a primary place of residence.\n\nOn the one hand this hints at a desire to elevate home ownership (by owner-occupiers), on the other hand it ignores the structure of single properties with multiple dwelling units, which cannot be made legally available for separate sale. And things get even more fuzzy when we get to purpose-built rental properties, which can produce misleading quirks in the data as we will discuss below.\nLikely for this reason the press release has focused on houses defined as single-detached houses, semi-detached houses, row houses, and mobile homes, and condominium apartment property types when reporting statistics and comparing across jurisdictions. That does avoid some of the problems by excluding properties with multiple dwelling units. It filters out suited single family homes as well as large portions of purpose-built rental buildings, depending on how they are legally structured. But it’s still a statistic that can produce highly misleading results running counter to expectations of the casual reader, as we will explain further below, and CHSP fails to provide proper guidance.\nIn summary, we believe the definitions in this data release aren’t particularly useful in providing meaningful information about Canadian housing, and they aren’t sufficiently contextualized to provide a non-expert the means to adequately interpret the data."
  },
  {
    "objectID": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#what-is-a-condominium-apartment",
    "href": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#what-is-a-condominium-apartment",
    "title": "Investing in definitions and framing",
    "section": "What is a condominium apartment?",
    "text": "What is a condominium apartment?\nMuch of the news about the release has focused on condominium apartments. Colloquially these are often referred to simply as “condos”, but there are some important differences between the colloquial understanding and the formal definition by StatCan.\nCondominium refers to the legal form of ownership, also called strata in some parts of the country, which divides air parcels or land (“bare land strata”) so it can be sold and owned separately while some elements are held in common ownership. It makes owning individual apartment units possible, but also applies to many semi-detached houses (also know colloquially as “duplex” in some parts of the country), townhouses, bare land, as well most recently to imaginary units created solely for property tax purposes (“air parcels”).\nApartment is a structural type of dwelling, which for StatCan refers generally to structures with three or more units.\nCondominium apartment consequently denotes the intersection of those two concepts, or as explained in the data release\n\nA condominium apartment refers to a set of living quarters that is owned individually, while land and common elements are held in joint ownership with others.\n\nBut there is important nuance to this, notably strata titled purpose-built rental apartments. These function and operate just like a purpose-built rental apartment structured legally as single ownership freehold, except it is legally structured as condominium, making each unit it’s separate legal entity but all units, while “owned individually” are held by the same owner. In the BC data underlying the CHSP building stock data, purpose-built rental apartment structured legally as condominium are identified by actual use codes 058 and 059, as opposed to actual use code 030 for condominium apartments which are generally sold and owned individually. The CHSP data does not distinguish between these two, and more importantly, fails to provide this context. This fundamentally challenges the framing provided in the overview analysis. The disparate treatment of purpose-built rental buildings depending on how they are legally structured causes problems when interpreting the data, in particular when the share of condominium titled purpose-built rental units among all condominium apartments varies across geographies."
  },
  {
    "objectID": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#purpose-built-rental-buildings",
    "href": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#purpose-built-rental-buildings",
    "title": "Investing in definitions and framing",
    "section": "Purpose-built rental buildings",
    "text": "Purpose-built rental buildings\nOne type of investment property that most people believe we should have more of is purpose-built rental buildings. These can be classified in a variety of ways, if the owner lives in one of the units it will be an owner-occupied investment property, if the owner does not live there it will be an investment property, and if the purpose-built rental building is legally structured as condominium, then the building will be classified as many investment properties, one for each dwelling unit (or strata lot). And this last point also shows how even simple statistics like the total share of properties that are investment properties in a given municipality can become very misleading.\nTo put it simply, big apartment buildings can show up as a lot of properties when structured into condominium, or a single property when structured like most purpose-built rental. But sometimes purpose-built rental is ALSO structured like condominium.\nFor example, focusing in on condominium apartments in the city of Vancouver as reported in the CHSP data 41.5% of the properties are classified as investment properties. But about 5% of these, roughly half of the 10.1% of the condominium apartment units owned by businesses, are in purpose-built rental apartments that happen to be legally structured as condominium. To give a specific example, the Peter Wall Yaletown at 1310 Richgards is a rental building that is legally structured as condominium; each individual unit is legally separated into it’s own strata lot.\nThe public generally is not attuned to how a purpose-built rental building is legally structured, renters won’t be able to tell the difference unless they look it up in land title data.\nIn Vancouver this does not make that much of a difference, the share of non-purpose-built rental condominium apartments that are investment properties is 38.4%, not much different from the overall. Where it does make a difference is when news articles and commentators talk up the business-owned condominium units and fail to provide this context. After all, the condominium titled purpose-built rental apartments make up a significant share of the business-owned investment condos.\nThis problem of inadequate framing from StatCan get’s even more concerning when condominium titled purpose-built rental buildings make up a much larger share of overall condominium units. In London, Ontario, news reports took StatCan data at face value and headlined [“More than 86 per cent of London’s apartment condos are owned by investors: StatsCan”]((https://www.cbc.ca/news/canada/london/london-ontario-investment-property-1.6739784) and falsely interpreted this as “a sign first-time buyers are being squeezed out of the market by investors, who are cashing in on the city’s severe undersupply of purpose-built rental apartments” instead of simply this being part of the purpose-built rental apartment supply. We can observe the echo of this by contextualizing the data with other data sources, in particular the Census and the CMHC condominium apartment secondary rental market survey.\nIt is important to recognize that these other data sources differ in how data was categorized and what concepts are measured. The CMHC condominium apartment universe will exclude apartments that are classified as part of the primary rental market, so it will excluded condominium titled purpose-built rental apartments. On the other hand, the CMHC survey may have gaps that can skew the data.\nCensus data only reports on condominium units that are someone’s usual place of residence, so it will miss units that are unoccupied or occupied by temporarily present persons like students who return to live with their parents over summer. The Census determines the condominium status of a dwelling unit by asking the respondent, which will yield a different response than when accessing the legal status from administrative data. For example residents of bare land strata will often incorrectly report not being part of a condominium development. More important for our purpose, renters may be much less aware whether their rental home is condominium or not. In particular, renters in a purpose-built rental apartment will generally not know about the underlying legal structure of their building and self-report as not living in a condominium unit, whereas renters renting from individual owners within a condominium apartment generally report as living in condominium.\nDespite of, and sometimes because of these differences, important information can be obtained by comparing these data sources.\n\nWe see that the overall CMHC condominium apartment universe is lower than the estimate of occupied (by usual residents) condominium apartment units from the Census, and the same is true for the number of rented condominium apartment units. Moreover, CHSP seems to under-estimate the owner-occupied stock, as should be expected given the biased error distribution inherent in the classification method. What stands out most, however, is the large difference in number of condominium apartment units in the CHSP data compared to the other two data sources, and that this can be to a very large extent attributed to business-owned condominium apartment units.\nTo confirm this we can compare the CMHC primary apartment rental market universe, which contains condominium titled purpose-built rental buildings, to the non-condominium apartment units identified in the census. One note here is that CMHC only counts structures with at least six rental units, while the census will identify buildings like single family homes with two secondary suites as apartments, so there are some differences to be mindful of.\n\nThe match between those two is quite strong, lending strong evidence to the hypothesis that a very large portion of the apartment condominium units are condominium titled purpose-built rental units.\nTo wrap this up we can put those together to understand the total apartment building stock, remembering that CMHC rental apartments only count units with six or more units.\n\nWe added in data from the 2016 census for which we have information on how many apartment units were occupied by temporarily present persons or were unoccupied, to have a more complete picture of the entire apartment dwelling stock as classified by the census. Unfortunately we don’t have this data for the 2021 census at this point. The apartment stock has likely grown between the 2016 and 2021 census. This is meant for reference to contextualize the 2021 census data on the apartment units which are occupied by usual residents. Also note that “unoccupied” apartments should not be confused with long term empty homes.\nThis shows that the overall number of units estimated by CMHC and the census still line up reasonably well. the CHSP data only counts condominium apartments, and comparing to the other data sources we conclude that the overwhelming majority of business investor condominium apartments are in purpose-built rental buildings that neither CMHC, nor renters self-reporting in the census consider to be “condos.” Here the legal structure departs from the common-sense and practical definition. These units are not available for sale to residents, nor are they governed in a similar fashion to other condominium units. But the CHSP release ignores this distinction, correspondingly boosting estimates of investor-owned properties relative to if they were treated as singular purpose-built rental properties. The gap between the totals from the CHSP data and the other data sources is attributable to London also having purpose-built rental buildings that are not legally structured as condominium. Moreover, this highlights the distinction between properties and dwelling units, which makes it impossible to complete the comparison by adding in non-condominium apartment units in the CHSP data since the CHSP reports only on properties and does not allow to enumerate dwelling units."
  },
  {
    "objectID": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#new-condo-stock",
    "href": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#new-condo-stock",
    "title": "Investing in definitions and framing",
    "section": "New condo stock",
    "text": "New condo stock\nTo touch on a few other points, new condominium apartments will show up with higher rates of investor ownership in the CHSP data for several reasons. A simple one is unsold new inventory, which CHSP data will identify as investor units held by a business. Apart from that we generally see higher rates of transactions, and associated transactional vacancies, for new condominium apartment stock due to condominium apartment financing. Moreover, CHSP methods will likely under-estimate owner-occupiers for recent movers. This is basic context for anyone familiar with housing, making it surprising to see commentators trying to interpret this as a trend of increasing investor activity."
  },
  {
    "objectID": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#takeaway",
    "href": "posts/2023-02-20-investing-in-definitions-and-framing/index.html#takeaway",
    "title": "Investing in definitions and framing",
    "section": "Takeaway",
    "text": "Takeaway\nIn the past CHSP has added valuable data to enhance the understanding of housing in Canada. Unfortunately this release didn’t. The definitions used in the CHSP data release aren’t particularly useful to gain meaningful insights into Canadian housing, and the CHSP release fails to properly contextualize the data. Given owners’ shifting motivations and relationships to properties, we maintain that the most important question is how properties are used rather than how they are owned. That’s not to say that the complexities and motivations of ownership aren’t also interesting. But they’re not well captured here.\nThe choices by CHSP on how to slice the data are questionable on multiple grounds. We’ve demonstrated special problems with regard to how strata titled purpose-built rental apartments show up. The complete lack of disclosure of this when discussing condominium apartment investors in the CHSP overview analysis is highly misleading and in some cases directly responsible for the outright wrong news coverage on this.\nOther context missing from the release is the historical data on multiple property ownership in Canada, readily available from other statistical datasets at StatCan, that would greatly assist in interpreting the data.\nTo move the discussion forward CHSP should focus on providing strong framing with data releases to avoid the media and public misinterpreting the data. Likewise, media, commentators, and the broader public need to pay more attention to definitions.\nThis does not mean that ownership of housing is not important; it is. But the motivations, expectations, and practices of property owners can prove elusive, and regularly shift over the course of the same underlying legal relationship. This is a challenge for trying to match data up to definitions of investment, let alone increasingly popular conceptualizations of terms like commodification and financialization as applied to housing, which rely upon strong assumptions about motivations, expectations, and practices. We’re very open to empirical contributions here, but ideally these contributions should directly address ambiguities and embrace underlying complexities, rather than sweeping them under the rug.\nLastly the meaning of ownership relative to renting is also complex and contingent. Aside from turning renters into owners, shifts in rental tenancy legislation can reset terms, providing access to the security many renters seek in ownership. In our mind a practical takeaway from the fact that many Canadians rent is that we should strengthen tenant rights, especially in the secondary market where most of the evictions happen.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt.\n\n\nReproducibility receipt\n\n## [1] \"2023-02-20 20:14:16 PST\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [df7e737] 2023-02-21: chsp investor post\n## R version 4.2.2 (2022-10-31)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Ventura 13.2.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] statcanXtabs_0.1.2 cancensus_0.5.5    cansim_0.3.14      cmhc_0.2.4        \n##  [5] forcats_1.0.0      stringr_1.5.0      dplyr_1.1.0        purrr_1.0.1       \n##  [9] readr_2.1.3        tidyr_1.3.0        tibble_3.1.8       ggplot2_3.4.0     \n## [13] tidyverse_1.3.2   \n## \n## loaded via a namespace (and not attached):\n##  [1] lubridate_1.9.1           assertthat_0.2.1         \n##  [3] digest_0.6.31             utf8_1.2.3               \n##  [5] R6_2.5.1                  cellranger_1.1.0         \n##  [7] backports_1.4.1           reprex_2.0.2             \n##  [9] evaluate_0.20             httr_1.4.4               \n## [11] blogdown_1.16             pillar_1.8.1             \n## [13] rlang_1.0.6               googlesheets4_1.0.1      \n## [15] readxl_1.4.1              rstudioapi_0.14          \n## [17] jquerylib_0.1.4           rmarkdown_2.20           \n## [19] googledrive_2.0.0         munsell_0.5.0            \n## [21] broom_1.0.3               compiler_4.2.2           \n## [23] modelr_0.1.10             xfun_0.37                \n## [25] pkgconfig_2.0.3           htmltools_0.5.4          \n## [27] tidyselect_1.2.0          bookdown_0.32            \n## [29] fansi_1.0.4               crayon_1.5.2             \n## [31] tzdb_0.3.0                dbplyr_2.3.0             \n## [33] withr_2.5.0               grid_4.2.2               \n## [35] jsonlite_1.8.4            gtable_0.3.1             \n## [37] lifecycle_1.0.3           DBI_1.1.3                \n## [39] git2r_0.31.0              magrittr_2.0.3           \n## [41] scales_1.2.1              cli_3.6.0                \n## [43] stringi_1.7.12            cachem_1.0.6             \n## [45] fs_1.6.0                  xml2_1.3.3               \n## [47] bslib_0.4.2               ellipsis_0.3.2           \n## [49] generics_0.1.3            vctrs_0.5.2              \n## [51] tools_4.2.2               mountainmathHelpers_0.1.4\n## [53] glue_1.6.2                hms_1.1.2                \n## [55] fastmap_1.1.0             yaml_2.3.7               \n## [57] timechange_0.2.0          colorspace_2.1-0         \n## [59] gargle_1.3.0              rvest_1.0.3              \n## [61] knitr_1.42                haven_2.5.1              \n## [63] sass_0.4.5"
  },
  {
    "objectID": "posts/2023-06-27-housing-targets/index.html",
    "href": "posts/2023-06-27-housing-targets/index.html",
    "title": "Housing targets",
    "section": "",
    "text": "(Joint with Nathan Lauster and cross-posted at HomeFreeSociology)\nMunicipalities in BC are required to submit Housing Needs reports, and integrate these into Official Community Plans and Regional Growth Strategies in something resembling housing targets. The BC Housing Supply Act now sharpens this process and adds some teeth, effectively enabling the province to define housing targets, accompanied by new provincial enforcement mechanisms, where the province selects municipalities not meeting housing need. Left unstated are the details of precisely how we should go about calculating housing needs or housing targets.\nIt’s most useful to think of this question in two parts:\nSetting housing targets for municipalities in BC is chiefly about the first. Non-market housing and housing supplements are generally the responsibility of higher levels of government. But overall targets and non-market housing are connected as part of a broader housing system, and ideally work together. Building more housing of all sorts reduces pressures and need for specifically non-market housing, and building non-market housing counts toward overall housing needed as well as having a stronger impact on affordability at the lower end of the market than building market housing alone.\nIn this post we focus on setting overall housing targets for municipalities, but we’ll revisit non-market need."
  },
  {
    "objectID": "posts/2023-06-27-housing-targets/index.html#estimating-current-housing-shortfall",
    "href": "posts/2023-06-27-housing-targets/index.html#estimating-current-housing-shortfall",
    "title": "Housing targets",
    "section": "Estimating current housing shortfall",
    "text": "Estimating current housing shortfall\nTo estimate current housing shortfall we focus on the type of housing that we can expect to be most effective in adding housing: multifamily apartment buildings. These also happen to be the type of housing most restricted by municipalities. While we could (and should) add single family homes by subdividing existing single family lots or stratifying them into row/townhouses, the main contribution of growth in housing will have to come from apartments. And while we are generally more concerned about rental than ownership housing, our data on rental is simply not good enough to use for this kind of analysis. Here we focus on prices, with the understanding that people trade off owning vs renting and ownership, and rental affordability moves in tandem in the medium to long term.\nWe estimate housing shortfall in three steps that together address our questions above, providing a relatively simple conceptual approach.\n\nEstimate the wedge between the price new housing fetches and what it costs to build, including land but assuming no or little restrictions on density or location.\nEstimate the demand elasticity to price, that is the shape of the demand curve.\nEstimate the housing shortfall, that is the amount of housing needed until the wedge is competed away.\n\nWhen the rubber hits the road it will get a little more complicated, but let’s take a look how this could be done for some Metro Vancouver municipalities.\n\nEstimating the wedge\nCMHC analysts have estimated the wedge for overall CMAs and municipalities within them based on 2018 prices. In their analysis the cost includes developer profit, but no land component. Glaeser suggests that around 20% of the sale price should be land cost in an unregulated market, so we apply this ratio to the wedge estimates from the report to obtain the “excess price” of housing. The idea here is that the optimum density is such that when everything is said and done the purchase price of the land makes up 20% of the sale price of the new housing built on top.\n\nNorth Vancouver here combines the District and the City. Of note, this suggests that Surrey builds housing roughly in line with demand. But the more centrally located municipalities of Vancouver, North Vancouver, Richmond and Burnaby, are under-building with respect to demand. This speaks to not just the overall housing shortage, but also the spatial misallocation of housing in the region, both of which are to a large extent promoted by Metro Vancouver regional planning.\n\n\nDemand elasticity\nTo translates our wedge estimates into current housing shortfall estimates we need to assess the demand elasticity of price. For BC overall CMHC has estimated a demand elasticity of -0.5, so all else being equal for every 1% of increase in housing supply prices fall by 2%, the inverse demand elasticity. Ideally one would try to get more detailed estimates for Vancouver, but the reality is that the shape of the demand curve is difficult to estimate and comes with large uncertainties, suggesting we should probably report ranges rather than point estimates. We add estimates using demand elasticities between -0.7 to -0.3 that roughly cover the range seen in the literature. Combining this with housing stock estimates this gives us estimates of the current (2018) housing shortfall, which should be interpreted as the number of dwelling units needed to compete away the excess cost.\nThere are some potential issues with these estimates, for example housing demand is not homogeneous, and given how far prices are from equilibrium of Minimum Profitable Production Cost our extrapolation may be off. Moreover, current construction tends to cater to the higher end of the market, we expect that to change as we slide down the demand curve, in which case we will see less fancy, and thus cheaper, construction, which increases the wedge and deepens the range of price effects we would see. We believe that using a broad range of demand elasticities should cover this and other uncertainties in this process. This gets us to the current housing shortfall, which to no surprise is concentrated in the central parts of the region, in particular the City of Vancouver.\n\nWe note that our simple estimate of a shortfall of 250k housing units for Metro Vancouver stacks up pretty well against other relevant estimates. For instance, the suppressed household estimate we calculated suggests a shortage of between 130k to 200k dwellings for Metro Vancouver looking at household maintainer rates for existing residents alone, but not including those outside the region. To put it differently, we would anticipate many more people moving into their own households instead of doubling up with others if more housing were available, but we would also expect more people remaining in or arriving to Vancouver.\nRegardless, these estimates are pegged to the present, and housing can’t be created instantly. To transform our estimates into housing targets we need to set a goal for some point in the future. But housing demand will likely grow in that time frame, necessitating more housing. This leads us to thinking not just about our current shortfall, but also about future shortfalls. How do we plan ahead?\nOne option is to benchmark against business as usual development, adding more on top. We see this with the CMHC housing target for BC of an additional 570k units over the next 10 years needed to bring affordability back in line with 2003-2004 levels. On first pass, our estimated shortfall of 250k households in Metro Vancouver fits pretty well with this target, insofar as Metro Vancouver contains over half the population of BC. But the fit isn’t perfect, and given that this is a different measure, speaking to a distinct affordability standard already considered unacceptable across the rest of Canada, and calculated out 10 years into the future, we shouldn’t expect it to be. How might we come up with our own housing target?"
  },
  {
    "objectID": "posts/2023-06-27-housing-targets/index.html#estimating-future-housing-demand",
    "href": "posts/2023-06-27-housing-targets/index.html#estimating-future-housing-demand",
    "title": "Housing targets",
    "section": "Estimating future housing demand",
    "text": "Estimating future housing demand\nThis is where we have to make some assumptions. How will the demand for housing grow in the future, say over the next 10 years? For the purposes if this post we take a very rough shortcut and piggy-back off of Metro Vancouver projected household growth. As we discussed in our last post, Metro Vancouver’s methods underlying projected household growth systematically underestimate demand, and we remedy this by assuming that they underestimate demand in line with the size of our current housing shortfall above. After all, the same growth management strategies baked into the Metro Vancouver projections is exactly what leads to our current housing shortfall.\nSo to get to projections of future housing demand we scale Metro household projections up by the ratio of the current shortfall to the existing dwelling stock. This may over-estimate demand a bit since traditionally Metro Vancouver fell short of meeting their household targets, but it may also underestimate demand insofar as Metro Vancouver projections are set to worsen the housing shortage. Also of note, Metro Vancouver projections are based on households, even though they mislabel them as “dwelling units”. To get to dwelling targets we add another 5% to allow for moving vacancies and other frictions.\n\nHere we break out the current shortfall estimated above, the current Metro Vancouver projections, our scaling up of Metro Vancouver projections by estimated shortfall to current stock, plus a factor to convert Metro Vancouver’s (mislabelled) household projections into dwelling projections. And voilà! Housing Targets!\nThis is all pretty rough and we have given up estimating the extent of our uncertainties here. Adding to those we already discussed we now add uncertainties in Metro Vancouver projections, and uncertainties how to adjust them for their methodological flaws. Since 2018 prices have climbed significantly, but so have construction costs. Incomes have also increased, as have interest rates. It would be worthwhile to redo this analysis for today’s conditions, while also paying more attention to compositional effects that lead to some curious quirks in the CMHC report. But that goes beyond the scope of a blog post.\nFor now this gives a baseline for planning to work with. It’s probably not realistic to expect half a million new dwelling units across Metro Vancouver, built in the places where people want to live. It takes time for the construction labour force to adjust, and redevelopment is contingent on properties selling. But it gives us a 10-year target to work toward."
  },
  {
    "objectID": "posts/2023-06-27-housing-targets/index.html#non-market-housing",
    "href": "posts/2023-06-27-housing-targets/index.html#non-market-housing",
    "title": "Housing targets",
    "section": "Non-market housing",
    "text": "Non-market housing\nTo close this section we note that we are talking about overall numbers, and using market mechanisms to estimate housing targets. People living in non-market housing don’t enter in the excess price metric we calculate as our wedge (renters enter implicitly, though not directly). But non-market housing is counted as part of the overall housing supply, and thus enters into our housing targets. The excess price metric implies that prices for new housing would fall by roughly 50% in Metro Vancouver, more in some sub-regions and less in others, and while these price reductions and related rent reductions would be expected to propagate in some way through the entire housing spectrum, it is not clear how exactly this would transform the lower end of the market. Nor is it clear the lower end of the market could meet everyone’s needs. We expect there would likely still be people left in inadequate or unaffordable housing, where dedicated non-market housing or cash supplements should be directed. But we also expect that alleviating overall housing shortages makes this work a lot easier to do.\nIdeally after addressing how much overall housing is needed, we’d return to the second part of our question to ask how much non-market housing is needed to insure everyone has access to housing. We have some ideas for how to go about this, involving calculating the effects of meeting overall targets on rent distributions by housing type, and drawing upon household characteristics to figure out what proportion of households wouldn’t find needs met by these new distributions. But that involves a lot more work and assumptions, and we’ll leave it for another post.\nFor now we’ll note that the work and assumptions involved in jointly assessing overall housing targets and specifically non-market housing targets point toward a persistent shortcoming in many current housing needs assessments. A lot of housing needs estimates simply look at how the need of the current population is met by the available housing. Sometimes assessments are pegged to household income brackets and look at how many households in each bracket face housing struggles. This is useful for understanding how people are doing in the current housing environment, but only of limited use when coming up with housing targets, where projection into the future includes people not currently resident in the region and involves a lot of moving parts operating within a broader and interconnected housing system."
  },
  {
    "objectID": "posts/2023-06-27-housing-targets/index.html#update-additional-scenario-on-effective-cost-assumptions-september-25-2023",
    "href": "posts/2023-06-27-housing-targets/index.html#update-additional-scenario-on-effective-cost-assumptions-september-25-2023",
    "title": "Housing targets",
    "section": "Update (additional scenario on effective cost assumptions, September 25, 2023)",
    "text": "Update (additional scenario on effective cost assumptions, September 25, 2023)\nOne caveat on our analysis above is that we added in a sizable land component to the marginal cost of housing. This may not be necessary, in a market where the amount of housing is not constrained by municipal regulation (maybe where municipalities even encourage more housing!) a developer could build up until the marginal (social) cost of production, including profit, equals the the price the housing sells for. The additional “land component” of 20% was added rather ad-hoc, with the implicit understanding that when we are far away from equilibrium developers would not just build one more floor but many, and the effective marginal cost of adding these floors is higher than the marginal cost of adding just one floor. This could also capture costs associated with externalities of increasing population density, which also increases the cost of providing services that may not be captured at the current level of DCC/DCLs (assuming for simplicity those are set properly to cover only the current incremental cost of new construction). Lastly this accounts for risk, where developers may opt to build below the economically optimal height to avoid running the risk of not being able to recover all their costs if it turns out that prices are lower than anticipated at the time of completion.\nThe 20% estimate is probably at the high end of the range of what could reasonably be expected here. To complement our previous estimates we re-run the analysis where we assume supply can be effectively added at 10% above current marginal cost of supply.\nUnder this scenario, we start out with a wedge that is ~10% larger.\n\nThis in turn gives significantly higher estimates of current shortfall.\n\nWhen spacing this out over 10 years this leads to again significantly higher 10 year targets, with higher baselines but also higher correction to the projection housing demand.\n\nAssuming supply can be effectively added at the current marginal cost would locate more than half of BC’s current dwelling shortfall in Metro Vancouver, and accordingly finds a higher dwelling shortfall especially in the more expensive cities within Metro Vancouver. This is probably a more realistic estimate of targets to return the market to a condition where the total amount of housing is not overly constrained by municipal restrictions limiting new housing construction.\n\n\nReproducibility receipt\n\n## [1] \"2023-09-25 22:31:07 PDT\"\n## Local:    master /Users/jens/Documents/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [164fe57] 2023-09-26: update (some of the) dead links in Metro Vancouver regimes post where Metro Vancouver website changed.\n## R version 4.3.0 (2023-04-21)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Ventura 13.6\n## \n## Matrix products: default\n## BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/Vancouver\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mountainmathHelpers_0.1.4 cancensus_0.5.6          \n##  [3] lubridate_1.9.2           forcats_1.0.0            \n##  [5] stringr_1.5.0             dplyr_1.1.2              \n##  [7] purrr_1.0.1               readr_2.1.4              \n##  [9] tidyr_1.3.0               tibble_3.2.1             \n## [11] ggplot2_3.4.2             tidyverse_2.0.0          \n## \n## loaded via a namespace (and not attached):\n##  [1] sass_0.4.5       utf8_1.2.3       generics_0.1.3   blogdown_1.18   \n##  [5] stringi_1.7.12   hms_1.1.3        digest_0.6.31    magrittr_2.0.3  \n##  [9] timechange_0.2.0 evaluate_0.20    grid_4.3.0       bookdown_0.34   \n## [13] fastmap_1.1.1    jsonlite_1.8.4   fansi_1.0.4      scales_1.2.1    \n## [17] jquerylib_0.1.4  cli_3.6.1        rlang_1.1.1      munsell_0.5.0   \n## [21] withr_2.5.0      cachem_1.0.8     yaml_2.3.7       tools_4.3.0     \n## [25] tzdb_0.3.0       colorspace_2.1-0 vctrs_0.6.2      R6_2.5.1        \n## [29] git2r_0.32.0     lifecycle_1.0.3  pkgconfig_2.0.3  pillar_1.9.0    \n## [33] bslib_0.4.2      gtable_0.3.3     glue_1.6.2       xfun_0.39       \n## [37] tidyselect_1.2.0 rstudioapi_0.14  knitr_1.42       htmltools_0.5.5 \n## [41] rmarkdown_2.23   compiler_4.3.0"
  },
  {
    "objectID": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html",
    "href": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html",
    "title": "First time buyer Lorenz curves revisited",
    "section": "",
    "text": "Three years ago we wrote a post on First Time Buyer Lorenz Curves, looking at what share of homes are in principle available to first-time home buyers.1 That post continues to be a more popular one, so we thought it would be good to update it with more recent data and expand some of the ideas further.\nIn this post we want to update this with 2021 data that has now become available, consider the effect of property taxes on affordability that we previously neglected, and introduce a new discretized version of this measure that condenses the information into two parameters, making is easier to digest and compare across different housing markets and allows the tracing of change over time.\nDifferent families have different housing needs, to simplify this we decided to base our affordability measure on adjusted after-tax family income deciles, which is a person-based measure that assigns each person the after tax family income divided by the square root of the number of members in the family unit. This is a standard Statistics Canada concept, based on the idea that when family members share a household and other expenses there are synergies that make the family unit more efficient than the sum of its parts, with efficiency factor roughly given by the square root of the size of the family unit. In short, a couple making a $100k and and person living alone making $71k have similar standards of living \\(\\left(\\frac{100}{\\sqrt{2}} \\approx \\frac{71}{\\sqrt{1}}\\right)\\).\nAn added advantage is this gives us a person-based measure, so instead of assessing the affordability of family units, where e.g. a single person gets compared to a family of four, we are comparing people and how they are effected by affordability. This is particulary important due to the composition of family types across metropolitan areas not being the same.\nOn the housing side, we showed we can similarly divide home prices by the square root of the number of bedrooms to get a stable measure of home prices that is comparable across different home sizes, as measured by number of bedrooms. Like the family example above, a $900k 5 bedroom house is considered to “cost” the same as a $400k 1 bedroom appartment \\(\\left(\\frac{900}{\\sqrt{5}} \\approx \\frac{400}{\\sqrt{1}}\\right)\\). A potentially better measure for scaling would likely be square footage, but that is not available in the census.\nThe adjusted GINI curve is then a visual representation of affordability of persons in first-time buyer family units. We believe this measure is useful to give an indication of how accessible ownership is in different housing markets. In particular this avoides the pitfalls of using household-based measures that are plaguing much of our housing discussions. Family units are the right kind of decision making units when it comes to buying (or renting) housing, so it’s family income that matters."
  },
  {
    "objectID": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#first-time-home-buyer-lorenz-curves-in-2021",
    "href": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#first-time-home-buyer-lorenz-curves-in-2021",
    "title": "First time buyer Lorenz curves revisited",
    "section": "First-time home buyer Lorenz curves in 2021",
    "text": "First-time home buyer Lorenz curves in 2021\nLet’s take a look what such a Lorenz curve for potential first-time buyers (essentially renters aged 20 through 49) looks like. To remind us we show the Lorenz curve for Calgary in 2021.\n\nIn Calgary, potential first-time buyers at the bottom end of the renter income can’t afford any of the homes. Someone at the 20th income percentile of potential first-time buyers can only afford 12% of Calgary homes. But after that things pick up fast and someone at the 60th percentile can afford 75%, at which point things even out and the Lorenz curve hugs the diagonal.\nThis says nothing about what kind of home someone can afford, or where in the metro area an affordable home may be. It also skirts the discussion about “starter homes” or more generally the question if there is a mismatch between homes first-time buyers are buying and the overall distribution of homes in the metro area in terms of quality or size. Or if there should be such a distinction. What this does is it gives a rough matchup between incomes of (potential) first-time buyers and home values.\nWith this we can update our Lorenz curves by showing both the 2016 and the new 2021 in the same plot, and adding in 2011 data for good measure, enabling us to see how the affordability distribution has shifted over the last 10 years.\n\nThis reveals that first time buyer affordability has shifted in different ways. The already largely unaffordable markets of Vancouver and Toronto have become further unaffordable. Hamilton shows the worst deterioration of affordability, as measured by the area between the 2016 and 2021 curve. On the other hand, Québec City has become more affordable.\nIt’s important to note that the 2021 census using 2020 income data, and especially low income people benefited from pandemic CERB payments that have lifted them up and explains some of the affordability improvements for the lower income percentiles. We note that this effect is only visible in some metro area, in others increasing home prices have more than offset the temporary pandemic income boost"
  },
  {
    "objectID": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#property-taxes",
    "href": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#property-taxes",
    "title": "First time buyer Lorenz curves revisited",
    "section": "Property taxes",
    "text": "Property taxes\nThis view might be overly optimistic, our affordability metric does not include property taxes and utilities. Cost of utilities are fairly uniform across Canada and don’t change much over time, but property taxes certainly do. And this matters to affordability, for example Québec City’s property tax rates are around 1% of the property value, compared to around 0.3% in Vancouver. Correspondingly people in Vancouver can afford higher-priced homes than people in Québec City on the same income.\nAdjusting our affordability metric to include property taxes, for this exercise taken from an Altus report, shows this effect on affordability quite clearly.\n\nIn Vancouver, with it’s relatively low property tax rate, this does not make much of a difference, whereas in other metro areas the difference is substantial.\nThis should not be taken to mean that property taxes have a causal effect on affordability in the sense that lowering property taxes will increase affordability of first-time buyers. This is because prices are highly endogenous to property taxes, if Québec City’s property tax rates were the same as Vancouver’s the net present value of a home in Québec would be around 31% higher, and exogenously changing property tax rates would cause prices to adjust to reflect that to a large extent. In Canada property tax rates are set by the ratio of aggregate home values to municipal budgets, so prices impact property tax rates. On the other hand, property tax rates impact home prices in the same way that interest rates do, with lower property tax rates leading to higher prices.\nWith this insight we continue by including property taxes in our affordability metric, with an updated graph for all years and metro areas."
  },
  {
    "objectID": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#condensing-the-lorenz-curve-into-a-discrete-metric",
    "href": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#condensing-the-lorenz-curve-into-a-discrete-metric",
    "title": "First time buyer Lorenz curves revisited",
    "section": "Condensing the Lorenz curve into a discrete metric",
    "text": "Condensing the Lorenz curve into a discrete metric\nAn obvious candidate for a discrete metric is the GINI coefficient. The GINI coefficient is just the area between the Lorenz curve and the diagonal. This gives a broad measure of affordability in a metro area, a positive coefficient means housing is less affordable, a negative coefficient means housing is more affordable. Perfect proportional affordability is where every potential first time buyer can access housing up to the percentile that corresponds to their percentile in the income distribution.\nHigh unaffordability is characterized by the Lorenz curve staying under the diagonal, high affordability by a curve staying above the diagonal. In many cases we see curves exhibiting high-income skew where they are crossing the diagonal somewhere in the middle of the income spectrum, with lower income people unable to afford homes corresponding to their income percentile while higher income people able to afford homes above their income percentile.\n\nBut there are several problems, the first time buyer Lorenz curves can also take values above the diagonal, and for example the two curves show above with zero GINI coefficient are qualitatively quite different. The high-income skew curve models a housing market like Calgary in 2021, where lower income potential first time buyers can’t afford housing corresponding to their income percentile, whereas higher income potential first time buyers can access more than housing that corresponds to their income percentile. The GINI coefficient can’t distinguish between these two cases, both have a GINI of zero, but they represent distinct distributions of (un-)affordability.For analysis and policy making, it is important our metrics summarizing the lorenz curve retains this information.\nIntuitively we can think of the two model cases of high (un)affordability and high-income skew as the first two harmonics relative to the diagonal representing perfect proportional affordability. More formally we discretize the Lorenz curves by taking the first two terms of the Fourier expansion of their difference from the diagonal, the perfect proportional affordability lorenz curve . A sine expansion will capture these reasonably well with just a few terms, where the first term is a good approximation of the (scaled) GINI coefficient representing the aggregate unaffordability and the second term gives the strength of the main oscillation around the diagonal, interpreted as high-income skew.\n\nExcept for the highly unaffordable metro areas of Vancouver, Toronto and Hamilton the first two (sine) Fourier coefficients capture the curve quite well, for Vancouver, Toronto and Hamilton the approximation struggles to properly capture how long the curve stays flat and how late it rises. This is yet another way how these city’s affordability, or the lack thereof, is extreme compared to the other metro areas.\nWe can now take these two coefficients and plot their movement over time.\n\nThis graph shows that Vancouver, Toronto and Hamilton are in a league of there own when it comes to housing affordability. They exhibit quite similar aggregate unaffordability and high-income skew. Vancouver has been doing quite poorly for the entire decade covered here, whereas Toronto and in particular Hamilton have see their housing affordability radically worsen over the past decade. Ottawa-Gatineau and Montréal are in mildly unaffordable territory, while Calgary, Edmonton and Winnipeg are fairly neutral in terms of affordability. Québec City is the most affordable market considered here, and its affordability has improved significantly.\nWe note that the values taken by the CMAs follow a rough frown shape. This is not coincidence, not all values in the range shown can be attained. Values in the lower left and lower right aren’t mathematically possible because the Lorenz curves are monotonic. Values in the top middle correspond to scenarios where lower income families have access to a higher share of housing than their position in the income distribution, whereas higher income households have access to a lower portion of housing. This is not how the housing system works."
  },
  {
    "objectID": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#property-tax-effects",
    "href": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#property-tax-effects",
    "title": "First time buyer Lorenz curves revisited",
    "section": "Property tax effects",
    "text": "Property tax effects\nWe can revisit what this would look like if we did not account for differences in property tax rates.\n\nOmitting property taxes decreases aggregate unaffordability, but it also changes the relative position of the cities. For example, when not accounting for property taxes, Vancouver has slightly higher aggregate unaffordability and slightly stronger high-income skew than Toronto, but this reverses after accounting for property taxes."
  },
  {
    "objectID": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#upshot",
    "href": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#upshot",
    "title": "First time buyer Lorenz curves revisited",
    "section": "Upshot",
    "text": "Upshot\nThis expands on our previous work on Lorenz curves and derived GINI index by adding in 2021 data, and adding the discretization into two metrics, the aggregate unaffordability that approximates the GINI index, and the high-income skew that captures the degree to which lower income earners do worse than higher income earners. We also included property taxes, as rates differ across metro areas and across time, and this makes a significant difference. Property tax rates were taken from the central municipalities, this could use refinement in future iterations.\nThis could be expanded on by using higher frequency data like home transactions and the Canadian Income Survey to allow closer monitoring of first time buyer affordability.\nAs usual, the code for this post is available on GitHub for anyone to reproduce or adapt for their own purposes.\n\n\nReproducibility receipt\n\n## [1] \"2024-01-17 10:05:59 PST\"\n## Local:    master /Users/jens/R/mountaindoodles\n## Remote:   master @ origin (https://github.com/mountainMath/doodles.git)\n## Head:     [3036abb] 2024-01-17: clarify legend in synthetic affordability graph to explicitly include skew\n## R version 4.3.2 (2023-10-31)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Sonoma 14.2.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/Vancouver\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] canpumf_0.1.8   patchwork_1.1.3 FinCal_0.6.3    lubridate_1.9.3\n##  [5] forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4     purrr_1.0.2    \n##  [9] readr_2.1.4     tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.4  \n## [13] tidyverse_2.0.0\n## \n## loaded via a namespace (and not attached):\n##  [1] sass_0.4.8                utf8_1.2.4               \n##  [3] generics_0.1.3            bitops_1.0-7             \n##  [5] blogdown_1.18             stringi_1.8.3            \n##  [7] hms_1.1.3                 digest_0.6.33            \n##  [9] magrittr_2.0.3            timechange_0.2.0         \n## [11] evaluate_0.23             grid_4.3.2               \n## [13] bookdown_0.37             fastmap_1.1.1            \n## [15] plyr_1.8.9                jsonlite_1.8.8           \n## [17] fansi_1.0.6               scales_1.3.0             \n## [19] jquerylib_0.1.4           cli_3.6.2                \n## [21] rlang_1.1.2               munsell_0.5.0            \n## [23] withr_2.5.2               cachem_1.0.8             \n## [25] yaml_2.3.7                tools_4.3.2              \n## [27] reshape2_1.4.4            tzdb_0.4.0               \n## [29] colorspace_2.1-0          vctrs_0.6.5              \n## [31] R6_2.5.1                  git2r_0.33.0             \n## [33] lifecycle_1.0.4           pkgconfig_2.0.3          \n## [35] pillar_1.9.0              bslib_0.6.1              \n## [37] gtable_0.3.4              Rcpp_1.0.11              \n## [39] glue_1.6.2                xfun_0.41                \n## [41] tidyselect_1.2.0          rstudioapi_0.15.0        \n## [43] knitr_1.45                htmltools_0.5.7          \n## [45] rmarkdown_2.25            mountainmathHelpers_0.1.4\n## [47] compiler_4.3.2            RCurl_1.98-1.13"
  },
  {
    "objectID": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#footnotes",
    "href": "posts/2024-01-16-first-time-buyer-lorenz-curves-revisited/index.html#footnotes",
    "title": "First time buyer Lorenz curves revisited",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe define first-time home buyers as members economic families with non-student primary household maintainers between the ages of 25 and 64 that are currently renting.↩︎"
  }
]